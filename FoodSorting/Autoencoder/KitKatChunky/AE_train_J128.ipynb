{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import axis, colorbar, imshow, show, figure, subplot\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=18)\n",
    "mpl.rc('xtick', labelsize=16)\n",
    "mpl.rc('ytick', labelsize=16)\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## ----- GPU ------------------------------------\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'    # use of GPU 0 (ERDA) (use before importing torch or tensorflow/keeas)\n",
    "## ----------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization, ReLU, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.activations import relu\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K                         #contains calls for tensor manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL_TRAIN_AUG:       /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTrainAugmentations\n",
      "NORMAL_VALIDATION_AUG:  /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalValidationAugmentations\n",
      "NORMAL_TEST_AUG:        /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTestAugmentations\n",
      "ANOMALY_AUG:            /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/AnomalyAugmentations\n"
     ]
    }
   ],
   "source": [
    "from utils_ae_kitkat_paths import *\n",
    "\n",
    "# Get work directions for augmentated data set:\n",
    "print (\"NORMAL_TRAIN_AUG:      \", NORMAL_TRAIN_AUG)\n",
    "print (\"NORMAL_VALIDATION_AUG: \", NORMAL_VAL_AUG)\n",
    "print (\"NORMAL_TEST_AUG:       \", NORMAL_TEST_AUG)\n",
    "print (\"ANOMALY_AUG:           \", ANOMALY_AUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which Folder The Models Are Saved In: saved_models/latent128\n"
     ]
    }
   ],
   "source": [
    "# What latent dimension and filter size are we using?:\n",
    "latent_dim = 128\n",
    "filters    = 32\n",
    "\n",
    "# Get work direction for saving files:\n",
    "SAVE_FOLDER = f'saved_models/latent{latent_dim}'\n",
    "print (\"Which Folder The Models Are Saved In:\", SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] deleting existing files in folder...\n",
      "         done in 0.013 minutes\n"
     ]
    }
   ],
   "source": [
    "# Delete all existing files in folder where models are saved:\n",
    "print(\"[INFO] deleting existing files in folder...\")\n",
    "t0 = time()\n",
    "\n",
    "files = glob.glob(f'{SAVE_FOLDER}/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "print (\"         done in %0.3f minutes\" % ((time() - t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_VAE_AE import get_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Investigation of Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"normal\" training images is:     1400\n",
      "Number of \"normal\" validation images is:   134\n",
      "Number of \"normal\" test images is:         266\n",
      "Number of \"anomaly\" images is:             600\n"
     ]
    }
   ],
   "source": [
    "# Glob the directories and get the lists of good and bad images\n",
    "good_train_fns = [f for f in os.listdir(NORMAL_TRAIN_AUG) if not f.startswith('.')]\n",
    "good_val_fns = [f for f in os.listdir(NORMAL_VAL_AUG) if not f.startswith('.')]\n",
    "good_test_fns = [f for f in os.listdir(NORMAL_TEST_AUG) if not f.startswith('.')]\n",
    "bad_fns = [f for f in os.listdir(ANOMALY_AUG) if not f.startswith('.')]\n",
    "\n",
    "print('Number of \"normal\" training images is:     {}'.format(len(good_train_fns)))\n",
    "print('Number of \"normal\" validation images is:   {}'.format(len(good_val_fns)))\n",
    "print('Number of \"normal\" test images is:         {}'.format(len(good_test_fns)))\n",
    "print('Number of \"anomaly\" images is:             {}'.format(len(bad_fns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Procentage of normal samples (ground truth):   75.00 %\n",
      "> Procentage of anomaly samples (ground truth):  25.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of bad images vs good images:\n",
    "normal_fns = len(good_train_fns) + len(good_val_fns) + len(good_test_fns)\n",
    "anomaly_fns = len(bad_fns)\n",
    "print(f\"> Procentage of normal samples (ground truth):   {(normal_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")\n",
    "print(f\"> Procentage of anomaly samples (ground truth):  {(anomaly_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Normal Data\n",
    "\n",
    "Load normal samples in pre-splitted data sets: train, valdiation and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal training images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal training images...\")\n",
    "trainX = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TRAIN_AUG}/*.png\")]  # read as grayscale\n",
    "trainX = np.array(trainX)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "trainY = get_labels(trainX, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal validation images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal validation images...\")\n",
    "x_normal_val = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_VAL_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_val = np.array(x_normal_val)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_val = get_labels(x_normal_val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal testing images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal testing images...\")\n",
    "x_normal_test = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TEST_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_test = np.array(x_normal_test)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_test = get_labels(x_normal_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Data\n",
    "\n",
    "\n",
    "Load anomaly samples in its singular training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly images...\")\n",
    "x_anomaly = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY_AUG}/*.png\")]  # read as grayscale\n",
    "x_anomaly = np.array(x_anomaly)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_anomaly = get_labels(x_anomaly, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine normal samples and anomaly samples and split them into training, validation and test sets\n",
    "\n",
    "`label 0` == Normal Samples\n",
    "\n",
    "`label 1` == Anomaly Samples\n",
    "\n",
    "Train and Validation sets only consists of `Normal Data`, aka. `label 0`.\n",
    "\n",
    "Testing sets consists of both  `Normal Data` (aka. `label 0`) and `Anomaly Data` (aka. `label 1`)\n",
    "\n",
    "________________\n",
    "\n",
    "Due to the very sparse original (preprossed) KitKat Chunky data, the validation set will be a mix of normal testing and validation data. The normal testing set will consists of all validation and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testvalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testvalY = get_labels(testvalX, 0)\n",
    "\n",
    "# randomly split in order to make new validation set:\n",
    "NoUsageX, valX, NoUsageY, valY = train_test_split(testvalX, testvalY, test_size=0.25, random_state=4345672)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testNormalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testNormalY = get_labels(testNormalX, 0)\n",
    "\n",
    "# anomaly class (only used for testing):\n",
    "testAnomalyX, testAnomalyY = x_anomaly, y_anomaly\n",
    "\n",
    "# Combine all testing data in one array:\n",
    "testAllX = np.vstack((testNormalX, testAnomalyX))\n",
    "testAllY = np.hstack((testNormalY, testAnomalyY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Dimensions and Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      " > images: (1400, 128, 128)\n",
      " > labels: (1400,)\n",
      "\n",
      "Validation set:\n",
      " > images: (100, 128, 128)\n",
      " > labels: (100,)\n",
      "\n",
      "Test set:\n",
      " > images: (1000, 128, 128)\n",
      " > labels: (1000,)\n",
      "\t'Normal' test set:\n",
      "\t  > images: (400, 128, 128)\n",
      "\t  > labels: (400,)\n",
      "\t'Anomaly' test set:\n",
      "\t  > images: (600, 128, 128)\n",
      "\t  > labels: (600,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "print(\" > images:\", trainX.shape)\n",
    "print(\" > labels:\", trainY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Validation set:\")\n",
    "print(\" > images:\", valX.shape)\n",
    "print(\" > labels:\", valY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Test set:\")\n",
    "print(\" > images:\", testAllX.shape)\n",
    "print(\" > labels:\", testAllY.shape)\n",
    "print(\"\\t'Normal' test set:\")\n",
    "print(\"\\t  > images:\", testNormalX.shape)\n",
    "print(\"\\t  > labels:\", testNormalY.shape)\n",
    "print(\"\\t'Anomaly' test set:\")\n",
    "print(\"\\t  > images:\", testAnomalyX.shape)\n",
    "print(\"\\t  > labels:\", testAnomalyY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 73.68 %\n",
      "Procentage of validation samples: 5.26 %\n",
      "Procentage of (only normal) testing samples: 21.05 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets (only normal data):\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (only normal) testing samples: {(len(testNormalX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 56.00 %\n",
      "Procentage of validation samples: 4.00 %\n",
      "Procentage of (total) testing samples: 40.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets:\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (total) testing samples: {(len(testAllX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for (un)balanced data\n",
    "\n",
    "In an ideal world the data should be balanced. However in the real world we expect there being around ~$99 \\%$ good samples and only ~$1 \\%$ bad samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['0: normal', '1: anomaly']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,4))\n",
    "ax[0].hist(trainY, align=\"left\"); ax[0].set_title('train', fontsize=18); ax[0].set_xticks([0,1]); ax[0].set_xticklabels(labels); ax[0].tick_params(axis='both', which='major', labelsize=16); ax[0].set_xlim(-0.5, 1.5);\n",
    "ax[1].hist(valY, align=\"left\"); ax[1].set_title('validation', fontsize=18); ax[1].set_xticks([0,1]); ax[1].set_xticklabels(labels); ax[1].tick_params(axis='both', which='major', labelsize=16); ax[1].set_xlim(-0.5, 1.5);\n",
    "ax[2].hist(testAllY, align=\"left\"); ax[2].set_title('test', fontsize=18); ax[2].set_xticks([0,1]); ax[2].set_xticklabels(labels); ax[2].tick_params(axis='both', which='major', labelsize=16); ax[2].set_xlim(-0.5, 1.5);\n",
    "\n",
    "ax[0].set_ylabel(\"Number of images\", fontsize=18)\n",
    "ax[1].set_xlabel(\"Class Labels\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of normal test samples: 40.00 %\n",
      "Procentage of total anomaly test samples: 60.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of normal test images vs total anomaly test images:\n",
    "print(f\"Procentage of normal test samples: {(len(testNormalX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of total anomaly test samples: {(len(testAnomalyX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Only normalization has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "img_width, img_height = trainX.shape[1], trainX.shape[2]      # input image dimensions\n",
    "num_channels = 1                                              # gray-channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Neural Networks learns faster with normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to be in  the [0, 1] range:\n",
    "trainX = trainX.astype('float32') / 255.\n",
    "valX = valX.astype('float32') / 255.\n",
    "testAllX = testAllX.astype('float32') / 255.\n",
    "\n",
    "# reshape data to keras inputs --> (n_samples, img_rows, img_cols, n_channels):\n",
    "trainX = trainX.reshape(trainX.shape[0], img_height, img_width, num_channels)\n",
    "valX = valX.reshape(valX.shape[0], img_height, img_width, num_channels)\n",
    "testAllX = testAllX.reshape(testAllX.shape[0], img_height, img_width, num_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import architecture of VAE model and its hyperparameters:\n",
    "from J128_AE_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Total AE Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 64, 64, 32)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "latent (Dense)               (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 32)      4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 128, 128, 32)      128       \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 128, 128, 1)       129       \n",
      "=================================================================\n",
      "Total params: 172,481\n",
      "Trainable params: 170,817\n",
      "Non-trainable params: 1,664\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "batch_size  = 256\n",
    "epochs      = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at 1'st epoch (inital model)\n",
    "\n",
    "https://stackoverflow.com/questions/54323960/save-keras-model-at-specific-epochs\n",
    "\"\"\"\n",
    "\n",
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 1:\n",
    "            self.model.save_weights(f\"{SAVE_FOLDER}/cp-0001.h5\")\n",
    "            \n",
    "# create and use callback:\n",
    "initial_checkpoint = CustomSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at every k'te epochs\n",
    "\"\"\"\n",
    "# Include the epoch in the file name (uses `str.format`):\n",
    "checkpoint_path = \"saved_models/latent128/cp-{epoch:04d}.h5\"\n",
    "\n",
    "# Create a callback that saves the model's weights every k'te epochs:\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path,\n",
    "                             monitor='loss',           # name of the metrics to monitor\n",
    "                             verbose=1, \n",
    "                             #save_best_only=False,     # if False, the best model will not be overridden.\n",
    "                             save_weights_only=True,   # if True, only the weights of the models will be saved.\n",
    "                                                        # if False, the whole models will be saved.\n",
    "                             #mode='auto', \n",
    "                             period=200                  # save after every k'te epochs\n",
    "                            )\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "ae.save_weights(checkpoint_path.format(epoch=0))          # save for zero epoch (inital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: Early stopping\n",
    "https://www.tensorflow.org/guide/keras/custom_callback\n",
    "\"\"\"\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=50):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create and Compile Model\"\"\"\n",
    "# import architecture of VAE model and its hyperparameters. learning rate choosen from graph above.\n",
    "ae = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "6/6 [==============================] - 1s 168ms/step - loss: 0.2919 - val_loss: 0.2155\n",
      "Epoch 2/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.2017 - val_loss: 0.2097\n",
      "Epoch 3/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.1422 - val_loss: 0.2023\n",
      "Epoch 4/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0957 - val_loss: 0.1933\n",
      "Epoch 5/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0668 - val_loss: 0.1831\n",
      "Epoch 6/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0514 - val_loss: 0.1723\n",
      "Epoch 7/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0429 - val_loss: 0.1613\n",
      "Epoch 8/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0373 - val_loss: 0.1505\n",
      "Epoch 9/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0331 - val_loss: 0.1398\n",
      "Epoch 10/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0293 - val_loss: 0.1295\n",
      "Epoch 11/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0260 - val_loss: 0.1195\n",
      "Epoch 12/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0232 - val_loss: 0.1099\n",
      "Epoch 13/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0204 - val_loss: 0.1013\n",
      "Epoch 14/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0181 - val_loss: 0.0934\n",
      "Epoch 15/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0160 - val_loss: 0.0864\n",
      "Epoch 16/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0145 - val_loss: 0.0802\n",
      "Epoch 17/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0128 - val_loss: 0.0749\n",
      "Epoch 18/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0116 - val_loss: 0.0705\n",
      "Epoch 19/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0107 - val_loss: 0.0666\n",
      "Epoch 20/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0096 - val_loss: 0.0634\n",
      "Epoch 21/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0094 - val_loss: 0.0607\n",
      "Epoch 22/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0084 - val_loss: 0.0584\n",
      "Epoch 23/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0078 - val_loss: 0.0564\n",
      "Epoch 24/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0074 - val_loss: 0.0548\n",
      "Epoch 25/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0071 - val_loss: 0.0534\n",
      "Epoch 26/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0068 - val_loss: 0.0523\n",
      "Epoch 27/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0061 - val_loss: 0.0513\n",
      "Epoch 28/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0059 - val_loss: 0.0504\n",
      "Epoch 29/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0056 - val_loss: 0.0498\n",
      "Epoch 30/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0052 - val_loss: 0.0492\n",
      "Epoch 31/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0051 - val_loss: 0.0488\n",
      "Epoch 32/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0049 - val_loss: 0.0483\n",
      "Epoch 33/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0047 - val_loss: 0.0478\n",
      "Epoch 34/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0046 - val_loss: 0.0475\n",
      "Epoch 35/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0044 - val_loss: 0.0471\n",
      "Epoch 36/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0042 - val_loss: 0.0465\n",
      "Epoch 37/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0042 - val_loss: 0.0463\n",
      "Epoch 38/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0041 - val_loss: 0.0454\n",
      "Epoch 39/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0039 - val_loss: 0.0445\n",
      "Epoch 40/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0037 - val_loss: 0.0442\n",
      "Epoch 41/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0036 - val_loss: 0.0436\n",
      "Epoch 42/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0037 - val_loss: 0.0432\n",
      "Epoch 43/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0034 - val_loss: 0.0425\n",
      "Epoch 44/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0034 - val_loss: 0.0418\n",
      "Epoch 45/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0033 - val_loss: 0.0414\n",
      "Epoch 46/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0032 - val_loss: 0.0402\n",
      "Epoch 47/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0030 - val_loss: 0.0392\n",
      "Epoch 48/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0030 - val_loss: 0.0386\n",
      "Epoch 49/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0029 - val_loss: 0.0380\n",
      "Epoch 50/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0029 - val_loss: 0.0369\n",
      "Epoch 51/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0029 - val_loss: 0.0363\n",
      "Epoch 52/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0028 - val_loss: 0.0358\n",
      "Epoch 53/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0031 - val_loss: 0.0343\n",
      "Epoch 54/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0337\n",
      "Epoch 55/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0025 - val_loss: 0.0322\n",
      "Epoch 56/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0309\n",
      "Epoch 57/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0296\n",
      "Epoch 58/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0025 - val_loss: 0.0303\n",
      "Epoch 59/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0026 - val_loss: 0.0280\n",
      "Epoch 60/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0023 - val_loss: 0.0270\n",
      "Epoch 61/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0252\n",
      "Epoch 62/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0024 - val_loss: 0.0235\n",
      "Epoch 63/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0231\n",
      "Epoch 64/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0023 - val_loss: 0.0221\n",
      "Epoch 65/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0022 - val_loss: 0.0205\n",
      "Epoch 66/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0212\n",
      "Epoch 67/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0022 - val_loss: 0.0184\n",
      "Epoch 68/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0021 - val_loss: 0.0185\n",
      "Epoch 69/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0168\n",
      "Epoch 70/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0020 - val_loss: 0.0164\n",
      "Epoch 71/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0021 - val_loss: 0.0154\n",
      "Epoch 72/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0133\n",
      "Epoch 73/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0120\n",
      "Epoch 74/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0122\n",
      "Epoch 75/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0020 - val_loss: 0.0106\n",
      "Epoch 76/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0019 - val_loss: 0.0104\n",
      "Epoch 77/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0093\n",
      "Epoch 78/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0090\n",
      "Epoch 79/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0088\n",
      "Epoch 80/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0018 - val_loss: 0.0072\n",
      "Epoch 81/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0071\n",
      "Epoch 82/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0017 - val_loss: 0.0057\n",
      "Epoch 83/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0018 - val_loss: 0.0052\n",
      "Epoch 84/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0017 - val_loss: 0.0048\n",
      "Epoch 85/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0017 - val_loss: 0.0046\n",
      "Epoch 86/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0047\n",
      "Epoch 87/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0018 - val_loss: 0.0037\n",
      "Epoch 88/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0017 - val_loss: 0.0040\n",
      "Epoch 89/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0037\n",
      "Epoch 90/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0035\n",
      "Epoch 91/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0032\n",
      "Epoch 92/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0033\n",
      "Epoch 93/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0015 - val_loss: 0.0033\n",
      "Epoch 94/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0030\n",
      "Epoch 95/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0029\n",
      "Epoch 96/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0030\n",
      "Epoch 97/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0029\n",
      "Epoch 98/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0015 - val_loss: 0.0031\n",
      "Epoch 99/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0028\n",
      "Epoch 100/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0015 - val_loss: 0.0025\n",
      "Epoch 101/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0024\n",
      "Epoch 102/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0015 - val_loss: 0.0024\n",
      "Epoch 103/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 104/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0023\n",
      "Epoch 105/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 106/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 107/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 108/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 109/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 110/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 111/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0014 - val_loss: 0.0022\n",
      "Epoch 112/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 113/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 114/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 115/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 116/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 117/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 118/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 119/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 120/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 121/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 122/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 123/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 124/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 125/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 126/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 127/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 128/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 129/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 130/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 131/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 132/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 133/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 134/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 135/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 136/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 137/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 138/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 139/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 140/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 141/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 142/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 143/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 144/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 145/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 146/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 147/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 148/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 149/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 150/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 151/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 152/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 153/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 154/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 155/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 156/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 157/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 158/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 159/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 160/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 161/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 162/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 163/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 164/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 165/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 166/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 167/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 168/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 169/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 170/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 171/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 172/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 173/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 174/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 175/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 176/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 177/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 178/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 179/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 180/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 181/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 182/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 183/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 184/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 185/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 186/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 187/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 188/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 189/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 190/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 191/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 192/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 193/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 194/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 195/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 9.6733e-04 - val_loss: 0.0012\n",
      "Epoch 196/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.7207e-04 - val_loss: 0.0012\n",
      "Epoch 197/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 9.5564e-04 - val_loss: 0.0011\n",
      "Epoch 198/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.8353e-04 - val_loss: 0.0012\n",
      "Epoch 199/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 200/1000\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 9.9747e-04\n",
      "Epoch 00200: saving model to saved_models/latent128/cp-0200.h5\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 201/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.9329e-04 - val_loss: 0.0012\n",
      "Epoch 202/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.7131e-04 - val_loss: 0.0012\n",
      "Epoch 203/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.8705e-04 - val_loss: 0.0012\n",
      "Epoch 204/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 205/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 206/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 9.3416e-04 - val_loss: 0.0012\n",
      "Epoch 207/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 208/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.6317e-04 - val_loss: 0.0012\n",
      "Epoch 209/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 210/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 211/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 9.4856e-04 - val_loss: 0.0011\n",
      "Epoch 212/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 213/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 214/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.8934e-04 - val_loss: 0.0011\n",
      "Epoch 215/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 9.7219e-04 - val_loss: 0.0011\n",
      "Epoch 216/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.4408e-04 - val_loss: 0.0012\n",
      "Epoch 217/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.6066e-04 - val_loss: 0.0011\n",
      "Epoch 218/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.7465e-04 - val_loss: 0.0011\n",
      "Epoch 219/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 9.0555e-04 - val_loss: 0.0011\n",
      "Epoch 220/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.4400e-04 - val_loss: 0.0011\n",
      "Epoch 221/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 9.8800e-04 - val_loss: 0.0011\n",
      "Epoch 222/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.5776e-04 - val_loss: 0.0011\n",
      "Epoch 223/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 8.9709e-04 - val_loss: 0.0011\n",
      "Epoch 224/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.0421e-04 - val_loss: 0.0011\n",
      "Epoch 225/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 9.2833e-04 - val_loss: 0.0011\n",
      "Epoch 226/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.3209e-04 - val_loss: 0.0011\n",
      "Epoch 227/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.6106e-04 - val_loss: 0.0011\n",
      "Epoch 228/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 8.9116e-04 - val_loss: 0.0011\n",
      "Epoch 229/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.3666e-04 - val_loss: 0.0010\n",
      "Epoch 230/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.3218e-04 - val_loss: 0.0011\n",
      "Epoch 231/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 9.1035e-04 - val_loss: 0.0011\n",
      "Epoch 232/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.2517e-04 - val_loss: 0.0011\n",
      "Epoch 233/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.1795e-04 - val_loss: 0.0011\n",
      "Epoch 234/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 8.5401e-04 - val_loss: 0.0010\n",
      "Epoch 235/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.1573e-04 - val_loss: 0.0011\n",
      "Epoch 236/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.4631e-04 - val_loss: 0.0011\n",
      "Epoch 237/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 9.1308e-04 - val_loss: 0.0010\n",
      "Epoch 238/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 9.1242e-04 - val_loss: 0.0011\n",
      "Epoch 239/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.3686e-04 - val_loss: 0.0010\n",
      "Epoch 240/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.4359e-04 - val_loss: 0.0011\n",
      "Epoch 241/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.6147e-04 - val_loss: 0.0010\n",
      "Epoch 242/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.9485e-04 - val_loss: 9.9037e-04\n",
      "Epoch 243/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.9325e-04 - val_loss: 0.0010\n",
      "Epoch 244/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.5874e-04 - val_loss: 9.7987e-04\n",
      "Epoch 245/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.7583e-04 - val_loss: 0.0010\n",
      "Epoch 246/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.9915e-04 - val_loss: 9.7166e-04\n",
      "Epoch 247/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.5583e-04 - val_loss: 0.0010\n",
      "Epoch 248/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 8.5027e-04 - val_loss: 9.7667e-04\n",
      "Epoch 249/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.3698e-04 - val_loss: 0.0011\n",
      "Epoch 250/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8769e-04 - val_loss: 9.5299e-04\n",
      "Epoch 251/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.1403e-04 - val_loss: 0.0010\n",
      "Epoch 252/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.2064e-04 - val_loss: 9.7859e-04\n",
      "Epoch 253/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.0862e-04 - val_loss: 0.0012\n",
      "Epoch 254/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.5507e-04 - val_loss: 9.9447e-04\n",
      "Epoch 255/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.7269e-04 - val_loss: 0.0010\n",
      "Epoch 256/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 8.2568e-04 - val_loss: 0.0011\n",
      "Epoch 257/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.3586e-04 - val_loss: 9.4813e-04\n",
      "Epoch 258/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.4913e-04 - val_loss: 0.0010\n",
      "Epoch 259/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.5405e-04 - val_loss: 0.0010\n",
      "Epoch 260/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.1610e-04 - val_loss: 0.0010\n",
      "Epoch 261/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.3292e-04 - val_loss: 0.0010\n",
      "Epoch 262/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.5534e-04 - val_loss: 9.5549e-04\n",
      "Epoch 263/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.4269e-04 - val_loss: 0.0011\n",
      "Epoch 264/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8335e-04 - val_loss: 9.6558e-04\n",
      "Epoch 265/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.9288e-04 - val_loss: 0.0010\n",
      "Epoch 266/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.2585e-04 - val_loss: 0.0011\n",
      "Epoch 267/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.4387e-04 - val_loss: 9.8857e-04\n",
      "Epoch 268/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.6955e-04 - val_loss: 0.0010\n",
      "Epoch 269/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.4056e-04 - val_loss: 0.0010\n",
      "Epoch 270/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 9.5173e-04 - val_loss: 0.0010\n",
      "Epoch 271/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.9096e-04 - val_loss: 0.0010\n",
      "Epoch 272/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.1535e-04 - val_loss: 9.5602e-04\n",
      "Epoch 273/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.3228e-04 - val_loss: 0.0010\n",
      "Epoch 274/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.1419e-04 - val_loss: 9.4879e-04\n",
      "Epoch 275/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.4592e-04 - val_loss: 9.7187e-04\n",
      "Epoch 276/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 8.0486e-04 - val_loss: 8.9629e-04\n",
      "Epoch 277/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 7.5772e-04 - val_loss: 9.3784e-04\n",
      "Epoch 278/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.1139e-04 - val_loss: 9.1855e-04\n",
      "Epoch 279/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.1554e-04 - val_loss: 9.0853e-04\n",
      "Epoch 280/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.6987e-04 - val_loss: 9.2848e-04\n",
      "Epoch 281/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.2409e-04 - val_loss: 9.4650e-04\n",
      "Epoch 282/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.3055e-04 - val_loss: 8.9368e-04\n",
      "Epoch 283/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.3131e-04 - val_loss: 9.7152e-04\n",
      "Epoch 284/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.7099e-04 - val_loss: 9.8901e-04\n",
      "Epoch 285/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.2254e-04 - val_loss: 9.7469e-04\n",
      "Epoch 286/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.3650e-04 - val_loss: 9.0923e-04\n",
      "Epoch 287/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.5037e-04 - val_loss: 0.0010\n",
      "Epoch 288/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.0630e-04 - val_loss: 9.3555e-04\n",
      "Epoch 289/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.3625e-04 - val_loss: 0.0010\n",
      "Epoch 290/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.2846e-04 - val_loss: 9.1172e-04\n",
      "Epoch 291/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.4225e-04 - val_loss: 9.2483e-04\n",
      "Epoch 292/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.6442e-04 - val_loss: 9.5179e-04\n",
      "Epoch 293/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.2801e-04 - val_loss: 9.4887e-04\n",
      "Epoch 294/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.3546e-04 - val_loss: 8.9778e-04\n",
      "Epoch 295/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.5122e-04 - val_loss: 8.9031e-04\n",
      "Epoch 296/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.1470e-04 - val_loss: 9.1377e-04\n",
      "Epoch 297/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.7382e-04 - val_loss: 8.8667e-04\n",
      "Epoch 298/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.2567e-04 - val_loss: 9.0775e-04\n",
      "Epoch 299/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.7224e-04 - val_loss: 8.7202e-04\n",
      "Epoch 300/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.1104e-04 - val_loss: 9.2183e-04\n",
      "Epoch 301/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.6354e-04 - val_loss: 9.3779e-04\n",
      "Epoch 302/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.9143e-04 - val_loss: 9.2309e-04\n",
      "Epoch 303/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.2294e-04 - val_loss: 9.6966e-04\n",
      "Epoch 304/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.9767e-04 - val_loss: 0.0010\n",
      "Epoch 305/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.9006e-04 - val_loss: 0.0010\n",
      "Epoch 306/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.7042e-04 - val_loss: 9.7318e-04\n",
      "Epoch 307/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.9746e-04 - val_loss: 0.0010\n",
      "Epoch 308/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.0757e-04 - val_loss: 9.5106e-04\n",
      "Epoch 309/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.8158e-04 - val_loss: 8.9463e-04\n",
      "Epoch 310/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.8036e-04 - val_loss: 9.3934e-04\n",
      "Epoch 311/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.7742e-04 - val_loss: 9.4922e-04\n",
      "Epoch 312/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 7.5544e-04 - val_loss: 8.9564e-04\n",
      "Epoch 313/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.8778e-04 - val_loss: 9.4134e-04\n",
      "Epoch 314/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.9155e-04 - val_loss: 9.6434e-04\n",
      "Epoch 315/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.2797e-04 - val_loss: 9.2123e-04\n",
      "Epoch 316/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 7.4885e-04 - val_loss: 8.4250e-04\n",
      "Epoch 317/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 6.9726e-04 - val_loss: 8.8629e-04\n",
      "Epoch 318/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 6.7367e-04 - val_loss: 8.7231e-04\n",
      "Epoch 319/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.3986e-04 - val_loss: 8.2366e-04\n",
      "Epoch 320/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.7044e-04 - val_loss: 9.1493e-04\n",
      "Epoch 321/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.3218e-04 - val_loss: 9.0084e-04\n",
      "Epoch 322/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.7543e-04 - val_loss: 8.7997e-04\n",
      "Epoch 323/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.2879e-04 - val_loss: 9.6143e-04\n",
      "Epoch 324/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.1547e-04 - val_loss: 8.4123e-04\n",
      "Epoch 325/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.3051e-04 - val_loss: 9.2853e-04\n",
      "Epoch 326/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.6341e-04 - val_loss: 8.9805e-04\n",
      "Epoch 327/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.7458e-04 - val_loss: 9.0356e-04\n",
      "Epoch 328/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.4739e-04 - val_loss: 8.3356e-04\n",
      "Epoch 329/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.0134e-04 - val_loss: 8.5274e-04\n",
      "Epoch 330/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.2460e-04 - val_loss: 8.1492e-04\n",
      "Epoch 331/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.1070e-04 - val_loss: 8.8446e-04\n",
      "Epoch 332/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.5094e-04 - val_loss: 8.2306e-04\n",
      "Epoch 333/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.7728e-04 - val_loss: 9.0684e-04\n",
      "Epoch 334/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.1183e-04 - val_loss: 9.2263e-04\n",
      "Epoch 335/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.7872e-04 - val_loss: 8.6452e-04\n",
      "Epoch 336/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.2418e-04 - val_loss: 8.5981e-04\n",
      "Epoch 337/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.8418e-04 - val_loss: 8.1437e-04\n",
      "Epoch 338/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.5482e-04 - val_loss: 8.5617e-04\n",
      "Epoch 339/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.9977e-04 - val_loss: 8.8275e-04\n",
      "Epoch 340/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.5740e-04 - val_loss: 8.9363e-04\n",
      "Epoch 341/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.7355e-04 - val_loss: 9.5717e-04\n",
      "Epoch 342/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.5034e-04 - val_loss: 9.4196e-04\n",
      "Epoch 343/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.8264e-04 - val_loss: 9.4662e-04\n",
      "Epoch 344/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.5727e-04 - val_loss: 9.7205e-04\n",
      "Epoch 345/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.8730e-04 - val_loss: 9.0343e-04\n",
      "Epoch 346/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.5400e-04 - val_loss: 9.3551e-04\n",
      "Epoch 347/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.1729e-04 - val_loss: 9.0069e-04\n",
      "Epoch 348/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.7754e-04 - val_loss: 9.1373e-04\n",
      "Epoch 349/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.9974e-04 - val_loss: 9.8101e-04\n",
      "Epoch 350/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.5562e-04 - val_loss: 8.7736e-04\n",
      "Epoch 351/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.7589e-04 - val_loss: 8.5909e-04\n",
      "Epoch 352/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.5930e-04 - val_loss: 9.0247e-04\n",
      "Epoch 353/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.3850e-04 - val_loss: 9.3757e-04\n",
      "Epoch 354/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.0448e-04 - val_loss: 8.4452e-04\n",
      "Epoch 355/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.1269e-04 - val_loss: 8.8061e-04\n",
      "Epoch 356/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.1957e-04 - val_loss: 8.4105e-04\n",
      "Epoch 357/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.2404e-04 - val_loss: 9.0895e-04\n",
      "Epoch 358/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.3038e-04 - val_loss: 8.5029e-04\n",
      "Epoch 359/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.6037e-04 - val_loss: 8.5496e-04\n",
      "Epoch 360/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.2275e-04 - val_loss: 8.6971e-04\n",
      "Epoch 361/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.4173e-04 - val_loss: 8.3669e-04\n",
      "Epoch 362/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.3371e-04 - val_loss: 8.6518e-04\n",
      "Epoch 363/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.3469e-04 - val_loss: 8.4303e-04\n",
      "Epoch 364/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.7752e-04 - val_loss: 8.4184e-04\n",
      "Epoch 365/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.4058e-04 - val_loss: 7.9324e-04\n",
      "Epoch 366/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 7.6357e-04 - val_loss: 9.3045e-04\n",
      "Epoch 367/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 6.8992e-04 - val_loss: 7.9286e-04\n",
      "Epoch 368/1000\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 7.0975e-04Restoring model weights from the end of the best epoch.\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 7.3500e-04 - val_loss: 8.6668e-04\n",
      "Epoch 00368: early stopping\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train VAE model with callbacks \"\"\"\n",
    "history = ae.fit(trainX, trainX, \n",
    "                  batch_size = batch_size, \n",
    "                  epochs = epochs, \n",
    "                  callbacks=[initial_checkpoint, checkpoint, EarlyStoppingAtMinLoss()],\n",
    "                  #callbacks=[initial_checkpoint, checkpoint],\n",
    "                  shuffle=True,\n",
    "                  validation_data=(valX, valX)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 6.0\n"
     ]
    }
   ],
   "source": [
    "# In this GPU implementation, it shows the number of batches/iterations for one epoch (and not the training samples), which is:\n",
    "print (\"Step size:\", np.ceil(trainX.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save the entire model \"\"\"\n",
    "# Save notes about hyperparameters used:\n",
    "with open(f'{SAVE_FOLDER}/notes.txt', 'w') as f:\n",
    "    f.write(f'Epochs: {epochs} \\n')\n",
    "    f.write(f'Batch size: {batch_size} \\n')\n",
    "    f.write(f'Latent dimension: {latent_dim} \\n')\n",
    "    f.write(f'Filter sizes: {filters} \\n')\n",
    "    f.write(f'img_width: {img_width} \\n')\n",
    "    f.write(f'img_height: {img_height} \\n')\n",
    "    f.write(f'num_channels: {num_channels}')\n",
    "    f.close()\n",
    "\n",
    "# Save encoder weights:\n",
    "encoder.save_weights(f'{SAVE_FOLDER}/ENCODER_WEIGHTS.h5')\n",
    "\n",
    "# Save the total AE model, i.e. its weights:\n",
    "ae.save_weights(f'{SAVE_FOLDER}/AE_WEIGHTS.h5')\n",
    "\n",
    "# Save the history at each epochs (cost of train and validation sets):\n",
    "np.save(f'{SAVE_FOLDER}/HISTORY.npy', history.history)\n",
    "\n",
    "# Save exact training, validation and testing data set (as numpy arrays):\n",
    "np.save(f'{SAVE_FOLDER}/trainX.npy', trainX)\n",
    "np.save(f'{SAVE_FOLDER}/valX.npy', valX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllX.npy', testAllX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllY.npy', testAllY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate training process\n",
    "\n",
    "Now that the training process is complete, let’s evaluate the average loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(13, 6))\n",
    "\n",
    "# summarize history for loss:\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='blue', alpha=0.4, label='validation')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Cost', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.title('Average Loss During Training', fontsize=18)\n",
    "#plt.xticks(range(0,epochs))\n",
    "plt.legend(loc='upper right', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
