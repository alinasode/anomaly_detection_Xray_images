{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import axis, colorbar, imshow, show, figure, subplot\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=18)\n",
    "mpl.rc('xtick', labelsize=16)\n",
    "mpl.rc('ytick', labelsize=16)\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## ----- GPU ------------------------------------\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'    # use of GPU 0 (ERDA) (use before importing torch or tensorflow/keeas)\n",
    "## ----------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization, ReLU, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.activations import relu\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K                         #contains calls for tensor manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL_TRAIN_AUG:       /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTrainAugmentations\n",
      "NORMAL_VALIDATION_AUG:  /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalValidationAugmentations\n",
      "NORMAL_TEST_AUG:        /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTestAugmentations\n",
      "ANOMALY_AUG:            /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/AnomalyAugmentations\n"
     ]
    }
   ],
   "source": [
    "from utils_ae_kitkat_paths import *\n",
    "\n",
    "# Get work directions for augmentated data set:\n",
    "print (\"NORMAL_TRAIN_AUG:      \", NORMAL_TRAIN_AUG)\n",
    "print (\"NORMAL_VALIDATION_AUG: \", NORMAL_VAL_AUG)\n",
    "print (\"NORMAL_TEST_AUG:       \", NORMAL_TEST_AUG)\n",
    "print (\"ANOMALY_AUG:           \", ANOMALY_AUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which Folder The Models Are Saved In: saved_models/latent16\n"
     ]
    }
   ],
   "source": [
    "# What latent dimension and filter size are we using?:\n",
    "latent_dim = 16\n",
    "filters    = 32\n",
    "\n",
    "# Get work direction for saving files:\n",
    "SAVE_FOLDER = f'saved_models/latent{latent_dim}'\n",
    "print (\"Which Folder The Models Are Saved In:\", SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] deleting existing files in folder...\n",
      "         done in 0.010 minutes\n"
     ]
    }
   ],
   "source": [
    "# Delete all existing files in folder where models are saved:\n",
    "print(\"[INFO] deleting existing files in folder...\")\n",
    "t0 = time()\n",
    "\n",
    "files = glob.glob(f'{SAVE_FOLDER}/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "print (\"         done in %0.3f minutes\" % ((time() - t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_VAE_AE import get_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Investigation of Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"normal\" training images is:     1400\n",
      "Number of \"normal\" validation images is:   134\n",
      "Number of \"normal\" test images is:         266\n",
      "Number of \"anomaly\" images is:             600\n"
     ]
    }
   ],
   "source": [
    "# Glob the directories and get the lists of good and bad images\n",
    "good_train_fns = [f for f in os.listdir(NORMAL_TRAIN_AUG) if not f.startswith('.')]\n",
    "good_val_fns = [f for f in os.listdir(NORMAL_VAL_AUG) if not f.startswith('.')]\n",
    "good_test_fns = [f for f in os.listdir(NORMAL_TEST_AUG) if not f.startswith('.')]\n",
    "bad_fns = [f for f in os.listdir(ANOMALY_AUG) if not f.startswith('.')]\n",
    "\n",
    "print('Number of \"normal\" training images is:     {}'.format(len(good_train_fns)))\n",
    "print('Number of \"normal\" validation images is:   {}'.format(len(good_val_fns)))\n",
    "print('Number of \"normal\" test images is:         {}'.format(len(good_test_fns)))\n",
    "print('Number of \"anomaly\" images is:             {}'.format(len(bad_fns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Procentage of normal samples (ground truth):   75.00 %\n",
      "> Procentage of anomaly samples (ground truth):  25.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of bad images vs good images:\n",
    "normal_fns = len(good_train_fns) + len(good_val_fns) + len(good_test_fns)\n",
    "anomaly_fns = len(bad_fns)\n",
    "print(f\"> Procentage of normal samples (ground truth):   {(normal_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")\n",
    "print(f\"> Procentage of anomaly samples (ground truth):  {(anomaly_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Normal Data\n",
    "\n",
    "Load normal samples in pre-splitted data sets: train, valdiation and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal training images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal training images...\")\n",
    "trainX = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TRAIN_AUG}/*.png\")]  # read as grayscale\n",
    "trainX = np.array(trainX)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "trainY = get_labels(trainX, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal validation images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal validation images...\")\n",
    "x_normal_val = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_VAL_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_val = np.array(x_normal_val)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_val = get_labels(x_normal_val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal testing images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal testing images...\")\n",
    "x_normal_test = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TEST_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_test = np.array(x_normal_test)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_test = get_labels(x_normal_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Data\n",
    "\n",
    "\n",
    "Load anomaly samples in its singular training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly images...\")\n",
    "x_anomaly = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY_AUG}/*.png\")]  # read as grayscale\n",
    "x_anomaly = np.array(x_anomaly)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_anomaly = get_labels(x_anomaly, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine normal samples and anomaly samples and split them into training, validation and test sets\n",
    "\n",
    "`label 0` == Normal Samples\n",
    "\n",
    "`label 1` == Anomaly Samples\n",
    "\n",
    "Train and Validation sets only consists of `Normal Data`, aka. `label 0`.\n",
    "\n",
    "Testing sets consists of both  `Normal Data` (aka. `label 0`) and `Anomaly Data` (aka. `label 1`)\n",
    "\n",
    "________________\n",
    "\n",
    "Due to the very sparse original (preprossed) KitKat Chunky data, the validation set will be a mix of normal testing and validation data. The normal testing set will consists of all validation and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testvalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testvalY = get_labels(testvalX, 0)\n",
    "\n",
    "# randomly split in order to make new validation set:\n",
    "NoUsageX, valX, NoUsageY, valY = train_test_split(testvalX, testvalY, test_size=0.25, random_state=4345672)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testNormalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testNormalY = get_labels(testNormalX, 0)\n",
    "\n",
    "# anomaly class (only used for testing):\n",
    "testAnomalyX, testAnomalyY = x_anomaly, y_anomaly\n",
    "\n",
    "# Combine all testing data in one array:\n",
    "testAllX = np.vstack((testNormalX, testAnomalyX))\n",
    "testAllY = np.hstack((testNormalY, testAnomalyY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Dimensions and Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      " > images: (1400, 128, 128)\n",
      " > labels: (1400,)\n",
      "\n",
      "Validation set:\n",
      " > images: (100, 128, 128)\n",
      " > labels: (100,)\n",
      "\n",
      "Test set:\n",
      " > images: (1000, 128, 128)\n",
      " > labels: (1000,)\n",
      "\t'Normal' test set:\n",
      "\t  > images: (400, 128, 128)\n",
      "\t  > labels: (400,)\n",
      "\t'Anomaly' test set:\n",
      "\t  > images: (600, 128, 128)\n",
      "\t  > labels: (600,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "print(\" > images:\", trainX.shape)\n",
    "print(\" > labels:\", trainY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Validation set:\")\n",
    "print(\" > images:\", valX.shape)\n",
    "print(\" > labels:\", valY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Test set:\")\n",
    "print(\" > images:\", testAllX.shape)\n",
    "print(\" > labels:\", testAllY.shape)\n",
    "print(\"\\t'Normal' test set:\")\n",
    "print(\"\\t  > images:\", testNormalX.shape)\n",
    "print(\"\\t  > labels:\", testNormalY.shape)\n",
    "print(\"\\t'Anomaly' test set:\")\n",
    "print(\"\\t  > images:\", testAnomalyX.shape)\n",
    "print(\"\\t  > labels:\", testAnomalyY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 73.68 %\n",
      "Procentage of validation samples: 5.26 %\n",
      "Procentage of (only normal) testing samples: 21.05 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets (only normal data):\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (only normal) testing samples: {(len(testNormalX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 56.00 %\n",
      "Procentage of validation samples: 4.00 %\n",
      "Procentage of (total) testing samples: 40.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets:\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (total) testing samples: {(len(testAllX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for (un)balanced data\n",
    "\n",
    "In an ideal world the data should be balanced. However in the real world we expect there being around ~$99 \\%$ good samples and only ~$1 \\%$ bad samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['0: normal', '1: anomaly']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,4))\n",
    "ax[0].hist(trainY, align=\"left\"); ax[0].set_title('train', fontsize=18); ax[0].set_xticks([0,1]); ax[0].set_xticklabels(labels); ax[0].tick_params(axis='both', which='major', labelsize=16); ax[0].set_xlim(-0.5, 1.5);\n",
    "ax[1].hist(valY, align=\"left\"); ax[1].set_title('validation', fontsize=18); ax[1].set_xticks([0,1]); ax[1].set_xticklabels(labels); ax[1].tick_params(axis='both', which='major', labelsize=16); ax[1].set_xlim(-0.5, 1.5);\n",
    "ax[2].hist(testAllY, align=\"left\"); ax[2].set_title('test', fontsize=18); ax[2].set_xticks([0,1]); ax[2].set_xticklabels(labels); ax[2].tick_params(axis='both', which='major', labelsize=16); ax[2].set_xlim(-0.5, 1.5);\n",
    "\n",
    "ax[0].set_ylabel(\"Number of images\", fontsize=18)\n",
    "ax[1].set_xlabel(\"Class Labels\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of normal test samples: 40.00 %\n",
      "Procentage of total anomaly test samples: 60.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of normal test images vs total anomaly test images:\n",
    "print(f\"Procentage of normal test samples: {(len(testNormalX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of total anomaly test samples: {(len(testAnomalyX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Only normalization has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "img_width, img_height = trainX.shape[1], trainX.shape[2]      # input image dimensions\n",
    "num_channels = 1                                              # gray-channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Neural Networks learns faster with normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to be in  the [0, 1] range:\n",
    "trainX = trainX.astype('float32') / 255.\n",
    "valX = valX.astype('float32') / 255.\n",
    "testAllX = testAllX.astype('float32') / 255.\n",
    "\n",
    "# reshape data to keras inputs --> (n_samples, img_rows, img_cols, n_channels):\n",
    "trainX = trainX.reshape(trainX.shape[0], img_height, img_width, num_channels)\n",
    "valX = valX.reshape(valX.shape[0], img_height, img_width, num_channels)\n",
    "testAllX = testAllX.reshape(testAllX.shape[0], img_height, img_width, num_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import architecture of VAE model and its hyperparameters:\n",
    "from J16_AE_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Total AE Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 64, 64, 32)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "latent (Dense)               (None, 16)                8208      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               8704      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 32)      4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 128, 128, 32)      128       \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 128, 128, 1)       129       \n",
      "=================================================================\n",
      "Total params: 57,681\n",
      "Trainable params: 56,017\n",
      "Non-trainable params: 1,664\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "batch_size  = 256\n",
    "epochs      = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at 1'st epoch (inital model)\n",
    "\n",
    "https://stackoverflow.com/questions/54323960/save-keras-model-at-specific-epochs\n",
    "\"\"\"\n",
    "\n",
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 1:\n",
    "            self.model.save_weights(f\"{SAVE_FOLDER}/cp-0001.h5\")\n",
    "            \n",
    "# create and use callback:\n",
    "initial_checkpoint = CustomSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at every k'te epochs\n",
    "\"\"\"\n",
    "# Include the epoch in the file name (uses `str.format`):\n",
    "checkpoint_path = \"saved_models/latent16/cp-{epoch:04d}.h5\"\n",
    "\n",
    "# Create a callback that saves the model's weights every k'te epochs:\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path,\n",
    "                             monitor='loss',           # name of the metrics to monitor\n",
    "                             verbose=1, \n",
    "                             #save_best_only=False,     # if False, the best model will not be overridden.\n",
    "                             save_weights_only=True,   # if True, only the weights of the models will be saved.\n",
    "                                                        # if False, the whole models will be saved.\n",
    "                             #mode='auto', \n",
    "                             period=200                  # save after every k'te epochs\n",
    "                            )\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "ae.save_weights(checkpoint_path.format(epoch=0))          # save for zero epoch (inital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: Early stopping\n",
    "https://www.tensorflow.org/guide/keras/custom_callback\n",
    "\"\"\"\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=50):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create and Compile Model\"\"\"\n",
    "# import architecture of VAE model and its hyperparameters. learning rate choosen from graph above.\n",
    "ae = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 0.2877 - val_loss: 0.2154\n",
      "Epoch 2/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.1998 - val_loss: 0.2097\n",
      "Epoch 3/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.1448 - val_loss: 0.2025\n",
      "Epoch 4/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0981 - val_loss: 0.1939\n",
      "Epoch 5/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0697 - val_loss: 0.1843\n",
      "Epoch 6/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0556 - val_loss: 0.1744\n",
      "Epoch 7/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0480 - val_loss: 0.1646\n",
      "Epoch 8/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0435 - val_loss: 0.1552\n",
      "Epoch 9/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0401 - val_loss: 0.1462\n",
      "Epoch 10/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0373 - val_loss: 0.1374\n",
      "Epoch 11/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0349 - val_loss: 0.1284\n",
      "Epoch 12/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0325 - val_loss: 0.1195\n",
      "Epoch 13/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0296 - val_loss: 0.1109\n",
      "Epoch 14/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0275 - val_loss: 0.1029\n",
      "Epoch 15/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0252 - val_loss: 0.0953\n",
      "Epoch 16/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0236 - val_loss: 0.0884\n",
      "Epoch 17/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0213 - val_loss: 0.0822\n",
      "Epoch 18/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0198 - val_loss: 0.0765\n",
      "Epoch 19/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0183 - val_loss: 0.0714\n",
      "Epoch 20/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0170 - val_loss: 0.0670\n",
      "Epoch 21/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0167 - val_loss: 0.0635\n",
      "Epoch 22/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0146 - val_loss: 0.0607\n",
      "Epoch 23/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0138 - val_loss: 0.0581\n",
      "Epoch 24/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0133 - val_loss: 0.0560\n",
      "Epoch 25/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0124 - val_loss: 0.0542\n",
      "Epoch 26/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0117 - val_loss: 0.0529\n",
      "Epoch 27/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0104 - val_loss: 0.0518\n",
      "Epoch 28/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0102 - val_loss: 0.0510\n",
      "Epoch 29/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0095 - val_loss: 0.0504\n",
      "Epoch 30/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0090 - val_loss: 0.0499\n",
      "Epoch 31/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0085 - val_loss: 0.0496\n",
      "Epoch 32/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0083 - val_loss: 0.0494\n",
      "Epoch 33/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0082 - val_loss: 0.0493\n",
      "Epoch 34/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0079 - val_loss: 0.0492\n",
      "Epoch 35/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0074 - val_loss: 0.0492\n",
      "Epoch 36/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0074 - val_loss: 0.0492\n",
      "Epoch 37/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0074 - val_loss: 0.0493\n",
      "Epoch 38/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0074 - val_loss: 0.0493\n",
      "Epoch 39/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0071 - val_loss: 0.0493\n",
      "Epoch 40/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0067 - val_loss: 0.0493\n",
      "Epoch 41/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0062 - val_loss: 0.0493\n",
      "Epoch 42/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0068 - val_loss: 0.0492\n",
      "Epoch 43/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0062 - val_loss: 0.0491\n",
      "Epoch 44/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0062 - val_loss: 0.0489\n",
      "Epoch 45/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0056 - val_loss: 0.0487\n",
      "Epoch 46/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0060 - val_loss: 0.0488\n",
      "Epoch 47/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0054 - val_loss: 0.0489\n",
      "Epoch 48/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0052 - val_loss: 0.0489\n",
      "Epoch 49/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0050 - val_loss: 0.0495\n",
      "Epoch 50/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0051 - val_loss: 0.0492\n",
      "Epoch 51/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0059 - val_loss: 0.0503\n",
      "Epoch 52/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0051 - val_loss: 0.0517\n",
      "Epoch 53/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0058 - val_loss: 0.0547\n",
      "Epoch 54/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0053 - val_loss: 0.0551\n",
      "Epoch 55/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0047 - val_loss: 0.0561\n",
      "Epoch 56/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0051 - val_loss: 0.0569\n",
      "Epoch 57/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0049 - val_loss: 0.0572\n",
      "Epoch 58/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0045 - val_loss: 0.0560\n",
      "Epoch 59/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0047 - val_loss: 0.0548\n",
      "Epoch 60/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0041 - val_loss: 0.0557\n",
      "Epoch 61/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0043 - val_loss: 0.0529\n",
      "Epoch 62/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0045 - val_loss: 0.0535\n",
      "Epoch 63/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0042 - val_loss: 0.0495\n",
      "Epoch 64/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0042 - val_loss: 0.0498\n",
      "Epoch 65/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0040 - val_loss: 0.0475\n",
      "Epoch 66/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0042 - val_loss: 0.0465\n",
      "Epoch 67/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0041 - val_loss: 0.0443\n",
      "Epoch 68/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0038 - val_loss: 0.0441\n",
      "Epoch 69/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0040 - val_loss: 0.0406\n",
      "Epoch 70/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0037 - val_loss: 0.0419\n",
      "Epoch 71/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0038 - val_loss: 0.0397\n",
      "Epoch 72/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0039 - val_loss: 0.0389\n",
      "Epoch 73/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0040 - val_loss: 0.0373\n",
      "Epoch 74/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0041 - val_loss: 0.0323\n",
      "Epoch 75/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0037 - val_loss: 0.0304\n",
      "Epoch 76/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0039 - val_loss: 0.0258\n",
      "Epoch 77/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0040 - val_loss: 0.0235\n",
      "Epoch 78/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0037 - val_loss: 0.0214\n",
      "Epoch 79/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0035 - val_loss: 0.0190\n",
      "Epoch 80/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0035 - val_loss: 0.0191\n",
      "Epoch 81/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0034 - val_loss: 0.0168\n",
      "Epoch 82/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0031 - val_loss: 0.0149\n",
      "Epoch 83/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0033 - val_loss: 0.0162\n",
      "Epoch 84/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0031 - val_loss: 0.0158\n",
      "Epoch 85/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0031 - val_loss: 0.0176\n",
      "Epoch 86/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0035 - val_loss: 0.0135\n",
      "Epoch 87/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0033 - val_loss: 0.0124\n",
      "Epoch 88/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0031 - val_loss: 0.0174\n",
      "Epoch 89/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0032 - val_loss: 0.0128\n",
      "Epoch 90/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0031 - val_loss: 0.0125\n",
      "Epoch 91/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0029 - val_loss: 0.0125\n",
      "Epoch 92/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0032 - val_loss: 0.0110\n",
      "Epoch 93/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0029 - val_loss: 0.0122\n",
      "Epoch 94/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0031 - val_loss: 0.0099\n",
      "Epoch 95/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0032 - val_loss: 0.0093\n",
      "Epoch 96/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0031 - val_loss: 0.0098\n",
      "Epoch 97/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0031 - val_loss: 0.0151\n",
      "Epoch 98/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0028 - val_loss: 0.0090\n",
      "Epoch 99/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0030 - val_loss: 0.0070\n",
      "Epoch 100/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0030 - val_loss: 0.0085\n",
      "Epoch 101/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0033 - val_loss: 0.0077\n",
      "Epoch 102/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0027 - val_loss: 0.0062\n",
      "Epoch 103/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0052\n",
      "Epoch 104/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0026 - val_loss: 0.0053\n",
      "Epoch 105/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0055\n",
      "Epoch 106/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0051\n",
      "Epoch 107/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0025 - val_loss: 0.0056\n",
      "Epoch 108/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0050\n",
      "Epoch 109/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0047\n",
      "Epoch 110/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0050\n",
      "Epoch 111/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0025 - val_loss: 0.0058\n",
      "Epoch 112/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0053\n",
      "Epoch 113/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0025 - val_loss: 0.0064\n",
      "Epoch 114/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0067\n",
      "Epoch 115/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0024 - val_loss: 0.0056\n",
      "Epoch 116/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0063\n",
      "Epoch 117/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0029 - val_loss: 0.0047\n",
      "Epoch 118/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0025 - val_loss: 0.0053\n",
      "Epoch 119/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0024 - val_loss: 0.0051\n",
      "Epoch 120/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0025 - val_loss: 0.0054\n",
      "Epoch 121/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0025 - val_loss: 0.0077\n",
      "Epoch 122/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0126\n",
      "Epoch 123/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0024 - val_loss: 0.0099\n",
      "Epoch 124/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0061\n",
      "Epoch 125/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0023 - val_loss: 0.0066\n",
      "Epoch 126/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0061\n",
      "Epoch 127/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0034 - val_loss: 0.0057\n",
      "Epoch 128/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0051\n",
      "Epoch 129/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0046\n",
      "Epoch 130/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0027 - val_loss: 0.0043\n",
      "Epoch 131/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0042\n",
      "Epoch 132/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0047\n",
      "Epoch 133/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0045\n",
      "Epoch 134/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0023 - val_loss: 0.0051\n",
      "Epoch 135/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0046\n",
      "Epoch 136/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0044\n",
      "Epoch 137/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0039\n",
      "Epoch 138/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0038\n",
      "Epoch 139/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0036\n",
      "Epoch 140/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0022 - val_loss: 0.0033\n",
      "Epoch 141/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0023 - val_loss: 0.0036\n",
      "Epoch 142/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0021 - val_loss: 0.0034\n",
      "Epoch 143/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0034\n",
      "Epoch 144/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0034\n",
      "Epoch 145/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0034\n",
      "Epoch 146/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0022 - val_loss: 0.0032\n",
      "Epoch 147/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0038\n",
      "Epoch 148/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0022 - val_loss: 0.0035\n",
      "Epoch 149/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0020 - val_loss: 0.0037\n",
      "Epoch 150/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0035\n",
      "Epoch 151/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0038\n",
      "Epoch 152/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0021 - val_loss: 0.0037\n",
      "Epoch 153/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0041\n",
      "Epoch 154/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0019 - val_loss: 0.0033\n",
      "Epoch 155/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0038\n",
      "Epoch 156/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0035\n",
      "Epoch 157/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0040\n",
      "Epoch 158/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0039\n",
      "Epoch 159/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0090\n",
      "Epoch 160/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0052\n",
      "Epoch 161/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0053\n",
      "Epoch 162/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0020 - val_loss: 0.0053\n",
      "Epoch 163/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0066\n",
      "Epoch 164/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0046\n",
      "Epoch 165/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0043\n",
      "Epoch 166/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0060\n",
      "Epoch 167/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0022 - val_loss: 0.0035\n",
      "Epoch 168/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0034\n",
      "Epoch 169/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0037\n",
      "Epoch 170/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0035\n",
      "Epoch 171/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0019 - val_loss: 0.0032\n",
      "Epoch 172/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0019 - val_loss: 0.0034\n",
      "Epoch 173/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0033\n",
      "Epoch 174/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0020 - val_loss: 0.0033\n",
      "Epoch 175/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0036\n",
      "Epoch 176/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0037\n",
      "Epoch 177/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0033\n",
      "Epoch 178/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0021 - val_loss: 0.0040\n",
      "Epoch 179/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0019 - val_loss: 0.0074\n",
      "Epoch 180/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0022 - val_loss: 0.0075\n",
      "Epoch 181/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0035\n",
      "Epoch 182/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0036\n",
      "Epoch 183/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0020 - val_loss: 0.0032\n",
      "Epoch 184/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0020 - val_loss: 0.0040\n",
      "Epoch 185/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0021 - val_loss: 0.0033\n",
      "Epoch 186/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0020 - val_loss: 0.0041\n",
      "Epoch 187/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0020 - val_loss: 0.0036\n",
      "Epoch 188/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0046\n",
      "Epoch 189/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0020 - val_loss: 0.0031\n",
      "Epoch 190/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0021 - val_loss: 0.0032\n",
      "Epoch 191/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0019 - val_loss: 0.0033\n",
      "Epoch 192/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0021 - val_loss: 0.0032\n",
      "Epoch 193/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0019 - val_loss: 0.0031\n",
      "Epoch 194/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0020 - val_loss: 0.0037\n",
      "Epoch 195/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0018 - val_loss: 0.0027\n",
      "Epoch 196/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0018 - val_loss: 0.0033\n",
      "Epoch 197/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0017 - val_loss: 0.0030\n",
      "Epoch 198/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0018 - val_loss: 0.0030\n",
      "Epoch 199/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0019 - val_loss: 0.0032\n",
      "Epoch 200/1000\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.0018\n",
      "Epoch 00200: saving model to saved_models/latent16/cp-0200.h5\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 0.0018 - val_loss: 0.0069\n",
      "Epoch 201/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0031\n",
      "Epoch 202/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0030\n",
      "Epoch 203/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0026\n",
      "Epoch 204/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0030\n",
      "Epoch 205/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0027\n",
      "Epoch 206/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0017 - val_loss: 0.0029\n",
      "Epoch 207/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0029\n",
      "Epoch 208/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0019 - val_loss: 0.0028\n",
      "Epoch 209/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0028\n",
      "Epoch 210/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0021 - val_loss: 0.0039\n",
      "Epoch 211/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0017 - val_loss: 0.0027\n",
      "Epoch 212/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0019 - val_loss: 0.0028\n",
      "Epoch 213/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0028\n",
      "Epoch 214/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0029\n",
      "Epoch 215/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0026\n",
      "Epoch 216/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0027\n",
      "Epoch 217/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0029\n",
      "Epoch 218/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0027\n",
      "Epoch 219/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0017 - val_loss: 0.0026\n",
      "Epoch 220/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0030\n",
      "Epoch 221/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0027\n",
      "Epoch 222/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0028\n",
      "Epoch 223/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0016 - val_loss: 0.0027\n",
      "Epoch 224/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0028\n",
      "Epoch 225/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0025\n",
      "Epoch 226/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0017 - val_loss: 0.0025\n",
      "Epoch 227/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 228/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0016 - val_loss: 0.0026\n",
      "Epoch 229/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0017 - val_loss: 0.0025\n",
      "Epoch 230/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0030\n",
      "Epoch 231/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0017 - val_loss: 0.0025\n",
      "Epoch 232/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0025\n",
      "Epoch 233/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0028\n",
      "Epoch 234/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0015 - val_loss: 0.0026\n",
      "Epoch 235/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0026\n",
      "Epoch 236/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 237/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0027\n",
      "Epoch 238/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0025\n",
      "Epoch 239/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 240/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0025\n",
      "Epoch 241/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0019 - val_loss: 0.0025\n",
      "Epoch 242/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 243/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 244/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 245/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0024\n",
      "Epoch 246/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 247/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0016 - val_loss: 0.0024\n",
      "Epoch 248/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 249/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 250/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 251/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0017 - val_loss: 0.0025\n",
      "Epoch 252/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 253/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 254/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 255/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0024\n",
      "Epoch 256/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 257/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 258/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 259/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 260/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 261/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0024\n",
      "Epoch 262/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 263/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 264/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0016 - val_loss: 0.0028\n",
      "Epoch 265/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 266/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0026\n",
      "Epoch 267/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0032\n",
      "Epoch 268/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0024\n",
      "Epoch 269/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 270/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 271/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0026\n",
      "Epoch 272/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 273/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 274/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 275/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 276/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 277/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 278/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 279/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 280/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 281/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 282/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 283/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 284/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 285/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 286/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 287/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 288/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 289/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 290/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 291/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 292/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 293/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 294/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 295/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 296/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0015 - val_loss: 0.0050\n",
      "Epoch 297/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0052\n",
      "Epoch 298/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0025\n",
      "Epoch 299/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 300/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 301/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0022\n",
      "Epoch 302/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0027\n",
      "Epoch 303/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 304/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 305/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 306/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 307/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 308/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 309/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 310/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 311/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 312/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 313/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 314/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 315/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 316/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 317/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 318/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0012 - val_loss: 0.0020\n",
      "Epoch 319/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 320/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 321/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 322/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 323/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 324/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0019\n",
      "Epoch 325/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 326/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 327/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 328/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 329/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 330/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 331/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 332/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 333/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 334/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 335/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 336/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 337/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 338/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 339/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 340/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 341/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 342/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 343/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 344/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 345/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 346/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 347/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 348/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 349/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 350/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 351/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 352/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 353/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 354/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 355/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 356/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 357/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 358/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 359/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 360/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 361/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 362/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 363/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 364/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 365/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 366/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 367/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 368/1000\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.0012Restoring model weights from the end of the best epoch.\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 00368: early stopping\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train VAE model with callbacks \"\"\"\n",
    "history = ae.fit(trainX, trainX, \n",
    "                  batch_size = batch_size, \n",
    "                  epochs = epochs, \n",
    "                  callbacks=[initial_checkpoint, checkpoint, EarlyStoppingAtMinLoss()],\n",
    "                  #callbacks=[initial_checkpoint, checkpoint],\n",
    "                  shuffle=True,\n",
    "                  validation_data=(valX, valX)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 6.0\n"
     ]
    }
   ],
   "source": [
    "# In this GPU implementation, it shows the number of batches/iterations for one epoch (and not the training samples), which is:\n",
    "print (\"Step size:\", np.ceil(trainX.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save the entire model \"\"\"\n",
    "# Save notes about hyperparameters used:\n",
    "with open(f'{SAVE_FOLDER}/notes.txt', 'w') as f:\n",
    "    f.write(f'Epochs: {epochs} \\n')\n",
    "    f.write(f'Batch size: {batch_size} \\n')\n",
    "    f.write(f'Latent dimension: {latent_dim} \\n')\n",
    "    f.write(f'Filter sizes: {filters} \\n')\n",
    "    f.write(f'img_width: {img_width} \\n')\n",
    "    f.write(f'img_height: {img_height} \\n')\n",
    "    f.write(f'num_channels: {num_channels}')\n",
    "    f.close()\n",
    "\n",
    "# Save encoder weights:\n",
    "encoder.save_weights(f'{SAVE_FOLDER}/ENCODER_WEIGHTS.h5')\n",
    "\n",
    "# Save the total AE model, i.e. its weights:\n",
    "ae.save_weights(f'{SAVE_FOLDER}/AE_WEIGHTS.h5')\n",
    "\n",
    "# Save the history at each epochs (cost of train and validation sets):\n",
    "np.save(f'{SAVE_FOLDER}/HISTORY.npy', history.history)\n",
    "\n",
    "# Save exact training, validation and testing data set (as numpy arrays):\n",
    "np.save(f'{SAVE_FOLDER}/trainX.npy', trainX)\n",
    "np.save(f'{SAVE_FOLDER}/valX.npy', valX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllX.npy', testAllX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllY.npy', testAllY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate training process\n",
    "\n",
    "Now that the training process is complete, let’s evaluate the average loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(13, 6))\n",
    "\n",
    "# summarize history for loss:\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='blue', alpha=0.4, label='validation')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Cost', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.title('Average Loss During Training', fontsize=18)\n",
    "#plt.xticks(range(0,epochs))\n",
    "plt.legend(loc='upper right', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
