{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import axis, colorbar, imshow, show, figure, subplot\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=18)\n",
    "mpl.rc('xtick', labelsize=16)\n",
    "mpl.rc('ytick', labelsize=16)\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## ----- GPU ------------------------------------\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'    # use of GPU 0 (ERDA) (use before importing torch or tensorflow/keeas)\n",
    "## ----------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization, ReLU, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.activations import relu\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K                         #contains calls for tensor manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL_TRAIN_AUG:       /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTrainAugmentations\n",
      "NORMAL_VALIDATION_AUG:  /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalValidationAugmentations\n",
      "NORMAL_TEST_AUG:        /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTestAugmentations\n",
      "ANOMALY_AUG:            /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/AnomalyAugmentations\n"
     ]
    }
   ],
   "source": [
    "from utils_ae_kitkat_paths import *\n",
    "\n",
    "# Get work directions for augmentated data set:\n",
    "print (\"NORMAL_TRAIN_AUG:      \", NORMAL_TRAIN_AUG)\n",
    "print (\"NORMAL_VALIDATION_AUG: \", NORMAL_VAL_AUG)\n",
    "print (\"NORMAL_TEST_AUG:       \", NORMAL_TEST_AUG)\n",
    "print (\"ANOMALY_AUG:           \", ANOMALY_AUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which Folder The Models Are Saved In: saved_models/latent32\n"
     ]
    }
   ],
   "source": [
    "# What latent dimension and filter size are we using?:\n",
    "latent_dim = 32\n",
    "filters    = 32\n",
    "\n",
    "# Get work direction for saving files:\n",
    "SAVE_FOLDER = f'saved_models/latent{latent_dim}'\n",
    "print (\"Which Folder The Models Are Saved In:\", SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] deleting existing files in folder...\n",
      "         done in 0.019 minutes\n"
     ]
    }
   ],
   "source": [
    "# Delete all existing files in folder where models are saved:\n",
    "print(\"[INFO] deleting existing files in folder...\")\n",
    "t0 = time()\n",
    "\n",
    "files = glob.glob(f'{SAVE_FOLDER}/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "print (\"         done in %0.3f minutes\" % ((time() - t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_VAE_AE import get_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Investigation of Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"normal\" training images is:     1400\n",
      "Number of \"normal\" validation images is:   134\n",
      "Number of \"normal\" test images is:         266\n",
      "Number of \"anomaly\" images is:             600\n"
     ]
    }
   ],
   "source": [
    "# Glob the directories and get the lists of good and bad images\n",
    "good_train_fns = [f for f in os.listdir(NORMAL_TRAIN_AUG) if not f.startswith('.')]\n",
    "good_val_fns = [f for f in os.listdir(NORMAL_VAL_AUG) if not f.startswith('.')]\n",
    "good_test_fns = [f for f in os.listdir(NORMAL_TEST_AUG) if not f.startswith('.')]\n",
    "bad_fns = [f for f in os.listdir(ANOMALY_AUG) if not f.startswith('.')]\n",
    "\n",
    "print('Number of \"normal\" training images is:     {}'.format(len(good_train_fns)))\n",
    "print('Number of \"normal\" validation images is:   {}'.format(len(good_val_fns)))\n",
    "print('Number of \"normal\" test images is:         {}'.format(len(good_test_fns)))\n",
    "print('Number of \"anomaly\" images is:             {}'.format(len(bad_fns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Procentage of normal samples (ground truth):   75.00 %\n",
      "> Procentage of anomaly samples (ground truth):  25.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of bad images vs good images:\n",
    "normal_fns = len(good_train_fns) + len(good_val_fns) + len(good_test_fns)\n",
    "anomaly_fns = len(bad_fns)\n",
    "print(f\"> Procentage of normal samples (ground truth):   {(normal_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")\n",
    "print(f\"> Procentage of anomaly samples (ground truth):  {(anomaly_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Normal Data\n",
    "\n",
    "Load normal samples in pre-splitted data sets: train, valdiation and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal training images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal training images...\")\n",
    "trainX = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TRAIN_AUG}/*.png\")]  # read as grayscale\n",
    "trainX = np.array(trainX)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "trainY = get_labels(trainX, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal validation images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal validation images...\")\n",
    "x_normal_val = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_VAL_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_val = np.array(x_normal_val)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_val = get_labels(x_normal_val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal testing images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal testing images...\")\n",
    "x_normal_test = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TEST_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_test = np.array(x_normal_test)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_test = get_labels(x_normal_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Data\n",
    "\n",
    "\n",
    "Load anomaly samples in its singular training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly images...\")\n",
    "x_anomaly = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY_AUG}/*.png\")]  # read as grayscale\n",
    "x_anomaly = np.array(x_anomaly)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_anomaly = get_labels(x_anomaly, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine normal samples and anomaly samples and split them into training, validation and test sets\n",
    "\n",
    "`label 0` == Normal Samples\n",
    "\n",
    "`label 1` == Anomaly Samples\n",
    "\n",
    "Train and Validation sets only consists of `Normal Data`, aka. `label 0`.\n",
    "\n",
    "Testing sets consists of both  `Normal Data` (aka. `label 0`) and `Anomaly Data` (aka. `label 1`)\n",
    "\n",
    "________________\n",
    "\n",
    "Due to the very sparse original (preprossed) KitKat Chunky data, the validation set will be a mix of normal testing and validation data. The normal testing set will consists of all validation and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testvalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testvalY = get_labels(testvalX, 0)\n",
    "\n",
    "# randomly split in order to make new validation set:\n",
    "NoUsageX, valX, NoUsageY, valY = train_test_split(testvalX, testvalY, test_size=0.25, random_state=4345672)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testNormalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testNormalY = get_labels(testNormalX, 0)\n",
    "\n",
    "# anomaly class (only used for testing):\n",
    "testAnomalyX, testAnomalyY = x_anomaly, y_anomaly\n",
    "\n",
    "# Combine all testing data in one array:\n",
    "testAllX = np.vstack((testNormalX, testAnomalyX))\n",
    "testAllY = np.hstack((testNormalY, testAnomalyY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Dimensions and Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      " > images: (1400, 128, 128)\n",
      " > labels: (1400,)\n",
      "\n",
      "Validation set:\n",
      " > images: (100, 128, 128)\n",
      " > labels: (100,)\n",
      "\n",
      "Test set:\n",
      " > images: (1000, 128, 128)\n",
      " > labels: (1000,)\n",
      "\t'Normal' test set:\n",
      "\t  > images: (400, 128, 128)\n",
      "\t  > labels: (400,)\n",
      "\t'Anomaly' test set:\n",
      "\t  > images: (600, 128, 128)\n",
      "\t  > labels: (600,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "print(\" > images:\", trainX.shape)\n",
    "print(\" > labels:\", trainY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Validation set:\")\n",
    "print(\" > images:\", valX.shape)\n",
    "print(\" > labels:\", valY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Test set:\")\n",
    "print(\" > images:\", testAllX.shape)\n",
    "print(\" > labels:\", testAllY.shape)\n",
    "print(\"\\t'Normal' test set:\")\n",
    "print(\"\\t  > images:\", testNormalX.shape)\n",
    "print(\"\\t  > labels:\", testNormalY.shape)\n",
    "print(\"\\t'Anomaly' test set:\")\n",
    "print(\"\\t  > images:\", testAnomalyX.shape)\n",
    "print(\"\\t  > labels:\", testAnomalyY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 73.68 %\n",
      "Procentage of validation samples: 5.26 %\n",
      "Procentage of (only normal) testing samples: 21.05 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets (only normal data):\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (only normal) testing samples: {(len(testNormalX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 56.00 %\n",
      "Procentage of validation samples: 4.00 %\n",
      "Procentage of (total) testing samples: 40.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets:\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (total) testing samples: {(len(testAllX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for (un)balanced data\n",
    "\n",
    "In an ideal world the data should be balanced. However in the real world we expect there being around ~$99 \\%$ good samples and only ~$1 \\%$ bad samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['0: normal', '1: anomaly']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,4))\n",
    "ax[0].hist(trainY, align=\"left\"); ax[0].set_title('train', fontsize=18); ax[0].set_xticks([0,1]); ax[0].set_xticklabels(labels); ax[0].tick_params(axis='both', which='major', labelsize=16); ax[0].set_xlim(-0.5, 1.5);\n",
    "ax[1].hist(valY, align=\"left\"); ax[1].set_title('validation', fontsize=18); ax[1].set_xticks([0,1]); ax[1].set_xticklabels(labels); ax[1].tick_params(axis='both', which='major', labelsize=16); ax[1].set_xlim(-0.5, 1.5);\n",
    "ax[2].hist(testAllY, align=\"left\"); ax[2].set_title('test', fontsize=18); ax[2].set_xticks([0,1]); ax[2].set_xticklabels(labels); ax[2].tick_params(axis='both', which='major', labelsize=16); ax[2].set_xlim(-0.5, 1.5);\n",
    "\n",
    "ax[0].set_ylabel(\"Number of images\", fontsize=18)\n",
    "ax[1].set_xlabel(\"Class Labels\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of normal test samples: 40.00 %\n",
      "Procentage of total anomaly test samples: 60.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of normal test images vs total anomaly test images:\n",
    "print(f\"Procentage of normal test samples: {(len(testNormalX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of total anomaly test samples: {(len(testAnomalyX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Only normalization has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "img_width, img_height = trainX.shape[1], trainX.shape[2]      # input image dimensions\n",
    "num_channels = 1                                              # gray-channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Neural Networks learns faster with normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to be in  the [0, 1] range:\n",
    "trainX = trainX.astype('float32') / 255.\n",
    "valX = valX.astype('float32') / 255.\n",
    "testAllX = testAllX.astype('float32') / 255.\n",
    "\n",
    "# reshape data to keras inputs --> (n_samples, img_rows, img_cols, n_channels):\n",
    "trainX = trainX.reshape(trainX.shape[0], img_height, img_width, num_channels)\n",
    "valX = valX.reshape(valX.shape[0], img_height, img_width, num_channels)\n",
    "testAllX = testAllX.reshape(testAllX.shape[0], img_height, img_width, num_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import architecture of VAE model and its hyperparameters:\n",
    "from J32_AE_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Total AE Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 64, 64, 32)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "latent (Dense)               (None, 32)                16416     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               16896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 32)      4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 128, 128, 32)      128       \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 128, 128, 1)       129       \n",
      "=================================================================\n",
      "Total params: 74,081\n",
      "Trainable params: 72,417\n",
      "Non-trainable params: 1,664\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "batch_size  = 256\n",
    "epochs      = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at 1'st epoch (inital model)\n",
    "\n",
    "https://stackoverflow.com/questions/54323960/save-keras-model-at-specific-epochs\n",
    "\"\"\"\n",
    "\n",
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 1:\n",
    "            self.model.save_weights(f\"{SAVE_FOLDER}/cp-0001.h5\")\n",
    "            \n",
    "# create and use callback:\n",
    "initial_checkpoint = CustomSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at every k'te epochs\n",
    "\"\"\"\n",
    "# Include the epoch in the file name (uses `str.format`):\n",
    "checkpoint_path = \"saved_models/latent32/cp-{epoch:04d}.h5\"\n",
    "\n",
    "# Create a callback that saves the model's weights every k'te epochs:\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path,\n",
    "                             monitor='loss',           # name of the metrics to monitor\n",
    "                             verbose=1, \n",
    "                             #save_best_only=False,     # if False, the best model will not be overridden.\n",
    "                             save_weights_only=True,   # if True, only the weights of the models will be saved.\n",
    "                                                        # if False, the whole models will be saved.\n",
    "                             #mode='auto', \n",
    "                             period=200                  # save after every k'te epochs\n",
    "                            )\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "ae.save_weights(checkpoint_path.format(epoch=0))          # save for zero epoch (inital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: Early stopping\n",
    "https://www.tensorflow.org/guide/keras/custom_callback\n",
    "\"\"\"\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=50):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create and Compile Model\"\"\"\n",
    "# import architecture of VAE model and its hyperparameters. learning rate choosen from graph above.\n",
    "ae = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "6/6 [==============================] - 1s 168ms/step - loss: 0.2870 - val_loss: 0.2154\n",
      "Epoch 2/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.1992 - val_loss: 0.2097\n",
      "Epoch 3/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.1453 - val_loss: 0.2025\n",
      "Epoch 4/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0992 - val_loss: 0.1939\n",
      "Epoch 5/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0701 - val_loss: 0.1843\n",
      "Epoch 6/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0560 - val_loss: 0.1741\n",
      "Epoch 7/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0481 - val_loss: 0.1641\n",
      "Epoch 8/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0430 - val_loss: 0.1546\n",
      "Epoch 9/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0394 - val_loss: 0.1452\n",
      "Epoch 10/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0365 - val_loss: 0.1359\n",
      "Epoch 11/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0337 - val_loss: 0.1269\n",
      "Epoch 12/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0310 - val_loss: 0.1180\n",
      "Epoch 13/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0281 - val_loss: 0.1096\n",
      "Epoch 14/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0252 - val_loss: 0.1012\n",
      "Epoch 15/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0229 - val_loss: 0.0934\n",
      "Epoch 16/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0210 - val_loss: 0.0863\n",
      "Epoch 17/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0186 - val_loss: 0.0799\n",
      "Epoch 18/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0175 - val_loss: 0.0745\n",
      "Epoch 19/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0155 - val_loss: 0.0697\n",
      "Epoch 20/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0142 - val_loss: 0.0657\n",
      "Epoch 21/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0137 - val_loss: 0.0620\n",
      "Epoch 22/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0124 - val_loss: 0.0591\n",
      "Epoch 23/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0113 - val_loss: 0.0569\n",
      "Epoch 24/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0113 - val_loss: 0.0549\n",
      "Epoch 25/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0105 - val_loss: 0.0536\n",
      "Epoch 26/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0101 - val_loss: 0.0523\n",
      "Epoch 27/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0090 - val_loss: 0.0513\n",
      "Epoch 28/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0089 - val_loss: 0.0506\n",
      "Epoch 29/1000\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0083 - val_loss: 0.0501\n",
      "Epoch 30/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0080 - val_loss: 0.0496\n",
      "Epoch 31/1000\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0074 - val_loss: 0.0493\n",
      "Epoch 32/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0073 - val_loss: 0.0492\n",
      "Epoch 33/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0068 - val_loss: 0.0490\n",
      "Epoch 34/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0067 - val_loss: 0.0490\n",
      "Epoch 35/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0066 - val_loss: 0.0490\n",
      "Epoch 36/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0064 - val_loss: 0.0489\n",
      "Epoch 37/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0065 - val_loss: 0.0488\n",
      "Epoch 38/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0059 - val_loss: 0.0488\n",
      "Epoch 39/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0061 - val_loss: 0.0487\n",
      "Epoch 40/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0059 - val_loss: 0.0484\n",
      "Epoch 41/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0056 - val_loss: 0.0484\n",
      "Epoch 42/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0057 - val_loss: 0.0480\n",
      "Epoch 43/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0052 - val_loss: 0.0479\n",
      "Epoch 44/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0053 - val_loss: 0.0473\n",
      "Epoch 45/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0054 - val_loss: 0.0470\n",
      "Epoch 46/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0050 - val_loss: 0.0467\n",
      "Epoch 47/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0048 - val_loss: 0.0458\n",
      "Epoch 48/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0050 - val_loss: 0.0450\n",
      "Epoch 49/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0047 - val_loss: 0.0442\n",
      "Epoch 50/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0047 - val_loss: 0.0436\n",
      "Epoch 51/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0046 - val_loss: 0.0428\n",
      "Epoch 52/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0044 - val_loss: 0.0416\n",
      "Epoch 53/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0049 - val_loss: 0.0411\n",
      "Epoch 54/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0052 - val_loss: 0.0418\n",
      "Epoch 55/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0041 - val_loss: 0.0398\n",
      "Epoch 56/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0044 - val_loss: 0.0392\n",
      "Epoch 57/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0042 - val_loss: 0.0385\n",
      "Epoch 58/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0041 - val_loss: 0.0371\n",
      "Epoch 59/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0042 - val_loss: 0.0369\n",
      "Epoch 60/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0038 - val_loss: 0.0355\n",
      "Epoch 61/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0041 - val_loss: 0.0370\n",
      "Epoch 62/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0043 - val_loss: 0.0349\n",
      "Epoch 63/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0037 - val_loss: 0.0342\n",
      "Epoch 64/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0043 - val_loss: 0.0364\n",
      "Epoch 65/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0040 - val_loss: 0.0294\n",
      "Epoch 66/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0040 - val_loss: 0.0344\n",
      "Epoch 67/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0042 - val_loss: 0.0382\n",
      "Epoch 68/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0036 - val_loss: 0.0437\n",
      "Epoch 69/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0037 - val_loss: 0.0416\n",
      "Epoch 70/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0034 - val_loss: 0.0452\n",
      "Epoch 71/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0034 - val_loss: 0.0451\n",
      "Epoch 72/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0034 - val_loss: 0.0455\n",
      "Epoch 73/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0037 - val_loss: 0.0455\n",
      "Epoch 74/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0034 - val_loss: 0.0460\n",
      "Epoch 75/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0031 - val_loss: 0.0437\n",
      "Epoch 76/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0033 - val_loss: 0.0449\n",
      "Epoch 77/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0034 - val_loss: 0.0408\n",
      "Epoch 78/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0032 - val_loss: 0.0407\n",
      "Epoch 79/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0032 - val_loss: 0.0426\n",
      "Epoch 80/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0031 - val_loss: 0.0418\n",
      "Epoch 81/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0031 - val_loss: 0.0410\n",
      "Epoch 82/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0029 - val_loss: 0.0400\n",
      "Epoch 83/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0031 - val_loss: 0.0355\n",
      "Epoch 84/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0028 - val_loss: 0.0381\n",
      "Epoch 85/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0030 - val_loss: 0.0338\n",
      "Epoch 86/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0030 - val_loss: 0.0308\n",
      "Epoch 87/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0029 - val_loss: 0.0326\n",
      "Epoch 88/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0027 - val_loss: 0.0191\n",
      "Epoch 89/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0028 - val_loss: 0.0131\n",
      "Epoch 90/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0028 - val_loss: 0.0109\n",
      "Epoch 91/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0027 - val_loss: 0.0124\n",
      "Epoch 92/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0119\n",
      "Epoch 93/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0025 - val_loss: 0.0106\n",
      "Epoch 94/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0026 - val_loss: 0.0110\n",
      "Epoch 95/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0030 - val_loss: 0.0083\n",
      "Epoch 96/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0026 - val_loss: 0.0104\n",
      "Epoch 97/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0089\n",
      "Epoch 98/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0025 - val_loss: 0.0103\n",
      "Epoch 99/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0026 - val_loss: 0.0083\n",
      "Epoch 100/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0025 - val_loss: 0.0082\n",
      "Epoch 101/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0024 - val_loss: 0.0081\n",
      "Epoch 102/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0023 - val_loss: 0.0080\n",
      "Epoch 103/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0079\n",
      "Epoch 104/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0068\n",
      "Epoch 105/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0074\n",
      "Epoch 106/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0023 - val_loss: 0.0071\n",
      "Epoch 107/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0024 - val_loss: 0.0074\n",
      "Epoch 108/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0024 - val_loss: 0.0070\n",
      "Epoch 109/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0022 - val_loss: 0.0072\n",
      "Epoch 110/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0030 - val_loss: 0.0067\n",
      "Epoch 111/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0075\n",
      "Epoch 112/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0023 - val_loss: 0.0072\n",
      "Epoch 113/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0022 - val_loss: 0.0061\n",
      "Epoch 114/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0069\n",
      "Epoch 115/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0022 - val_loss: 0.0064\n",
      "Epoch 116/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0021 - val_loss: 0.0063\n",
      "Epoch 117/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0071\n",
      "Epoch 118/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0023 - val_loss: 0.0063\n",
      "Epoch 119/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0066\n",
      "Epoch 120/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0022 - val_loss: 0.0067\n",
      "Epoch 121/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0024 - val_loss: 0.0064\n",
      "Epoch 122/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0022 - val_loss: 0.0064\n",
      "Epoch 123/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0022 - val_loss: 0.0056\n",
      "Epoch 124/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0050\n",
      "Epoch 125/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0053\n",
      "Epoch 126/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0020 - val_loss: 0.0052\n",
      "Epoch 127/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0051\n",
      "Epoch 128/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0052\n",
      "Epoch 129/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0024 - val_loss: 0.0057\n",
      "Epoch 130/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0054\n",
      "Epoch 131/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0058\n",
      "Epoch 132/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0060\n",
      "Epoch 133/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0055\n",
      "Epoch 134/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0020 - val_loss: 0.0053\n",
      "Epoch 135/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0052\n",
      "Epoch 136/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0021 - val_loss: 0.0056\n",
      "Epoch 137/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0054\n",
      "Epoch 138/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0023 - val_loss: 0.0054\n",
      "Epoch 139/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0022 - val_loss: 0.0055\n",
      "Epoch 140/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0021 - val_loss: 0.0047\n",
      "Epoch 141/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0019 - val_loss: 0.0053\n",
      "Epoch 142/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0019 - val_loss: 0.0053\n",
      "Epoch 143/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0053\n",
      "Epoch 144/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0046\n",
      "Epoch 145/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0048\n",
      "Epoch 146/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0052\n",
      "Epoch 147/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 148/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0057\n",
      "Epoch 149/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0058\n",
      "Epoch 150/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0053\n",
      "Epoch 151/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0057\n",
      "Epoch 152/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 153/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0019 - val_loss: 0.0055\n",
      "Epoch 154/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0050\n",
      "Epoch 155/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0048\n",
      "Epoch 156/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0046\n",
      "Epoch 157/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0042\n",
      "Epoch 158/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0024 - val_loss: 0.0042\n",
      "Epoch 159/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0021 - val_loss: 0.0085\n",
      "Epoch 160/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0110\n",
      "Epoch 161/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0031 - val_loss: 0.0038\n",
      "Epoch 162/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0027 - val_loss: 0.0049\n",
      "Epoch 163/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0026 - val_loss: 0.0059\n",
      "Epoch 164/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0058\n",
      "Epoch 165/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0022 - val_loss: 0.0051\n",
      "Epoch 166/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0045\n",
      "Epoch 167/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0039\n",
      "Epoch 168/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0033\n",
      "Epoch 169/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0019 - val_loss: 0.0031\n",
      "Epoch 170/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0019 - val_loss: 0.0025\n",
      "Epoch 171/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0018 - val_loss: 0.0025\n",
      "Epoch 172/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 173/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 174/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0023\n",
      "Epoch 175/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 176/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0023\n",
      "Epoch 177/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 178/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0023\n",
      "Epoch 179/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 180/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 181/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 182/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 183/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 184/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 185/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 186/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 187/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 188/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 189/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 190/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 191/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 192/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 193/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 194/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 195/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 196/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 197/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 198/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 199/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 200/1000\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00200: saving model to saved_models/latent32/cp-0200.h5\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 201/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 202/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 203/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 204/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 205/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 206/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 207/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 208/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 209/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 210/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 211/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0015 - val_loss: 0.0025\n",
      "Epoch 212/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0017 - val_loss: 0.0026\n",
      "Epoch 213/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 214/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0026\n",
      "Epoch 215/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0015 - val_loss: 0.0028\n",
      "Epoch 216/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0015 - val_loss: 0.0026\n",
      "Epoch 217/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0016 - val_loss: 0.0024\n",
      "Epoch 218/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0026\n",
      "Epoch 219/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0028\n",
      "Epoch 220/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0031\n",
      "Epoch 221/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0029\n",
      "Epoch 222/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0029\n",
      "Epoch 223/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0015 - val_loss: 0.0031\n",
      "Epoch 224/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0032\n",
      "Epoch 225/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0028\n",
      "Epoch 226/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0016 - val_loss: 0.0028\n",
      "Epoch 227/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0028\n",
      "Epoch 228/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0014 - val_loss: 0.0022\n",
      "Epoch 229/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 230/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0027\n",
      "Epoch 231/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0032\n",
      "Epoch 232/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0033\n",
      "Epoch 233/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0015 - val_loss: 0.0034\n",
      "Epoch 234/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0013 - val_loss: 0.0035\n",
      "Epoch 235/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0029\n",
      "Epoch 236/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0030\n",
      "Epoch 237/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0025\n",
      "Epoch 238/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0026\n",
      "Epoch 239/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0030\n",
      "Epoch 240/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0030\n",
      "Epoch 241/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0017 - val_loss: 0.0028\n",
      "Epoch 242/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0015 - val_loss: 0.0030\n",
      "Epoch 243/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0028\n",
      "Epoch 244/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0028\n",
      "Epoch 245/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0027\n",
      "Epoch 246/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 247/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 248/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0013 - val_loss: 0.0024\n",
      "Epoch 249/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 250/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 251/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 252/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 253/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 254/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 255/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 256/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 257/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 258/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 259/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0026\n",
      "Epoch 260/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0025\n",
      "Epoch 261/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0022\n",
      "Epoch 262/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 263/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 264/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 265/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 266/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 267/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 268/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 269/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 270/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 271/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 272/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 273/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 274/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 275/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 276/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 277/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 278/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 279/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 280/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 281/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 282/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 283/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 284/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 285/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 286/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 287/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 288/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 289/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 290/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 291/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 292/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 293/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 294/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 295/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 296/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 297/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 298/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 299/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 300/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 301/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0019\n",
      "Epoch 302/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 303/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 304/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 305/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 306/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 307/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 308/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 309/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 310/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 311/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 312/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 313/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 314/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 315/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 316/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0021\n",
      "Epoch 317/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0011 - val_loss: 0.0021\n",
      "Epoch 318/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0011 - val_loss: 0.0022\n",
      "Epoch 319/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0019\n",
      "Epoch 320/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 321/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0024\n",
      "Epoch 322/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 323/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0021\n",
      "Epoch 324/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0023\n",
      "Epoch 325/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0023\n",
      "Epoch 326/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0012 - val_loss: 0.0024\n",
      "Epoch 327/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0022\n",
      "Epoch 328/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0022\n",
      "Epoch 329/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0011 - val_loss: 0.0020\n",
      "Epoch 330/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0018\n",
      "Epoch 331/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 332/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 333/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0012 - val_loss: 0.0019\n",
      "Epoch 334/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 335/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 336/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 337/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 338/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 339/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 340/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 341/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 342/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 343/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 344/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 345/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 346/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 347/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 348/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0019\n",
      "Epoch 349/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 350/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 351/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 352/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 353/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 354/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 355/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 356/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 357/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 358/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 359/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 360/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 361/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 362/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 363/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 364/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 365/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 366/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 367/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 368/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 369/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 370/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 371/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 372/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 373/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 374/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 375/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 376/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 377/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 378/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 379/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 380/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 381/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 382/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 383/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 384/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 385/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 386/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 387/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 388/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 389/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 390/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 391/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 392/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 393/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0010 - val_loss: 0.0016\n",
      "Epoch 394/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 395/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 396/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 397/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 398/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 399/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 400/1000\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.0012\n",
      "Epoch 00400: saving model to saved_models/latent32/cp-0400.h5\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 401/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 402/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 403/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 404/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 405/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 406/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 407/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 408/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 409/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 410/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 411/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 412/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0010 - val_loss: 0.0016\n",
      "Epoch 413/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 414/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 415/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0021\n",
      "Epoch 416/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0010 - val_loss: 0.0018\n",
      "Epoch 417/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0020\n",
      "Epoch 418/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0018\n",
      "Epoch 419/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 420/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 421/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 422/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 9.8460e-04 - val_loss: 0.0013\n",
      "Epoch 423/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 424/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.8949e-04 - val_loss: 0.0013\n",
      "Epoch 425/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 426/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 427/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 428/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 429/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 430/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 431/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 432/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 433/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0018\n",
      "Epoch 434/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 435/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0017\n",
      "Epoch 436/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 437/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0016\n",
      "Epoch 438/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 439/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.9274e-04 - val_loss: 0.0018\n",
      "Epoch 440/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0016\n",
      "Epoch 441/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0018\n",
      "Epoch 442/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0021\n",
      "Epoch 443/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.9538e-04 - val_loss: 0.0023\n",
      "Epoch 444/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 9.6357e-04 - val_loss: 0.0021\n",
      "Epoch 445/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0010 - val_loss: 0.0017\n",
      "Epoch 446/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0018\n",
      "Epoch 447/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0015\n",
      "Epoch 448/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 449/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 450/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 451/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 452/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 453/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 454/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 455/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 456/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.9098e-04 - val_loss: 0.0013\n",
      "Epoch 457/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0015\n",
      "Epoch 458/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.8402e-04 - val_loss: 0.0014\n",
      "Epoch 459/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 460/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 461/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 462/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 463/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 464/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 465/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 9.5320e-04 - val_loss: 0.0013\n",
      "Epoch 466/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 467/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 468/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 469/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 470/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.8193e-04 - val_loss: 0.0014\n",
      "Epoch 471/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 472/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 473/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 9.5296e-04 - val_loss: 0.0015\n",
      "Epoch 474/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 475/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 476/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 477/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.6495e-04 - val_loss: 0.0013\n",
      "Epoch 478/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 479/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 480/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 481/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.7301e-04 - val_loss: 0.0014\n",
      "Epoch 482/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 483/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 484/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 485/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 486/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.7325e-04 - val_loss: 0.0012\n",
      "Epoch 487/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 488/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.9171e-04 - val_loss: 0.0014\n",
      "Epoch 489/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 9.2870e-04 - val_loss: 0.0012\n",
      "Epoch 490/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.5504e-04 - val_loss: 0.0013\n",
      "Epoch 491/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.4146e-04 - val_loss: 0.0012\n",
      "Epoch 492/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.5249e-04 - val_loss: 0.0013\n",
      "Epoch 493/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 494/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.7634e-04 - val_loss: 0.0014\n",
      "Epoch 495/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 496/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 497/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 9.8016e-04 - val_loss: 0.0013\n",
      "Epoch 498/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 9.1947e-04 - val_loss: 0.0014\n",
      "Epoch 499/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.8293e-04 - val_loss: 0.0014\n",
      "Epoch 500/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.2583e-04 - val_loss: 0.0012\n",
      "Epoch 501/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.6192e-04 - val_loss: 0.0014\n",
      "Epoch 502/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.3277e-04 - val_loss: 0.0013\n",
      "Epoch 503/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 9.7473e-04 - val_loss: 0.0013\n",
      "Epoch 504/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 505/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.6097e-04 - val_loss: 0.0013\n",
      "Epoch 506/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 8.9681e-04 - val_loss: 0.0015\n",
      "Epoch 507/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.4656e-04 - val_loss: 0.0016\n",
      "Epoch 508/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.3559e-04 - val_loss: 0.0016\n",
      "Epoch 509/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.8418e-04 - val_loss: 0.0013\n",
      "Epoch 510/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 9.3393e-04 - val_loss: 0.0014\n",
      "Epoch 511/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.7621e-04 - val_loss: 0.0013\n",
      "Epoch 512/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 8.7837e-04 - val_loss: 0.0013\n",
      "Epoch 513/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.5108e-04 - val_loss: 0.0012\n",
      "Epoch 514/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 515/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.9578e-04 - val_loss: 0.0014\n",
      "Epoch 516/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.4905e-04 - val_loss: 0.0013\n",
      "Epoch 517/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.3255e-04 - val_loss: 0.0014\n",
      "Epoch 518/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.9753e-04 - val_loss: 0.0013\n",
      "Epoch 519/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.8403e-04 - val_loss: 0.0012\n",
      "Epoch 520/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.5563e-04 - val_loss: 0.0013\n",
      "Epoch 521/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.6995e-04 - val_loss: 0.0012\n",
      "Epoch 522/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8244e-04 - val_loss: 0.0012\n",
      "Epoch 523/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8089e-04 - val_loss: 0.0012\n",
      "Epoch 524/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 8.5886e-04 - val_loss: 0.0013\n",
      "Epoch 525/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.1209e-04 - val_loss: 0.0012\n",
      "Epoch 526/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.5958e-04 - val_loss: 0.0012\n",
      "Epoch 527/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.0489e-04 - val_loss: 0.0012\n",
      "Epoch 528/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.6468e-04 - val_loss: 0.0013\n",
      "Epoch 529/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 530/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.0699e-04 - val_loss: 0.0012\n",
      "Epoch 531/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 9.8610e-04 - val_loss: 0.0013\n",
      "Epoch 532/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 533/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.9198e-04 - val_loss: 0.0013\n",
      "Epoch 534/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.3370e-04 - val_loss: 0.0014\n",
      "Epoch 535/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.9956e-04 - val_loss: 0.0012\n",
      "Epoch 536/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 537/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.6612e-04 - val_loss: 0.0013\n",
      "Epoch 538/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.7232e-04 - val_loss: 0.0011\n",
      "Epoch 539/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 8.5303e-04 - val_loss: 0.0012\n",
      "Epoch 540/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.9789e-04 - val_loss: 0.0011\n",
      "Epoch 541/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.1851e-04 - val_loss: 0.0011\n",
      "Epoch 542/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.9525e-04 - val_loss: 0.0012\n",
      "Epoch 543/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.5985e-04 - val_loss: 0.0012\n",
      "Epoch 544/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.8344e-04 - val_loss: 0.0011\n",
      "Epoch 545/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.1300e-04 - val_loss: 0.0011\n",
      "Epoch 546/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.2148e-04 - val_loss: 0.0012\n",
      "Epoch 547/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8348e-04 - val_loss: 0.0013\n",
      "Epoch 548/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.0145e-04 - val_loss: 0.0012\n",
      "Epoch 549/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8416e-04 - val_loss: 0.0012\n",
      "Epoch 550/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 551/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.7174e-04 - val_loss: 0.0015\n",
      "Epoch 552/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.3498e-04 - val_loss: 0.0014\n",
      "Epoch 553/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 554/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.6536e-04 - val_loss: 0.0012\n",
      "Epoch 555/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.1809e-04 - val_loss: 0.0012\n",
      "Epoch 556/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.9722e-04 - val_loss: 0.0013\n",
      "Epoch 557/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.7633e-04 - val_loss: 0.0012\n",
      "Epoch 558/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.1470e-04 - val_loss: 0.0012\n",
      "Epoch 559/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.9380e-04 - val_loss: 0.0011\n",
      "Epoch 560/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.4815e-04 - val_loss: 0.0012\n",
      "Epoch 561/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 562/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.1492e-04 - val_loss: 0.0013\n",
      "Epoch 563/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.2448e-04 - val_loss: 0.0013\n",
      "Epoch 564/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.3527e-04 - val_loss: 0.0012\n",
      "Epoch 565/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.6896e-04 - val_loss: 0.0013\n",
      "Epoch 566/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.9756e-04 - val_loss: 0.0012\n",
      "Epoch 567/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 8.4714e-04 - val_loss: 0.0012\n",
      "Epoch 568/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.9378e-04 - val_loss: 0.0013\n",
      "Epoch 569/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.9253e-04 - val_loss: 0.0012\n",
      "Epoch 570/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.7249e-04 - val_loss: 0.0011\n",
      "Epoch 571/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.7811e-04 - val_loss: 0.0012\n",
      "Epoch 572/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.2815e-04 - val_loss: 0.0012\n",
      "Epoch 573/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.7768e-04 - val_loss: 0.0011\n",
      "Epoch 574/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.6675e-04 - val_loss: 0.0012\n",
      "Epoch 575/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.3339e-04 - val_loss: 0.0012\n",
      "Epoch 576/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.4771e-04 - val_loss: 0.0011\n",
      "Epoch 577/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.7353e-04 - val_loss: 0.0012\n",
      "Epoch 578/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 8.2437e-04 - val_loss: 0.0012\n",
      "Epoch 579/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.5231e-04 - val_loss: 0.0011\n",
      "Epoch 580/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.6696e-04 - val_loss: 0.0013\n",
      "Epoch 581/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.3250e-04 - val_loss: 0.0012\n",
      "Epoch 582/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.2995e-04 - val_loss: 0.0012\n",
      "Epoch 583/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8881e-04 - val_loss: 0.0011\n",
      "Epoch 584/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.7634e-04 - val_loss: 0.0012\n",
      "Epoch 585/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.6323e-04 - val_loss: 0.0012\n",
      "Epoch 586/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.4895e-04 - val_loss: 0.0012\n",
      "Epoch 587/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.5310e-04 - val_loss: 0.0012\n",
      "Epoch 588/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8803e-04 - val_loss: 0.0013\n",
      "Epoch 589/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.7554e-04 - val_loss: 0.0013\n",
      "Epoch 590/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 9.7125e-04 - val_loss: 0.0012\n",
      "Epoch 591/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.9384e-04 - val_loss: 0.0012\n",
      "Epoch 592/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 593/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.2796e-04 - val_loss: 0.0012\n",
      "Epoch 594/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 9.0074e-04 - val_loss: 0.0011\n",
      "Epoch 595/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.1612e-04 - val_loss: 0.0012\n",
      "Epoch 596/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.7644e-04 - val_loss: 0.0011\n",
      "Epoch 597/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.2498e-04 - val_loss: 0.0014\n",
      "Epoch 598/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.8669e-04 - val_loss: 0.0014\n",
      "Epoch 599/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.6514e-04 - val_loss: 0.0014\n",
      "Epoch 600/1000\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 8.5183e-04\n",
      "Epoch 00600: saving model to saved_models/latent32/cp-0600.h5\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 8.9809e-04 - val_loss: 0.0013\n",
      "Epoch 601/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.2778e-04 - val_loss: 0.0013\n",
      "Epoch 602/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.3499e-04 - val_loss: 0.0014\n",
      "Epoch 603/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.4832e-04 - val_loss: 0.0011\n",
      "Epoch 604/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 9.1155e-04 - val_loss: 0.0012\n",
      "Epoch 605/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 606/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.7254e-04 - val_loss: 0.0014\n",
      "Epoch 607/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.1670e-04 - val_loss: 0.0013\n",
      "Epoch 608/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.3347e-04 - val_loss: 0.0015\n",
      "Epoch 609/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.7096e-04 - val_loss: 0.0013\n",
      "Epoch 610/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.8451e-04 - val_loss: 0.0011\n",
      "Epoch 611/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.6145e-04 - val_loss: 0.0014\n",
      "Epoch 612/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.8913e-04 - val_loss: 0.0015\n",
      "Epoch 613/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.5598e-04 - val_loss: 0.0013\n",
      "Epoch 614/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.3169e-04 - val_loss: 0.0013\n",
      "Epoch 615/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.6301e-04 - val_loss: 0.0012\n",
      "Epoch 616/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.7007e-04 - val_loss: 0.0012\n",
      "Epoch 617/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 618/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 8.0827e-04 - val_loss: 0.0014\n",
      "Epoch 619/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.5531e-04 - val_loss: 0.0013\n",
      "Epoch 620/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.6236e-04 - val_loss: 0.0014\n",
      "Epoch 621/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.7366e-04 - val_loss: 0.0011\n",
      "Epoch 622/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.0224e-04 - val_loss: 0.0012\n",
      "Epoch 623/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.4339e-04 - val_loss: 0.0011\n",
      "Epoch 624/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 7.7712e-04 - val_loss: 0.0013\n",
      "Epoch 625/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.4258e-04 - val_loss: 0.0013\n",
      "Epoch 626/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.9199e-04 - val_loss: 0.0012\n",
      "Epoch 627/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.1577e-04 - val_loss: 0.0013\n",
      "Epoch 628/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.2638e-04 - val_loss: 0.0012\n",
      "Epoch 629/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.7051e-04 - val_loss: 0.0012\n",
      "Epoch 630/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.3982e-04 - val_loss: 0.0015\n",
      "Epoch 631/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.5663e-04 - val_loss: 0.0016\n",
      "Epoch 632/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.1120e-04 - val_loss: 0.0012\n",
      "Epoch 633/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.3975e-04 - val_loss: 0.0012\n",
      "Epoch 634/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.2680e-04 - val_loss: 0.0012\n",
      "Epoch 635/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.0913e-04 - val_loss: 0.0012\n",
      "Epoch 636/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.5434e-04 - val_loss: 0.0012\n",
      "Epoch 637/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.9662e-04 - val_loss: 0.0011\n",
      "Epoch 638/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.0389e-04 - val_loss: 0.0012\n",
      "Epoch 639/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8263e-04 - val_loss: 0.0011\n",
      "Epoch 640/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.6908e-04 - val_loss: 0.0013\n",
      "Epoch 641/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.7644e-04 - val_loss: 0.0011\n",
      "Epoch 642/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.3073e-04 - val_loss: 0.0011\n",
      "Epoch 643/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.0478e-04 - val_loss: 0.0011\n",
      "Epoch 644/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.9557e-04 - val_loss: 0.0013\n",
      "Epoch 645/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.4814e-04 - val_loss: 0.0010\n",
      "Epoch 646/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.3513e-04 - val_loss: 0.0011\n",
      "Epoch 647/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.9068e-04 - val_loss: 0.0011\n",
      "Epoch 648/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8120e-04 - val_loss: 0.0013\n",
      "Epoch 649/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.3986e-04 - val_loss: 0.0012\n",
      "Epoch 650/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.9855e-04 - val_loss: 0.0011\n",
      "Epoch 651/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.0448e-04 - val_loss: 0.0011\n",
      "Epoch 652/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.7094e-04 - val_loss: 0.0012\n",
      "Epoch 653/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.0323e-04 - val_loss: 0.0011\n",
      "Epoch 654/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.8402e-04 - val_loss: 0.0011\n",
      "Epoch 655/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.9368e-04 - val_loss: 0.0012\n",
      "Epoch 656/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.9107e-04 - val_loss: 0.0011\n",
      "Epoch 657/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.5383e-04 - val_loss: 0.0012\n",
      "Epoch 658/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.6631e-04 - val_loss: 0.0011\n",
      "Epoch 659/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.1635e-04 - val_loss: 0.0012\n",
      "Epoch 660/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8746e-04 - val_loss: 0.0011\n",
      "Epoch 661/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.3931e-04 - val_loss: 0.0011\n",
      "Epoch 662/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.5385e-04 - val_loss: 0.0012\n",
      "Epoch 663/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.2159e-04 - val_loss: 0.0012\n",
      "Epoch 664/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.4657e-04 - val_loss: 0.0011\n",
      "Epoch 665/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.1824e-04 - val_loss: 0.0011\n",
      "Epoch 666/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.3347e-04 - val_loss: 0.0011\n",
      "Epoch 667/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.4743e-04 - val_loss: 0.0011\n",
      "Epoch 668/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.7812e-04 - val_loss: 0.0011\n",
      "Epoch 669/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.1527e-04 - val_loss: 0.0011\n",
      "Epoch 670/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.0442e-04 - val_loss: 0.0011\n",
      "Epoch 671/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.9341e-04 - val_loss: 0.0012\n",
      "Epoch 672/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 7.7350e-04 - val_loss: 0.0012\n",
      "Epoch 673/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.9057e-04 - val_loss: 0.0013\n",
      "Epoch 674/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.0516e-04 - val_loss: 0.0011\n",
      "Epoch 675/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.4283e-04 - val_loss: 0.0013\n",
      "Epoch 676/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.1073e-04 - val_loss: 0.0012\n",
      "Epoch 677/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8948e-04 - val_loss: 0.0012\n",
      "Epoch 678/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.8768e-04 - val_loss: 0.0013\n",
      "Epoch 679/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.2860e-04 - val_loss: 0.0012\n",
      "Epoch 680/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.9512e-04 - val_loss: 0.0012\n",
      "Epoch 681/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.1581e-04 - val_loss: 0.0011\n",
      "Epoch 682/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.7369e-04 - val_loss: 0.0014\n",
      "Epoch 683/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 7.3888e-04 - val_loss: 0.0011\n",
      "Epoch 684/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 7.3733e-04 - val_loss: 0.0014\n",
      "Epoch 685/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 7.5618e-04 - val_loss: 0.0013\n",
      "Epoch 686/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.0180e-04 - val_loss: 0.0012\n",
      "Epoch 687/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.8032e-04 - val_loss: 0.0012\n",
      "Epoch 688/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.3948e-04 - val_loss: 0.0012\n",
      "Epoch 689/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.3726e-04 - val_loss: 0.0012\n",
      "Epoch 690/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.0633e-04 - val_loss: 0.0012\n",
      "Epoch 691/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.4251e-04 - val_loss: 0.0012\n",
      "Epoch 692/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.7530e-04 - val_loss: 0.0012\n",
      "Epoch 693/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.2288e-04 - val_loss: 0.0012\n",
      "Epoch 694/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.6458e-04 - val_loss: 0.0011\n",
      "Epoch 695/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.6186e-04 - val_loss: 0.0015\n",
      "Epoch 696/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.4378e-04 - val_loss: 0.0015\n",
      "Epoch 697/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.6170e-04 - val_loss: 0.0017\n",
      "Epoch 698/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.3096e-04 - val_loss: 0.0014\n",
      "Epoch 699/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.6149e-04 - val_loss: 0.0017\n",
      "Epoch 700/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.0741e-04 - val_loss: 0.0014\n",
      "Epoch 701/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.1203e-04 - val_loss: 0.0016\n",
      "Epoch 702/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.7277e-04 - val_loss: 0.0012\n",
      "Epoch 703/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.5729e-04 - val_loss: 0.0012\n",
      "Epoch 704/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.2257e-04 - val_loss: 0.0011\n",
      "Epoch 705/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.3350e-04 - val_loss: 0.0012\n",
      "Epoch 706/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.0897e-04 - val_loss: 0.0011\n",
      "Epoch 707/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.1679e-04 - val_loss: 0.0011\n",
      "Epoch 708/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.2291e-04 - val_loss: 0.0011\n",
      "Epoch 709/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.4898e-04 - val_loss: 0.0012\n",
      "Epoch 710/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.4990e-04 - val_loss: 0.0011\n",
      "Epoch 711/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.1997e-04 - val_loss: 0.0013\n",
      "Epoch 712/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.1448e-04 - val_loss: 0.0011\n",
      "Epoch 713/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.8073e-04 - val_loss: 0.0013\n",
      "Epoch 714/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.4537e-04 - val_loss: 0.0010\n",
      "Epoch 715/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8452e-04 - val_loss: 0.0013\n",
      "Epoch 716/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.9471e-04 - val_loss: 0.0015\n",
      "Epoch 717/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.3282e-04 - val_loss: 0.0016\n",
      "Epoch 718/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.8584e-04 - val_loss: 0.0015\n",
      "Epoch 719/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.9844e-04 - val_loss: 0.0013\n",
      "Epoch 720/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.3885e-04 - val_loss: 0.0014\n",
      "Epoch 721/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.6954e-04 - val_loss: 0.0015\n",
      "Epoch 722/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.1105e-04 - val_loss: 0.0015\n",
      "Epoch 723/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.1079e-04 - val_loss: 0.0013\n",
      "Epoch 724/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.9833e-04 - val_loss: 0.0015\n",
      "Epoch 725/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.0569e-04 - val_loss: 0.0012\n",
      "Epoch 726/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.7589e-04 - val_loss: 0.0016\n",
      "Epoch 727/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.9732e-04 - val_loss: 0.0014\n",
      "Epoch 728/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.1344e-04 - val_loss: 0.0013\n",
      "Epoch 729/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 9.7903e-04 - val_loss: 0.0013\n",
      "Epoch 730/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.6826e-04 - val_loss: 0.0013\n",
      "Epoch 731/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.0868e-04 - val_loss: 0.0014\n",
      "Epoch 732/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.3050e-04 - val_loss: 0.0014\n",
      "Epoch 733/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.9560e-04 - val_loss: 0.0013\n",
      "Epoch 734/1000\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 7.3686e-04Restoring model weights from the end of the best epoch.\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 7.7195e-04 - val_loss: 0.0015\n",
      "Epoch 00734: early stopping\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train VAE model with callbacks \"\"\"\n",
    "history = ae.fit(trainX, trainX, \n",
    "                  batch_size = batch_size, \n",
    "                  epochs = epochs, \n",
    "                  callbacks=[initial_checkpoint, checkpoint, EarlyStoppingAtMinLoss()],\n",
    "                  #callbacks=[initial_checkpoint, checkpoint],\n",
    "                  shuffle=True,\n",
    "                  validation_data=(valX, valX)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 6.0\n"
     ]
    }
   ],
   "source": [
    "# In this GPU implementation, it shows the number of batches/iterations for one epoch (and not the training samples), which is:\n",
    "print (\"Step size:\", np.ceil(trainX.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save the entire model \"\"\"\n",
    "# Save notes about hyperparameters used:\n",
    "with open(f'{SAVE_FOLDER}/notes.txt', 'w') as f:\n",
    "    f.write(f'Epochs: {epochs} \\n')\n",
    "    f.write(f'Batch size: {batch_size} \\n')\n",
    "    f.write(f'Latent dimension: {latent_dim} \\n')\n",
    "    f.write(f'Filter sizes: {filters} \\n')\n",
    "    f.write(f'img_width: {img_width} \\n')\n",
    "    f.write(f'img_height: {img_height} \\n')\n",
    "    f.write(f'num_channels: {num_channels}')\n",
    "    f.close()\n",
    "\n",
    "# Save encoder weights:\n",
    "encoder.save_weights(f'{SAVE_FOLDER}/ENCODER_WEIGHTS.h5')\n",
    "\n",
    "# Save the total AE model, i.e. its weights:\n",
    "ae.save_weights(f'{SAVE_FOLDER}/AE_WEIGHTS.h5')\n",
    "\n",
    "# Save the history at each epochs (cost of train and validation sets):\n",
    "np.save(f'{SAVE_FOLDER}/HISTORY.npy', history.history)\n",
    "\n",
    "# Save exact training, validation and testing data set (as numpy arrays):\n",
    "np.save(f'{SAVE_FOLDER}/trainX.npy', trainX)\n",
    "np.save(f'{SAVE_FOLDER}/valX.npy', valX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllX.npy', testAllX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllY.npy', testAllY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate training process\n",
    "\n",
    "Now that the training process is complete, let’s evaluate the average loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(13, 6))\n",
    "\n",
    "# summarize history for loss:\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='blue', alpha=0.4, label='validation')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Cost', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.title('Average Loss During Training', fontsize=18)\n",
    "#plt.xticks(range(0,epochs))\n",
    "plt.legend(loc='upper right', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
