{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import axis, colorbar, imshow, show, figure, subplot\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=18)\n",
    "mpl.rc('xtick', labelsize=16)\n",
    "mpl.rc('ytick', labelsize=16)\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## ----- GPU ------------------------------------\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'    # use of GPU 0 (ERDA) (use before importing torch or tensorflow/keeas)\n",
    "## ----------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization, ReLU, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.activations import relu\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K                         #contains calls for tensor manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL_TRAIN_AUG:       /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTrainAugmentations\n",
      "NORMAL_VALIDATION_AUG:  /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalValidationAugmentations\n",
      "NORMAL_TEST_AUG:        /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTestAugmentations\n",
      "ANOMALY_AUG:            /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/AnomalyAugmentations\n"
     ]
    }
   ],
   "source": [
    "from utils_ae_kitkat_paths import *\n",
    "\n",
    "# Get work directions for augmentated data set:\n",
    "print (\"NORMAL_TRAIN_AUG:      \", NORMAL_TRAIN_AUG)\n",
    "print (\"NORMAL_VALIDATION_AUG: \", NORMAL_VAL_AUG)\n",
    "print (\"NORMAL_TEST_AUG:       \", NORMAL_TEST_AUG)\n",
    "print (\"ANOMALY_AUG:           \", ANOMALY_AUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which Folder The Models Are Saved In: saved_models/latent4\n"
     ]
    }
   ],
   "source": [
    "# What latent dimension and filter size are we using?:\n",
    "latent_dim = 4\n",
    "filters    = 32\n",
    "\n",
    "# Get work direction for saving files:\n",
    "SAVE_FOLDER = f'saved_models/latent{latent_dim}'\n",
    "print (\"Which Folder The Models Are Saved In:\", SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] deleting existing files in folder...\n",
      "         done in 0.021 minutes\n"
     ]
    }
   ],
   "source": [
    "# Delete all existing files in folder where models are saved:\n",
    "print(\"[INFO] deleting existing files in folder...\")\n",
    "t0 = time()\n",
    "\n",
    "files = glob.glob(f'{SAVE_FOLDER}/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "print (\"         done in %0.3f minutes\" % ((time() - t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_VAE_AE import get_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Investigation of Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"normal\" training images is:     1400\n",
      "Number of \"normal\" validation images is:   134\n",
      "Number of \"normal\" test images is:         266\n",
      "Number of \"anomaly\" images is:             600\n"
     ]
    }
   ],
   "source": [
    "# Glob the directories and get the lists of good and bad images\n",
    "good_train_fns = [f for f in os.listdir(NORMAL_TRAIN_AUG) if not f.startswith('.')]\n",
    "good_val_fns = [f for f in os.listdir(NORMAL_VAL_AUG) if not f.startswith('.')]\n",
    "good_test_fns = [f for f in os.listdir(NORMAL_TEST_AUG) if not f.startswith('.')]\n",
    "bad_fns = [f for f in os.listdir(ANOMALY_AUG) if not f.startswith('.')]\n",
    "\n",
    "print('Number of \"normal\" training images is:     {}'.format(len(good_train_fns)))\n",
    "print('Number of \"normal\" validation images is:   {}'.format(len(good_val_fns)))\n",
    "print('Number of \"normal\" test images is:         {}'.format(len(good_test_fns)))\n",
    "print('Number of \"anomaly\" images is:             {}'.format(len(bad_fns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Procentage of normal samples (ground truth):   75.00 %\n",
      "> Procentage of anomaly samples (ground truth):  25.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of bad images vs good images:\n",
    "normal_fns = len(good_train_fns) + len(good_val_fns) + len(good_test_fns)\n",
    "anomaly_fns = len(bad_fns)\n",
    "print(f\"> Procentage of normal samples (ground truth):   {(normal_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")\n",
    "print(f\"> Procentage of anomaly samples (ground truth):  {(anomaly_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Normal Data\n",
    "\n",
    "Load normal samples in pre-splitted data sets: train, valdiation and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal training images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal training images...\")\n",
    "trainX = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TRAIN_AUG}/*.png\")]  # read as grayscale\n",
    "trainX = np.array(trainX)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "trainY = get_labels(trainX, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal validation images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal validation images...\")\n",
    "x_normal_val = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_VAL_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_val = np.array(x_normal_val)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_val = get_labels(x_normal_val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal testing images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal testing images...\")\n",
    "x_normal_test = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TEST_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_test = np.array(x_normal_test)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_test = get_labels(x_normal_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Data\n",
    "\n",
    "\n",
    "Load anomaly samples in its singular training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly images...\")\n",
    "x_anomaly = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY_AUG}/*.png\")]  # read as grayscale\n",
    "x_anomaly = np.array(x_anomaly)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_anomaly = get_labels(x_anomaly, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine normal samples and anomaly samples and split them into training, validation and test sets\n",
    "\n",
    "`label 0` == Normal Samples\n",
    "\n",
    "`label 1` == Anomaly Samples\n",
    "\n",
    "Train and Validation sets only consists of `Normal Data`, aka. `label 0`.\n",
    "\n",
    "Testing sets consists of both  `Normal Data` (aka. `label 0`) and `Anomaly Data` (aka. `label 1`)\n",
    "\n",
    "________________\n",
    "\n",
    "Due to the very sparse original (preprossed) KitKat Chunky data, the validation set will be a mix of normal testing and validation data. The normal testing set will consists of all validation and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testvalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testvalY = get_labels(testvalX, 0)\n",
    "\n",
    "# randomly split in order to make new validation set:\n",
    "NoUsageX, valX, NoUsageY, valY = train_test_split(testvalX, testvalY, test_size=0.25, random_state=4345672)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testNormalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testNormalY = get_labels(testNormalX, 0)\n",
    "\n",
    "# anomaly class (only used for testing):\n",
    "testAnomalyX, testAnomalyY = x_anomaly, y_anomaly\n",
    "\n",
    "# Combine all testing data in one array:\n",
    "testAllX = np.vstack((testNormalX, testAnomalyX))\n",
    "testAllY = np.hstack((testNormalY, testAnomalyY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Dimensions and Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      " > images: (1400, 128, 128)\n",
      " > labels: (1400,)\n",
      "\n",
      "Validation set:\n",
      " > images: (100, 128, 128)\n",
      " > labels: (100,)\n",
      "\n",
      "Test set:\n",
      " > images: (1000, 128, 128)\n",
      " > labels: (1000,)\n",
      "\t'Normal' test set:\n",
      "\t  > images: (400, 128, 128)\n",
      "\t  > labels: (400,)\n",
      "\t'Anomaly' test set:\n",
      "\t  > images: (600, 128, 128)\n",
      "\t  > labels: (600,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "print(\" > images:\", trainX.shape)\n",
    "print(\" > labels:\", trainY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Validation set:\")\n",
    "print(\" > images:\", valX.shape)\n",
    "print(\" > labels:\", valY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Test set:\")\n",
    "print(\" > images:\", testAllX.shape)\n",
    "print(\" > labels:\", testAllY.shape)\n",
    "print(\"\\t'Normal' test set:\")\n",
    "print(\"\\t  > images:\", testNormalX.shape)\n",
    "print(\"\\t  > labels:\", testNormalY.shape)\n",
    "print(\"\\t'Anomaly' test set:\")\n",
    "print(\"\\t  > images:\", testAnomalyX.shape)\n",
    "print(\"\\t  > labels:\", testAnomalyY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 73.68 %\n",
      "Procentage of validation samples: 5.26 %\n",
      "Procentage of (only normal) testing samples: 21.05 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets (only normal data):\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (only normal) testing samples: {(len(testNormalX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 56.00 %\n",
      "Procentage of validation samples: 4.00 %\n",
      "Procentage of (total) testing samples: 40.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets:\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (total) testing samples: {(len(testAllX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for (un)balanced data\n",
    "\n",
    "In an ideal world the data should be balanced. However in the real world we expect there being around ~$99 \\%$ good samples and only ~$1 \\%$ bad samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['0: normal', '1: anomaly']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,4))\n",
    "ax[0].hist(trainY, align=\"left\"); ax[0].set_title('train', fontsize=18); ax[0].set_xticks([0,1]); ax[0].set_xticklabels(labels); ax[0].tick_params(axis='both', which='major', labelsize=16); ax[0].set_xlim(-0.5, 1.5);\n",
    "ax[1].hist(valY, align=\"left\"); ax[1].set_title('validation', fontsize=18); ax[1].set_xticks([0,1]); ax[1].set_xticklabels(labels); ax[1].tick_params(axis='both', which='major', labelsize=16); ax[1].set_xlim(-0.5, 1.5);\n",
    "ax[2].hist(testAllY, align=\"left\"); ax[2].set_title('test', fontsize=18); ax[2].set_xticks([0,1]); ax[2].set_xticklabels(labels); ax[2].tick_params(axis='both', which='major', labelsize=16); ax[2].set_xlim(-0.5, 1.5);\n",
    "\n",
    "ax[0].set_ylabel(\"Number of images\", fontsize=18)\n",
    "ax[1].set_xlabel(\"Class Labels\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of normal test samples: 40.00 %\n",
      "Procentage of total anomaly test samples: 60.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of normal test images vs total anomaly test images:\n",
    "print(f\"Procentage of normal test samples: {(len(testNormalX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of total anomaly test samples: {(len(testAnomalyX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Only normalization has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "img_width, img_height = trainX.shape[1], trainX.shape[2]      # input image dimensions\n",
    "num_channels = 1                                              # gray-channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Neural Networks learns faster with normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to be in  the [0, 1] range:\n",
    "trainX = trainX.astype('float32') / 255.\n",
    "valX = valX.astype('float32') / 255.\n",
    "testAllX = testAllX.astype('float32') / 255.\n",
    "\n",
    "# reshape data to keras inputs --> (n_samples, img_rows, img_cols, n_channels):\n",
    "trainX = trainX.reshape(trainX.shape[0], img_height, img_width, num_channels)\n",
    "valX = valX.reshape(valX.shape[0], img_height, img_width, num_channels)\n",
    "testAllX = testAllX.reshape(testAllX.shape[0], img_height, img_width, num_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import architecture of VAE model and its hyperparameters:\n",
    "from J4_AE_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Total AE Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 64, 64, 32)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "latent (Dense)               (None, 4)                 2052      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               2560      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 32)      4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 128, 128, 32)      128       \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 128, 128, 1)       129       \n",
      "=================================================================\n",
      "Total params: 45,381\n",
      "Trainable params: 43,717\n",
      "Non-trainable params: 1,664\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "batch_size  = 256\n",
    "epochs      = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at 1'st epoch (inital model)\n",
    "\n",
    "https://stackoverflow.com/questions/54323960/save-keras-model-at-specific-epochs\n",
    "\"\"\"\n",
    "\n",
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 1:\n",
    "            self.model.save_weights(f\"{SAVE_FOLDER}/cp-0001.h5\")\n",
    "            \n",
    "# create and use callback:\n",
    "initial_checkpoint = CustomSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at every k'te epochs\n",
    "\"\"\"\n",
    "# Include the epoch in the file name (uses `str.format`):\n",
    "checkpoint_path = \"saved_models/latent4/cp-{epoch:04d}.h5\"\n",
    "\n",
    "# Create a callback that saves the model's weights every k'te epochs:\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path,\n",
    "                             monitor='loss',           # name of the metrics to monitor\n",
    "                             verbose=1, \n",
    "                             #save_best_only=False,     # if False, the best model will not be overridden.\n",
    "                             save_weights_only=True,   # if True, only the weights of the models will be saved.\n",
    "                                                        # if False, the whole models will be saved.\n",
    "                             #mode='auto', \n",
    "                             period=200                  # save after every k'te epochs\n",
    "                            )\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "ae.save_weights(checkpoint_path.format(epoch=0))          # save for zero epoch (inital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: Early stopping\n",
    "https://www.tensorflow.org/guide/keras/custom_callback\n",
    "\"\"\"\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=50):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create and Compile Model\"\"\"\n",
    "# import architecture of VAE model and its hyperparameters. learning rate choosen from graph above.\n",
    "ae = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 0.2807 - val_loss: 0.2154\n",
      "Epoch 2/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.1976 - val_loss: 0.2096\n",
      "Epoch 3/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.1480 - val_loss: 0.2023\n",
      "Epoch 4/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.1055 - val_loss: 0.1935\n",
      "Epoch 5/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0755 - val_loss: 0.1842\n",
      "Epoch 6/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0596 - val_loss: 0.1740\n",
      "Epoch 7/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0510 - val_loss: 0.1639\n",
      "Epoch 8/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0465 - val_loss: 0.1546\n",
      "Epoch 9/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0434 - val_loss: 0.1459\n",
      "Epoch 10/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0414 - val_loss: 0.1372\n",
      "Epoch 11/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0398 - val_loss: 0.1288\n",
      "Epoch 12/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0379 - val_loss: 0.1202\n",
      "Epoch 13/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0363 - val_loss: 0.1117\n",
      "Epoch 14/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0344 - val_loss: 0.1037\n",
      "Epoch 15/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0324 - val_loss: 0.0959\n",
      "Epoch 16/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0304 - val_loss: 0.0887\n",
      "Epoch 17/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0285 - val_loss: 0.0822\n",
      "Epoch 18/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0264 - val_loss: 0.0765\n",
      "Epoch 19/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0246 - val_loss: 0.0714\n",
      "Epoch 20/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0229 - val_loss: 0.0669\n",
      "Epoch 21/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0219 - val_loss: 0.0630\n",
      "Epoch 22/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0208 - val_loss: 0.0599\n",
      "Epoch 23/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0194 - val_loss: 0.0575\n",
      "Epoch 24/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0187 - val_loss: 0.0555\n",
      "Epoch 25/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0177 - val_loss: 0.0538\n",
      "Epoch 26/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0169 - val_loss: 0.0524\n",
      "Epoch 27/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0158 - val_loss: 0.0514\n",
      "Epoch 28/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0152 - val_loss: 0.0506\n",
      "Epoch 29/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0146 - val_loss: 0.0502\n",
      "Epoch 30/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0142 - val_loss: 0.0499\n",
      "Epoch 31/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0136 - val_loss: 0.0496\n",
      "Epoch 32/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0131 - val_loss: 0.0495\n",
      "Epoch 33/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0128 - val_loss: 0.0495\n",
      "Epoch 34/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0129 - val_loss: 0.0496\n",
      "Epoch 35/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0125 - val_loss: 0.0497\n",
      "Epoch 36/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0128 - val_loss: 0.0498\n",
      "Epoch 37/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0120 - val_loss: 0.0500\n",
      "Epoch 38/1000\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.0114 - val_loss: 0.0501\n",
      "Epoch 39/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0109 - val_loss: 0.0503\n",
      "Epoch 40/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0106 - val_loss: 0.0505\n",
      "Epoch 41/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0102 - val_loss: 0.0507\n",
      "Epoch 42/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0110 - val_loss: 0.0508\n",
      "Epoch 43/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0104 - val_loss: 0.0510\n",
      "Epoch 44/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0102 - val_loss: 0.0511\n",
      "Epoch 45/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0094 - val_loss: 0.0512\n",
      "Epoch 46/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0092 - val_loss: 0.0514\n",
      "Epoch 47/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0089 - val_loss: 0.0514\n",
      "Epoch 48/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0085 - val_loss: 0.0515\n",
      "Epoch 49/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0084 - val_loss: 0.0515\n",
      "Epoch 50/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0084 - val_loss: 0.0514\n",
      "Epoch 51/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0081 - val_loss: 0.0513\n",
      "Epoch 52/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0078 - val_loss: 0.0512\n",
      "Epoch 53/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0086 - val_loss: 0.0509\n",
      "Epoch 54/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0084 - val_loss: 0.0506\n",
      "Epoch 55/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0076 - val_loss: 0.0504\n",
      "Epoch 56/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0076 - val_loss: 0.0501\n",
      "Epoch 57/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0078 - val_loss: 0.0494\n",
      "Epoch 58/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0075 - val_loss: 0.0494\n",
      "Epoch 59/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0077 - val_loss: 0.0483\n",
      "Epoch 60/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0071 - val_loss: 0.0481\n",
      "Epoch 61/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0071 - val_loss: 0.0472\n",
      "Epoch 62/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0072 - val_loss: 0.0460\n",
      "Epoch 63/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0071 - val_loss: 0.0460\n",
      "Epoch 64/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0073 - val_loss: 0.0454\n",
      "Epoch 65/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0066 - val_loss: 0.0442\n",
      "Epoch 66/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0067 - val_loss: 0.0434\n",
      "Epoch 67/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0067 - val_loss: 0.0432\n",
      "Epoch 68/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0062 - val_loss: 0.0424\n",
      "Epoch 69/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0066 - val_loss: 0.0421\n",
      "Epoch 70/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0062 - val_loss: 0.0414\n",
      "Epoch 71/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0062 - val_loss: 0.0409\n",
      "Epoch 72/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0060 - val_loss: 0.0403\n",
      "Epoch 73/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0065 - val_loss: 0.0393\n",
      "Epoch 74/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0062 - val_loss: 0.0395\n",
      "Epoch 75/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0061 - val_loss: 0.0383\n",
      "Epoch 76/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0059 - val_loss: 0.0388\n",
      "Epoch 77/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0061 - val_loss: 0.0375\n",
      "Epoch 78/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0058 - val_loss: 0.0380\n",
      "Epoch 79/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0058 - val_loss: 0.0383\n",
      "Epoch 80/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0055 - val_loss: 0.0382\n",
      "Epoch 81/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0057 - val_loss: 0.0381\n",
      "Epoch 82/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0053 - val_loss: 0.0374\n",
      "Epoch 83/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0058 - val_loss: 0.0371\n",
      "Epoch 84/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0054 - val_loss: 0.0361\n",
      "Epoch 85/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0056 - val_loss: 0.0345\n",
      "Epoch 86/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0056 - val_loss: 0.0327\n",
      "Epoch 87/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0054 - val_loss: 0.0318\n",
      "Epoch 88/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0050 - val_loss: 0.0331\n",
      "Epoch 89/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0050 - val_loss: 0.0330\n",
      "Epoch 90/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0051 - val_loss: 0.0323\n",
      "Epoch 91/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0051 - val_loss: 0.0327\n",
      "Epoch 92/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0054 - val_loss: 0.0330\n",
      "Epoch 93/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0049 - val_loss: 0.0309\n",
      "Epoch 94/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0312\n",
      "Epoch 95/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0053 - val_loss: 0.0286\n",
      "Epoch 96/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0049 - val_loss: 0.0311\n",
      "Epoch 97/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0292\n",
      "Epoch 98/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0049 - val_loss: 0.0296\n",
      "Epoch 99/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0051 - val_loss: 0.0281\n",
      "Epoch 100/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0049 - val_loss: 0.0233\n",
      "Epoch 101/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0046 - val_loss: 0.0228\n",
      "Epoch 102/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0048 - val_loss: 0.0269\n",
      "Epoch 103/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0045 - val_loss: 0.0245\n",
      "Epoch 104/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0046 - val_loss: 0.0246\n",
      "Epoch 105/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0049 - val_loss: 0.0218\n",
      "Epoch 106/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0047 - val_loss: 0.0177\n",
      "Epoch 107/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0044 - val_loss: 0.0246\n",
      "Epoch 108/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0046 - val_loss: 0.0191\n",
      "Epoch 109/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0043 - val_loss: 0.0237\n",
      "Epoch 110/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0045 - val_loss: 0.0199\n",
      "Epoch 111/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0044 - val_loss: 0.0283\n",
      "Epoch 112/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0046 - val_loss: 0.0283\n",
      "Epoch 113/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0044 - val_loss: 0.0194\n",
      "Epoch 114/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0044 - val_loss: 0.0208\n",
      "Epoch 115/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0043 - val_loss: 0.0231\n",
      "Epoch 116/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0044 - val_loss: 0.0089\n",
      "Epoch 117/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0049 - val_loss: 0.0233\n",
      "Epoch 118/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0045 - val_loss: 0.0115\n",
      "Epoch 119/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0043 - val_loss: 0.0075\n",
      "Epoch 120/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0046 - val_loss: 0.0121\n",
      "Epoch 121/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0046 - val_loss: 0.0156\n",
      "Epoch 122/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0042 - val_loss: 0.0093\n",
      "Epoch 123/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0045 - val_loss: 0.0073\n",
      "Epoch 124/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0045 - val_loss: 0.0057\n",
      "Epoch 125/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0042 - val_loss: 0.0074\n",
      "Epoch 126/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0041 - val_loss: 0.0062\n",
      "Epoch 127/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0045 - val_loss: 0.0057\n",
      "Epoch 128/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 129/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0044 - val_loss: 0.0052\n",
      "Epoch 130/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0043 - val_loss: 0.0059\n",
      "Epoch 131/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0041 - val_loss: 0.0052\n",
      "Epoch 132/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0043 - val_loss: 0.0051\n",
      "Epoch 133/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0044 - val_loss: 0.0068\n",
      "Epoch 134/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0041 - val_loss: 0.0074\n",
      "Epoch 135/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0042 - val_loss: 0.0060\n",
      "Epoch 136/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0040 - val_loss: 0.0062\n",
      "Epoch 137/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0041 - val_loss: 0.0051\n",
      "Epoch 138/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0043 - val_loss: 0.0046\n",
      "Epoch 139/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0041 - val_loss: 0.0047\n",
      "Epoch 140/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0039 - val_loss: 0.0047\n",
      "Epoch 141/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0038 - val_loss: 0.0047\n",
      "Epoch 142/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0037 - val_loss: 0.0044\n",
      "Epoch 143/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0040 - val_loss: 0.0044\n",
      "Epoch 144/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0041 - val_loss: 0.0047\n",
      "Epoch 145/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0036 - val_loss: 0.0045\n",
      "Epoch 146/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0037 - val_loss: 0.0042\n",
      "Epoch 147/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0037 - val_loss: 0.0043\n",
      "Epoch 148/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0040 - val_loss: 0.0047\n",
      "Epoch 149/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0035 - val_loss: 0.0041\n",
      "Epoch 150/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0037 - val_loss: 0.0043\n",
      "Epoch 151/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0034 - val_loss: 0.0043\n",
      "Epoch 152/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0038 - val_loss: 0.0042\n",
      "Epoch 153/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 154/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0034 - val_loss: 0.0041\n",
      "Epoch 155/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0037 - val_loss: 0.0044\n",
      "Epoch 156/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0039 - val_loss: 0.0053\n",
      "Epoch 157/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 158/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0036 - val_loss: 0.0043\n",
      "Epoch 159/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0036 - val_loss: 0.0046\n",
      "Epoch 160/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0036 - val_loss: 0.0045\n",
      "Epoch 161/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0039 - val_loss: 0.0051\n",
      "Epoch 162/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0036 - val_loss: 0.0043\n",
      "Epoch 163/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0042 - val_loss: 0.0056\n",
      "Epoch 164/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0037 - val_loss: 0.0047\n",
      "Epoch 165/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0040 - val_loss: 0.0052\n",
      "Epoch 166/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0037 - val_loss: 0.0049\n",
      "Epoch 167/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0038 - val_loss: 0.0046\n",
      "Epoch 168/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0036 - val_loss: 0.0048\n",
      "Epoch 169/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0037 - val_loss: 0.0053\n",
      "Epoch 170/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0037 - val_loss: 0.0045\n",
      "Epoch 171/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0033 - val_loss: 0.0051\n",
      "Epoch 172/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0033 - val_loss: 0.0050\n",
      "Epoch 173/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0034 - val_loss: 0.0048\n",
      "Epoch 174/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0035 - val_loss: 0.0047\n",
      "Epoch 175/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0035 - val_loss: 0.0047\n",
      "Epoch 176/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0034 - val_loss: 0.0043\n",
      "Epoch 177/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0035 - val_loss: 0.0045\n",
      "Epoch 178/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0037 - val_loss: 0.0042\n",
      "Epoch 179/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0034 - val_loss: 0.0044\n",
      "Epoch 180/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0038 - val_loss: 0.0041\n",
      "Epoch 181/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0037 - val_loss: 0.0038\n",
      "Epoch 182/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0033 - val_loss: 0.0041\n",
      "Epoch 183/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0033 - val_loss: 0.0040\n",
      "Epoch 184/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0034 - val_loss: 0.0038\n",
      "Epoch 185/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 186/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 187/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0033 - val_loss: 0.0036\n",
      "Epoch 188/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 189/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 190/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0038 - val_loss: 0.0042\n",
      "Epoch 191/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0033 - val_loss: 0.0038\n",
      "Epoch 192/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0037 - val_loss: 0.0043\n",
      "Epoch 193/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0033 - val_loss: 0.0046\n",
      "Epoch 194/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0033 - val_loss: 0.0045\n",
      "Epoch 195/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 196/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0032 - val_loss: 0.0040\n",
      "Epoch 197/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0032 - val_loss: 0.0048\n",
      "Epoch 198/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0032 - val_loss: 0.0038\n",
      "Epoch 199/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0033 - val_loss: 0.0048\n",
      "Epoch 200/1000\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.0031\n",
      "Epoch 00200: saving model to saved_models/latent4/cp-0200.h5\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 0.0032 - val_loss: 0.0043\n",
      "Epoch 201/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0032 - val_loss: 0.0038\n",
      "Epoch 202/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0029 - val_loss: 0.0041\n",
      "Epoch 203/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0032 - val_loss: 0.0036\n",
      "Epoch 204/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 205/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0032 - val_loss: 0.0036\n",
      "Epoch 206/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0030 - val_loss: 0.0036\n",
      "Epoch 207/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 208/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0033 - val_loss: 0.0037\n",
      "Epoch 209/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0033 - val_loss: 0.0043\n",
      "Epoch 210/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 211/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0032 - val_loss: 0.0038\n",
      "Epoch 212/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 213/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0034 - val_loss: 0.0041\n",
      "Epoch 214/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0030 - val_loss: 0.0038\n",
      "Epoch 215/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0030 - val_loss: 0.0037\n",
      "Epoch 216/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0030 - val_loss: 0.0036\n",
      "Epoch 217/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0031 - val_loss: 0.0036\n",
      "Epoch 218/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0032 - val_loss: 0.0051\n",
      "Epoch 219/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0030 - val_loss: 0.0044\n",
      "Epoch 220/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 221/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0034 - val_loss: 0.0039\n",
      "Epoch 222/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0036 - val_loss: 0.0043\n",
      "Epoch 223/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 224/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0029 - val_loss: 0.0043\n",
      "Epoch 225/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0032 - val_loss: 0.0036\n",
      "Epoch 226/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 227/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 228/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0028 - val_loss: 0.0049\n",
      "Epoch 229/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 230/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0031 - val_loss: 0.0045\n",
      "Epoch 231/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0043\n",
      "Epoch 232/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0045\n",
      "Epoch 233/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 234/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0027 - val_loss: 0.0038\n",
      "Epoch 235/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0029 - val_loss: 0.0035\n",
      "Epoch 236/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0030 - val_loss: 0.0034\n",
      "Epoch 237/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0033\n",
      "Epoch 238/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 239/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0029 - val_loss: 0.0036\n",
      "Epoch 240/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0034\n",
      "Epoch 241/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0035 - val_loss: 0.0032\n",
      "Epoch 242/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 243/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0032 - val_loss: 0.0035\n",
      "Epoch 244/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0031 - val_loss: 0.0036\n",
      "Epoch 245/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0030 - val_loss: 0.0033\n",
      "Epoch 246/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 247/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0030 - val_loss: 0.0034\n",
      "Epoch 248/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0035\n",
      "Epoch 249/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 250/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 251/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0033 - val_loss: 0.0035\n",
      "Epoch 252/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0029 - val_loss: 0.0034\n",
      "Epoch 253/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0029 - val_loss: 0.0032\n",
      "Epoch 254/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0032\n",
      "Epoch 255/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0035\n",
      "Epoch 256/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0026 - val_loss: 0.0036\n",
      "Epoch 257/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0025 - val_loss: 0.0034\n",
      "Epoch 258/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 259/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0028 - val_loss: 0.0031\n",
      "Epoch 260/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 261/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0039\n",
      "Epoch 262/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0033\n",
      "Epoch 263/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0027 - val_loss: 0.0036\n",
      "Epoch 264/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0030 - val_loss: 0.0036\n",
      "Epoch 265/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0029 - val_loss: 0.0043\n",
      "Epoch 266/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0030 - val_loss: 0.0032\n",
      "Epoch 267/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0041\n",
      "Epoch 268/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0039\n",
      "Epoch 269/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0029 - val_loss: 0.0047\n",
      "Epoch 270/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0032 - val_loss: 0.0037\n",
      "Epoch 271/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0033\n",
      "Epoch 272/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0026 - val_loss: 0.0031\n",
      "Epoch 273/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0031\n",
      "Epoch 274/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0027 - val_loss: 0.0030\n",
      "Epoch 275/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0031\n",
      "Epoch 276/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0044\n",
      "Epoch 277/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0025 - val_loss: 0.0035\n",
      "Epoch 278/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0034\n",
      "Epoch 279/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0033\n",
      "Epoch 280/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0029 - val_loss: 0.0031\n",
      "Epoch 281/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0031\n",
      "Epoch 282/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0031\n",
      "Epoch 283/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0029 - val_loss: 0.0034\n",
      "Epoch 284/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0025 - val_loss: 0.0032\n",
      "Epoch 285/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 286/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 287/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 288/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0028\n",
      "Epoch 289/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0031\n",
      "Epoch 290/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0033\n",
      "Epoch 291/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0034\n",
      "Epoch 292/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0029 - val_loss: 0.0036\n",
      "Epoch 293/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0029 - val_loss: 0.0034\n",
      "Epoch 294/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0033\n",
      "Epoch 295/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 296/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0029\n",
      "Epoch 297/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0030\n",
      "Epoch 298/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 299/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0033\n",
      "Epoch 300/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0028\n",
      "Epoch 301/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0028\n",
      "Epoch 302/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0029 - val_loss: 0.0035\n",
      "Epoch 303/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0037\n",
      "Epoch 304/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0036\n",
      "Epoch 305/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0029 - val_loss: 0.0038\n",
      "Epoch 306/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0036\n",
      "Epoch 307/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0030\n",
      "Epoch 308/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 309/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0034\n",
      "Epoch 310/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0026 - val_loss: 0.0033\n",
      "Epoch 311/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0037\n",
      "Epoch 312/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0029\n",
      "Epoch 313/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 314/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 315/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 316/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0025 - val_loss: 0.0030\n",
      "Epoch 317/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0023 - val_loss: 0.0032\n",
      "Epoch 318/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0022 - val_loss: 0.0027\n",
      "Epoch 319/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0027\n",
      "Epoch 320/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0027\n",
      "Epoch 321/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0022 - val_loss: 0.0026\n",
      "Epoch 322/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 323/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0026\n",
      "Epoch 324/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0022 - val_loss: 0.0029\n",
      "Epoch 325/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0027\n",
      "Epoch 326/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0028\n",
      "Epoch 327/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0040\n",
      "Epoch 328/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0038\n",
      "Epoch 329/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0033\n",
      "Epoch 330/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0028\n",
      "Epoch 331/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0028\n",
      "Epoch 332/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 333/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0027 - val_loss: 0.0030\n",
      "Epoch 334/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0030\n",
      "Epoch 335/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 336/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0042\n",
      "Epoch 337/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0037\n",
      "Epoch 338/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0031 - val_loss: 0.0047\n",
      "Epoch 339/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0047\n",
      "Epoch 340/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0049\n",
      "Epoch 341/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0043\n",
      "Epoch 342/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0034\n",
      "Epoch 343/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0030\n",
      "Epoch 344/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0027\n",
      "Epoch 345/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0029\n",
      "Epoch 346/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 347/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0033 - val_loss: 0.0035\n",
      "Epoch 348/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 349/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0030 - val_loss: 0.0033\n",
      "Epoch 350/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 351/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0035\n",
      "Epoch 352/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0029\n",
      "Epoch 353/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0027\n",
      "Epoch 354/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0022 - val_loss: 0.0027\n",
      "Epoch 355/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0023 - val_loss: 0.0029\n",
      "Epoch 356/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0026 - val_loss: 0.0030\n",
      "Epoch 357/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0027\n",
      "Epoch 358/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0026\n",
      "Epoch 359/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0031\n",
      "Epoch 360/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0026\n",
      "Epoch 361/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 362/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0033\n",
      "Epoch 363/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0031\n",
      "Epoch 364/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0022 - val_loss: 0.0032\n",
      "Epoch 365/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0028\n",
      "Epoch 366/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0029\n",
      "Epoch 367/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0028\n",
      "Epoch 368/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0027\n",
      "Epoch 369/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0027\n",
      "Epoch 370/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0025\n",
      "Epoch 371/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0026\n",
      "Epoch 372/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0035\n",
      "Epoch 373/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0040\n",
      "Epoch 374/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0040\n",
      "Epoch 375/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0021 - val_loss: 0.0027\n",
      "Epoch 376/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0024 - val_loss: 0.0027\n",
      "Epoch 377/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0026\n",
      "Epoch 378/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0031\n",
      "Epoch 379/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0026\n",
      "Epoch 380/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0033\n",
      "Epoch 381/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0034\n",
      "Epoch 382/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0031 - val_loss: 0.0049\n",
      "Epoch 383/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0046\n",
      "Epoch 384/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0033\n",
      "Epoch 385/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0061\n",
      "Epoch 386/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0027\n",
      "Epoch 387/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0028\n",
      "Epoch 388/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0021 - val_loss: 0.0025\n",
      "Epoch 389/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0036\n",
      "Epoch 390/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0092\n",
      "Epoch 391/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0025 - val_loss: 0.0027\n",
      "Epoch 392/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0028\n",
      "Epoch 393/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0028\n",
      "Epoch 394/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0025\n",
      "Epoch 395/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 396/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 397/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0032 - val_loss: 0.0027\n",
      "Epoch 398/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 399/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 400/1000\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.0024\n",
      "Epoch 00400: saving model to saved_models/latent4/cp-0400.h5\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 401/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 402/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0028\n",
      "Epoch 403/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 404/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 405/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 406/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0027\n",
      "Epoch 407/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0028\n",
      "Epoch 408/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 409/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0032\n",
      "Epoch 410/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0037\n",
      "Epoch 411/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0040\n",
      "Epoch 412/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0031\n",
      "Epoch 413/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0027\n",
      "Epoch 414/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0020 - val_loss: 0.0026\n",
      "Epoch 415/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 416/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0026\n",
      "Epoch 417/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 418/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 419/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0026\n",
      "Epoch 420/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0025\n",
      "Epoch 421/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0027\n",
      "Epoch 422/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0025\n",
      "Epoch 423/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0030\n",
      "Epoch 424/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0026\n",
      "Epoch 425/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0030\n",
      "Epoch 426/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0030\n",
      "Epoch 427/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 428/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 429/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0060\n",
      "Epoch 430/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0023 - val_loss: 0.0027\n",
      "Epoch 431/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 432/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0025\n",
      "Epoch 433/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0023\n",
      "Epoch 434/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0027\n",
      "Epoch 435/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0028\n",
      "Epoch 436/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0026\n",
      "Epoch 437/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0030\n",
      "Epoch 438/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 439/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0026\n",
      "Epoch 440/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 441/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0027\n",
      "Epoch 442/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0022 - val_loss: 0.0030\n",
      "Epoch 443/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0023\n",
      "Epoch 444/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0026\n",
      "Epoch 445/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0027\n",
      "Epoch 446/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0027\n",
      "Epoch 447/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0025\n",
      "Epoch 448/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0027\n",
      "Epoch 449/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0028\n",
      "Epoch 450/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0028\n",
      "Epoch 451/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 452/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 453/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 454/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0030\n",
      "Epoch 455/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0028 - val_loss: 0.0025\n",
      "Epoch 456/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0028\n",
      "Epoch 457/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0022 - val_loss: 0.0029\n",
      "Epoch 458/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0031\n",
      "Epoch 459/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 460/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0030\n",
      "Epoch 461/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0052\n",
      "Epoch 462/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0024 - val_loss: 0.0068\n",
      "Epoch 463/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0032\n",
      "Epoch 464/1000\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.0022Restoring model weights from the end of the best epoch.\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0022 - val_loss: 0.0035\n",
      "Epoch 00464: early stopping\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train VAE model with callbacks \"\"\"\n",
    "history = ae.fit(trainX, trainX, \n",
    "                  batch_size = batch_size, \n",
    "                  epochs = epochs, \n",
    "                  callbacks=[initial_checkpoint, checkpoint, EarlyStoppingAtMinLoss()],\n",
    "                  #callbacks=[initial_checkpoint, checkpoint],\n",
    "                  shuffle=True,\n",
    "                  validation_data=(valX, valX)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 6.0\n"
     ]
    }
   ],
   "source": [
    "# In this GPU implementation, it shows the number of batches/iterations for one epoch (and not the training samples), which is:\n",
    "print (\"Step size:\", np.ceil(trainX.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save the entire model \"\"\"\n",
    "# Save notes about hyperparameters used:\n",
    "with open(f'{SAVE_FOLDER}/notes.txt', 'w') as f:\n",
    "    f.write(f'Epochs: {epochs} \\n')\n",
    "    f.write(f'Batch size: {batch_size} \\n')\n",
    "    f.write(f'Latent dimension: {latent_dim} \\n')\n",
    "    f.write(f'Filter sizes: {filters} \\n')\n",
    "    f.write(f'img_width: {img_width} \\n')\n",
    "    f.write(f'img_height: {img_height} \\n')\n",
    "    f.write(f'num_channels: {num_channels}')\n",
    "    f.close()\n",
    "\n",
    "# Save encoder weights:\n",
    "encoder.save_weights(f'{SAVE_FOLDER}/ENCODER_WEIGHTS.h5')\n",
    "\n",
    "# Save the total AE model, i.e. its weights:\n",
    "ae.save_weights(f'{SAVE_FOLDER}/AE_WEIGHTS.h5')\n",
    "\n",
    "# Save the history at each epochs (cost of train and validation sets):\n",
    "np.save(f'{SAVE_FOLDER}/HISTORY.npy', history.history)\n",
    "\n",
    "# Save exact training, validation and testing data set (as numpy arrays):\n",
    "np.save(f'{SAVE_FOLDER}/trainX.npy', trainX)\n",
    "np.save(f'{SAVE_FOLDER}/valX.npy', valX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllX.npy', testAllX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllY.npy', testAllY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate training process\n",
    "\n",
    "Now that the training process is complete, let’s evaluate the average loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(13, 6))\n",
    "\n",
    "# summarize history for loss:\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='blue', alpha=0.4, label='validation')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Cost', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.title('Average Loss During Training', fontsize=18)\n",
    "#plt.xticks(range(0,epochs))\n",
    "plt.legend(loc='upper right', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
