{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import axis, colorbar, imshow, show, figure, subplot\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=18)\n",
    "mpl.rc('xtick', labelsize=16)\n",
    "mpl.rc('ytick', labelsize=16)\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## ----- GPU ------------------------------------\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'    # use of GPU 0 (ERDA) (use before importing torch or tensorflow/keeas)\n",
    "## ----------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization, ReLU, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.activations import relu\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K                         #contains calls for tensor manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL_TRAIN_AUG:       /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTrainAugmentations\n",
      "NORMAL_VALIDATION_AUG:  /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalValidationAugmentations\n",
      "NORMAL_TEST_AUG:        /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTestAugmentations\n",
      "ANOMALY_AUG:            /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/AnomalyAugmentations\n"
     ]
    }
   ],
   "source": [
    "from utils_ae_kitkat_paths import *\n",
    "\n",
    "# Get work directions for augmentated data set:\n",
    "print (\"NORMAL_TRAIN_AUG:      \", NORMAL_TRAIN_AUG)\n",
    "print (\"NORMAL_VALIDATION_AUG: \", NORMAL_VAL_AUG)\n",
    "print (\"NORMAL_TEST_AUG:       \", NORMAL_TEST_AUG)\n",
    "print (\"ANOMALY_AUG:           \", ANOMALY_AUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which Folder The Models Are Saved In: saved_models/latent8\n"
     ]
    }
   ],
   "source": [
    "# What latent dimension and filter size are we using?:\n",
    "latent_dim = 8\n",
    "filters    = 32\n",
    "\n",
    "# Get work direction for saving files:\n",
    "SAVE_FOLDER = f'saved_models/latent{latent_dim}'\n",
    "print (\"Which Folder The Models Are Saved In:\", SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] deleting existing files in folder...\n",
      "         done in 0.034 minutes\n"
     ]
    }
   ],
   "source": [
    "# Delete all existing files in folder where models are saved:\n",
    "print(\"[INFO] deleting existing files in folder...\")\n",
    "t0 = time()\n",
    "\n",
    "files = glob.glob(f'{SAVE_FOLDER}/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "print (\"         done in %0.3f minutes\" % ((time() - t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_VAE_AE import get_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Investigation of Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"normal\" training images is:     1400\n",
      "Number of \"normal\" validation images is:   134\n",
      "Number of \"normal\" test images is:         266\n",
      "Number of \"anomaly\" images is:             600\n"
     ]
    }
   ],
   "source": [
    "# Glob the directories and get the lists of good and bad images\n",
    "good_train_fns = [f for f in os.listdir(NORMAL_TRAIN_AUG) if not f.startswith('.')]\n",
    "good_val_fns = [f for f in os.listdir(NORMAL_VAL_AUG) if not f.startswith('.')]\n",
    "good_test_fns = [f for f in os.listdir(NORMAL_TEST_AUG) if not f.startswith('.')]\n",
    "bad_fns = [f for f in os.listdir(ANOMALY_AUG) if not f.startswith('.')]\n",
    "\n",
    "print('Number of \"normal\" training images is:     {}'.format(len(good_train_fns)))\n",
    "print('Number of \"normal\" validation images is:   {}'.format(len(good_val_fns)))\n",
    "print('Number of \"normal\" test images is:         {}'.format(len(good_test_fns)))\n",
    "print('Number of \"anomaly\" images is:             {}'.format(len(bad_fns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Procentage of normal samples (ground truth):   75.00 %\n",
      "> Procentage of anomaly samples (ground truth):  25.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of bad images vs good images:\n",
    "normal_fns = len(good_train_fns) + len(good_val_fns) + len(good_test_fns)\n",
    "anomaly_fns = len(bad_fns)\n",
    "print(f\"> Procentage of normal samples (ground truth):   {(normal_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")\n",
    "print(f\"> Procentage of anomaly samples (ground truth):  {(anomaly_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Normal Data\n",
    "\n",
    "Load normal samples in pre-splitted data sets: train, valdiation and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal training images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal training images...\")\n",
    "trainX = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TRAIN_AUG}/*.png\")]  # read as grayscale\n",
    "trainX = np.array(trainX)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "trainY = get_labels(trainX, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal validation images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal validation images...\")\n",
    "x_normal_val = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_VAL_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_val = np.array(x_normal_val)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_val = get_labels(x_normal_val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal testing images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal testing images...\")\n",
    "x_normal_test = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TEST_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_test = np.array(x_normal_test)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_test = get_labels(x_normal_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Data\n",
    "\n",
    "\n",
    "Load anomaly samples in its singular training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly images...\")\n",
    "x_anomaly = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY_AUG}/*.png\")]  # read as grayscale\n",
    "x_anomaly = np.array(x_anomaly)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_anomaly = get_labels(x_anomaly, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine normal samples and anomaly samples and split them into training, validation and test sets\n",
    "\n",
    "`label 0` == Normal Samples\n",
    "\n",
    "`label 1` == Anomaly Samples\n",
    "\n",
    "Train and Validation sets only consists of `Normal Data`, aka. `label 0`.\n",
    "\n",
    "Testing sets consists of both  `Normal Data` (aka. `label 0`) and `Anomaly Data` (aka. `label 1`)\n",
    "\n",
    "________________\n",
    "\n",
    "Due to the very sparse original (preprossed) KitKat Chunky data, the validation set will be a mix of normal testing and validation data. The normal testing set will consists of all validation and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testvalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testvalY = get_labels(testvalX, 0)\n",
    "\n",
    "# randomly split in order to make new validation set:\n",
    "NoUsageX, valX, NoUsageY, valY = train_test_split(testvalX, testvalY, test_size=0.25, random_state=4345672)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testNormalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testNormalY = get_labels(testNormalX, 0)\n",
    "\n",
    "# anomaly class (only used for testing):\n",
    "testAnomalyX, testAnomalyY = x_anomaly, y_anomaly\n",
    "\n",
    "# Combine all testing data in one array:\n",
    "testAllX = np.vstack((testNormalX, testAnomalyX))\n",
    "testAllY = np.hstack((testNormalY, testAnomalyY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Dimensions and Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      " > images: (1400, 128, 128)\n",
      " > labels: (1400,)\n",
      "\n",
      "Validation set:\n",
      " > images: (100, 128, 128)\n",
      " > labels: (100,)\n",
      "\n",
      "Test set:\n",
      " > images: (1000, 128, 128)\n",
      " > labels: (1000,)\n",
      "\t'Normal' test set:\n",
      "\t  > images: (400, 128, 128)\n",
      "\t  > labels: (400,)\n",
      "\t'Anomaly' test set:\n",
      "\t  > images: (600, 128, 128)\n",
      "\t  > labels: (600,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "print(\" > images:\", trainX.shape)\n",
    "print(\" > labels:\", trainY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Validation set:\")\n",
    "print(\" > images:\", valX.shape)\n",
    "print(\" > labels:\", valY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Test set:\")\n",
    "print(\" > images:\", testAllX.shape)\n",
    "print(\" > labels:\", testAllY.shape)\n",
    "print(\"\\t'Normal' test set:\")\n",
    "print(\"\\t  > images:\", testNormalX.shape)\n",
    "print(\"\\t  > labels:\", testNormalY.shape)\n",
    "print(\"\\t'Anomaly' test set:\")\n",
    "print(\"\\t  > images:\", testAnomalyX.shape)\n",
    "print(\"\\t  > labels:\", testAnomalyY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 73.68 %\n",
      "Procentage of validation samples: 5.26 %\n",
      "Procentage of (only normal) testing samples: 21.05 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets (only normal data):\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (only normal) testing samples: {(len(testNormalX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 56.00 %\n",
      "Procentage of validation samples: 4.00 %\n",
      "Procentage of (total) testing samples: 40.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets:\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (total) testing samples: {(len(testAllX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for (un)balanced data\n",
    "\n",
    "In an ideal world the data should be balanced. However in the real world we expect there being around ~$99 \\%$ good samples and only ~$1 \\%$ bad samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['0: normal', '1: anomaly']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,4))\n",
    "ax[0].hist(trainY, align=\"left\"); ax[0].set_title('train', fontsize=18); ax[0].set_xticks([0,1]); ax[0].set_xticklabels(labels); ax[0].tick_params(axis='both', which='major', labelsize=16); ax[0].set_xlim(-0.5, 1.5);\n",
    "ax[1].hist(valY, align=\"left\"); ax[1].set_title('validation', fontsize=18); ax[1].set_xticks([0,1]); ax[1].set_xticklabels(labels); ax[1].tick_params(axis='both', which='major', labelsize=16); ax[1].set_xlim(-0.5, 1.5);\n",
    "ax[2].hist(testAllY, align=\"left\"); ax[2].set_title('test', fontsize=18); ax[2].set_xticks([0,1]); ax[2].set_xticklabels(labels); ax[2].tick_params(axis='both', which='major', labelsize=16); ax[2].set_xlim(-0.5, 1.5);\n",
    "\n",
    "ax[0].set_ylabel(\"Number of images\", fontsize=18)\n",
    "ax[1].set_xlabel(\"Class Labels\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of normal test samples: 40.00 %\n",
      "Procentage of total anomaly test samples: 60.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of normal test images vs total anomaly test images:\n",
    "print(f\"Procentage of normal test samples: {(len(testNormalX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of total anomaly test samples: {(len(testAnomalyX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Only normalization has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "img_width, img_height = trainX.shape[1], trainX.shape[2]      # input image dimensions\n",
    "num_channels = 1                                              # gray-channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Neural Networks learns faster with normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to be in  the [0, 1] range:\n",
    "trainX = trainX.astype('float32') / 255.\n",
    "valX = valX.astype('float32') / 255.\n",
    "testAllX = testAllX.astype('float32') / 255.\n",
    "\n",
    "# reshape data to keras inputs --> (n_samples, img_rows, img_cols, n_channels):\n",
    "trainX = trainX.reshape(trainX.shape[0], img_height, img_width, num_channels)\n",
    "valX = valX.reshape(valX.shape[0], img_height, img_width, num_channels)\n",
    "testAllX = testAllX.reshape(testAllX.shape[0], img_height, img_width, num_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import architecture of VAE model and its hyperparameters:\n",
    "from J8_AE_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Total AE Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 64, 64, 32)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "latent (Dense)               (None, 8)                 4104      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               4608      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 32)      4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 128, 128, 32)      128       \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 128, 128, 1)       129       \n",
      "=================================================================\n",
      "Total params: 49,481\n",
      "Trainable params: 47,817\n",
      "Non-trainable params: 1,664\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "batch_size  = 256\n",
    "epochs      = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at 1'st epoch (inital model)\n",
    "\n",
    "https://stackoverflow.com/questions/54323960/save-keras-model-at-specific-epochs\n",
    "\"\"\"\n",
    "\n",
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 1:\n",
    "            self.model.save_weights(f\"{SAVE_FOLDER}/cp-0001.h5\")\n",
    "            \n",
    "# create and use callback:\n",
    "initial_checkpoint = CustomSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at every k'te epochs\n",
    "\"\"\"\n",
    "# Include the epoch in the file name (uses `str.format`):\n",
    "checkpoint_path = \"saved_models/latent8/cp-{epoch:04d}.h5\"\n",
    "\n",
    "# Create a callback that saves the model's weights every k'te epochs:\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path,\n",
    "                             monitor='loss',           # name of the metrics to monitor\n",
    "                             verbose=1, \n",
    "                             #save_best_only=False,     # if False, the best model will not be overridden.\n",
    "                             save_weights_only=True,   # if True, only the weights of the models will be saved.\n",
    "                                                        # if False, the whole models will be saved.\n",
    "                             #mode='auto', \n",
    "                             period=200                  # save after every k'te epochs\n",
    "                            )\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "ae.save_weights(checkpoint_path.format(epoch=0))          # save for zero epoch (inital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: Early stopping\n",
    "https://www.tensorflow.org/guide/keras/custom_callback\n",
    "\"\"\"\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=50):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create and Compile Model\"\"\"\n",
    "# import architecture of VAE model and its hyperparameters. learning rate choosen from graph above.\n",
    "ae = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "6/6 [==============================] - 1s 168ms/step - loss: 0.2822 - val_loss: 0.2154\n",
      "Epoch 2/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.1985 - val_loss: 0.2098\n",
      "Epoch 3/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.1538 - val_loss: 0.2028\n",
      "Epoch 4/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.1100 - val_loss: 0.1943\n",
      "Epoch 5/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0788 - val_loss: 0.1850\n",
      "Epoch 6/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0591 - val_loss: 0.1750\n",
      "Epoch 7/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0498 - val_loss: 0.1646\n",
      "Epoch 8/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0442 - val_loss: 0.1540\n",
      "Epoch 9/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0406 - val_loss: 0.1441\n",
      "Epoch 10/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0380 - val_loss: 0.1348\n",
      "Epoch 11/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0356 - val_loss: 0.1259\n",
      "Epoch 12/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0334 - val_loss: 0.1174\n",
      "Epoch 13/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0309 - val_loss: 0.1090\n",
      "Epoch 14/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0280 - val_loss: 0.1008\n",
      "Epoch 15/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0253 - val_loss: 0.0930\n",
      "Epoch 16/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0236 - val_loss: 0.0861\n",
      "Epoch 17/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0205 - val_loss: 0.0796\n",
      "Epoch 18/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0191 - val_loss: 0.0738\n",
      "Epoch 19/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0172 - val_loss: 0.0691\n",
      "Epoch 20/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0159 - val_loss: 0.0649\n",
      "Epoch 21/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0156 - val_loss: 0.0614\n",
      "Epoch 22/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0139 - val_loss: 0.0585\n",
      "Epoch 23/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0129 - val_loss: 0.0563\n",
      "Epoch 24/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0128 - val_loss: 0.0545\n",
      "Epoch 25/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0113 - val_loss: 0.0532\n",
      "Epoch 26/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0109 - val_loss: 0.0520\n",
      "Epoch 27/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0100 - val_loss: 0.0511\n",
      "Epoch 28/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0098 - val_loss: 0.0504\n",
      "Epoch 29/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0095 - val_loss: 0.0500\n",
      "Epoch 30/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0089 - val_loss: 0.0498\n",
      "Epoch 31/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0085 - val_loss: 0.0496\n",
      "Epoch 32/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0081 - val_loss: 0.0495\n",
      "Epoch 33/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0078 - val_loss: 0.0496\n",
      "Epoch 34/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0078 - val_loss: 0.0496\n",
      "Epoch 35/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0073 - val_loss: 0.0498\n",
      "Epoch 36/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0074 - val_loss: 0.0500\n",
      "Epoch 37/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0069 - val_loss: 0.0501\n",
      "Epoch 38/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0068 - val_loss: 0.0503\n",
      "Epoch 39/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0065 - val_loss: 0.0505\n",
      "Epoch 40/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0069 - val_loss: 0.0507\n",
      "Epoch 41/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0067 - val_loss: 0.0509\n",
      "Epoch 42/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0070 - val_loss: 0.0510\n",
      "Epoch 43/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0068 - val_loss: 0.0512\n",
      "Epoch 44/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0065 - val_loss: 0.0513\n",
      "Epoch 45/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0062 - val_loss: 0.0516\n",
      "Epoch 46/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0059 - val_loss: 0.0518\n",
      "Epoch 47/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0055 - val_loss: 0.0520\n",
      "Epoch 48/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0056 - val_loss: 0.0522\n",
      "Epoch 49/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0052 - val_loss: 0.0524\n",
      "Epoch 50/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0053 - val_loss: 0.0526\n",
      "Epoch 51/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0054 - val_loss: 0.0527\n",
      "Epoch 52/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0048 - val_loss: 0.0528\n",
      "Epoch 53/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0055 - val_loss: 0.0531\n",
      "Epoch 54/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0049 - val_loss: 0.0532\n",
      "Epoch 55/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0047 - val_loss: 0.0533\n",
      "Epoch 56/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0047 - val_loss: 0.0533\n",
      "Epoch 57/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0051 - val_loss: 0.0533\n",
      "Epoch 58/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0046 - val_loss: 0.0544\n",
      "Epoch 59/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0047 - val_loss: 0.0540\n",
      "Epoch 60/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0043 - val_loss: 0.0555\n",
      "Epoch 61/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0044 - val_loss: 0.0549\n",
      "Epoch 62/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0045 - val_loss: 0.0556\n",
      "Epoch 63/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0042 - val_loss: 0.0572\n",
      "Epoch 64/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0043 - val_loss: 0.0570\n",
      "Epoch 65/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0042 - val_loss: 0.0579\n",
      "Epoch 66/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0042 - val_loss: 0.0574\n",
      "Epoch 67/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0041 - val_loss: 0.0602\n",
      "Epoch 68/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0039 - val_loss: 0.0599\n",
      "Epoch 69/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0039 - val_loss: 0.0630\n",
      "Epoch 70/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0037 - val_loss: 0.0612\n",
      "Epoch 71/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0040 - val_loss: 0.0663\n",
      "Epoch 72/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0038 - val_loss: 0.0620\n",
      "Epoch 73/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0041 - val_loss: 0.0630\n",
      "Epoch 74/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0038 - val_loss: 0.0645\n",
      "Epoch 75/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0036 - val_loss: 0.0642\n",
      "Epoch 76/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0035 - val_loss: 0.0696\n",
      "Epoch 77/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0039 - val_loss: 0.0679\n",
      "Epoch 78/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0038 - val_loss: 0.0699\n",
      "Epoch 79/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0038 - val_loss: 0.0684\n",
      "Epoch 80/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0035 - val_loss: 0.0664\n",
      "Epoch 81/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0036 - val_loss: 0.0657\n",
      "Epoch 82/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0033 - val_loss: 0.0635\n",
      "Epoch 83/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0040 - val_loss: 0.0619\n",
      "Epoch 84/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0035 - val_loss: 0.0594\n",
      "Epoch 85/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0036 - val_loss: 0.0586\n",
      "Epoch 86/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0036 - val_loss: 0.0560\n",
      "Epoch 87/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0036 - val_loss: 0.0428\n",
      "Epoch 88/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0032 - val_loss: 0.0361\n",
      "Epoch 89/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0034 - val_loss: 0.0451\n",
      "Epoch 90/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0035 - val_loss: 0.0386\n",
      "Epoch 91/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0035 - val_loss: 0.0386\n",
      "Epoch 92/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0034 - val_loss: 0.0310\n",
      "Epoch 93/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0032 - val_loss: 0.0284\n",
      "Epoch 94/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0034 - val_loss: 0.0298\n",
      "Epoch 95/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0033 - val_loss: 0.0297\n",
      "Epoch 96/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0031 - val_loss: 0.0247\n",
      "Epoch 97/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0032 - val_loss: 0.0217\n",
      "Epoch 98/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0029 - val_loss: 0.0238\n",
      "Epoch 99/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0030 - val_loss: 0.0226\n",
      "Epoch 100/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0029 - val_loss: 0.0217\n",
      "Epoch 101/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0028 - val_loss: 0.0184\n",
      "Epoch 102/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0028 - val_loss: 0.0191\n",
      "Epoch 103/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0029 - val_loss: 0.0153\n",
      "Epoch 104/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0028 - val_loss: 0.0130\n",
      "Epoch 105/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0030 - val_loss: 0.0122\n",
      "Epoch 106/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0029 - val_loss: 0.0114\n",
      "Epoch 107/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0027 - val_loss: 0.0124\n",
      "Epoch 108/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0029 - val_loss: 0.0089\n",
      "Epoch 109/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0026 - val_loss: 0.0100\n",
      "Epoch 110/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0094\n",
      "Epoch 111/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0083\n",
      "Epoch 112/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0030 - val_loss: 0.0075\n",
      "Epoch 113/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0082\n",
      "Epoch 114/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0031 - val_loss: 0.0075\n",
      "Epoch 115/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0075\n",
      "Epoch 116/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0075\n",
      "Epoch 117/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0031 - val_loss: 0.0080\n",
      "Epoch 118/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0095\n",
      "Epoch 119/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0072\n",
      "Epoch 120/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0029 - val_loss: 0.0052\n",
      "Epoch 121/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0050\n",
      "Epoch 122/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0051\n",
      "Epoch 123/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0054\n",
      "Epoch 124/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0055\n",
      "Epoch 125/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0041\n",
      "Epoch 126/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0041\n",
      "Epoch 127/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 128/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0026 - val_loss: 0.0035\n",
      "Epoch 129/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0028 - val_loss: 0.0050\n",
      "Epoch 130/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 131/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0028 - val_loss: 0.0073\n",
      "Epoch 132/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0107\n",
      "Epoch 133/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0027 - val_loss: 0.0111\n",
      "Epoch 134/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0025 - val_loss: 0.0156\n",
      "Epoch 135/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0174\n",
      "Epoch 136/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0025 - val_loss: 0.0156\n",
      "Epoch 137/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0081\n",
      "Epoch 138/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0027 - val_loss: 0.0060\n",
      "Epoch 139/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0051\n",
      "Epoch 140/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0025 - val_loss: 0.0064\n",
      "Epoch 141/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0024 - val_loss: 0.0044\n",
      "Epoch 142/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0022 - val_loss: 0.0044\n",
      "Epoch 143/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0025 - val_loss: 0.0029\n",
      "Epoch 144/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0025 - val_loss: 0.0029\n",
      "Epoch 145/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0023 - val_loss: 0.0028\n",
      "Epoch 146/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0025 - val_loss: 0.0028\n",
      "Epoch 147/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0024 - val_loss: 0.0028\n",
      "Epoch 148/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0025 - val_loss: 0.0030\n",
      "Epoch 149/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0023 - val_loss: 0.0029\n",
      "Epoch 150/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0023 - val_loss: 0.0028\n",
      "Epoch 151/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0022 - val_loss: 0.0032\n",
      "Epoch 152/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0023 - val_loss: 0.0026\n",
      "Epoch 153/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0022 - val_loss: 0.0029\n",
      "Epoch 154/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0023 - val_loss: 0.0025\n",
      "Epoch 155/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 156/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 157/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 158/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0023 - val_loss: 0.0025\n",
      "Epoch 159/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0025\n",
      "Epoch 160/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0026\n",
      "Epoch 161/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0026\n",
      "Epoch 162/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0021 - val_loss: 0.0028\n",
      "Epoch 163/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 164/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0022 - val_loss: 0.0026\n",
      "Epoch 165/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0029\n",
      "Epoch 166/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0022 - val_loss: 0.0026\n",
      "Epoch 167/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0023 - val_loss: 0.0026\n",
      "Epoch 168/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0022 - val_loss: 0.0030\n",
      "Epoch 169/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0021 - val_loss: 0.0026\n",
      "Epoch 170/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 171/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0031\n",
      "Epoch 172/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0020 - val_loss: 0.0031\n",
      "Epoch 173/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0025\n",
      "Epoch 174/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0026\n",
      "Epoch 175/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0026\n",
      "Epoch 176/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0027\n",
      "Epoch 177/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0026\n",
      "Epoch 178/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0022 - val_loss: 0.0029\n",
      "Epoch 179/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0020 - val_loss: 0.0030\n",
      "Epoch 180/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0023 - val_loss: 0.0033\n",
      "Epoch 181/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0023 - val_loss: 0.0028\n",
      "Epoch 182/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0021 - val_loss: 0.0025\n",
      "Epoch 183/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 184/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 185/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 186/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0024\n",
      "Epoch 187/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0025\n",
      "Epoch 188/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 189/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0021 - val_loss: 0.0024\n",
      "Epoch 190/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0027\n",
      "Epoch 191/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0021 - val_loss: 0.0030\n",
      "Epoch 192/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0024 - val_loss: 0.0022\n",
      "Epoch 193/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0021 - val_loss: 0.0025\n",
      "Epoch 194/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0021 - val_loss: 0.0026\n",
      "Epoch 195/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0019 - val_loss: 0.0026\n",
      "Epoch 196/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0027\n",
      "Epoch 197/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 198/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0034\n",
      "Epoch 199/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 200/1000\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.0020\n",
      "Epoch 00200: saving model to saved_models/latent8/cp-0200.h5\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 201/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0027\n",
      "Epoch 202/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0027\n",
      "Epoch 203/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0026\n",
      "Epoch 204/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0027\n",
      "Epoch 205/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0020 - val_loss: 0.0025\n",
      "Epoch 206/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0020 - val_loss: 0.0029\n",
      "Epoch 207/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0022 - val_loss: 0.0024\n",
      "Epoch 208/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0020 - val_loss: 0.0030\n",
      "Epoch 209/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0022 - val_loss: 0.0026\n",
      "Epoch 210/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0029\n",
      "Epoch 211/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0019 - val_loss: 0.0026\n",
      "Epoch 212/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0027\n",
      "Epoch 213/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0021 - val_loss: 0.0030\n",
      "Epoch 214/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0019 - val_loss: 0.0038\n",
      "Epoch 215/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0018 - val_loss: 0.0038\n",
      "Epoch 216/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0019 - val_loss: 0.0031\n",
      "Epoch 217/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0019 - val_loss: 0.0027\n",
      "Epoch 218/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0026\n",
      "Epoch 219/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0019 - val_loss: 0.0027\n",
      "Epoch 220/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0019 - val_loss: 0.0023\n",
      "Epoch 221/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0022 - val_loss: 0.0027\n",
      "Epoch 222/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0021 - val_loss: 0.0026\n",
      "Epoch 223/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0020 - val_loss: 0.0027\n",
      "Epoch 224/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0025\n",
      "Epoch 225/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 226/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 227/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 228/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 229/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 230/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 231/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 232/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0044\n",
      "Epoch 233/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0026 - val_loss: 0.0086\n",
      "Epoch 234/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0023 - val_loss: 0.0084\n",
      "Epoch 235/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0023 - val_loss: 0.0101\n",
      "Epoch 236/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0024 - val_loss: 0.0070\n",
      "Epoch 237/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0048\n",
      "Epoch 238/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0048\n",
      "Epoch 239/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0068\n",
      "Epoch 240/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0079\n",
      "Epoch 241/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0079\n",
      "Epoch 242/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0019 - val_loss: 0.0075\n",
      "Epoch 243/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0065\n",
      "Epoch 244/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0018 - val_loss: 0.0046\n",
      "Epoch 245/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0017 - val_loss: 0.0053\n",
      "Epoch 246/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0040\n",
      "Epoch 247/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0025\n",
      "Epoch 248/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0017 - val_loss: 0.0026\n",
      "Epoch 249/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0025\n",
      "Epoch 250/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 251/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 252/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 253/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 254/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 255/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 256/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0025\n",
      "Epoch 257/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 258/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 259/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 260/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0019\n",
      "Epoch 261/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 262/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 263/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 264/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 265/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 266/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 267/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 268/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 269/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 270/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0023\n",
      "Epoch 271/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 272/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 273/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 274/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0024\n",
      "Epoch 275/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0025\n",
      "Epoch 276/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 277/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 278/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 279/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 280/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 281/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0018 - val_loss: 0.0027\n",
      "Epoch 282/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 283/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 284/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 285/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 286/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 287/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 288/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 289/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 290/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 291/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 292/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 293/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 294/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0038\n",
      "Epoch 295/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0042\n",
      "Epoch 296/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0030\n",
      "Epoch 297/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0016 - val_loss: 0.0026\n",
      "Epoch 298/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 299/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 300/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 301/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0027\n",
      "Epoch 302/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 303/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 304/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 305/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 306/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 307/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 308/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 309/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 310/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 311/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 312/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 313/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 314/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 315/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 316/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 317/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 318/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 319/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 320/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 321/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0022\n",
      "Epoch 322/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 323/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0023\n",
      "Epoch 324/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 325/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 326/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 327/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 328/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0024\n",
      "Epoch 329/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 330/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 331/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 332/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 333/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 334/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 335/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 336/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 337/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 338/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0046\n",
      "Epoch 339/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0046\n",
      "Epoch 340/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0046\n",
      "Epoch 341/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0040\n",
      "Epoch 342/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0024\n",
      "Epoch 343/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 344/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0033\n",
      "Epoch 345/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 346/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0027\n",
      "Epoch 347/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 348/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 349/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 350/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 351/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 352/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 353/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0014 - val_loss: 0.0026\n",
      "Epoch 354/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0028\n",
      "Epoch 355/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0048\n",
      "Epoch 356/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0032\n",
      "Epoch 357/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0028\n",
      "Epoch 358/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 359/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 360/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 361/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 362/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 363/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 364/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 365/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 366/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 367/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 368/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 369/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 370/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0031\n",
      "Epoch 371/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0046\n",
      "Epoch 372/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0049\n",
      "Epoch 373/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0031\n",
      "Epoch 374/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 375/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0016 - val_loss: 0.0031\n",
      "Epoch 376/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0087\n",
      "Epoch 377/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0107\n",
      "Epoch 378/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0093\n",
      "Epoch 379/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0043\n",
      "Epoch 380/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0037\n",
      "Epoch 381/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0053\n",
      "Epoch 382/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0018 - val_loss: 0.0056\n",
      "Epoch 383/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0035\n",
      "Epoch 384/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 385/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 386/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 387/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 388/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 389/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 390/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 391/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 392/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 393/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 394/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 395/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 396/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 397/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 398/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 399/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 400/1000\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00400: saving model to saved_models/latent8/cp-0400.h5\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 401/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 402/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 403/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 404/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 405/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 406/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 407/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 408/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 409/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 410/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 411/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 412/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 413/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 414/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 415/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 416/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 417/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 418/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 419/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 420/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 421/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 422/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 423/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 424/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 425/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 426/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 427/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 428/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0022\n",
      "Epoch 429/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 430/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 431/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 432/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 433/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 434/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 435/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 436/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 437/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0030\n",
      "Epoch 438/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 439/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0024\n",
      "Epoch 440/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 441/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 442/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 443/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 444/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 445/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 446/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 447/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 448/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 449/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 450/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 451/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 452/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 453/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 454/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 455/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 456/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 457/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 458/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 459/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 460/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 461/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 462/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 463/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 464/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 465/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0033\n",
      "Epoch 466/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0052\n",
      "Epoch 467/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0022\n",
      "Epoch 468/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0030\n",
      "Epoch 469/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0015 - val_loss: 0.0025\n",
      "Epoch 470/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0013 - val_loss: 0.0034\n",
      "Epoch 471/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0055\n",
      "Epoch 472/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0025\n",
      "Epoch 473/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 474/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 475/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 476/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 477/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0023\n",
      "Epoch 478/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0026\n",
      "Epoch 479/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 480/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 481/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 482/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 483/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 484/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 485/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 486/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 487/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 488/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 489/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 490/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 491/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 492/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 493/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 494/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 495/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 496/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 497/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 498/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 499/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 500/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 501/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 502/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 503/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 504/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 505/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 506/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 507/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 508/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 509/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 510/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 511/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 512/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 513/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 514/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 515/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 516/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 517/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 518/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 519/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 520/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 521/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 522/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 523/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 524/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 525/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 526/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 527/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 528/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 529/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 530/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 531/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 532/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 533/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0020\n",
      "Epoch 534/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 535/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 536/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 537/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 538/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 539/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 540/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 541/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 542/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 543/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 544/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 545/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 546/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0018\n",
      "Epoch 547/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 548/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 549/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 550/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 551/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 552/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0019\n",
      "Epoch 553/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 554/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 555/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0022\n",
      "Epoch 556/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 557/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0013 - val_loss: 0.0027\n",
      "Epoch 558/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 559/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 560/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 561/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 562/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 563/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 564/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0013 - val_loss: 0.0022\n",
      "Epoch 565/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0012 - val_loss: 0.0027\n",
      "Epoch 566/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0035\n",
      "Epoch 567/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0011 - val_loss: 0.0046\n",
      "Epoch 568/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0025\n",
      "Epoch 569/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0023\n",
      "Epoch 570/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 571/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 572/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 573/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 574/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0020\n",
      "Epoch 575/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 576/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 577/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 578/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 579/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 580/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 581/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0010 - val_loss: 0.0015\n",
      "Epoch 582/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 583/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0019\n",
      "Epoch 584/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0022\n",
      "Epoch 585/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0021\n",
      "Epoch 586/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 587/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 588/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0019\n",
      "Epoch 589/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 590/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 591/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 592/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 593/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 594/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 595/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 596/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 597/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0019\n",
      "Epoch 598/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 599/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 600/1000\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.0013\n",
      "Epoch 00600: saving model to saved_models/latent8/cp-0600.h5\n",
      "6/6 [==============================] - 1s 112ms/step - loss: 0.0013 - val_loss: 0.0021\n",
      "Epoch 601/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 602/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 603/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 604/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 605/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 606/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 607/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 608/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 609/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0019\n",
      "Epoch 610/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 611/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0015\n",
      "Epoch 612/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 613/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0018\n",
      "Epoch 614/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0021\n",
      "Epoch 615/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0032\n",
      "Epoch 616/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 617/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0021\n",
      "Epoch 618/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0010 - val_loss: 0.0016\n",
      "Epoch 619/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0020\n",
      "Epoch 620/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0012 - val_loss: 0.0019\n",
      "Epoch 621/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0012 - val_loss: 0.0030\n",
      "Epoch 622/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0019\n",
      "Epoch 623/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0028\n",
      "Epoch 624/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0047\n",
      "Epoch 625/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 626/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 627/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0010 - val_loss: 0.0015\n",
      "Epoch 628/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 629/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 630/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 631/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 632/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 9.8945e-04 - val_loss: 0.0013\n",
      "Epoch 633/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 634/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 635/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 636/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 637/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0020\n",
      "Epoch 638/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0027\n",
      "Epoch 639/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0023\n",
      "Epoch 640/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 641/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 642/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 643/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 644/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 645/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 646/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 647/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 648/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 649/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 650/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 651/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 652/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 653/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0020\n",
      "Epoch 654/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0028\n",
      "Epoch 655/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0018\n",
      "Epoch 656/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 657/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0020\n",
      "Epoch 658/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0019\n",
      "Epoch 659/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0022\n",
      "Epoch 660/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 661/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 662/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 663/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0018\n",
      "Epoch 664/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0024\n",
      "Epoch 665/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0018\n",
      "Epoch 666/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0017\n",
      "Epoch 667/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0019\n",
      "Epoch 668/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0020\n",
      "Epoch 669/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0024\n",
      "Epoch 670/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0015\n",
      "Epoch 671/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 672/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0010 - val_loss: 0.0016\n",
      "Epoch 673/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 674/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0024\n",
      "Epoch 675/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0047\n",
      "Epoch 676/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0047\n",
      "Epoch 677/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0023\n",
      "Epoch 678/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 9.8191e-04 - val_loss: 0.0014\n",
      "Epoch 679/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0021\n",
      "Epoch 680/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0028\n",
      "Epoch 681/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 682/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 683/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 9.7431e-04 - val_loss: 0.0012\n",
      "Epoch 684/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.8944e-04 - val_loss: 0.0015\n",
      "Epoch 685/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0016\n",
      "Epoch 686/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 687/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 688/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 689/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0019\n",
      "Epoch 690/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0010 - val_loss: 0.0021\n",
      "Epoch 691/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 692/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 693/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.9718e-04 - val_loss: 0.0014\n",
      "Epoch 694/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0023\n",
      "Epoch 695/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0027\n",
      "Epoch 696/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0020\n",
      "Epoch 697/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 698/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 699/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 700/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 701/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0029\n",
      "Epoch 702/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0030\n",
      "Epoch 703/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0027\n",
      "Epoch 704/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0046\n",
      "Epoch 705/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0028\n",
      "Epoch 706/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0029\n",
      "Epoch 707/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 708/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0019\n",
      "Epoch 709/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 710/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 711/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 712/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0018\n",
      "Epoch 713/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 714/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 715/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 716/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0010 - val_loss: 0.0060\n",
      "Epoch 717/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0021\n",
      "Epoch 718/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0039\n",
      "Epoch 719/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0010 - val_loss: 0.0025\n",
      "Epoch 720/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 721/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0049\n",
      "Epoch 722/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0019\n",
      "Epoch 723/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0010 - val_loss: 0.0018\n",
      "Epoch 724/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 725/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 726/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 727/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0016\n",
      "Epoch 728/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 729/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 730/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0011 - val_loss: 0.0025\n",
      "Epoch 731/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0022\n",
      "Epoch 732/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 733/1000\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.0011Restoring model weights from the end of the best epoch.\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0011 - val_loss: 0.0020\n",
      "Epoch 00733: early stopping\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train VAE model with callbacks \"\"\"\n",
    "history = ae.fit(trainX, trainX, \n",
    "                  batch_size = batch_size, \n",
    "                  epochs = epochs, \n",
    "                  callbacks=[initial_checkpoint, checkpoint, EarlyStoppingAtMinLoss()],\n",
    "                  #callbacks=[initial_checkpoint, checkpoint],\n",
    "                  shuffle=True,\n",
    "                  validation_data=(valX, valX)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 6.0\n"
     ]
    }
   ],
   "source": [
    "# In this GPU implementation, it shows the number of batches/iterations for one epoch (and not the training samples), which is:\n",
    "print (\"Step size:\", np.ceil(trainX.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save the entire model \"\"\"\n",
    "# Save notes about hyperparameters used:\n",
    "with open(f'{SAVE_FOLDER}/notes.txt', 'w') as f:\n",
    "    f.write(f'Epochs: {epochs} \\n')\n",
    "    f.write(f'Batch size: {batch_size} \\n')\n",
    "    f.write(f'Latent dimension: {latent_dim} \\n')\n",
    "    f.write(f'Filter sizes: {filters} \\n')\n",
    "    f.write(f'img_width: {img_width} \\n')\n",
    "    f.write(f'img_height: {img_height} \\n')\n",
    "    f.write(f'num_channels: {num_channels}')\n",
    "    f.close()\n",
    "\n",
    "# Save encoder weights:\n",
    "encoder.save_weights(f'{SAVE_FOLDER}/ENCODER_WEIGHTS.h5')\n",
    "\n",
    "# Save the total AE model, i.e. its weights:\n",
    "ae.save_weights(f'{SAVE_FOLDER}/AE_WEIGHTS.h5')\n",
    "\n",
    "# Save the history at each epochs (cost of train and validation sets):\n",
    "np.save(f'{SAVE_FOLDER}/HISTORY.npy', history.history)\n",
    "\n",
    "# Save exact training, validation and testing data set (as numpy arrays):\n",
    "np.save(f'{SAVE_FOLDER}/trainX.npy', trainX)\n",
    "np.save(f'{SAVE_FOLDER}/valX.npy', valX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllX.npy', testAllX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllY.npy', testAllY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate training process\n",
    "\n",
    "Now that the training process is complete, let’s evaluate the average loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(13, 6))\n",
    "\n",
    "# summarize history for loss:\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='blue', alpha=0.4, label='validation')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Cost', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.title('Average Loss During Training', fontsize=18)\n",
    "#plt.xticks(range(0,epochs))\n",
    "plt.legend(loc='upper right', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
