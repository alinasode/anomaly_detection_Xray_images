{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import axis, colorbar, imshow, show, figure, subplot\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=18)\n",
    "mpl.rc('xtick', labelsize=16)\n",
    "mpl.rc('ytick', labelsize=16)\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## ----- GPU ------------------------------------\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'    # use of GPU 0 (ERDA) (use before importing torch or tensorflow/keeas)\n",
    "## ----------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization, ReLU, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.activations import relu\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K                         #contains calls for tensor manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL_TRAIN_AUG:       /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTrainAugmentations\n",
      "NORMAL_VALIDATION_AUG:  /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalValidationAugmentations\n",
      "NORMAL_TEST_AUG:        /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTestAugmentations\n",
      "ANOMALY_AUG:            /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/AnomalyAugmentations\n"
     ]
    }
   ],
   "source": [
    "from utils_vae_kitkat_paths import *\n",
    "\n",
    "# Get work directions for augmentated data set:\n",
    "print (\"NORMAL_TRAIN_AUG:      \", NORMAL_TRAIN_AUG)\n",
    "print (\"NORMAL_VALIDATION_AUG: \", NORMAL_VAL_AUG)\n",
    "print (\"NORMAL_TEST_AUG:       \", NORMAL_TEST_AUG)\n",
    "print (\"ANOMALY_AUG:           \", ANOMALY_AUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which Folder The Models Are Saved In: saved_models/latent16\n"
     ]
    }
   ],
   "source": [
    "# What latent dimension and filter size are we using?:\n",
    "latent_dim = 16\n",
    "filters    = 32\n",
    "\n",
    "# Get work direction for saving files:\n",
    "SAVE_FOLDER = f'saved_models/latent{latent_dim}'\n",
    "print (\"Which Folder The Models Are Saved In:\", SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] deleting existing files in folder...\n",
      "         done in 0.041 minutes\n"
     ]
    }
   ],
   "source": [
    "# Delete all existing files in folder where models are saved:\n",
    "print(\"[INFO] deleting existing files in folder...\")\n",
    "t0 = time()\n",
    "\n",
    "files = glob.glob(f'{SAVE_FOLDER}/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "print (\"         done in %0.3f minutes\" % ((time() - t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_VAE_AE import get_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Investigation of Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"normal\" training images is:     1400\n",
      "Number of \"normal\" validation images is:   134\n",
      "Number of \"normal\" test images is:         266\n",
      "Number of \"anomaly\" images is:             600\n"
     ]
    }
   ],
   "source": [
    "# Glob the directories and get the lists of good and bad images\n",
    "good_train_fns = [f for f in os.listdir(NORMAL_TRAIN_AUG) if not f.startswith('.')]\n",
    "good_val_fns = [f for f in os.listdir(NORMAL_VAL_AUG) if not f.startswith('.')]\n",
    "good_test_fns = [f for f in os.listdir(NORMAL_TEST_AUG) if not f.startswith('.')]\n",
    "bad_fns = [f for f in os.listdir(ANOMALY_AUG) if not f.startswith('.')]\n",
    "\n",
    "print('Number of \"normal\" training images is:     {}'.format(len(good_train_fns)))\n",
    "print('Number of \"normal\" validation images is:   {}'.format(len(good_val_fns)))\n",
    "print('Number of \"normal\" test images is:         {}'.format(len(good_test_fns)))\n",
    "print('Number of \"anomaly\" images is:             {}'.format(len(bad_fns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Procentage of normal samples (ground truth):   75.00 %\n",
      "> Procentage of anomaly samples (ground truth):  25.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of bad images vs good images:\n",
    "normal_fns = len(good_train_fns) + len(good_val_fns) + len(good_test_fns)\n",
    "anomaly_fns = len(bad_fns)\n",
    "print(f\"> Procentage of normal samples (ground truth):   {(normal_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")\n",
    "print(f\"> Procentage of anomaly samples (ground truth):  {(anomaly_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Normal Data\n",
    "\n",
    "Load normal samples in pre-splitted data sets: train, valdiation and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal training images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal training images...\")\n",
    "trainX = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TRAIN_AUG}/*.png\")]  # read as grayscale\n",
    "trainX = np.array(trainX)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "trainY = get_labels(trainX, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal validation images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal validation images...\")\n",
    "x_normal_val = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_VAL_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_val = np.array(x_normal_val)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_val = get_labels(x_normal_val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal testing images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal testing images...\")\n",
    "x_normal_test = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TEST_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_test = np.array(x_normal_test)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_test = get_labels(x_normal_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Data\n",
    "\n",
    "\n",
    "Load anomaly samples in its singular training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly images...\")\n",
    "x_anomaly = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY_AUG}/*.png\")]  # read as grayscale\n",
    "x_anomaly = np.array(x_anomaly)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_anomaly = get_labels(x_anomaly, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine normal samples and anomaly samples and split them into training, validation and test sets\n",
    "\n",
    "`label 0` == Normal Samples\n",
    "\n",
    "`label 1` == Anomaly Samples\n",
    "\n",
    "Train and Validation sets only consists of `Normal Data`, aka. `label 0`.\n",
    "\n",
    "Testing sets consists of both  `Normal Data` (aka. `label 0`) and `Anomaly Data` (aka. `label 1`)\n",
    "\n",
    "________________\n",
    "\n",
    "Due to the very sparse original (preprossed) KitKat Chunky data, the validation set will be a mix of normal testing and validation data. The normal testing set will consists of all validation and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testvalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testvalY = get_labels(testvalX, 0)\n",
    "\n",
    "# randomly split in order to make new validation set:\n",
    "NoUsageX, valX, NoUsageY, valY = train_test_split(testvalX, testvalY, test_size=0.25, random_state=4345672)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testNormalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testNormalY = get_labels(testNormalX, 0)\n",
    "\n",
    "# anomaly class (only used for testing):\n",
    "testAnomalyX, testAnomalyY = x_anomaly, y_anomaly\n",
    "\n",
    "# Combine all testing data in one array:\n",
    "testAllX = np.vstack((testNormalX, testAnomalyX))\n",
    "testAllY = np.hstack((testNormalY, testAnomalyY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Dimensions and Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      " > images: (1400, 128, 128)\n",
      " > labels: (1400,)\n",
      "\n",
      "Validation set:\n",
      " > images: (100, 128, 128)\n",
      " > labels: (100,)\n",
      "\n",
      "Test set:\n",
      " > images: (1000, 128, 128)\n",
      " > labels: (1000,)\n",
      "\t'Normal' test set:\n",
      "\t  > images: (400, 128, 128)\n",
      "\t  > labels: (400,)\n",
      "\t'Anomaly' test set:\n",
      "\t  > images: (600, 128, 128)\n",
      "\t  > labels: (600,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "print(\" > images:\", trainX.shape)\n",
    "print(\" > labels:\", trainY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Validation set:\")\n",
    "print(\" > images:\", valX.shape)\n",
    "print(\" > labels:\", valY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Test set:\")\n",
    "print(\" > images:\", testAllX.shape)\n",
    "print(\" > labels:\", testAllY.shape)\n",
    "print(\"\\t'Normal' test set:\")\n",
    "print(\"\\t  > images:\", testNormalX.shape)\n",
    "print(\"\\t  > labels:\", testNormalY.shape)\n",
    "print(\"\\t'Anomaly' test set:\")\n",
    "print(\"\\t  > images:\", testAnomalyX.shape)\n",
    "print(\"\\t  > labels:\", testAnomalyY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 73.68 %\n",
      "Procentage of validation samples: 5.26 %\n",
      "Procentage of (only normal) testing samples: 21.05 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets (only normal data):\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (only normal) testing samples: {(len(testNormalX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 56.00 %\n",
      "Procentage of validation samples: 4.00 %\n",
      "Procentage of (total) testing samples: 40.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets:\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (total) testing samples: {(len(testAllX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for (un)balanced data\n",
    "\n",
    "In an ideal world the data should be balanced. However in the real world we expect there being around ~$99 \\%$ good samples and only ~$1 \\%$ bad samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['0: normal', '1: anomaly']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,4))\n",
    "ax[0].hist(trainY, align=\"left\"); ax[0].set_title('train', fontsize=18); ax[0].set_xticks([0,1]); ax[0].set_xticklabels(labels); ax[0].tick_params(axis='both', which='major', labelsize=16); ax[0].set_xlim(-0.5, 1.5);\n",
    "ax[1].hist(valY, align=\"left\"); ax[1].set_title('validation', fontsize=18); ax[1].set_xticks([0,1]); ax[1].set_xticklabels(labels); ax[1].tick_params(axis='both', which='major', labelsize=16); ax[1].set_xlim(-0.5, 1.5);\n",
    "ax[2].hist(testAllY, align=\"left\"); ax[2].set_title('test', fontsize=18); ax[2].set_xticks([0,1]); ax[2].set_xticklabels(labels); ax[2].tick_params(axis='both', which='major', labelsize=16); ax[2].set_xlim(-0.5, 1.5);\n",
    "\n",
    "ax[0].set_ylabel(\"Number of images\", fontsize=18)\n",
    "ax[1].set_xlabel(\"Class Labels\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of normal test samples: 40.00 %\n",
      "Procentage of total anomaly test samples: 60.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of normal test images vs total anomaly test images:\n",
    "print(f\"Procentage of normal test samples: {(len(testNormalX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of total anomaly test samples: {(len(testAnomalyX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Only normalization has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "img_width, img_height = trainX.shape[1], trainX.shape[2]      # input image dimensions\n",
    "num_channels = 1                                              # gray-channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Neural Networks learns faster with normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to be in  the [0, 1] range:\n",
    "trainX = trainX.astype('float32') / 255.\n",
    "valX = valX.astype('float32') / 255.\n",
    "testAllX = testAllX.astype('float32') / 255.\n",
    "\n",
    "# reshape data to keras inputs --> (n_samples, img_rows, img_cols, n_channels):\n",
    "trainX = trainX.reshape(trainX.shape[0], img_height, img_width, num_channels)\n",
    "valX = valX.reshape(valX.shape[0], img_height, img_width, num_channels)\n",
    "testAllX = testAllX.reshape(testAllX.shape[0], img_height, img_width, num_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import architecture of VAE model and its hyperparameters:\n",
    "from J16_VAE_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Encoder Part*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 32)   320         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 64, 64, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 32)   9248        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 32, 32, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 32)   9248        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 16, 16, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 8, 32)     9248        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 8, 8, 32)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 4, 4, 32)     9248        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 4, 4, 32)     0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "latent (Dense)                  (None, 200)          102600      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 200)          0           latent[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 16)           3216        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_log_mean (Dense)              (None, 16)           3216        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 16)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_mean[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 146,344\n",
      "Trainable params: 146,344\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Decoder Part*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               8704      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 128, 128, 1)       289       \n",
      "=================================================================\n",
      "Total params: 55,233\n",
      "Trainable params: 55,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Total VAE Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         [(None, 16), (None, 16),  146344    \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 128, 128, 1)       55233     \n",
      "=================================================================\n",
      "Total params: 201,577\n",
      "Trainable params: 201,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "batch_size  = 256\n",
    "epochs      = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at 1'st epoch (inital model)\n",
    "\n",
    "https://stackoverflow.com/questions/54323960/save-keras-model-at-specific-epochs\n",
    "\"\"\"\n",
    "\n",
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 1:\n",
    "            self.model.save_weights(f\"{SAVE_FOLDER}/cp-0001.h5\")\n",
    "            \n",
    "# create and use callback:\n",
    "initial_checkpoint = CustomSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at every k'te epochs\n",
    "\"\"\"\n",
    "# Include the epoch in the file name (uses `str.format`):\n",
    "checkpoint_path = \"saved_models/latent16/cp-{epoch:04d}.h5\"\n",
    "\n",
    "# Create a callback that saves the model's weights every k'te epochs:\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path,\n",
    "                             monitor='loss',           # name of the metrics to monitor\n",
    "                             verbose=1, \n",
    "                             #save_best_only=False,     # if False, the best model will not be overridden.\n",
    "                             save_weights_only=True,   # if True, only the weights of the models will be saved.\n",
    "                                                        # if False, the whole models will be saved.\n",
    "                             #mode='auto', \n",
    "                             period=200                  # save after every k'te epochs\n",
    "                            )\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "vae.save_weights(checkpoint_path.format(epoch=0))          # save for zero epoch (inital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: Early stopping\n",
    "https://www.tensorflow.org/guide/keras/custom_callback\n",
    "\"\"\"\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=100):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create and Compile Model\"\"\"\n",
    "# import architecture of VAE model and its hyperparameters. learning rate choosen from graph above.\n",
    "vae = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "6/6 [==============================] - 1s 207ms/step - loss: 3524.4905 - val_loss: 3429.9050\n",
      "Epoch 2/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 3219.0876 - val_loss: 2803.4014\n",
      "Epoch 3/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 2111.6321 - val_loss: 1220.5630\n",
      "Epoch 4/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 960.8035 - val_loss: 917.4836\n",
      "Epoch 5/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 829.6448 - val_loss: 892.2521\n",
      "Epoch 6/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 807.0098 - val_loss: 842.4833\n",
      "Epoch 7/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 736.6832 - val_loss: 760.3042\n",
      "Epoch 8/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 658.6260 - val_loss: 687.1044\n",
      "Epoch 9/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 612.3138 - val_loss: 651.0820\n",
      "Epoch 10/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 593.9975 - val_loss: 638.4211\n",
      "Epoch 11/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 582.1785 - val_loss: 627.3613\n",
      "Epoch 12/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 571.9207 - val_loss: 620.0364\n",
      "Epoch 13/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 566.2129 - val_loss: 614.6485\n",
      "Epoch 14/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 562.7476 - val_loss: 610.3353\n",
      "Epoch 15/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 557.5455 - val_loss: 607.1711\n",
      "Epoch 16/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 554.0788 - val_loss: 605.1961\n",
      "Epoch 17/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 551.9753 - val_loss: 602.1315\n",
      "Epoch 18/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 549.3448 - val_loss: 599.4196\n",
      "Epoch 19/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 545.9020 - val_loss: 596.5641\n",
      "Epoch 20/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 543.3092 - val_loss: 593.7719\n",
      "Epoch 21/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 542.2061 - val_loss: 589.3118\n",
      "Epoch 22/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 536.7767 - val_loss: 579.6295\n",
      "Epoch 23/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 523.4345 - val_loss: 549.4465\n",
      "Epoch 24/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 490.2036 - val_loss: 500.7480\n",
      "Epoch 25/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 410.1461 - val_loss: 399.2538\n",
      "Epoch 26/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 345.4526 - val_loss: 357.6901\n",
      "Epoch 27/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 311.3456 - val_loss: 327.4716\n",
      "Epoch 28/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 291.4031 - val_loss: 309.6458\n",
      "Epoch 29/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 277.3768 - val_loss: 295.3735\n",
      "Epoch 30/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 265.6342 - val_loss: 285.6148\n",
      "Epoch 31/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 256.3795 - val_loss: 271.4618\n",
      "Epoch 32/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 247.7762 - val_loss: 265.5214\n",
      "Epoch 33/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 239.5751 - val_loss: 254.4663\n",
      "Epoch 34/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 231.8906 - val_loss: 245.6023\n",
      "Epoch 35/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 225.6815 - val_loss: 238.1261\n",
      "Epoch 36/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 218.4046 - val_loss: 231.1481\n",
      "Epoch 37/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 212.8300 - val_loss: 222.6140\n",
      "Epoch 38/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 205.3335 - val_loss: 213.6131\n",
      "Epoch 39/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 196.6616 - val_loss: 206.3585\n",
      "Epoch 40/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 186.3293 - val_loss: 197.4448\n",
      "Epoch 41/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 175.3831 - val_loss: 189.6567\n",
      "Epoch 42/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 168.5159 - val_loss: 181.3436\n",
      "Epoch 43/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 159.9997 - val_loss: 176.1081\n",
      "Epoch 44/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 154.9960 - val_loss: 170.1308\n",
      "Epoch 45/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 150.0958 - val_loss: 168.7100\n",
      "Epoch 46/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 146.8032 - val_loss: 163.9432\n",
      "Epoch 47/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 143.7852 - val_loss: 160.2358\n",
      "Epoch 48/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 140.9201 - val_loss: 157.5318\n",
      "Epoch 49/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 138.5650 - val_loss: 156.6319\n",
      "Epoch 50/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 136.2015 - val_loss: 156.2032\n",
      "Epoch 51/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 134.8066 - val_loss: 153.8733\n",
      "Epoch 52/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 134.9684 - val_loss: 151.5472\n",
      "Epoch 53/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 133.5156 - val_loss: 151.1639\n",
      "Epoch 54/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 131.3103 - val_loss: 149.3024\n",
      "Epoch 55/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 129.8058 - val_loss: 148.9547\n",
      "Epoch 56/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 129.8957 - val_loss: 149.4713\n",
      "Epoch 57/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 129.7007 - val_loss: 150.8188\n",
      "Epoch 58/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 128.6978 - val_loss: 144.2587\n",
      "Epoch 59/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 124.9273 - val_loss: 141.5001\n",
      "Epoch 60/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 123.2374 - val_loss: 141.7214\n",
      "Epoch 61/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 122.5694 - val_loss: 140.3703\n",
      "Epoch 62/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 121.4356 - val_loss: 138.7640\n",
      "Epoch 63/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 120.2722 - val_loss: 137.3413\n",
      "Epoch 64/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 118.8985 - val_loss: 136.3842\n",
      "Epoch 65/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 119.4046 - val_loss: 139.8063\n",
      "Epoch 66/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 118.9176 - val_loss: 134.1790\n",
      "Epoch 67/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 117.3395 - val_loss: 135.4064\n",
      "Epoch 68/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 116.1140 - val_loss: 132.8165\n",
      "Epoch 69/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 115.9769 - val_loss: 129.7867\n",
      "Epoch 70/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 113.7794 - val_loss: 134.4250\n",
      "Epoch 71/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 113.7900 - val_loss: 129.3445\n",
      "Epoch 72/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 112.1025 - val_loss: 128.7690\n",
      "Epoch 73/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 111.4329 - val_loss: 129.8014\n",
      "Epoch 74/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 111.7599 - val_loss: 126.0726\n",
      "Epoch 75/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 110.3010 - val_loss: 125.4173\n",
      "Epoch 76/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 108.7515 - val_loss: 126.1572\n",
      "Epoch 77/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 108.0537 - val_loss: 124.0370\n",
      "Epoch 78/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 107.0324 - val_loss: 125.5250\n",
      "Epoch 79/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 107.8921 - val_loss: 121.3896\n",
      "Epoch 80/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 105.7880 - val_loss: 122.6112\n",
      "Epoch 81/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 104.5794 - val_loss: 122.5419\n",
      "Epoch 82/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 104.4591 - val_loss: 121.2184\n",
      "Epoch 83/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 103.1861 - val_loss: 120.2222\n",
      "Epoch 84/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 102.4520 - val_loss: 120.2025\n",
      "Epoch 85/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 102.0047 - val_loss: 120.2022\n",
      "Epoch 86/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 101.5145 - val_loss: 118.1437\n",
      "Epoch 87/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 101.1422 - val_loss: 118.9288\n",
      "Epoch 88/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 101.1234 - val_loss: 119.8905\n",
      "Epoch 89/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 100.5726 - val_loss: 116.0874\n",
      "Epoch 90/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 99.8847 - val_loss: 117.1641\n",
      "Epoch 91/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 99.9487 - val_loss: 115.9106\n",
      "Epoch 92/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 99.3692 - val_loss: 117.7359\n",
      "Epoch 93/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 98.6477 - val_loss: 115.4241\n",
      "Epoch 94/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 98.2474 - val_loss: 114.8770\n",
      "Epoch 95/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 98.0380 - val_loss: 113.9040\n",
      "Epoch 96/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 97.7082 - val_loss: 113.2803\n",
      "Epoch 97/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 96.7650 - val_loss: 114.6800\n",
      "Epoch 98/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 96.6509 - val_loss: 114.7719\n",
      "Epoch 99/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 96.3784 - val_loss: 112.3616\n",
      "Epoch 100/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 96.4762 - val_loss: 112.9814\n",
      "Epoch 101/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 96.1616 - val_loss: 109.2478\n",
      "Epoch 102/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 95.2721 - val_loss: 110.4413\n",
      "Epoch 103/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 94.9121 - val_loss: 115.5511\n",
      "Epoch 104/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 96.5546 - val_loss: 110.4475\n",
      "Epoch 105/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 94.2435 - val_loss: 111.4087\n",
      "Epoch 106/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 93.1764 - val_loss: 111.1599\n",
      "Epoch 107/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 93.0059 - val_loss: 108.0757\n",
      "Epoch 108/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 91.8852 - val_loss: 106.7378\n",
      "Epoch 109/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 90.8595 - val_loss: 107.8337\n",
      "Epoch 110/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 90.8796 - val_loss: 105.4944\n",
      "Epoch 111/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 90.2112 - val_loss: 104.1543\n",
      "Epoch 112/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 89.7604 - val_loss: 105.7629\n",
      "Epoch 113/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 89.6957 - val_loss: 104.2816\n",
      "Epoch 114/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 89.6262 - val_loss: 104.0507\n",
      "Epoch 115/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 88.6718 - val_loss: 104.0780\n",
      "Epoch 116/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 88.6027 - val_loss: 105.5338\n",
      "Epoch 117/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 87.8520 - val_loss: 103.0385\n",
      "Epoch 118/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 87.4231 - val_loss: 103.4400\n",
      "Epoch 119/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 87.0828 - val_loss: 102.6937\n",
      "Epoch 120/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 87.0276 - val_loss: 102.3398\n",
      "Epoch 121/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 86.5443 - val_loss: 101.9310\n",
      "Epoch 122/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 86.1004 - val_loss: 102.3525\n",
      "Epoch 123/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 86.4118 - val_loss: 101.6837\n",
      "Epoch 124/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 85.4197 - val_loss: 101.3319\n",
      "Epoch 125/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 85.1854 - val_loss: 100.6020\n",
      "Epoch 126/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 84.9364 - val_loss: 100.9012\n",
      "Epoch 127/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 84.0426 - val_loss: 99.2544\n",
      "Epoch 128/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 83.8558 - val_loss: 98.6472\n",
      "Epoch 129/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 83.7745 - val_loss: 98.8648\n",
      "Epoch 130/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 83.7465 - val_loss: 99.7087\n",
      "Epoch 131/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 83.6568 - val_loss: 97.3793\n",
      "Epoch 132/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 82.8429 - val_loss: 98.7103\n",
      "Epoch 133/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 83.1943 - val_loss: 98.8889\n",
      "Epoch 134/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 83.0753 - val_loss: 96.7822\n",
      "Epoch 135/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 82.4389 - val_loss: 96.4897\n",
      "Epoch 136/2000\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 81.3821 - val_loss: 95.3400\n",
      "Epoch 137/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 81.8201 - val_loss: 96.8901\n",
      "Epoch 138/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 81.7399 - val_loss: 95.5107\n",
      "Epoch 139/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 81.0814 - val_loss: 94.6447\n",
      "Epoch 140/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 80.3859 - val_loss: 94.8733\n",
      "Epoch 141/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 80.0513 - val_loss: 93.6551\n",
      "Epoch 142/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 79.4016 - val_loss: 93.8752\n",
      "Epoch 143/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 79.2112 - val_loss: 93.1609\n",
      "Epoch 144/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 78.8059 - val_loss: 94.2680\n",
      "Epoch 145/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 78.9925 - val_loss: 92.3941\n",
      "Epoch 146/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 78.1024 - val_loss: 92.7701\n",
      "Epoch 147/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 77.7061 - val_loss: 92.1877\n",
      "Epoch 148/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 77.5611 - val_loss: 92.1984\n",
      "Epoch 149/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 76.8971 - val_loss: 91.5676\n",
      "Epoch 150/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 77.0057 - val_loss: 91.6947\n",
      "Epoch 151/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 77.3076 - val_loss: 91.1059\n",
      "Epoch 152/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 76.7956 - val_loss: 91.3459\n",
      "Epoch 153/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 76.7835 - val_loss: 89.6499\n",
      "Epoch 154/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 76.4012 - val_loss: 90.2012\n",
      "Epoch 155/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 76.3428 - val_loss: 88.8283\n",
      "Epoch 156/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 75.9876 - val_loss: 89.1410\n",
      "Epoch 157/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 75.3782 - val_loss: 90.2949\n",
      "Epoch 158/2000\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 75.2114 - val_loss: 89.2152\n",
      "Epoch 159/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 75.1299 - val_loss: 86.9507\n",
      "Epoch 160/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 74.3425 - val_loss: 87.2167\n",
      "Epoch 161/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 73.9193 - val_loss: 86.8654\n",
      "Epoch 162/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 71.8082 - val_loss: 83.5899\n",
      "Epoch 163/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 68.9713 - val_loss: 78.0570\n",
      "Epoch 164/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 64.4366 - val_loss: 71.3437\n",
      "Epoch 165/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 59.3911 - val_loss: 64.2980\n",
      "Epoch 166/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 55.2043 - val_loss: 59.3187\n",
      "Epoch 167/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 52.0804 - val_loss: 55.4659\n",
      "Epoch 168/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 51.0768 - val_loss: 53.1996\n",
      "Epoch 169/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.4571 - val_loss: 50.1349\n",
      "Epoch 170/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 49.9147 - val_loss: 50.5556\n",
      "Epoch 171/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.5318 - val_loss: 49.8760\n",
      "Epoch 172/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.0282 - val_loss: 49.6970\n",
      "Epoch 173/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 49.1515 - val_loss: 50.3292\n",
      "Epoch 174/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 48.5954 - val_loss: 49.6647\n",
      "Epoch 175/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.8706 - val_loss: 51.0176\n",
      "Epoch 176/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.6207 - val_loss: 51.4898\n",
      "Epoch 177/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.9510 - val_loss: 50.8393\n",
      "Epoch 178/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.4864 - val_loss: 50.6535\n",
      "Epoch 179/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.5626 - val_loss: 51.4906\n",
      "Epoch 180/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.8793 - val_loss: 51.3192\n",
      "Epoch 181/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.1900 - val_loss: 51.7156\n",
      "Epoch 182/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.9444 - val_loss: 49.6013\n",
      "Epoch 183/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 48.3383 - val_loss: 50.4149\n",
      "Epoch 184/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.8484 - val_loss: 47.8940\n",
      "Epoch 185/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.1278 - val_loss: 48.1275\n",
      "Epoch 186/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 46.7897 - val_loss: 48.1454\n",
      "Epoch 187/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.5501 - val_loss: 48.2680\n",
      "Epoch 188/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.0181 - val_loss: 48.6853\n",
      "Epoch 189/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.8466 - val_loss: 48.4219\n",
      "Epoch 190/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.1283 - val_loss: 47.2564\n",
      "Epoch 191/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.2099 - val_loss: 49.2641\n",
      "Epoch 192/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.4306 - val_loss: 47.4941\n",
      "Epoch 193/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.0522 - val_loss: 46.7663\n",
      "Epoch 194/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 45.6034 - val_loss: 47.0062\n",
      "Epoch 195/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 45.8464 - val_loss: 48.3421\n",
      "Epoch 196/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.7521 - val_loss: 47.5108\n",
      "Epoch 197/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.7039 - val_loss: 48.1613\n",
      "Epoch 198/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 44.9732 - val_loss: 45.0669\n",
      "Epoch 199/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 44.3850 - val_loss: 45.6978\n",
      "Epoch 200/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 44.7204\n",
      "Epoch 00200: saving model to saved_models/latent16/cp-0200.h5\n",
      "6/6 [==============================] - 1s 173ms/step - loss: 44.7204 - val_loss: 46.2474\n",
      "Epoch 201/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 44.1272 - val_loss: 45.5911\n",
      "Epoch 202/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.6373 - val_loss: 46.3006\n",
      "Epoch 203/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.2090 - val_loss: 45.5326\n",
      "Epoch 204/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 43.8183 - val_loss: 44.1918\n",
      "Epoch 205/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 43.5925 - val_loss: 44.5170\n",
      "Epoch 206/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 43.2334 - val_loss: 45.0696\n",
      "Epoch 207/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.5945 - val_loss: 44.1780\n",
      "Epoch 208/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 42.8930 - val_loss: 43.6377\n",
      "Epoch 209/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 42.2815 - val_loss: 44.4218\n",
      "Epoch 210/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.0414 - val_loss: 46.7995\n",
      "Epoch 211/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.8928 - val_loss: 44.6767\n",
      "Epoch 212/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 42.7190 - val_loss: 43.7202\n",
      "Epoch 213/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 42.1166 - val_loss: 44.7976\n",
      "Epoch 214/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 41.6932 - val_loss: 44.9629\n",
      "Epoch 215/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 41.5380 - val_loss: 43.6833\n",
      "Epoch 216/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 41.1541 - val_loss: 44.1151\n",
      "Epoch 217/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 41.2695 - val_loss: 43.1517\n",
      "Epoch 218/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 41.1017 - val_loss: 42.2957\n",
      "Epoch 219/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 40.8742 - val_loss: 42.9469\n",
      "Epoch 220/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 40.6900 - val_loss: 43.2277\n",
      "Epoch 221/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 40.4498 - val_loss: 43.7343\n",
      "Epoch 222/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.7617 - val_loss: 42.9231\n",
      "Epoch 223/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 40.9417 - val_loss: 42.1402\n",
      "Epoch 224/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 40.1514 - val_loss: 41.1348\n",
      "Epoch 225/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 39.7607 - val_loss: 42.7677\n",
      "Epoch 226/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.9736 - val_loss: 42.6023\n",
      "Epoch 227/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 40.0979 - val_loss: 42.5018\n",
      "Epoch 228/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 39.7960 - val_loss: 40.4362\n",
      "Epoch 229/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 39.0362 - val_loss: 42.3381\n",
      "Epoch 230/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.6514 - val_loss: 42.2067\n",
      "Epoch 231/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 39.7391 - val_loss: 40.6581\n",
      "Epoch 232/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.0336 - val_loss: 41.5458\n",
      "Epoch 233/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.0650 - val_loss: 40.1580\n",
      "Epoch 234/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 38.7498 - val_loss: 40.2300\n",
      "Epoch 235/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 39.1443 - val_loss: 40.7068\n",
      "Epoch 236/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.7594 - val_loss: 40.6441\n",
      "Epoch 237/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 38.6608 - val_loss: 41.2188\n",
      "Epoch 238/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 38.7738 - val_loss: 40.0882\n",
      "Epoch 239/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.5537 - val_loss: 41.8515\n",
      "Epoch 240/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 39.0914 - val_loss: 40.9077\n",
      "Epoch 241/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 38.5428 - val_loss: 41.5372\n",
      "Epoch 242/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.8098 - val_loss: 40.7905\n",
      "Epoch 243/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 38.8888 - val_loss: 40.8067\n",
      "Epoch 244/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.7162 - val_loss: 42.2508\n",
      "Epoch 245/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.9673 - val_loss: 41.8202\n",
      "Epoch 246/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.5728 - val_loss: 41.6709\n",
      "Epoch 247/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 38.0643 - val_loss: 39.7426\n",
      "Epoch 248/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 37.8836 - val_loss: 39.9335\n",
      "Epoch 249/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 37.7859 - val_loss: 40.2200\n",
      "Epoch 250/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.5397 - val_loss: 41.0134\n",
      "Epoch 251/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.9560 - val_loss: 39.7911\n",
      "Epoch 252/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.7372 - val_loss: 37.4584\n",
      "Epoch 253/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.3977 - val_loss: 40.3094\n",
      "Epoch 254/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.3697 - val_loss: 39.4328\n",
      "Epoch 255/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.9813 - val_loss: 38.8927\n",
      "Epoch 256/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.9082 - val_loss: 39.4106\n",
      "Epoch 257/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.1230 - val_loss: 39.7715\n",
      "Epoch 258/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.9849 - val_loss: 39.3825\n",
      "Epoch 259/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.8056 - val_loss: 38.7377\n",
      "Epoch 260/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 36.7221 - val_loss: 39.5360\n",
      "Epoch 261/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 36.6217 - val_loss: 39.6395\n",
      "Epoch 262/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.2161 - val_loss: 38.2396\n",
      "Epoch 263/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.7528 - val_loss: 38.7611\n",
      "Epoch 264/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.6872 - val_loss: 40.0090\n",
      "Epoch 265/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.5768 - val_loss: 39.0232\n",
      "Epoch 266/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.3790 - val_loss: 39.7855\n",
      "Epoch 267/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.5154 - val_loss: 39.6221\n",
      "Epoch 268/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.1901 - val_loss: 38.0232\n",
      "Epoch 269/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.0194 - val_loss: 38.5464\n",
      "Epoch 270/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.1098 - val_loss: 38.7076\n",
      "Epoch 271/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.8630 - val_loss: 37.5791\n",
      "Epoch 272/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.0582 - val_loss: 38.5809\n",
      "Epoch 273/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.9195 - val_loss: 38.6467\n",
      "Epoch 274/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.7972 - val_loss: 39.7199\n",
      "Epoch 275/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.8945 - val_loss: 38.1908\n",
      "Epoch 276/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.0378 - val_loss: 38.2013\n",
      "Epoch 277/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.8081 - val_loss: 37.9582\n",
      "Epoch 278/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.9545 - val_loss: 37.8270\n",
      "Epoch 279/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 35.6361 - val_loss: 38.1563\n",
      "Epoch 280/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.8215 - val_loss: 37.7896\n",
      "Epoch 281/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.6505 - val_loss: 37.8623\n",
      "Epoch 282/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.3057 - val_loss: 37.2219\n",
      "Epoch 283/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.1364 - val_loss: 37.2835\n",
      "Epoch 284/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.9903 - val_loss: 36.4124\n",
      "Epoch 285/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.0737 - val_loss: 37.0159\n",
      "Epoch 286/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.2250 - val_loss: 37.8461\n",
      "Epoch 287/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.4810 - val_loss: 38.1460\n",
      "Epoch 288/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.4078 - val_loss: 38.6536\n",
      "Epoch 289/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.1104 - val_loss: 37.6419\n",
      "Epoch 290/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 34.6944 - val_loss: 36.7297\n",
      "Epoch 291/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 34.3602 - val_loss: 37.0713\n",
      "Epoch 292/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.7604 - val_loss: 38.1640\n",
      "Epoch 293/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.5961 - val_loss: 36.6241\n",
      "Epoch 294/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.0681 - val_loss: 37.2965\n",
      "Epoch 295/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.0169 - val_loss: 37.2311\n",
      "Epoch 296/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.7087 - val_loss: 37.6555\n",
      "Epoch 297/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.6483 - val_loss: 37.3264\n",
      "Epoch 298/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.6070 - val_loss: 35.8581\n",
      "Epoch 299/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.6668 - val_loss: 37.4784\n",
      "Epoch 300/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.6500 - val_loss: 36.5094\n",
      "Epoch 301/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.2189 - val_loss: 36.9732\n",
      "Epoch 302/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.4266 - val_loss: 37.5699\n",
      "Epoch 303/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.5552 - val_loss: 35.9850\n",
      "Epoch 304/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.2883 - val_loss: 35.8375\n",
      "Epoch 305/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.5504 - val_loss: 37.0526\n",
      "Epoch 306/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.6496 - val_loss: 37.2469\n",
      "Epoch 307/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.5239 - val_loss: 37.1949\n",
      "Epoch 308/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.2452 - val_loss: 38.4706\n",
      "Epoch 309/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.6899 - val_loss: 36.5774\n",
      "Epoch 310/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.6088 - val_loss: 36.7676\n",
      "Epoch 311/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.1818 - val_loss: 36.2395\n",
      "Epoch 312/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.4150 - val_loss: 36.5318\n",
      "Epoch 313/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.4057 - val_loss: 37.2589\n",
      "Epoch 314/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.0555 - val_loss: 35.8978\n",
      "Epoch 315/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 33.8610 - val_loss: 36.0757\n",
      "Epoch 316/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.8296 - val_loss: 36.5310\n",
      "Epoch 317/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.1829 - val_loss: 37.7939\n",
      "Epoch 318/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.7146 - val_loss: 35.4777\n",
      "Epoch 319/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.0734 - val_loss: 37.0076\n",
      "Epoch 320/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.4833 - val_loss: 37.4846\n",
      "Epoch 321/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.7281 - val_loss: 35.5042\n",
      "Epoch 322/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.4668 - val_loss: 35.6615\n",
      "Epoch 323/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.1688 - val_loss: 35.4639\n",
      "Epoch 324/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.3415 - val_loss: 36.3312\n",
      "Epoch 325/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.5714 - val_loss: 35.2788\n",
      "Epoch 326/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.3132 - val_loss: 35.1360\n",
      "Epoch 327/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 32.8821 - val_loss: 34.7293\n",
      "Epoch 328/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.2061 - val_loss: 35.7895\n",
      "Epoch 329/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.2978 - val_loss: 35.6841\n",
      "Epoch 330/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.4451 - val_loss: 36.1076\n",
      "Epoch 331/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.2832 - val_loss: 34.9000\n",
      "Epoch 332/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.2970 - val_loss: 35.0772\n",
      "Epoch 333/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.5436 - val_loss: 36.4139\n",
      "Epoch 334/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.5966 - val_loss: 35.8176\n",
      "Epoch 335/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.2555 - val_loss: 35.0231\n",
      "Epoch 336/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.9046 - val_loss: 35.3436\n",
      "Epoch 337/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.1225 - val_loss: 35.2777\n",
      "Epoch 338/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.7010 - val_loss: 34.8208\n",
      "Epoch 339/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.6661 - val_loss: 34.1948\n",
      "Epoch 340/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.5168 - val_loss: 36.2942\n",
      "Epoch 341/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.0647 - val_loss: 34.7705\n",
      "Epoch 342/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.7412 - val_loss: 35.5191\n",
      "Epoch 343/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.9531 - val_loss: 34.5467\n",
      "Epoch 344/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 32.3815 - val_loss: 35.4695\n",
      "Epoch 345/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.9256 - val_loss: 35.6959\n",
      "Epoch 346/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.2933 - val_loss: 36.4628\n",
      "Epoch 347/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.3855 - val_loss: 36.4484\n",
      "Epoch 348/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.1810 - val_loss: 35.0812\n",
      "Epoch 349/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.8847 - val_loss: 34.4403\n",
      "Epoch 350/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.9117 - val_loss: 34.6231\n",
      "Epoch 351/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.7398 - val_loss: 35.1966\n",
      "Epoch 352/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.6304 - val_loss: 34.7824\n",
      "Epoch 353/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.8333 - val_loss: 35.4378\n",
      "Epoch 354/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.9129 - val_loss: 35.8931\n",
      "Epoch 355/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.3730 - val_loss: 34.4012\n",
      "Epoch 356/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.2600 - val_loss: 34.4689\n",
      "Epoch 357/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.3522 - val_loss: 34.0896\n",
      "Epoch 358/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.2727 - val_loss: 34.5215\n",
      "Epoch 359/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.4185 - val_loss: 34.0870\n",
      "Epoch 360/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 32.0287 - val_loss: 33.7191\n",
      "Epoch 361/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.2231 - val_loss: 34.1569\n",
      "Epoch 362/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.4840 - val_loss: 34.7665\n",
      "Epoch 363/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.5060 - val_loss: 33.6175\n",
      "Epoch 364/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.2538 - val_loss: 34.1799\n",
      "Epoch 365/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.0772 - val_loss: 34.5001\n",
      "Epoch 366/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.1415 - val_loss: 35.3184\n",
      "Epoch 367/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.9720 - val_loss: 34.3884\n",
      "Epoch 368/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.7057 - val_loss: 34.0034\n",
      "Epoch 369/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 31.5839 - val_loss: 34.7376\n",
      "Epoch 370/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.2534 - val_loss: 33.6398\n",
      "Epoch 371/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.0147 - val_loss: 34.5662\n",
      "Epoch 372/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.3673 - val_loss: 34.3784\n",
      "Epoch 373/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.1162 - val_loss: 34.3459\n",
      "Epoch 374/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.9104 - val_loss: 33.9353\n",
      "Epoch 375/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.9354 - val_loss: 35.5889\n",
      "Epoch 376/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.0343 - val_loss: 35.1759\n",
      "Epoch 377/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.0915 - val_loss: 33.2419\n",
      "Epoch 378/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.7901 - val_loss: 34.0532\n",
      "Epoch 379/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.1278 - val_loss: 35.3884\n",
      "Epoch 380/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.4785 - val_loss: 34.0544\n",
      "Epoch 381/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.9448 - val_loss: 34.9186\n",
      "Epoch 382/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.2257 - val_loss: 34.2776\n",
      "Epoch 383/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.0339 - val_loss: 34.1334\n",
      "Epoch 384/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.9056 - val_loss: 34.6694\n",
      "Epoch 385/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.9330 - val_loss: 33.7413\n",
      "Epoch 386/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.1304 - val_loss: 35.1993\n",
      "Epoch 387/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.3690 - val_loss: 35.6171\n",
      "Epoch 388/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.3108 - val_loss: 34.3850\n",
      "Epoch 389/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.8683 - val_loss: 33.0399\n",
      "Epoch 390/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.7957 - val_loss: 33.5640\n",
      "Epoch 391/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.8225 - val_loss: 33.4359\n",
      "Epoch 392/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.7880 - val_loss: 33.7005\n",
      "Epoch 393/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.2469 - val_loss: 34.1112\n",
      "Epoch 394/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.5018 - val_loss: 33.9316\n",
      "Epoch 395/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.5619 - val_loss: 34.2837\n",
      "Epoch 396/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.3353 - val_loss: 33.7382\n",
      "Epoch 397/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.2899 - val_loss: 34.0148\n",
      "Epoch 398/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.5514 - val_loss: 33.6767\n",
      "Epoch 399/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.5592 - val_loss: 34.5757\n",
      "Epoch 400/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 31.4530\n",
      "Epoch 00400: saving model to saved_models/latent16/cp-0400.h5\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 31.4530 - val_loss: 33.5131\n",
      "Epoch 401/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.9944 - val_loss: 34.3128\n",
      "Epoch 402/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.7539 - val_loss: 33.0579\n",
      "Epoch 403/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.5226 - val_loss: 34.2013\n",
      "Epoch 404/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.6263 - val_loss: 33.5137\n",
      "Epoch 405/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.2946 - val_loss: 34.0724\n",
      "Epoch 406/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.7412 - val_loss: 33.0009\n",
      "Epoch 407/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.2859 - val_loss: 34.0424\n",
      "Epoch 408/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.4783 - val_loss: 33.9852\n",
      "Epoch 409/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.4045 - val_loss: 34.4025\n",
      "Epoch 410/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.3234 - val_loss: 34.2376\n",
      "Epoch 411/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.7204 - val_loss: 33.5862\n",
      "Epoch 412/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.6581 - val_loss: 32.9609\n",
      "Epoch 413/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.5441 - val_loss: 33.0495\n",
      "Epoch 414/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 31.2295 - val_loss: 33.4982\n",
      "Epoch 415/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.9170 - val_loss: 34.0036\n",
      "Epoch 416/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.1080 - val_loss: 32.7644\n",
      "Epoch 417/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.1196 - val_loss: 33.3567\n",
      "Epoch 418/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 30.8165 - val_loss: 32.4348\n",
      "Epoch 419/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.8153 - val_loss: 33.9190\n",
      "Epoch 420/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.2674 - val_loss: 33.8519\n",
      "Epoch 421/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.0980 - val_loss: 33.4856\n",
      "Epoch 422/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.0673 - val_loss: 33.5881\n",
      "Epoch 423/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.2447 - val_loss: 33.5271\n",
      "Epoch 424/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.7627 - val_loss: 32.1754\n",
      "Epoch 425/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.4223 - val_loss: 33.3138\n",
      "Epoch 426/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.0261 - val_loss: 33.1036\n",
      "Epoch 427/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.8364 - val_loss: 33.1462\n",
      "Epoch 428/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.5889 - val_loss: 32.8120\n",
      "Epoch 429/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.3871 - val_loss: 34.0875\n",
      "Epoch 430/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.7928 - val_loss: 33.2337\n",
      "Epoch 431/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.7881 - val_loss: 32.5018\n",
      "Epoch 432/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.8376 - val_loss: 33.4490\n",
      "Epoch 433/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.8225 - val_loss: 32.9475\n",
      "Epoch 434/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.6918 - val_loss: 32.7026\n",
      "Epoch 435/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.4787 - val_loss: 33.1854\n",
      "Epoch 436/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.6158 - val_loss: 32.3642\n",
      "Epoch 437/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.5522 - val_loss: 32.8013\n",
      "Epoch 438/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.5971 - val_loss: 32.7519\n",
      "Epoch 439/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.5864 - val_loss: 33.5105\n",
      "Epoch 440/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.6360 - val_loss: 33.1615\n",
      "Epoch 441/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.7405 - val_loss: 32.4979\n",
      "Epoch 442/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.9183 - val_loss: 34.4205\n",
      "Epoch 443/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.7601 - val_loss: 32.8243\n",
      "Epoch 444/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.4065 - val_loss: 33.2147\n",
      "Epoch 445/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.6463 - val_loss: 31.8404\n",
      "Epoch 446/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.9825 - val_loss: 32.5934\n",
      "Epoch 447/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.4707 - val_loss: 33.0053\n",
      "Epoch 448/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.4359 - val_loss: 33.1745\n",
      "Epoch 449/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.7801 - val_loss: 32.8894\n",
      "Epoch 450/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.7103 - val_loss: 33.2554\n",
      "Epoch 451/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.5565 - val_loss: 32.4026\n",
      "Epoch 452/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.4635 - val_loss: 32.3249\n",
      "Epoch 453/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.2105 - val_loss: 33.2173\n",
      "Epoch 454/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.3323 - val_loss: 32.0331\n",
      "Epoch 455/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.4848 - val_loss: 32.5994\n",
      "Epoch 456/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.9569 - val_loss: 32.5612\n",
      "Epoch 457/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.0675 - val_loss: 32.8953\n",
      "Epoch 458/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.9808 - val_loss: 32.4064\n",
      "Epoch 459/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.0330 - val_loss: 31.7384\n",
      "Epoch 460/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.0268 - val_loss: 33.8875\n",
      "Epoch 461/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.1121 - val_loss: 32.0781\n",
      "Epoch 462/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.1210 - val_loss: 32.3859\n",
      "Epoch 463/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.1931 - val_loss: 33.0856\n",
      "Epoch 464/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.2587 - val_loss: 32.6103\n",
      "Epoch 465/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.3859 - val_loss: 33.4437\n",
      "Epoch 466/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.4194 - val_loss: 31.7939\n",
      "Epoch 467/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.9621 - val_loss: 32.2332\n",
      "Epoch 468/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.0753 - val_loss: 32.0218\n",
      "Epoch 469/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.2775 - val_loss: 31.9093\n",
      "Epoch 470/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.0465 - val_loss: 31.9516\n",
      "Epoch 471/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.0553 - val_loss: 34.0143\n",
      "Epoch 472/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.8006 - val_loss: 32.3756\n",
      "Epoch 473/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.1976 - val_loss: 31.6123\n",
      "Epoch 474/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.9987 - val_loss: 32.1181\n",
      "Epoch 475/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.9984 - val_loss: 32.8064\n",
      "Epoch 476/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.5550 - val_loss: 33.5338\n",
      "Epoch 477/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.5104 - val_loss: 33.2413\n",
      "Epoch 478/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.9374 - val_loss: 33.0296\n",
      "Epoch 479/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.6013 - val_loss: 31.5957\n",
      "Epoch 480/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.1806 - val_loss: 31.9641\n",
      "Epoch 481/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.1696 - val_loss: 32.1080\n",
      "Epoch 482/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 29.5892 - val_loss: 32.1091\n",
      "Epoch 483/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.9631 - val_loss: 32.1006\n",
      "Epoch 484/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.3123 - val_loss: 32.5166\n",
      "Epoch 485/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.2966 - val_loss: 33.1326\n",
      "Epoch 486/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.2252 - val_loss: 32.3002\n",
      "Epoch 487/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.0756 - val_loss: 33.0489\n",
      "Epoch 488/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.9990 - val_loss: 31.6503\n",
      "Epoch 489/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.1683 - val_loss: 31.8528\n",
      "Epoch 490/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.9995 - val_loss: 32.0166\n",
      "Epoch 491/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.8181 - val_loss: 31.8054\n",
      "Epoch 492/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.9024 - val_loss: 32.3983\n",
      "Epoch 493/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.6183 - val_loss: 31.2249\n",
      "Epoch 494/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.1367 - val_loss: 33.2806\n",
      "Epoch 495/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.6859 - val_loss: 32.2618\n",
      "Epoch 496/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.7170 - val_loss: 31.2170\n",
      "Epoch 497/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 29.2981 - val_loss: 31.3387\n",
      "Epoch 498/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.4281 - val_loss: 31.5415\n",
      "Epoch 499/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3337 - val_loss: 32.1367\n",
      "Epoch 500/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.4112 - val_loss: 31.3397\n",
      "Epoch 501/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.5433 - val_loss: 31.2636\n",
      "Epoch 502/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.4461 - val_loss: 31.4060\n",
      "Epoch 503/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.4794 - val_loss: 31.4275\n",
      "Epoch 504/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3909 - val_loss: 31.1512\n",
      "Epoch 505/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3545 - val_loss: 31.3005\n",
      "Epoch 506/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.4000 - val_loss: 31.8239\n",
      "Epoch 507/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.5032 - val_loss: 31.6367\n",
      "Epoch 508/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.6076 - val_loss: 32.2631\n",
      "Epoch 509/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.5720 - val_loss: 31.9820\n",
      "Epoch 510/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3824 - val_loss: 30.9174\n",
      "Epoch 511/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.7302 - val_loss: 32.0271\n",
      "Epoch 512/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.8159 - val_loss: 32.3063\n",
      "Epoch 513/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.8959 - val_loss: 32.5122\n",
      "Epoch 514/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.6761 - val_loss: 32.2498\n",
      "Epoch 515/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.5719 - val_loss: 30.7561\n",
      "Epoch 516/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.4477 - val_loss: 31.6088\n",
      "Epoch 517/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 29.1292 - val_loss: 30.9599\n",
      "Epoch 518/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.8521 - val_loss: 31.2051\n",
      "Epoch 519/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9909 - val_loss: 30.7645\n",
      "Epoch 520/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3577 - val_loss: 32.3148\n",
      "Epoch 521/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.8926 - val_loss: 31.4634\n",
      "Epoch 522/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.6098 - val_loss: 30.8419\n",
      "Epoch 523/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3813 - val_loss: 31.8060\n",
      "Epoch 524/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.0039 - val_loss: 31.2091\n",
      "Epoch 525/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9600 - val_loss: 31.0849\n",
      "Epoch 526/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2587 - val_loss: 32.0692\n",
      "Epoch 527/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.4745 - val_loss: 33.2273\n",
      "Epoch 528/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.8229 - val_loss: 32.7558\n",
      "Epoch 529/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3253 - val_loss: 32.3850\n",
      "Epoch 530/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3956 - val_loss: 31.4274\n",
      "Epoch 531/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.1427 - val_loss: 31.1263\n",
      "Epoch 532/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.5539 - val_loss: 31.2820\n",
      "Epoch 533/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.4616 - val_loss: 31.3665\n",
      "Epoch 534/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.5943 - val_loss: 31.6142\n",
      "Epoch 535/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.6372 - val_loss: 31.4821\n",
      "Epoch 536/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3696 - val_loss: 31.7310\n",
      "Epoch 537/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3141 - val_loss: 30.9451\n",
      "Epoch 538/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3607 - val_loss: 30.9217\n",
      "Epoch 539/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3277 - val_loss: 31.9091\n",
      "Epoch 540/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 29.5023 - val_loss: 32.3461\n",
      "Epoch 541/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.1305 - val_loss: 31.8032\n",
      "Epoch 542/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.4181 - val_loss: 32.1264\n",
      "Epoch 543/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.5783 - val_loss: 32.0383\n",
      "Epoch 544/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.1859 - val_loss: 30.7794\n",
      "Epoch 545/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9148 - val_loss: 30.6346\n",
      "Epoch 546/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.8773 - val_loss: 31.0882\n",
      "Epoch 547/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.8726 - val_loss: 30.4575\n",
      "Epoch 548/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 28.8373 - val_loss: 31.8373\n",
      "Epoch 549/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.0614 - val_loss: 30.5799\n",
      "Epoch 550/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.8053 - val_loss: 31.1083\n",
      "Epoch 551/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.6632 - val_loss: 30.9685\n",
      "Epoch 552/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.6806 - val_loss: 31.8778\n",
      "Epoch 553/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7671 - val_loss: 31.0623\n",
      "Epoch 554/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.6451 - val_loss: 30.2992\n",
      "Epoch 555/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7266 - val_loss: 30.5598\n",
      "Epoch 556/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.8550 - val_loss: 31.6249\n",
      "Epoch 557/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9084 - val_loss: 31.2848\n",
      "Epoch 558/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9248 - val_loss: 30.6932\n",
      "Epoch 559/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2689 - val_loss: 31.3272\n",
      "Epoch 560/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.4358 - val_loss: 31.4014\n",
      "Epoch 561/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3051 - val_loss: 31.5667\n",
      "Epoch 562/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2424 - val_loss: 31.6335\n",
      "Epoch 563/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3724 - val_loss: 31.8927\n",
      "Epoch 564/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9716 - val_loss: 30.9229\n",
      "Epoch 565/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7242 - val_loss: 31.7801\n",
      "Epoch 566/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.6686 - val_loss: 31.2594\n",
      "Epoch 567/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.5166 - val_loss: 31.1402\n",
      "Epoch 568/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.5335 - val_loss: 30.7420\n",
      "Epoch 569/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.4734 - val_loss: 30.8118\n",
      "Epoch 570/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7730 - val_loss: 30.8384\n",
      "Epoch 571/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.1220 - val_loss: 30.2448\n",
      "Epoch 572/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.8600 - val_loss: 31.3956\n",
      "Epoch 573/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.5902 - val_loss: 30.7189\n",
      "Epoch 574/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 28.2605 - val_loss: 30.7399\n",
      "Epoch 575/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.4363 - val_loss: 30.4840\n",
      "Epoch 576/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.4665 - val_loss: 30.6839\n",
      "Epoch 577/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.1907 - val_loss: 32.0081\n",
      "Epoch 578/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9089 - val_loss: 30.3471\n",
      "Epoch 579/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.8642 - val_loss: 31.7094\n",
      "Epoch 580/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7810 - val_loss: 30.5412\n",
      "Epoch 581/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.6696 - val_loss: 31.1877\n",
      "Epoch 582/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.4820 - val_loss: 30.7036\n",
      "Epoch 583/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.2102 - val_loss: 30.9058\n",
      "Epoch 584/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 28.0782 - val_loss: 30.7241\n",
      "Epoch 585/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2344 - val_loss: 30.3677\n",
      "Epoch 586/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1590 - val_loss: 30.7054\n",
      "Epoch 587/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.5823 - val_loss: 30.1235\n",
      "Epoch 588/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1584 - val_loss: 30.8691\n",
      "Epoch 589/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2190 - val_loss: 29.6927\n",
      "Epoch 590/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1753 - val_loss: 31.0916\n",
      "Epoch 591/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.3405 - val_loss: 30.4872\n",
      "Epoch 592/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.3834 - val_loss: 29.9265\n",
      "Epoch 593/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.0310 - val_loss: 30.3341\n",
      "Epoch 594/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1511 - val_loss: 30.1640\n",
      "Epoch 595/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.3208 - val_loss: 30.4923\n",
      "Epoch 596/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.3907 - val_loss: 30.7631\n",
      "Epoch 597/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2939 - val_loss: 29.9711\n",
      "Epoch 598/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.2682 - val_loss: 30.2033\n",
      "Epoch 599/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.0122 - val_loss: 30.2297\n",
      "Epoch 600/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 28.2025\n",
      "Epoch 00600: saving model to saved_models/latent16/cp-0600.h5\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 28.2025 - val_loss: 30.9448\n",
      "Epoch 601/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.6794 - val_loss: 30.5182\n",
      "Epoch 602/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9615 - val_loss: 32.7132\n",
      "Epoch 603/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.0909 - val_loss: 30.7631\n",
      "Epoch 604/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7742 - val_loss: 30.8547\n",
      "Epoch 605/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.5278 - val_loss: 31.0916\n",
      "Epoch 606/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.3594 - val_loss: 30.1675\n",
      "Epoch 607/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.3775 - val_loss: 29.8466\n",
      "Epoch 608/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2775 - val_loss: 31.2215\n",
      "Epoch 609/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2810 - val_loss: 30.3424\n",
      "Epoch 610/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0620 - val_loss: 30.9330\n",
      "Epoch 611/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1464 - val_loss: 29.8283\n",
      "Epoch 612/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.9599 - val_loss: 30.3981\n",
      "Epoch 613/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0580 - val_loss: 30.3650\n",
      "Epoch 614/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.4664 - val_loss: 29.9852\n",
      "Epoch 615/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.3155 - val_loss: 30.3801\n",
      "Epoch 616/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.5046 - val_loss: 31.0171\n",
      "Epoch 617/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0993 - val_loss: 30.4647\n",
      "Epoch 618/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2062 - val_loss: 29.6604\n",
      "Epoch 619/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1462 - val_loss: 30.1436\n",
      "Epoch 620/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1092 - val_loss: 30.2634\n",
      "Epoch 621/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1932 - val_loss: 30.1092\n",
      "Epoch 622/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0657 - val_loss: 30.0356\n",
      "Epoch 623/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9854 - val_loss: 29.6321\n",
      "Epoch 624/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.6052 - val_loss: 30.1295\n",
      "Epoch 625/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.6000 - val_loss: 29.9934\n",
      "Epoch 626/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8165 - val_loss: 30.0979\n",
      "Epoch 627/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4076 - val_loss: 30.1225\n",
      "Epoch 628/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5538 - val_loss: 30.1381\n",
      "Epoch 629/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4666 - val_loss: 30.0053\n",
      "Epoch 630/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6178 - val_loss: 30.2601\n",
      "Epoch 631/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5433 - val_loss: 30.1945\n",
      "Epoch 632/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.3946 - val_loss: 29.7931\n",
      "Epoch 633/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4615 - val_loss: 29.3244\n",
      "Epoch 634/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4508 - val_loss: 30.0006\n",
      "Epoch 635/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5763 - val_loss: 29.3110\n",
      "Epoch 636/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7146 - val_loss: 29.7430\n",
      "Epoch 637/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7329 - val_loss: 30.1522\n",
      "Epoch 638/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5496 - val_loss: 30.3067\n",
      "Epoch 639/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6312 - val_loss: 30.3189\n",
      "Epoch 640/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7797 - val_loss: 28.6350\n",
      "Epoch 641/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8041 - val_loss: 30.4172\n",
      "Epoch 642/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7779 - val_loss: 29.7162\n",
      "Epoch 643/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5037 - val_loss: 30.3172\n",
      "Epoch 644/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9579 - val_loss: 29.8439\n",
      "Epoch 645/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0909 - val_loss: 30.4562\n",
      "Epoch 646/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6432 - val_loss: 29.8250\n",
      "Epoch 647/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5263 - val_loss: 29.7357\n",
      "Epoch 648/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5517 - val_loss: 29.9454\n",
      "Epoch 649/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5994 - val_loss: 29.6191\n",
      "Epoch 650/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4426 - val_loss: 28.4957\n",
      "Epoch 651/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.3856 - val_loss: 30.4011\n",
      "Epoch 652/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4706 - val_loss: 30.5749\n",
      "Epoch 653/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.6349 - val_loss: 29.5379\n",
      "Epoch 654/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4266 - val_loss: 30.5480\n",
      "Epoch 655/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.2103 - val_loss: 30.2536\n",
      "Epoch 656/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7447 - val_loss: 29.8626\n",
      "Epoch 657/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8137 - val_loss: 29.4695\n",
      "Epoch 658/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4320 - val_loss: 29.8666\n",
      "Epoch 659/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3532 - val_loss: 29.3124\n",
      "Epoch 660/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.0802 - val_loss: 29.0053\n",
      "Epoch 661/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3351 - val_loss: 29.7862\n",
      "Epoch 662/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6723 - val_loss: 29.8694\n",
      "Epoch 663/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5540 - val_loss: 30.1870\n",
      "Epoch 664/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7041 - val_loss: 30.1972\n",
      "Epoch 665/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5099 - val_loss: 30.3894\n",
      "Epoch 666/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2865 - val_loss: 29.2653\n",
      "Epoch 667/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3071 - val_loss: 29.9075\n",
      "Epoch 668/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.5938 - val_loss: 29.4210\n",
      "Epoch 669/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1022 - val_loss: 29.5202\n",
      "Epoch 670/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.0433 - val_loss: 29.4720\n",
      "Epoch 671/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1829 - val_loss: 28.7487\n",
      "Epoch 672/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.3215 - val_loss: 30.0425\n",
      "Epoch 673/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4695 - val_loss: 30.5175\n",
      "Epoch 674/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6753 - val_loss: 30.2140\n",
      "Epoch 675/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6364 - val_loss: 29.8366\n",
      "Epoch 676/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6470 - val_loss: 29.6636\n",
      "Epoch 677/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7065 - val_loss: 28.8946\n",
      "Epoch 678/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5371 - val_loss: 29.7994\n",
      "Epoch 679/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4746 - val_loss: 29.1265\n",
      "Epoch 680/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.5441 - val_loss: 29.1457\n",
      "Epoch 681/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2710 - val_loss: 29.6367\n",
      "Epoch 682/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1063 - val_loss: 30.0292\n",
      "Epoch 683/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9284 - val_loss: 29.3923\n",
      "Epoch 684/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0583 - val_loss: 28.6132\n",
      "Epoch 685/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9485 - val_loss: 29.5163\n",
      "Epoch 686/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0544 - val_loss: 29.5017\n",
      "Epoch 687/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0925 - val_loss: 28.7011\n",
      "Epoch 688/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7777 - val_loss: 28.7914\n",
      "Epoch 689/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9864 - val_loss: 28.6375\n",
      "Epoch 690/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7904 - val_loss: 29.2532\n",
      "Epoch 691/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9392 - val_loss: 29.3837\n",
      "Epoch 692/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5426 - val_loss: 28.7985\n",
      "Epoch 693/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9163 - val_loss: 29.2031\n",
      "Epoch 694/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3001 - val_loss: 29.4327\n",
      "Epoch 695/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4734 - val_loss: 31.1586\n",
      "Epoch 696/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.4703 - val_loss: 32.6745\n",
      "Epoch 697/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2374 - val_loss: 30.3936\n",
      "Epoch 698/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6797 - val_loss: 30.1076\n",
      "Epoch 699/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3247 - val_loss: 29.2259\n",
      "Epoch 700/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1592 - val_loss: 29.4271\n",
      "Epoch 701/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0407 - val_loss: 28.9404\n",
      "Epoch 702/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8381 - val_loss: 28.8560\n",
      "Epoch 703/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9757 - val_loss: 29.5488\n",
      "Epoch 704/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1065 - val_loss: 29.2144\n",
      "Epoch 705/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8329 - val_loss: 29.7696\n",
      "Epoch 706/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7428 - val_loss: 29.3431\n",
      "Epoch 707/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8919 - val_loss: 30.5142\n",
      "Epoch 708/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2876 - val_loss: 28.9174\n",
      "Epoch 709/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2402 - val_loss: 29.7823\n",
      "Epoch 710/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9458 - val_loss: 29.8582\n",
      "Epoch 711/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2394 - val_loss: 29.9513\n",
      "Epoch 712/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2832 - val_loss: 29.2063\n",
      "Epoch 713/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0023 - val_loss: 28.7440\n",
      "Epoch 714/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7663 - val_loss: 29.2516\n",
      "Epoch 715/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7922 - val_loss: 28.8956\n",
      "Epoch 716/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7596 - val_loss: 29.0645\n",
      "Epoch 717/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6082 - val_loss: 28.9819\n",
      "Epoch 718/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5705 - val_loss: 29.4350\n",
      "Epoch 719/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9100 - val_loss: 29.8417\n",
      "Epoch 720/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8913 - val_loss: 30.1633\n",
      "Epoch 721/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9708 - val_loss: 28.9984\n",
      "Epoch 722/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6790 - val_loss: 29.1387\n",
      "Epoch 723/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1377 - val_loss: 30.0861\n",
      "Epoch 724/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1444 - val_loss: 28.9940\n",
      "Epoch 725/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.5107 - val_loss: 29.4133\n",
      "Epoch 726/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7310 - val_loss: 28.5968\n",
      "Epoch 727/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4472 - val_loss: 28.0998\n",
      "Epoch 728/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6936 - val_loss: 29.2703\n",
      "Epoch 729/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9084 - val_loss: 29.2177\n",
      "Epoch 730/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8650 - val_loss: 28.7082\n",
      "Epoch 731/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6784 - val_loss: 28.4126\n",
      "Epoch 732/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6454 - val_loss: 28.5120\n",
      "Epoch 733/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6864 - val_loss: 28.4075\n",
      "Epoch 734/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5625 - val_loss: 29.3640\n",
      "Epoch 735/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4432 - val_loss: 28.9502\n",
      "Epoch 736/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4420 - val_loss: 28.4054\n",
      "Epoch 737/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7791 - val_loss: 28.4444\n",
      "Epoch 738/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6992 - val_loss: 28.3937\n",
      "Epoch 739/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7573 - val_loss: 29.5720\n",
      "Epoch 740/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5352 - val_loss: 28.7547\n",
      "Epoch 741/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4981 - val_loss: 29.4759\n",
      "Epoch 742/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8639 - val_loss: 29.3169\n",
      "Epoch 743/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0126 - val_loss: 28.9180\n",
      "Epoch 744/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7922 - val_loss: 29.4898\n",
      "Epoch 745/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.1231 - val_loss: 28.5868\n",
      "Epoch 746/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8693 - val_loss: 28.8873\n",
      "Epoch 747/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8160 - val_loss: 29.2414\n",
      "Epoch 748/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7288 - val_loss: 29.4599\n",
      "Epoch 749/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5857 - val_loss: 28.3877\n",
      "Epoch 750/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4681 - val_loss: 28.2979\n",
      "Epoch 751/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.2373 - val_loss: 28.5104\n",
      "Epoch 752/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 26.1344 - val_loss: 28.4159\n",
      "Epoch 753/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2988 - val_loss: 28.4631\n",
      "Epoch 754/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2764 - val_loss: 28.6434\n",
      "Epoch 755/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4468 - val_loss: 29.5042\n",
      "Epoch 756/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4426 - val_loss: 28.2179\n",
      "Epoch 757/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5213 - val_loss: 28.3793\n",
      "Epoch 758/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2476 - val_loss: 28.2074\n",
      "Epoch 759/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3630 - val_loss: 29.1773\n",
      "Epoch 760/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.5488 - val_loss: 29.0064\n",
      "Epoch 761/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4531 - val_loss: 28.1531\n",
      "Epoch 762/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2790 - val_loss: 28.8534\n",
      "Epoch 763/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5102 - val_loss: 29.0922\n",
      "Epoch 764/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3955 - val_loss: 28.8236\n",
      "Epoch 765/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6344 - val_loss: 28.5536\n",
      "Epoch 766/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3183 - val_loss: 27.4717\n",
      "Epoch 767/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3646 - val_loss: 28.5869\n",
      "Epoch 768/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3264 - val_loss: 28.8380\n",
      "Epoch 769/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6223 - val_loss: 29.3847\n",
      "Epoch 770/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6556 - val_loss: 28.3937\n",
      "Epoch 771/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5460 - val_loss: 29.2367\n",
      "Epoch 772/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6217 - val_loss: 28.4612\n",
      "Epoch 773/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6045 - val_loss: 29.2005\n",
      "Epoch 774/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6327 - val_loss: 28.9176\n",
      "Epoch 775/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4959 - val_loss: 28.6707\n",
      "Epoch 776/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4272 - val_loss: 29.0466\n",
      "Epoch 777/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3537 - val_loss: 28.9523\n",
      "Epoch 778/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2616 - val_loss: 28.9144\n",
      "Epoch 779/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7166 - val_loss: 28.3457\n",
      "Epoch 780/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2839 - val_loss: 28.7124\n",
      "Epoch 781/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3935 - val_loss: 28.4903\n",
      "Epoch 782/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4194 - val_loss: 29.1334\n",
      "Epoch 783/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1810 - val_loss: 29.3704\n",
      "Epoch 784/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.0953 - val_loss: 28.0114\n",
      "Epoch 785/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8995 - val_loss: 28.4156\n",
      "Epoch 786/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0415 - val_loss: 28.3229\n",
      "Epoch 787/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2395 - val_loss: 29.3133\n",
      "Epoch 788/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2723 - val_loss: 28.4431\n",
      "Epoch 789/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6469 - val_loss: 27.8944\n",
      "Epoch 790/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0922 - val_loss: 29.1095\n",
      "Epoch 791/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2320 - val_loss: 28.7781\n",
      "Epoch 792/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1683 - val_loss: 29.0420\n",
      "Epoch 793/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2534 - val_loss: 28.6683\n",
      "Epoch 794/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2360 - val_loss: 28.8902\n",
      "Epoch 795/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3054 - val_loss: 29.0425\n",
      "Epoch 796/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2367 - val_loss: 28.8287\n",
      "Epoch 797/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0996 - val_loss: 27.8951\n",
      "Epoch 798/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.8903 - val_loss: 28.2878\n",
      "Epoch 799/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9677 - val_loss: 28.3520\n",
      "Epoch 800/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 25.8918\n",
      "Epoch 00800: saving model to saved_models/latent16/cp-0800.h5\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 25.8918 - val_loss: 27.5052\n",
      "Epoch 801/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9360 - val_loss: 28.2798\n",
      "Epoch 802/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2982 - val_loss: 29.0642\n",
      "Epoch 803/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2981 - val_loss: 28.2797\n",
      "Epoch 804/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3564 - val_loss: 28.3411\n",
      "Epoch 805/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2374 - val_loss: 28.3953\n",
      "Epoch 806/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9877 - val_loss: 28.1089\n",
      "Epoch 807/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9611 - val_loss: 27.9668\n",
      "Epoch 808/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9597 - val_loss: 28.2760\n",
      "Epoch 809/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1971 - val_loss: 28.2461\n",
      "Epoch 810/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0107 - val_loss: 29.7788\n",
      "Epoch 811/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4108 - val_loss: 28.5646\n",
      "Epoch 812/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2244 - val_loss: 28.9563\n",
      "Epoch 813/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5676 - val_loss: 29.1212\n",
      "Epoch 814/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9000 - val_loss: 29.9527\n",
      "Epoch 815/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9928 - val_loss: 28.5742\n",
      "Epoch 816/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5620 - val_loss: 28.6498\n",
      "Epoch 817/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2464 - val_loss: 28.9470\n",
      "Epoch 818/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4047 - val_loss: 28.4358\n",
      "Epoch 819/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1728 - val_loss: 28.9756\n",
      "Epoch 820/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0067 - val_loss: 28.4288\n",
      "Epoch 821/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8182 - val_loss: 28.1418\n",
      "Epoch 822/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7372 - val_loss: 28.4044\n",
      "Epoch 823/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0022 - val_loss: 28.1696\n",
      "Epoch 824/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1234 - val_loss: 28.0155\n",
      "Epoch 825/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9911 - val_loss: 29.1130\n",
      "Epoch 826/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2860 - val_loss: 28.6636\n",
      "Epoch 827/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2384 - val_loss: 28.3413\n",
      "Epoch 828/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0757 - val_loss: 29.0972\n",
      "Epoch 829/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0747 - val_loss: 29.3621\n",
      "Epoch 830/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1968 - val_loss: 28.7994\n",
      "Epoch 831/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1215 - val_loss: 28.4045\n",
      "Epoch 832/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7855 - val_loss: 28.5126\n",
      "Epoch 833/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0838 - val_loss: 28.6114\n",
      "Epoch 834/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3858 - val_loss: 29.7877\n",
      "Epoch 835/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5237 - val_loss: 29.0531\n",
      "Epoch 836/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0183 - val_loss: 28.0861\n",
      "Epoch 837/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9965 - val_loss: 28.4835\n",
      "Epoch 838/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3197 - val_loss: 28.1994\n",
      "Epoch 839/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3269 - val_loss: 28.1948\n",
      "Epoch 840/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0290 - val_loss: 28.0038\n",
      "Epoch 841/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9032 - val_loss: 27.7991\n",
      "Epoch 842/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9511 - val_loss: 28.6274\n",
      "Epoch 843/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7753 - val_loss: 27.5828\n",
      "Epoch 844/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8618 - val_loss: 28.6889\n",
      "Epoch 845/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1575 - val_loss: 28.6634\n",
      "Epoch 846/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4671 - val_loss: 29.6436\n",
      "Epoch 847/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2609 - val_loss: 28.4996\n",
      "Epoch 848/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0460 - val_loss: 28.4736\n",
      "Epoch 849/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8406 - val_loss: 28.0227\n",
      "Epoch 850/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7773 - val_loss: 27.7674\n",
      "Epoch 851/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8104 - val_loss: 27.6997\n",
      "Epoch 852/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7170 - val_loss: 27.8133\n",
      "Epoch 853/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.6053 - val_loss: 27.4448\n",
      "Epoch 854/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6357 - val_loss: 27.6543\n",
      "Epoch 855/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.5687 - val_loss: 27.8476\n",
      "Epoch 856/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.5435 - val_loss: 28.4241\n",
      "Epoch 857/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6405 - val_loss: 27.8646\n",
      "Epoch 858/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7709 - val_loss: 28.9073\n",
      "Epoch 859/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8414 - val_loss: 27.7597\n",
      "Epoch 860/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8802 - val_loss: 29.0300\n",
      "Epoch 861/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2392 - val_loss: 27.8267\n",
      "Epoch 862/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2235 - val_loss: 28.5558\n",
      "Epoch 863/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1304 - val_loss: 29.1210\n",
      "Epoch 864/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3673 - val_loss: 28.8420\n",
      "Epoch 865/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0955 - val_loss: 28.8018\n",
      "Epoch 866/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9178 - val_loss: 28.3768\n",
      "Epoch 867/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6550 - val_loss: 28.6593\n",
      "Epoch 868/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5789 - val_loss: 29.0245\n",
      "Epoch 869/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9439 - val_loss: 28.4581\n",
      "Epoch 870/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6137 - val_loss: 27.5192\n",
      "Epoch 871/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7089 - val_loss: 27.8993\n",
      "Epoch 872/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6756 - val_loss: 27.8652\n",
      "Epoch 873/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5067 - val_loss: 27.6014\n",
      "Epoch 874/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9221 - val_loss: 28.8510\n",
      "Epoch 875/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9569 - val_loss: 28.1792\n",
      "Epoch 876/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6172 - val_loss: 27.8773\n",
      "Epoch 877/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5145 - val_loss: 27.5022\n",
      "Epoch 878/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7130 - val_loss: 28.2450\n",
      "Epoch 879/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7114 - val_loss: 27.8482\n",
      "Epoch 880/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9935 - val_loss: 27.7615\n",
      "Epoch 881/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9398 - val_loss: 29.1752\n",
      "Epoch 882/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9745 - val_loss: 28.4265\n",
      "Epoch 883/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0293 - val_loss: 28.2553\n",
      "Epoch 884/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9977 - val_loss: 29.6388\n",
      "Epoch 885/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3257 - val_loss: 28.7930\n",
      "Epoch 886/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0938 - val_loss: 28.2820\n",
      "Epoch 887/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8269 - val_loss: 28.9041\n",
      "Epoch 888/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7446 - val_loss: 28.2417\n",
      "Epoch 889/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5883 - val_loss: 27.8167\n",
      "Epoch 890/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4183 - val_loss: 27.8214\n",
      "Epoch 891/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3113 - val_loss: 27.2968\n",
      "Epoch 892/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6298 - val_loss: 27.8721\n",
      "Epoch 893/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9118 - val_loss: 29.0131\n",
      "Epoch 894/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8350 - val_loss: 28.7023\n",
      "Epoch 895/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6694 - val_loss: 27.4247\n",
      "Epoch 896/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5740 - val_loss: 27.8166\n",
      "Epoch 897/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5736 - val_loss: 28.0436\n",
      "Epoch 898/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4539 - val_loss: 28.0396\n",
      "Epoch 899/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7214 - val_loss: 28.3629\n",
      "Epoch 900/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9695 - val_loss: 28.0279\n",
      "Epoch 901/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4608 - val_loss: 27.7962\n",
      "Epoch 902/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8328 - val_loss: 28.3939\n",
      "Epoch 903/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7034 - val_loss: 27.9892\n",
      "Epoch 904/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6291 - val_loss: 28.0324\n",
      "Epoch 905/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4413 - val_loss: 27.3430\n",
      "Epoch 906/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4167 - val_loss: 27.9058\n",
      "Epoch 907/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6182 - val_loss: 27.8645\n",
      "Epoch 908/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6315 - val_loss: 28.0804\n",
      "Epoch 909/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7031 - val_loss: 27.9613\n",
      "Epoch 910/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0671 - val_loss: 27.8143\n",
      "Epoch 911/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9112 - val_loss: 28.9324\n",
      "Epoch 912/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8987 - val_loss: 28.4455\n",
      "Epoch 913/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9262 - val_loss: 28.8595\n",
      "Epoch 914/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7661 - val_loss: 28.2232\n",
      "Epoch 915/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7459 - val_loss: 28.5139\n",
      "Epoch 916/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5035 - val_loss: 27.1184\n",
      "Epoch 917/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4613 - val_loss: 28.2131\n",
      "Epoch 918/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8033 - val_loss: 28.0921\n",
      "Epoch 919/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3501 - val_loss: 28.0742\n",
      "Epoch 920/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7488 - val_loss: 28.2965\n",
      "Epoch 921/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5460 - val_loss: 29.0780\n",
      "Epoch 922/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7623 - val_loss: 28.3015\n",
      "Epoch 923/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4634 - val_loss: 27.9929\n",
      "Epoch 924/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4976 - val_loss: 27.5750\n",
      "Epoch 925/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5497 - val_loss: 27.8342\n",
      "Epoch 926/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4465 - val_loss: 27.6659\n",
      "Epoch 927/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5697 - val_loss: 27.8077\n",
      "Epoch 928/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7152 - val_loss: 27.6114\n",
      "Epoch 929/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6411 - val_loss: 29.6509\n",
      "Epoch 930/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8484 - val_loss: 28.0083\n",
      "Epoch 931/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7500 - val_loss: 28.5470\n",
      "Epoch 932/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8223 - val_loss: 28.3268\n",
      "Epoch 933/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5834 - val_loss: 28.0748\n",
      "Epoch 934/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4595 - val_loss: 27.5080\n",
      "Epoch 935/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.3043 - val_loss: 28.3693\n",
      "Epoch 936/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5117 - val_loss: 28.1561\n",
      "Epoch 937/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3198 - val_loss: 27.6013\n",
      "Epoch 938/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5428 - val_loss: 27.8607\n",
      "Epoch 939/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5022 - val_loss: 27.9084\n",
      "Epoch 940/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3884 - val_loss: 27.7553\n",
      "Epoch 941/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2463 - val_loss: 27.8067\n",
      "Epoch 942/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6410 - val_loss: 27.4009\n",
      "Epoch 943/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4985 - val_loss: 27.8984\n",
      "Epoch 944/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1202 - val_loss: 27.7711\n",
      "Epoch 945/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7054 - val_loss: 28.0468\n",
      "Epoch 946/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5143 - val_loss: 28.0250\n",
      "Epoch 947/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5285 - val_loss: 27.6635\n",
      "Epoch 948/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2107 - val_loss: 27.8641\n",
      "Epoch 949/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3576 - val_loss: 27.8083\n",
      "Epoch 950/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2159 - val_loss: 27.7566\n",
      "Epoch 951/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3400 - val_loss: 27.8686\n",
      "Epoch 952/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3096 - val_loss: 27.2985\n",
      "Epoch 953/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3923 - val_loss: 27.9641\n",
      "Epoch 954/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7024 - val_loss: 28.7437\n",
      "Epoch 955/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.6767 - val_loss: 28.5464\n",
      "Epoch 956/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3447 - val_loss: 27.7656\n",
      "Epoch 957/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2921 - val_loss: 27.2577\n",
      "Epoch 958/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3264 - val_loss: 27.5540\n",
      "Epoch 959/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2379 - val_loss: 27.6754\n",
      "Epoch 960/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2659 - val_loss: 28.5787\n",
      "Epoch 961/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6461 - val_loss: 27.8275\n",
      "Epoch 962/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2716 - val_loss: 27.1799\n",
      "Epoch 963/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3712 - val_loss: 27.2906\n",
      "Epoch 964/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3420 - val_loss: 27.8168\n",
      "Epoch 965/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4124 - val_loss: 27.0452\n",
      "Epoch 966/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3655 - val_loss: 28.6481\n",
      "Epoch 967/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4581 - val_loss: 27.7070\n",
      "Epoch 968/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3508 - val_loss: 27.6324\n",
      "Epoch 969/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2493 - val_loss: 27.5078\n",
      "Epoch 970/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4118 - val_loss: 28.1546\n",
      "Epoch 971/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9598 - val_loss: 28.2979\n",
      "Epoch 972/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4993 - val_loss: 28.4094\n",
      "Epoch 973/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7049 - val_loss: 28.1850\n",
      "Epoch 974/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 25.4065 - val_loss: 27.9314\n",
      "Epoch 975/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5821 - val_loss: 27.7234\n",
      "Epoch 976/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1949 - val_loss: 27.6370\n",
      "Epoch 977/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1890 - val_loss: 27.6040\n",
      "Epoch 978/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4145 - val_loss: 27.7444\n",
      "Epoch 979/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2535 - val_loss: 27.8227\n",
      "Epoch 980/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2217 - val_loss: 27.6241\n",
      "Epoch 981/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4788 - val_loss: 28.1622\n",
      "Epoch 982/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4889 - val_loss: 27.6366\n",
      "Epoch 983/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.0202 - val_loss: 28.0421\n",
      "Epoch 984/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3165 - val_loss: 27.4976\n",
      "Epoch 985/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2324 - val_loss: 27.2210\n",
      "Epoch 986/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2685 - val_loss: 27.7982\n",
      "Epoch 987/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3431 - val_loss: 27.4455\n",
      "Epoch 988/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.9841 - val_loss: 28.0072\n",
      "Epoch 989/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4135 - val_loss: 27.9590\n",
      "Epoch 990/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3278 - val_loss: 27.8620\n",
      "Epoch 991/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2977 - val_loss: 27.7780\n",
      "Epoch 992/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2681 - val_loss: 27.7406\n",
      "Epoch 993/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5486 - val_loss: 27.3602\n",
      "Epoch 994/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3548 - val_loss: 26.7035\n",
      "Epoch 995/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1163 - val_loss: 27.6968\n",
      "Epoch 996/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3382 - val_loss: 27.4462\n",
      "Epoch 997/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5277 - val_loss: 28.6888\n",
      "Epoch 998/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8522 - val_loss: 28.4598\n",
      "Epoch 999/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6630 - val_loss: 27.9461\n",
      "Epoch 1000/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 25.4130\n",
      "Epoch 01000: saving model to saved_models/latent16/cp-1000.h5\n",
      "6/6 [==============================] - 1s 169ms/step - loss: 25.4130 - val_loss: 28.1537\n",
      "Epoch 1001/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4640 - val_loss: 27.3393\n",
      "Epoch 1002/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3885 - val_loss: 27.8398\n",
      "Epoch 1003/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1797 - val_loss: 27.7352\n",
      "Epoch 1004/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3987 - val_loss: 28.0991\n",
      "Epoch 1005/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3131 - val_loss: 27.4597\n",
      "Epoch 1006/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0969 - val_loss: 27.0978\n",
      "Epoch 1007/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0984 - val_loss: 27.8471\n",
      "Epoch 1008/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0505 - val_loss: 27.1821\n",
      "Epoch 1009/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.9771 - val_loss: 27.2912\n",
      "Epoch 1010/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9058 - val_loss: 27.8044\n",
      "Epoch 1011/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1315 - val_loss: 27.5075\n",
      "Epoch 1012/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1761 - val_loss: 28.2894\n",
      "Epoch 1013/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6093 - val_loss: 28.3936\n",
      "Epoch 1014/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.4216 - val_loss: 27.7631\n",
      "Epoch 1015/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3219 - val_loss: 27.7595\n",
      "Epoch 1016/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2731 - val_loss: 28.2484\n",
      "Epoch 1017/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5309 - val_loss: 28.5568\n",
      "Epoch 1018/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4671 - val_loss: 27.4995\n",
      "Epoch 1019/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5598 - val_loss: 28.7425\n",
      "Epoch 1020/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9520 - val_loss: 28.7416\n",
      "Epoch 1021/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6572 - val_loss: 28.9534\n",
      "Epoch 1022/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4312 - val_loss: 28.0174\n",
      "Epoch 1023/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8356 - val_loss: 27.3613\n",
      "Epoch 1024/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2103 - val_loss: 27.3166\n",
      "Epoch 1025/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1602 - val_loss: 27.5392\n",
      "Epoch 1026/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3377 - val_loss: 27.8506\n",
      "Epoch 1027/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2414 - val_loss: 27.2494\n",
      "Epoch 1028/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0536 - val_loss: 27.6125\n",
      "Epoch 1029/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0844 - val_loss: 28.0804\n",
      "Epoch 1030/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5137 - val_loss: 27.8676\n",
      "Epoch 1031/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6609 - val_loss: 28.7323\n",
      "Epoch 1032/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8572 - val_loss: 28.1988\n",
      "Epoch 1033/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5475 - val_loss: 28.3105\n",
      "Epoch 1034/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3310 - val_loss: 27.4437\n",
      "Epoch 1035/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4837 - val_loss: 27.7418\n",
      "Epoch 1036/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6359 - val_loss: 28.0524\n",
      "Epoch 1037/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4744 - val_loss: 27.6268\n",
      "Epoch 1038/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4904 - val_loss: 27.6505\n",
      "Epoch 1039/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1251 - val_loss: 28.4621\n",
      "Epoch 1040/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2420 - val_loss: 27.3844\n",
      "Epoch 1041/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9807 - val_loss: 27.5195\n",
      "Epoch 1042/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0382 - val_loss: 27.4704\n",
      "Epoch 1043/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9450 - val_loss: 27.8720\n",
      "Epoch 1044/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1026 - val_loss: 27.1911\n",
      "Epoch 1045/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0673 - val_loss: 27.4413\n",
      "Epoch 1046/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8857 - val_loss: 28.3055\n",
      "Epoch 1047/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3084 - val_loss: 27.3601\n",
      "Epoch 1048/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0207 - val_loss: 27.8523\n",
      "Epoch 1049/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.7655 - val_loss: 26.9997\n",
      "Epoch 1050/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0500 - val_loss: 27.1782\n",
      "Epoch 1051/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0087 - val_loss: 27.6324\n",
      "Epoch 1052/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0476 - val_loss: 27.3049\n",
      "Epoch 1053/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0692 - val_loss: 27.5434\n",
      "Epoch 1054/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0445 - val_loss: 27.5621\n",
      "Epoch 1055/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0324 - val_loss: 28.3043\n",
      "Epoch 1056/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2299 - val_loss: 27.2388\n",
      "Epoch 1057/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3339 - val_loss: 28.3010\n",
      "Epoch 1058/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2827 - val_loss: 28.3055\n",
      "Epoch 1059/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2982 - val_loss: 27.7959\n",
      "Epoch 1060/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2539 - val_loss: 27.7465\n",
      "Epoch 1061/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1554 - val_loss: 27.4995\n",
      "Epoch 1062/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0709 - val_loss: 27.1555\n",
      "Epoch 1063/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8473 - val_loss: 27.3760\n",
      "Epoch 1064/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1930 - val_loss: 27.6057\n",
      "Epoch 1065/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8280 - val_loss: 27.5376\n",
      "Epoch 1066/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0806 - val_loss: 27.5473\n",
      "Epoch 1067/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0627 - val_loss: 27.9551\n",
      "Epoch 1068/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3267 - val_loss: 27.6434\n",
      "Epoch 1069/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0450 - val_loss: 27.9168\n",
      "Epoch 1070/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1711 - val_loss: 27.4896\n",
      "Epoch 1071/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1307 - val_loss: 28.2285\n",
      "Epoch 1072/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3955 - val_loss: 27.9390\n",
      "Epoch 1073/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6474 - val_loss: 27.9615\n",
      "Epoch 1074/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1526 - val_loss: 27.5588\n",
      "Epoch 1075/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3058 - val_loss: 27.4107\n",
      "Epoch 1076/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9685 - val_loss: 26.8258\n",
      "Epoch 1077/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7371 - val_loss: 27.9359\n",
      "Epoch 1078/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0825 - val_loss: 27.0069\n",
      "Epoch 1079/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0881 - val_loss: 27.6190\n",
      "Epoch 1080/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8831 - val_loss: 26.9837\n",
      "Epoch 1081/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7696 - val_loss: 27.1748\n",
      "Epoch 1082/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9536 - val_loss: 27.6151\n",
      "Epoch 1083/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1322 - val_loss: 28.1204\n",
      "Epoch 1084/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3100 - val_loss: 27.9144\n",
      "Epoch 1085/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9595 - val_loss: 27.0140\n",
      "Epoch 1086/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0739 - val_loss: 27.8295\n",
      "Epoch 1087/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9868 - val_loss: 27.8241\n",
      "Epoch 1088/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3812 - val_loss: 29.0435\n",
      "Epoch 1089/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7411 - val_loss: 27.5076\n",
      "Epoch 1090/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2237 - val_loss: 28.1809\n",
      "Epoch 1091/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1977 - val_loss: 27.4371\n",
      "Epoch 1092/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8557 - val_loss: 27.6653\n",
      "Epoch 1093/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8834 - val_loss: 27.0772\n",
      "Epoch 1094/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0671 - val_loss: 27.4461\n",
      "Epoch 1095/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8638 - val_loss: 27.0090\n",
      "Epoch 1096/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8542 - val_loss: 27.2016\n",
      "Epoch 1097/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7626 - val_loss: 27.4982\n",
      "Epoch 1098/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8876 - val_loss: 28.1873\n",
      "Epoch 1099/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9419 - val_loss: 27.5623\n",
      "Epoch 1100/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9178 - val_loss: 27.3185\n",
      "Epoch 1101/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0788 - val_loss: 27.5423\n",
      "Epoch 1102/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9050 - val_loss: 26.5491\n",
      "Epoch 1103/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7933 - val_loss: 27.0976\n",
      "Epoch 1104/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8446 - val_loss: 27.2955\n",
      "Epoch 1105/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0777 - val_loss: 28.0388\n",
      "Epoch 1106/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3141 - val_loss: 28.1709\n",
      "Epoch 1107/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4935 - val_loss: 27.2905\n",
      "Epoch 1108/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3983 - val_loss: 27.9574\n",
      "Epoch 1109/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1739 - val_loss: 27.5398\n",
      "Epoch 1110/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4259 - val_loss: 27.6178\n",
      "Epoch 1111/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0232 - val_loss: 27.3250\n",
      "Epoch 1112/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.9647 - val_loss: 27.6481\n",
      "Epoch 1113/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2027 - val_loss: 27.1417\n",
      "Epoch 1114/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0517 - val_loss: 27.5395\n",
      "Epoch 1115/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8347 - val_loss: 26.9713\n",
      "Epoch 1116/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8429 - val_loss: 27.3151\n",
      "Epoch 1117/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8020 - val_loss: 27.0276\n",
      "Epoch 1118/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6539 - val_loss: 26.8237\n",
      "Epoch 1119/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6566 - val_loss: 26.8808\n",
      "Epoch 1120/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5322 - val_loss: 27.9704\n",
      "Epoch 1121/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8286 - val_loss: 27.2605\n",
      "Epoch 1122/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8262 - val_loss: 27.7462\n",
      "Epoch 1123/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3211 - val_loss: 28.6723\n",
      "Epoch 1124/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1212 - val_loss: 27.5617\n",
      "Epoch 1125/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1231 - val_loss: 27.7080\n",
      "Epoch 1126/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0643 - val_loss: 27.8919\n",
      "Epoch 1127/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9633 - val_loss: 27.1688\n",
      "Epoch 1128/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5754 - val_loss: 27.7085\n",
      "Epoch 1129/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0251 - val_loss: 27.8144\n",
      "Epoch 1130/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1277 - val_loss: 27.5893\n",
      "Epoch 1131/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4233 - val_loss: 27.7949\n",
      "Epoch 1132/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3755 - val_loss: 27.7176\n",
      "Epoch 1133/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8530 - val_loss: 27.5535\n",
      "Epoch 1134/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1644 - val_loss: 27.8402\n",
      "Epoch 1135/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2976 - val_loss: 27.6242\n",
      "Epoch 1136/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9325 - val_loss: 28.1604\n",
      "Epoch 1137/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9021 - val_loss: 27.4975\n",
      "Epoch 1138/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7900 - val_loss: 27.7624\n",
      "Epoch 1139/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9535 - val_loss: 28.4277\n",
      "Epoch 1140/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1480 - val_loss: 27.3075\n",
      "Epoch 1141/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9708 - val_loss: 27.5591\n",
      "Epoch 1142/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8548 - val_loss: 26.7978\n",
      "Epoch 1143/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7391 - val_loss: 27.6458\n",
      "Epoch 1144/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7879 - val_loss: 26.5676\n",
      "Epoch 1145/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8003 - val_loss: 27.9835\n",
      "Epoch 1146/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2834 - val_loss: 27.3352\n",
      "Epoch 1147/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1881 - val_loss: 28.9642\n",
      "Epoch 1148/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4041 - val_loss: 27.9652\n",
      "Epoch 1149/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2180 - val_loss: 26.7833\n",
      "Epoch 1150/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1360 - val_loss: 27.5138\n",
      "Epoch 1151/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9900 - val_loss: 26.7580\n",
      "Epoch 1152/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9096 - val_loss: 27.2187\n",
      "Epoch 1153/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8561 - val_loss: 26.7857\n",
      "Epoch 1154/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7148 - val_loss: 27.3987\n",
      "Epoch 1155/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7997 - val_loss: 27.3165\n",
      "Epoch 1156/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1494 - val_loss: 27.3339\n",
      "Epoch 1157/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8089 - val_loss: 26.8354\n",
      "Epoch 1158/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7058 - val_loss: 27.4838\n",
      "Epoch 1159/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8882 - val_loss: 28.1173\n",
      "Epoch 1160/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8827 - val_loss: 27.1445\n",
      "Epoch 1161/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9048 - val_loss: 26.7994\n",
      "Epoch 1162/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6166 - val_loss: 27.5384\n",
      "Epoch 1163/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0151 - val_loss: 27.2421\n",
      "Epoch 1164/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7846 - val_loss: 27.0002\n",
      "Epoch 1165/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6738 - val_loss: 27.2309\n",
      "Epoch 1166/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7263 - val_loss: 27.2422\n",
      "Epoch 1167/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9294 - val_loss: 27.6496\n",
      "Epoch 1168/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6393 - val_loss: 27.6536\n",
      "Epoch 1169/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8243 - val_loss: 27.4340\n",
      "Epoch 1170/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6791 - val_loss: 28.2524\n",
      "Epoch 1171/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0741 - val_loss: 27.8696\n",
      "Epoch 1172/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9192 - val_loss: 26.8895\n",
      "Epoch 1173/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9162 - val_loss: 26.6891\n",
      "Epoch 1174/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6291 - val_loss: 27.0418\n",
      "Epoch 1175/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6449 - val_loss: 27.2117\n",
      "Epoch 1176/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7017 - val_loss: 27.5713\n",
      "Epoch 1177/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4694 - val_loss: 26.6549\n",
      "Epoch 1178/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6659 - val_loss: 26.9573\n",
      "Epoch 1179/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6919 - val_loss: 26.6096\n",
      "Epoch 1180/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6469 - val_loss: 26.5515\n",
      "Epoch 1181/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4935 - val_loss: 27.4764\n",
      "Epoch 1182/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8112 - val_loss: 27.0952\n",
      "Epoch 1183/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6950 - val_loss: 26.7593\n",
      "Epoch 1184/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.5781 - val_loss: 27.1987\n",
      "Epoch 1185/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6968 - val_loss: 26.7548\n",
      "Epoch 1186/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6285 - val_loss: 27.5395\n",
      "Epoch 1187/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8865 - val_loss: 27.7060\n",
      "Epoch 1188/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0189 - val_loss: 27.4949\n",
      "Epoch 1189/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9801 - val_loss: 28.3947\n",
      "Epoch 1190/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2628 - val_loss: 27.6787\n",
      "Epoch 1191/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1949 - val_loss: 27.6593\n",
      "Epoch 1192/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1081 - val_loss: 28.2672\n",
      "Epoch 1193/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1174 - val_loss: 27.2458\n",
      "Epoch 1194/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9635 - val_loss: 27.2517\n",
      "Epoch 1195/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1330 - val_loss: 27.2288\n",
      "Epoch 1196/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8682 - val_loss: 28.6756\n",
      "Epoch 1197/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0568 - val_loss: 26.5268\n",
      "Epoch 1198/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7425 - val_loss: 27.6582\n",
      "Epoch 1199/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7100 - val_loss: 27.1858\n",
      "Epoch 1200/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 24.8030\n",
      "Epoch 01200: saving model to saved_models/latent16/cp-1200.h5\n",
      "6/6 [==============================] - 1s 184ms/step - loss: 24.8030 - val_loss: 26.5520\n",
      "Epoch 1201/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9708 - val_loss: 27.2896\n",
      "Epoch 1202/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8770 - val_loss: 27.4178\n",
      "Epoch 1203/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0969 - val_loss: 27.3371\n",
      "Epoch 1204/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7535 - val_loss: 26.6347\n",
      "Epoch 1205/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6943 - val_loss: 27.8858\n",
      "Epoch 1206/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9765 - val_loss: 27.0369\n",
      "Epoch 1207/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7968 - val_loss: 27.6455\n",
      "Epoch 1208/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9013 - val_loss: 27.6179\n",
      "Epoch 1209/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2755 - val_loss: 27.4115\n",
      "Epoch 1210/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8504 - val_loss: 27.8065\n",
      "Epoch 1211/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2327 - val_loss: 27.4160\n",
      "Epoch 1212/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9342 - val_loss: 27.4394\n",
      "Epoch 1213/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7360 - val_loss: 27.5594\n",
      "Epoch 1214/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5106 - val_loss: 27.5987\n",
      "Epoch 1215/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7317 - val_loss: 27.8908\n",
      "Epoch 1216/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7572 - val_loss: 26.6158\n",
      "Epoch 1217/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.4137 - val_loss: 27.1129\n",
      "Epoch 1218/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6035 - val_loss: 26.8283\n",
      "Epoch 1219/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4494 - val_loss: 27.0578\n",
      "Epoch 1220/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6307 - val_loss: 27.7878\n",
      "Epoch 1221/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7152 - val_loss: 27.2848\n",
      "Epoch 1222/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0991 - val_loss: 28.0253\n",
      "Epoch 1223/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2430 - val_loss: 28.2462\n",
      "Epoch 1224/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1092 - val_loss: 27.4445\n",
      "Epoch 1225/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5698 - val_loss: 27.5427\n",
      "Epoch 1226/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5672 - val_loss: 26.4477\n",
      "Epoch 1227/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5392 - val_loss: 27.3614\n",
      "Epoch 1228/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4233 - val_loss: 27.2300\n",
      "Epoch 1229/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8106 - val_loss: 26.6428\n",
      "Epoch 1230/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5724 - val_loss: 26.9365\n",
      "Epoch 1231/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5908 - val_loss: 26.7630\n",
      "Epoch 1232/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5353 - val_loss: 27.1102\n",
      "Epoch 1233/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4750 - val_loss: 27.5821\n",
      "Epoch 1234/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.3652 - val_loss: 27.3344\n",
      "Epoch 1235/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5991 - val_loss: 27.0877\n",
      "Epoch 1236/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4941 - val_loss: 26.7353\n",
      "Epoch 1237/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2544 - val_loss: 26.8821\n",
      "Epoch 1238/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4316 - val_loss: 27.0248\n",
      "Epoch 1239/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3841 - val_loss: 26.5149\n",
      "Epoch 1240/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5678 - val_loss: 27.2787\n",
      "Epoch 1241/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4820 - val_loss: 26.9271\n",
      "Epoch 1242/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4222 - val_loss: 26.9826\n",
      "Epoch 1243/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6479 - val_loss: 27.1299\n",
      "Epoch 1244/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6318 - val_loss: 27.6975\n",
      "Epoch 1245/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5886 - val_loss: 26.7691\n",
      "Epoch 1246/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5453 - val_loss: 27.1202\n",
      "Epoch 1247/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8008 - val_loss: 27.3449\n",
      "Epoch 1248/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5449 - val_loss: 26.8058\n",
      "Epoch 1249/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4088 - val_loss: 27.2279\n",
      "Epoch 1250/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7555 - val_loss: 27.4532\n",
      "Epoch 1251/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6353 - val_loss: 27.0546\n",
      "Epoch 1252/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3638 - val_loss: 26.6117\n",
      "Epoch 1253/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6389 - val_loss: 27.5158\n",
      "Epoch 1254/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7186 - val_loss: 27.5310\n",
      "Epoch 1255/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7584 - val_loss: 27.3115\n",
      "Epoch 1256/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7213 - val_loss: 26.7640\n",
      "Epoch 1257/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3650 - val_loss: 26.7155\n",
      "Epoch 1258/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4851 - val_loss: 27.1285\n",
      "Epoch 1259/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3793 - val_loss: 26.6802\n",
      "Epoch 1260/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4118 - val_loss: 27.3415\n",
      "Epoch 1261/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6536 - val_loss: 28.1573\n",
      "Epoch 1262/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8081 - val_loss: 27.7619\n",
      "Epoch 1263/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7828 - val_loss: 27.3384\n",
      "Epoch 1264/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8687 - val_loss: 27.4000\n",
      "Epoch 1265/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6287 - val_loss: 27.6250\n",
      "Epoch 1266/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6602 - val_loss: 26.6126\n",
      "Epoch 1267/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5638 - val_loss: 28.2193\n",
      "Epoch 1268/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7277 - val_loss: 27.0935\n",
      "Epoch 1269/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5886 - val_loss: 27.9568\n",
      "Epoch 1270/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4982 - val_loss: 27.3878\n",
      "Epoch 1271/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 24.5085 - val_loss: 27.0800\n",
      "Epoch 1272/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8544 - val_loss: 27.6176\n",
      "Epoch 1273/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6546 - val_loss: 27.3184\n",
      "Epoch 1274/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6413 - val_loss: 27.2301\n",
      "Epoch 1275/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7790 - val_loss: 28.2085\n",
      "Epoch 1276/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9061 - val_loss: 27.6465\n",
      "Epoch 1277/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9794 - val_loss: 27.5658\n",
      "Epoch 1278/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4816 - val_loss: 27.3188\n",
      "Epoch 1279/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7812 - val_loss: 27.1516\n",
      "Epoch 1280/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5893 - val_loss: 27.2611\n",
      "Epoch 1281/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4918 - val_loss: 26.5485\n",
      "Epoch 1282/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5171 - val_loss: 27.5904\n",
      "Epoch 1283/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9101 - val_loss: 27.0440\n",
      "Epoch 1284/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7819 - val_loss: 26.7061\n",
      "Epoch 1285/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5329 - val_loss: 26.2115\n",
      "Epoch 1286/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4821 - val_loss: 26.8257\n",
      "Epoch 1287/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3149 - val_loss: 26.5564\n",
      "Epoch 1288/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7278 - val_loss: 27.2844\n",
      "Epoch 1289/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6880 - val_loss: 28.0687\n",
      "Epoch 1290/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0591 - val_loss: 28.2345\n",
      "Epoch 1291/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9982 - val_loss: 27.5410\n",
      "Epoch 1292/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6860 - val_loss: 27.3659\n",
      "Epoch 1293/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3221 - val_loss: 27.4870\n",
      "Epoch 1294/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4270 - val_loss: 26.7793\n",
      "Epoch 1295/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2113 - val_loss: 27.2163\n",
      "Epoch 1296/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3825 - val_loss: 27.2556\n",
      "Epoch 1297/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3904 - val_loss: 26.6472\n",
      "Epoch 1298/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4491 - val_loss: 26.9640\n",
      "Epoch 1299/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3625 - val_loss: 27.5823\n",
      "Epoch 1300/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3409 - val_loss: 26.9847\n",
      "Epoch 1301/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4247 - val_loss: 27.3683\n",
      "Epoch 1302/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3809 - val_loss: 27.2060\n",
      "Epoch 1303/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0033 - val_loss: 27.7388\n",
      "Epoch 1304/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7178 - val_loss: 26.7061\n",
      "Epoch 1305/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4949 - val_loss: 27.1781\n",
      "Epoch 1306/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 24.3647 - val_loss: 27.7528\n",
      "Epoch 1307/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4658 - val_loss: 27.3507\n",
      "Epoch 1308/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5001 - val_loss: 27.2480\n",
      "Epoch 1309/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5167 - val_loss: 27.2854\n",
      "Epoch 1310/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3802 - val_loss: 27.1557\n",
      "Epoch 1311/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3061 - val_loss: 26.5864\n",
      "Epoch 1312/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4769 - val_loss: 26.8692\n",
      "Epoch 1313/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5698 - val_loss: 27.3299\n",
      "Epoch 1314/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4361 - val_loss: 27.1342\n",
      "Epoch 1315/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2686 - val_loss: 26.8391\n",
      "Epoch 1316/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4053 - val_loss: 27.3341\n",
      "Epoch 1317/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3892 - val_loss: 27.4986\n",
      "Epoch 1318/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7526 - val_loss: 27.6806\n",
      "Epoch 1319/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6323 - val_loss: 27.1201\n",
      "Epoch 1320/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6373 - val_loss: 26.9466\n",
      "Epoch 1321/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2757 - val_loss: 27.0594\n",
      "Epoch 1322/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4505 - val_loss: 27.0437\n",
      "Epoch 1323/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4439 - val_loss: 27.4212\n",
      "Epoch 1324/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4494 - val_loss: 28.1258\n",
      "Epoch 1325/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9364 - val_loss: 27.6444\n",
      "Epoch 1326/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7819 - val_loss: 27.3612\n",
      "Epoch 1327/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7696 - val_loss: 27.7317\n",
      "Epoch 1328/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8517 - val_loss: 27.4398\n",
      "Epoch 1329/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6516 - val_loss: 27.6862\n",
      "Epoch 1330/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8204 - val_loss: 27.5178\n",
      "Epoch 1331/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6598 - val_loss: 26.3983\n",
      "Epoch 1332/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2971 - val_loss: 26.8767\n",
      "Epoch 1333/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5323 - val_loss: 27.3559\n",
      "Epoch 1334/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3185 - val_loss: 26.1372\n",
      "Epoch 1335/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1981 - val_loss: 26.5567\n",
      "Epoch 1336/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1166 - val_loss: 26.8272\n",
      "Epoch 1337/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3302 - val_loss: 27.1293\n",
      "Epoch 1338/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2234 - val_loss: 27.0977\n",
      "Epoch 1339/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3557 - val_loss: 27.0497\n",
      "Epoch 1340/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1533 - val_loss: 26.4609\n",
      "Epoch 1341/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3131 - val_loss: 26.8973\n",
      "Epoch 1342/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1958 - val_loss: 26.6549\n",
      "Epoch 1343/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4279 - val_loss: 27.1289\n",
      "Epoch 1344/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2745 - val_loss: 26.5653\n",
      "Epoch 1345/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3608 - val_loss: 27.0324\n",
      "Epoch 1346/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3418 - val_loss: 26.6571\n",
      "Epoch 1347/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3908 - val_loss: 27.4051\n",
      "Epoch 1348/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5030 - val_loss: 27.2488\n",
      "Epoch 1349/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5425 - val_loss: 26.9412\n",
      "Epoch 1350/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4886 - val_loss: 27.3338\n",
      "Epoch 1351/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4718 - val_loss: 27.5420\n",
      "Epoch 1352/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5577 - val_loss: 27.5961\n",
      "Epoch 1353/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7724 - val_loss: 27.8422\n",
      "Epoch 1354/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8451 - val_loss: 27.8669\n",
      "Epoch 1355/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4618 - val_loss: 26.1442\n",
      "Epoch 1356/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2848 - val_loss: 26.6896\n",
      "Epoch 1357/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4369 - val_loss: 27.2160\n",
      "Epoch 1358/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6326 - val_loss: 26.4245\n",
      "Epoch 1359/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4949 - val_loss: 27.4466\n",
      "Epoch 1360/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5444 - val_loss: 26.6306\n",
      "Epoch 1361/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3699 - val_loss: 27.0464\n",
      "Epoch 1362/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5138 - val_loss: 26.9666\n",
      "Epoch 1363/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2900 - val_loss: 27.0402\n",
      "Epoch 1364/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4335 - val_loss: 26.5062\n",
      "Epoch 1365/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1849 - val_loss: 26.2520\n",
      "Epoch 1366/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4669 - val_loss: 26.6715\n",
      "Epoch 1367/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5183 - val_loss: 27.7514\n",
      "Epoch 1368/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4091 - val_loss: 27.5470\n",
      "Epoch 1369/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0660 - val_loss: 26.1790\n",
      "Epoch 1370/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2341 - val_loss: 27.1994\n",
      "Epoch 1371/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2256 - val_loss: 27.0737\n",
      "Epoch 1372/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2974 - val_loss: 26.7411\n",
      "Epoch 1373/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2022 - val_loss: 27.6015\n",
      "Epoch 1374/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2078 - val_loss: 27.1259\n",
      "Epoch 1375/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1951 - val_loss: 27.0465\n",
      "Epoch 1376/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3126 - val_loss: 26.6373\n",
      "Epoch 1377/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2187 - val_loss: 26.9334\n",
      "Epoch 1378/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3160 - val_loss: 27.6491\n",
      "Epoch 1379/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1970 - val_loss: 27.3269\n",
      "Epoch 1380/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2238 - val_loss: 26.3241\n",
      "Epoch 1381/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1281 - val_loss: 26.5768\n",
      "Epoch 1382/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1786 - val_loss: 26.9561\n",
      "Epoch 1383/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2191 - val_loss: 26.2127\n",
      "Epoch 1384/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1819 - val_loss: 26.5717\n",
      "Epoch 1385/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3933 - val_loss: 26.7464\n",
      "Epoch 1386/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1422 - val_loss: 26.2985\n",
      "Epoch 1387/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1412 - val_loss: 26.8069\n",
      "Epoch 1388/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1347 - val_loss: 26.7487\n",
      "Epoch 1389/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1793 - val_loss: 26.5701\n",
      "Epoch 1390/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0068 - val_loss: 26.5033\n",
      "Epoch 1391/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3352 - val_loss: 27.6328\n",
      "Epoch 1392/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3795 - val_loss: 27.1411\n",
      "Epoch 1393/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2691 - val_loss: 27.3506\n",
      "Epoch 1394/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4844 - val_loss: 27.2609\n",
      "Epoch 1395/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5414 - val_loss: 27.1055\n",
      "Epoch 1396/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2633 - val_loss: 27.1403\n",
      "Epoch 1397/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2619 - val_loss: 26.6302\n",
      "Epoch 1398/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2217 - val_loss: 27.3508\n",
      "Epoch 1399/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3416 - val_loss: 26.8470\n",
      "Epoch 1400/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 24.3126\n",
      "Epoch 01400: saving model to saved_models/latent16/cp-1400.h5\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 24.3126 - val_loss: 26.8421\n",
      "Epoch 1401/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2216 - val_loss: 26.9057\n",
      "Epoch 1402/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2519 - val_loss: 26.7839\n",
      "Epoch 1403/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1132 - val_loss: 27.0990\n",
      "Epoch 1404/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4460 - val_loss: 26.4169\n",
      "Epoch 1405/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2195 - val_loss: 26.9329\n",
      "Epoch 1406/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3530 - val_loss: 27.7714\n",
      "Epoch 1407/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3516 - val_loss: 26.4359\n",
      "Epoch 1408/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3341 - val_loss: 27.0807\n",
      "Epoch 1409/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2730 - val_loss: 26.9933\n",
      "Epoch 1410/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3773 - val_loss: 26.6206\n",
      "Epoch 1411/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3041 - val_loss: 26.5845\n",
      "Epoch 1412/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3531 - val_loss: 27.2570\n",
      "Epoch 1413/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1461 - val_loss: 26.8782\n",
      "Epoch 1414/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2723 - val_loss: 27.0094\n",
      "Epoch 1415/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1356 - val_loss: 27.2189\n",
      "Epoch 1416/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4221 - val_loss: 26.2979\n",
      "Epoch 1417/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3767 - val_loss: 26.7333\n",
      "Epoch 1418/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3341 - val_loss: 27.9056\n",
      "Epoch 1419/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4461 - val_loss: 27.0272\n",
      "Epoch 1420/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2385 - val_loss: 27.6129\n",
      "Epoch 1421/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4447 - val_loss: 26.2048\n",
      "Epoch 1422/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2507 - val_loss: 27.8049\n",
      "Epoch 1423/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5722 - val_loss: 27.7885\n",
      "Epoch 1424/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4360 - val_loss: 26.4498\n",
      "Epoch 1425/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3045 - val_loss: 28.6335\n",
      "Epoch 1426/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4329 - val_loss: 26.5742\n",
      "Epoch 1427/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5584 - val_loss: 27.4970\n",
      "Epoch 1428/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4705 - val_loss: 27.6907\n",
      "Epoch 1429/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3970 - val_loss: 26.4499\n",
      "Epoch 1430/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9044 - val_loss: 26.4445\n",
      "Epoch 1431/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1826 - val_loss: 26.4948\n",
      "Epoch 1432/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0464 - val_loss: 26.5021\n",
      "Epoch 1433/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0472 - val_loss: 27.0437\n",
      "Epoch 1434/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2870 - val_loss: 27.0730\n",
      "Epoch 1435/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1446 - val_loss: 27.2397\n",
      "Epoch 1436/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1291 - val_loss: 26.8999\n",
      "Epoch 1437/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3224 - val_loss: 26.7076\n",
      "Epoch 1438/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2747 - val_loss: 26.7827\n",
      "Epoch 1439/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2070 - val_loss: 26.8065\n",
      "Epoch 1440/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4016 - val_loss: 27.1129\n",
      "Epoch 1441/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1428 - val_loss: 25.9579\n",
      "Epoch 1442/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0079 - val_loss: 26.4784\n",
      "Epoch 1443/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1273 - val_loss: 27.1037\n",
      "Epoch 1444/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2037 - val_loss: 26.8587\n",
      "Epoch 1445/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1125 - val_loss: 26.2730\n",
      "Epoch 1446/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0225 - val_loss: 26.1522\n",
      "Epoch 1447/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0649 - val_loss: 27.1879\n",
      "Epoch 1448/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1222 - val_loss: 27.0208\n",
      "Epoch 1449/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1943 - val_loss: 26.9802\n",
      "Epoch 1450/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4928 - val_loss: 26.4853\n",
      "Epoch 1451/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1118 - val_loss: 27.3358\n",
      "Epoch 1452/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2556 - val_loss: 27.5144\n",
      "Epoch 1453/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3491 - val_loss: 26.9276\n",
      "Epoch 1454/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2155 - val_loss: 26.9366\n",
      "Epoch 1455/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2255 - val_loss: 27.5821\n",
      "Epoch 1456/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4990 - val_loss: 26.5785\n",
      "Epoch 1457/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4100 - val_loss: 27.3006\n",
      "Epoch 1458/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3740 - val_loss: 26.4102\n",
      "Epoch 1459/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2932 - val_loss: 26.1068\n",
      "Epoch 1460/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1874 - val_loss: 26.7453\n",
      "Epoch 1461/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0311 - val_loss: 26.5821\n",
      "Epoch 1462/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0409 - val_loss: 26.6920\n",
      "Epoch 1463/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1911 - val_loss: 26.5009\n",
      "Epoch 1464/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1827 - val_loss: 26.3455\n",
      "Epoch 1465/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4655 - val_loss: 26.7148\n",
      "Epoch 1466/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2654 - val_loss: 26.6664\n",
      "Epoch 1467/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1554 - val_loss: 27.4147\n",
      "Epoch 1468/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2855 - val_loss: 27.1220\n",
      "Epoch 1469/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2694 - val_loss: 26.4032\n",
      "Epoch 1470/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3458 - val_loss: 26.9111\n",
      "Epoch 1471/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1870 - val_loss: 27.1752\n",
      "Epoch 1472/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4438 - val_loss: 26.8159\n",
      "Epoch 1473/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0883 - val_loss: 26.5052\n",
      "Epoch 1474/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8101 - val_loss: 25.8985\n",
      "Epoch 1475/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0579 - val_loss: 26.4583\n",
      "Epoch 1476/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3413 - val_loss: 26.3947\n",
      "Epoch 1477/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3172 - val_loss: 27.2592\n",
      "Epoch 1478/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4205 - val_loss: 27.8698\n",
      "Epoch 1479/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4601 - val_loss: 27.5697\n",
      "Epoch 1480/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1005 - val_loss: 27.5892\n",
      "Epoch 1481/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1429 - val_loss: 26.8852\n",
      "Epoch 1482/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5941 - val_loss: 27.9958\n",
      "Epoch 1483/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.5940 - val_loss: 27.9172\n",
      "Epoch 1484/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3455 - val_loss: 26.5078\n",
      "Epoch 1485/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1835 - val_loss: 26.4460\n",
      "Epoch 1486/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8496 - val_loss: 26.5229\n",
      "Epoch 1487/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1587 - val_loss: 27.1261\n",
      "Epoch 1488/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8732 - val_loss: 26.7462\n",
      "Epoch 1489/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9742 - val_loss: 26.8677\n",
      "Epoch 1490/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0182 - val_loss: 26.5404\n",
      "Epoch 1491/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9880 - val_loss: 26.2481\n",
      "Epoch 1492/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9673 - val_loss: 25.9634\n",
      "Epoch 1493/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9902 - val_loss: 26.6994\n",
      "Epoch 1494/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9217 - val_loss: 26.3929\n",
      "Epoch 1495/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8323 - val_loss: 26.2546\n",
      "Epoch 1496/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7218 - val_loss: 26.5958\n",
      "Epoch 1497/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0133 - val_loss: 26.1821\n",
      "Epoch 1498/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1494 - val_loss: 26.7596\n",
      "Epoch 1499/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4566 - val_loss: 26.4107\n",
      "Epoch 1500/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4922 - val_loss: 26.5987\n",
      "Epoch 1501/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3175 - val_loss: 27.9035\n",
      "Epoch 1502/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3447 - val_loss: 26.8078\n",
      "Epoch 1503/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1777 - val_loss: 26.4577\n",
      "Epoch 1504/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1252 - val_loss: 26.4338\n",
      "Epoch 1505/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2142 - val_loss: 26.6214\n",
      "Epoch 1506/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1396 - val_loss: 27.5199\n",
      "Epoch 1507/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4172 - val_loss: 27.4849\n",
      "Epoch 1508/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4239 - val_loss: 26.9514\n",
      "Epoch 1509/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3531 - val_loss: 26.5887\n",
      "Epoch 1510/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2106 - val_loss: 27.4170\n",
      "Epoch 1511/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4094 - val_loss: 26.5506\n",
      "Epoch 1512/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1818 - val_loss: 26.8592\n",
      "Epoch 1513/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0852 - val_loss: 27.3462\n",
      "Epoch 1514/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2624 - val_loss: 26.6236\n",
      "Epoch 1515/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0745 - val_loss: 26.8351\n",
      "Epoch 1516/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9529 - val_loss: 26.6144\n",
      "Epoch 1517/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9625 - val_loss: 26.6388\n",
      "Epoch 1518/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8821 - val_loss: 26.9184\n",
      "Epoch 1519/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.6734 - val_loss: 26.2374\n",
      "Epoch 1520/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7315 - val_loss: 26.2899\n",
      "Epoch 1521/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9443 - val_loss: 26.8449\n",
      "Epoch 1522/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9948 - val_loss: 27.1698\n",
      "Epoch 1523/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0032 - val_loss: 26.8708\n",
      "Epoch 1524/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1124 - val_loss: 26.6148\n",
      "Epoch 1525/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2240 - val_loss: 27.1521\n",
      "Epoch 1526/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0453 - val_loss: 26.9031\n",
      "Epoch 1527/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1409 - val_loss: 26.6795\n",
      "Epoch 1528/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3731 - val_loss: 26.6486\n",
      "Epoch 1529/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0766 - val_loss: 26.9985\n",
      "Epoch 1530/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3449 - val_loss: 26.5539\n",
      "Epoch 1531/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3009 - val_loss: 27.7681\n",
      "Epoch 1532/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6676 - val_loss: 27.5358\n",
      "Epoch 1533/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6217 - val_loss: 26.1527\n",
      "Epoch 1534/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3575 - val_loss: 26.7712\n",
      "Epoch 1535/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9871 - val_loss: 26.7377\n",
      "Epoch 1536/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9078 - val_loss: 26.7452\n",
      "Epoch 1537/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0540 - val_loss: 26.8272\n",
      "Epoch 1538/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1462 - val_loss: 26.9339\n",
      "Epoch 1539/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9466 - val_loss: 27.4155\n",
      "Epoch 1540/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8458 - val_loss: 26.0228\n",
      "Epoch 1541/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8790 - val_loss: 26.8710\n",
      "Epoch 1542/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0345 - val_loss: 26.7705\n",
      "Epoch 1543/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8589 - val_loss: 26.1845\n",
      "Epoch 1544/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8268 - val_loss: 26.9576\n",
      "Epoch 1545/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7173 - val_loss: 26.5721\n",
      "Epoch 1546/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6920 - val_loss: 27.1546\n",
      "Epoch 1547/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8763 - val_loss: 26.1213\n",
      "Epoch 1548/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6651 - val_loss: 26.4278\n",
      "Epoch 1549/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9049 - val_loss: 26.4065\n",
      "Epoch 1550/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8929 - val_loss: 26.7542\n",
      "Epoch 1551/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9184 - val_loss: 26.0829\n",
      "Epoch 1552/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9118 - val_loss: 27.5554\n",
      "Epoch 1553/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0721 - val_loss: 26.5654\n",
      "Epoch 1554/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0481 - val_loss: 26.5821\n",
      "Epoch 1555/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0442 - val_loss: 26.3706\n",
      "Epoch 1556/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9639 - val_loss: 26.4130\n",
      "Epoch 1557/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7681 - val_loss: 26.9451\n",
      "Epoch 1558/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8730 - val_loss: 26.9690\n",
      "Epoch 1559/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1078 - val_loss: 26.6745\n",
      "Epoch 1560/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2984 - val_loss: 26.6641\n",
      "Epoch 1561/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3144 - val_loss: 26.9191\n",
      "Epoch 1562/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0969 - val_loss: 27.0456\n",
      "Epoch 1563/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2113 - val_loss: 27.0031\n",
      "Epoch 1564/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1410 - val_loss: 26.5201\n",
      "Epoch 1565/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1795 - val_loss: 26.2083\n",
      "Epoch 1566/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0671 - val_loss: 26.7964\n",
      "Epoch 1567/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9425 - val_loss: 25.9531\n",
      "Epoch 1568/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8545 - val_loss: 26.7734\n",
      "Epoch 1569/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9884 - val_loss: 26.4916\n",
      "Epoch 1570/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7646 - val_loss: 26.4283\n",
      "Epoch 1571/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8584 - val_loss: 27.0338\n",
      "Epoch 1572/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9730 - val_loss: 26.4923\n",
      "Epoch 1573/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0625 - val_loss: 26.1560\n",
      "Epoch 1574/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1496 - val_loss: 26.6301\n",
      "Epoch 1575/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9900 - val_loss: 26.8529\n",
      "Epoch 1576/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9837 - val_loss: 27.2822\n",
      "Epoch 1577/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0623 - val_loss: 27.4893\n",
      "Epoch 1578/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2413 - val_loss: 26.3862\n",
      "Epoch 1579/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6327 - val_loss: 27.2953\n",
      "Epoch 1580/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2978 - val_loss: 26.9718\n",
      "Epoch 1581/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3054 - val_loss: 26.5718\n",
      "Epoch 1582/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3129 - val_loss: 26.7876\n",
      "Epoch 1583/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2406 - val_loss: 27.4449\n",
      "Epoch 1584/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2541 - val_loss: 26.4898\n",
      "Epoch 1585/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0198 - val_loss: 26.6809\n",
      "Epoch 1586/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8441 - val_loss: 26.5793\n",
      "Epoch 1587/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9251 - val_loss: 27.3433\n",
      "Epoch 1588/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0343 - val_loss: 26.4496\n",
      "Epoch 1589/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8845 - val_loss: 27.4831\n",
      "Epoch 1590/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0435 - val_loss: 26.3773\n",
      "Epoch 1591/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8560 - val_loss: 26.5767\n",
      "Epoch 1592/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8136 - val_loss: 26.4274\n",
      "Epoch 1593/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7654 - val_loss: 26.3466\n",
      "Epoch 1594/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7440 - val_loss: 25.8907\n",
      "Epoch 1595/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0169 - val_loss: 26.5073\n",
      "Epoch 1596/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9719 - val_loss: 25.9478\n",
      "Epoch 1597/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.9110 - val_loss: 26.1436\n",
      "Epoch 1598/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8201 - val_loss: 26.6543\n",
      "Epoch 1599/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6847 - val_loss: 26.2480\n",
      "Epoch 1600/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 23.7583\n",
      "Epoch 01600: saving model to saved_models/latent16/cp-1600.h5\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 23.7583 - val_loss: 26.3513\n",
      "Epoch 1601/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7406 - val_loss: 26.9642\n",
      "Epoch 1602/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6508 - val_loss: 26.3306\n",
      "Epoch 1603/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7353 - val_loss: 27.1090\n",
      "Epoch 1604/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6120 - val_loss: 26.7293\n",
      "Epoch 1605/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5528 - val_loss: 26.5848\n",
      "Epoch 1606/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6499 - val_loss: 26.1535\n",
      "Epoch 1607/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7996 - val_loss: 26.9190\n",
      "Epoch 1608/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9364 - val_loss: 26.9560\n",
      "Epoch 1609/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7943 - val_loss: 26.4156\n",
      "Epoch 1610/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8048 - val_loss: 26.8649\n",
      "Epoch 1611/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8564 - val_loss: 26.4893\n",
      "Epoch 1612/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6318 - val_loss: 25.9415\n",
      "Epoch 1613/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7108 - val_loss: 26.2002\n",
      "Epoch 1614/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7605 - val_loss: 25.8083\n",
      "Epoch 1615/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8769 - val_loss: 26.6622\n",
      "Epoch 1616/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8446 - val_loss: 26.4340\n",
      "Epoch 1617/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6280 - val_loss: 26.5581\n",
      "Epoch 1618/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7860 - val_loss: 26.5626\n",
      "Epoch 1619/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6322 - val_loss: 26.6167\n",
      "Epoch 1620/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5626 - val_loss: 26.2996\n",
      "Epoch 1621/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6504 - val_loss: 27.0586\n",
      "Epoch 1622/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0614 - val_loss: 26.9871\n",
      "Epoch 1623/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3668 - val_loss: 27.0297\n",
      "Epoch 1624/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5273 - val_loss: 27.5280\n",
      "Epoch 1625/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2182 - val_loss: 26.7037\n",
      "Epoch 1626/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3530 - val_loss: 26.8914\n",
      "Epoch 1627/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1166 - val_loss: 26.9592\n",
      "Epoch 1628/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8368 - val_loss: 26.4777\n",
      "Epoch 1629/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7421 - val_loss: 26.0016\n",
      "Epoch 1630/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7542 - val_loss: 26.3367\n",
      "Epoch 1631/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7380 - val_loss: 26.8388\n",
      "Epoch 1632/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7740 - val_loss: 26.2955\n",
      "Epoch 1633/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5779 - val_loss: 26.8483\n",
      "Epoch 1634/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6294 - val_loss: 26.4913\n",
      "Epoch 1635/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9620 - val_loss: 26.0916\n",
      "Epoch 1636/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9526 - val_loss: 26.1736\n",
      "Epoch 1637/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6556 - val_loss: 26.3932\n",
      "Epoch 1638/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5531 - val_loss: 27.2637\n",
      "Epoch 1639/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7443 - val_loss: 26.2659\n",
      "Epoch 1640/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6426 - val_loss: 26.4894\n",
      "Epoch 1641/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7998 - val_loss: 27.0123\n",
      "Epoch 1642/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9048 - val_loss: 26.6901\n",
      "Epoch 1643/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8646 - val_loss: 27.1000\n",
      "Epoch 1644/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1417 - val_loss: 26.0282\n",
      "Epoch 1645/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9695 - val_loss: 26.7907\n",
      "Epoch 1646/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8058 - val_loss: 26.8670\n",
      "Epoch 1647/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8198 - val_loss: 25.9057\n",
      "Epoch 1648/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8384 - val_loss: 26.3788\n",
      "Epoch 1649/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8590 - val_loss: 26.8407\n",
      "Epoch 1650/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1071 - val_loss: 26.2194\n",
      "Epoch 1651/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9355 - val_loss: 27.0373\n",
      "Epoch 1652/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9031 - val_loss: 26.2959\n",
      "Epoch 1653/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6572 - val_loss: 26.5041\n",
      "Epoch 1654/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6241 - val_loss: 26.7705\n",
      "Epoch 1655/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8457 - val_loss: 26.3150\n",
      "Epoch 1656/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7614 - val_loss: 26.6405\n",
      "Epoch 1657/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9347 - val_loss: 27.0621\n",
      "Epoch 1658/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5910 - val_loss: 26.0966\n",
      "Epoch 1659/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5850 - val_loss: 26.4040\n",
      "Epoch 1660/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.6898 - val_loss: 26.1810\n",
      "Epoch 1661/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7226 - val_loss: 26.2142\n",
      "Epoch 1662/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9322 - val_loss: 26.6599\n",
      "Epoch 1663/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6885 - val_loss: 26.0053\n",
      "Epoch 1664/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8962 - val_loss: 26.9006\n",
      "Epoch 1665/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6859 - val_loss: 26.5552\n",
      "Epoch 1666/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6730 - val_loss: 26.5144\n",
      "Epoch 1667/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.5231 - val_loss: 27.2044\n",
      "Epoch 1668/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8312 - val_loss: 26.9624\n",
      "Epoch 1669/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8106 - val_loss: 26.1689\n",
      "Epoch 1670/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7379 - val_loss: 27.0097\n",
      "Epoch 1671/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7827 - val_loss: 26.7548\n",
      "Epoch 1672/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7799 - val_loss: 25.4277\n",
      "Epoch 1673/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6013 - val_loss: 26.8805\n",
      "Epoch 1674/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8492 - val_loss: 26.7479\n",
      "Epoch 1675/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6641 - val_loss: 26.2406\n",
      "Epoch 1676/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.4561 - val_loss: 27.4270\n",
      "Epoch 1677/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5070 - val_loss: 26.6351\n",
      "Epoch 1678/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6580 - val_loss: 25.6972\n",
      "Epoch 1679/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.4441 - val_loss: 26.5080\n",
      "Epoch 1680/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4639 - val_loss: 25.9620\n",
      "Epoch 1681/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6302 - val_loss: 26.7809\n",
      "Epoch 1682/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8220 - val_loss: 26.2588\n",
      "Epoch 1683/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8602 - val_loss: 27.4506\n",
      "Epoch 1684/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9343 - val_loss: 26.4684\n",
      "Epoch 1685/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9724 - val_loss: 27.1525\n",
      "Epoch 1686/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7855 - val_loss: 26.3086\n",
      "Epoch 1687/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8444 - val_loss: 25.8080\n",
      "Epoch 1688/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8071 - val_loss: 26.5863\n",
      "Epoch 1689/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6557 - val_loss: 26.4624\n",
      "Epoch 1690/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9452 - val_loss: 26.7606\n",
      "Epoch 1691/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7965 - val_loss: 26.7495\n",
      "Epoch 1692/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1034 - val_loss: 26.9223\n",
      "Epoch 1693/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9131 - val_loss: 26.0920\n",
      "Epoch 1694/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6428 - val_loss: 26.1274\n",
      "Epoch 1695/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5161 - val_loss: 27.1115\n",
      "Epoch 1696/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4611 - val_loss: 26.9938\n",
      "Epoch 1697/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.4269 - val_loss: 26.7366\n",
      "Epoch 1698/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5900 - val_loss: 26.2578\n",
      "Epoch 1699/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.3912 - val_loss: 26.0532\n",
      "Epoch 1700/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5834 - val_loss: 26.0401\n",
      "Epoch 1701/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7062 - val_loss: 26.7919\n",
      "Epoch 1702/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8693 - val_loss: 26.0135\n",
      "Epoch 1703/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0645 - val_loss: 26.2584\n",
      "Epoch 1704/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6992 - val_loss: 26.9987\n",
      "Epoch 1705/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.7804 - val_loss: 26.3930\n",
      "Epoch 1706/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6663 - val_loss: 26.4905\n",
      "Epoch 1707/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4012 - val_loss: 26.0959\n",
      "Epoch 1708/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4728 - val_loss: 25.7737\n",
      "Epoch 1709/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.2967 - val_loss: 26.1627\n",
      "Epoch 1710/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5468 - val_loss: 26.3118\n",
      "Epoch 1711/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4583 - val_loss: 25.8800\n",
      "Epoch 1712/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.4601 - val_loss: 26.7096\n",
      "Epoch 1713/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3308 - val_loss: 26.8425\n",
      "Epoch 1714/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7160 - val_loss: 27.1027\n",
      "Epoch 1715/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1538 - val_loss: 26.6125\n",
      "Epoch 1716/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8775 - val_loss: 26.7170\n",
      "Epoch 1717/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7411 - val_loss: 26.3861\n",
      "Epoch 1718/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4806 - val_loss: 26.1183\n",
      "Epoch 1719/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4042 - val_loss: 26.2539\n",
      "Epoch 1720/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4250 - val_loss: 25.9388\n",
      "Epoch 1721/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4399 - val_loss: 25.7180\n",
      "Epoch 1722/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4299 - val_loss: 26.1384\n",
      "Epoch 1723/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3288 - val_loss: 26.2427\n",
      "Epoch 1724/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3533 - val_loss: 26.0365\n",
      "Epoch 1725/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7399 - val_loss: 25.8342\n",
      "Epoch 1726/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3305 - val_loss: 26.2136\n",
      "Epoch 1727/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7422 - val_loss: 26.1076\n",
      "Epoch 1728/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.4258 - val_loss: 25.8223\n",
      "Epoch 1729/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.3195 - val_loss: 25.9262\n",
      "Epoch 1730/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3552 - val_loss: 26.6172\n",
      "Epoch 1731/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7624 - val_loss: 26.4300\n",
      "Epoch 1732/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8452 - val_loss: 26.5178\n",
      "Epoch 1733/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0148 - val_loss: 26.3341\n",
      "Epoch 1734/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.2719 - val_loss: 26.1995\n",
      "Epoch 1735/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3446 - val_loss: 26.4874\n",
      "Epoch 1736/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5550 - val_loss: 26.5623\n",
      "Epoch 1737/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3634 - val_loss: 25.6923\n",
      "Epoch 1738/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6347 - val_loss: 26.5390\n",
      "Epoch 1739/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4902 - val_loss: 26.0113\n",
      "Epoch 1740/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6732 - val_loss: 26.2832\n",
      "Epoch 1741/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5476 - val_loss: 26.3840\n",
      "Epoch 1742/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.2246 - val_loss: 25.2762\n",
      "Epoch 1743/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.1899 - val_loss: 26.7203\n",
      "Epoch 1744/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2210 - val_loss: 26.2979\n",
      "Epoch 1745/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4255 - val_loss: 25.6705\n",
      "Epoch 1746/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4436 - val_loss: 25.7487\n",
      "Epoch 1747/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.4639 - val_loss: 25.9450\n",
      "Epoch 1748/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5555 - val_loss: 26.8129\n",
      "Epoch 1749/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8388 - val_loss: 26.8855\n",
      "Epoch 1750/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6330 - val_loss: 26.7396\n",
      "Epoch 1751/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7182 - val_loss: 26.4158\n",
      "Epoch 1752/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3326 - val_loss: 25.9623\n",
      "Epoch 1753/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.4623 - val_loss: 26.6660\n",
      "Epoch 1754/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3597 - val_loss: 26.5317\n",
      "Epoch 1755/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5705 - val_loss: 26.1411\n",
      "Epoch 1756/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6187 - val_loss: 26.4660\n",
      "Epoch 1757/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9637 - val_loss: 26.6242\n",
      "Epoch 1758/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5913 - val_loss: 25.8218\n",
      "Epoch 1759/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5461 - val_loss: 26.6333\n",
      "Epoch 1760/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6194 - val_loss: 25.8950\n",
      "Epoch 1761/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3906 - val_loss: 25.7963\n",
      "Epoch 1762/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.2642 - val_loss: 27.0117\n",
      "Epoch 1763/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7775 - val_loss: 25.9534\n",
      "Epoch 1764/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9846 - val_loss: 26.4562\n",
      "Epoch 1765/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5439 - val_loss: 27.1474\n",
      "Epoch 1766/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3446 - val_loss: 26.4825\n",
      "Epoch 1767/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4625 - val_loss: 26.2406\n",
      "Epoch 1768/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6057 - val_loss: 26.8033\n",
      "Epoch 1769/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3712 - val_loss: 26.0189\n",
      "Epoch 1770/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4131 - val_loss: 26.9417\n",
      "Epoch 1771/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4492 - val_loss: 26.7955\n",
      "Epoch 1772/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3757 - val_loss: 26.0833\n",
      "Epoch 1773/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2030 - val_loss: 26.0605\n",
      "Epoch 1774/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.1985 - val_loss: 26.8011\n",
      "Epoch 1775/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2157 - val_loss: 26.4960\n",
      "Epoch 1776/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4989 - val_loss: 25.6818\n",
      "Epoch 1777/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3231 - val_loss: 26.3665\n",
      "Epoch 1778/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2851 - val_loss: 26.5980\n",
      "Epoch 1779/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3015 - val_loss: 25.3083\n",
      "Epoch 1780/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5296 - val_loss: 26.5219\n",
      "Epoch 1781/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3938 - val_loss: 26.8801\n",
      "Epoch 1782/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5840 - val_loss: 26.8803\n",
      "Epoch 1783/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8671 - val_loss: 26.9837\n",
      "Epoch 1784/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9233 - val_loss: 26.3191\n",
      "Epoch 1785/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5556 - val_loss: 26.6646\n",
      "Epoch 1786/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4876 - val_loss: 26.5578\n",
      "Epoch 1787/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5288 - val_loss: 26.2117\n",
      "Epoch 1788/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5687 - val_loss: 25.9035\n",
      "Epoch 1789/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5854 - val_loss: 25.4385\n",
      "Epoch 1790/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5317 - val_loss: 25.7758\n",
      "Epoch 1791/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3800 - val_loss: 26.9025\n",
      "Epoch 1792/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6100 - val_loss: 26.2230\n",
      "Epoch 1793/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3525 - val_loss: 25.8680\n",
      "Epoch 1794/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3265 - val_loss: 25.5987\n",
      "Epoch 1795/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.0972 - val_loss: 27.0688\n",
      "Epoch 1796/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3487 - val_loss: 25.5504\n",
      "Epoch 1797/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3692 - val_loss: 26.1090\n",
      "Epoch 1798/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4514 - val_loss: 26.8160\n",
      "Epoch 1799/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4384 - val_loss: 25.8020\n",
      "Epoch 1800/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 23.3555\n",
      "Epoch 01800: saving model to saved_models/latent16/cp-1800.h5\n",
      "6/6 [==============================] - 1s 170ms/step - loss: 23.3555 - val_loss: 26.3603\n",
      "Epoch 1801/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5177 - val_loss: 25.9175\n",
      "Epoch 1802/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4243 - val_loss: 26.0920\n",
      "Epoch 1803/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2628 - val_loss: 26.3038\n",
      "Epoch 1804/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.9843 - val_loss: 25.8436\n",
      "Epoch 1805/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1459 - val_loss: 26.8156\n",
      "Epoch 1806/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2203 - val_loss: 25.7369\n",
      "Epoch 1807/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4184 - val_loss: 25.6303\n",
      "Epoch 1808/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6016 - val_loss: 26.4268\n",
      "Epoch 1809/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2424 - val_loss: 26.0965\n",
      "Epoch 1810/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2605 - val_loss: 26.0348\n",
      "Epoch 1811/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1692 - val_loss: 25.3340\n",
      "Epoch 1812/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2945 - val_loss: 25.7364\n",
      "Epoch 1813/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2590 - val_loss: 25.8983\n",
      "Epoch 1814/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2188 - val_loss: 26.4054\n",
      "Epoch 1815/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2897 - val_loss: 25.6597\n",
      "Epoch 1816/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1804 - val_loss: 25.8424\n",
      "Epoch 1817/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4756 - val_loss: 26.5886\n",
      "Epoch 1818/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3026 - val_loss: 25.6852\n",
      "Epoch 1819/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3848 - val_loss: 26.0908\n",
      "Epoch 1820/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1946 - val_loss: 26.0503\n",
      "Epoch 1821/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3417 - val_loss: 26.5397\n",
      "Epoch 1822/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3595 - val_loss: 25.3720\n",
      "Epoch 1823/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2607 - val_loss: 26.3561\n",
      "Epoch 1824/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5120 - val_loss: 25.3877\n",
      "Epoch 1825/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3488 - val_loss: 26.3056\n",
      "Epoch 1826/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1125 - val_loss: 25.4390\n",
      "Epoch 1827/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0324 - val_loss: 26.0184\n",
      "Epoch 1828/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.0416 - val_loss: 25.8583\n",
      "Epoch 1829/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1457 - val_loss: 25.8494\n",
      "Epoch 1830/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2915 - val_loss: 26.3804\n",
      "Epoch 1831/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1476 - val_loss: 26.1181\n",
      "Epoch 1832/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0933 - val_loss: 26.0114\n",
      "Epoch 1833/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 22.9710 - val_loss: 25.5647\n",
      "Epoch 1834/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0477 - val_loss: 25.9371\n",
      "Epoch 1835/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0704 - val_loss: 26.4216\n",
      "Epoch 1836/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1413 - val_loss: 25.8013\n",
      "Epoch 1837/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0781 - val_loss: 25.9712\n",
      "Epoch 1838/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0233 - val_loss: 26.2470\n",
      "Epoch 1839/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3386 - val_loss: 25.6713\n",
      "Epoch 1840/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0415 - val_loss: 25.5690\n",
      "Epoch 1841/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.9598 - val_loss: 25.5637\n",
      "Epoch 1842/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0202 - val_loss: 26.0554\n",
      "Epoch 1843/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0075 - val_loss: 25.2169\n",
      "Epoch 1844/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.8978 - val_loss: 25.7854\n",
      "Epoch 1845/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.8387 - val_loss: 26.1063\n",
      "Epoch 1846/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.8239 - val_loss: 25.3361\n",
      "Epoch 1847/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9863 - val_loss: 26.7334\n",
      "Epoch 1848/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4176 - val_loss: 26.0663\n",
      "Epoch 1849/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0808 - val_loss: 24.9109\n",
      "Epoch 1850/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8448 - val_loss: 25.6670\n",
      "Epoch 1851/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8813 - val_loss: 25.6623\n",
      "Epoch 1852/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.6285 - val_loss: 25.5448\n",
      "Epoch 1853/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7792 - val_loss: 25.3865\n",
      "Epoch 1854/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8526 - val_loss: 25.3933\n",
      "Epoch 1855/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9263 - val_loss: 25.5104\n",
      "Epoch 1856/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9022 - val_loss: 25.1432\n",
      "Epoch 1857/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9432 - val_loss: 25.6213\n",
      "Epoch 1858/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0554 - val_loss: 26.2941\n",
      "Epoch 1859/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4816 - val_loss: 25.3145\n",
      "Epoch 1860/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1082 - val_loss: 26.0113\n",
      "Epoch 1861/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9905 - val_loss: 25.5972\n",
      "Epoch 1862/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9123 - val_loss: 25.5994\n",
      "Epoch 1863/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7901 - val_loss: 25.8201\n",
      "Epoch 1864/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9938 - val_loss: 26.2471\n",
      "Epoch 1865/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1538 - val_loss: 25.8880\n",
      "Epoch 1866/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1621 - val_loss: 25.2582\n",
      "Epoch 1867/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2430 - val_loss: 25.9283\n",
      "Epoch 1868/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0910 - val_loss: 25.7968\n",
      "Epoch 1869/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0554 - val_loss: 26.1844\n",
      "Epoch 1870/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2127 - val_loss: 26.1452\n",
      "Epoch 1871/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3162 - val_loss: 26.4402\n",
      "Epoch 1872/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4170 - val_loss: 26.0157\n",
      "Epoch 1873/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3570 - val_loss: 26.3356\n",
      "Epoch 1874/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1364 - val_loss: 25.9177\n",
      "Epoch 1875/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8324 - val_loss: 26.1910\n",
      "Epoch 1876/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8299 - val_loss: 25.7771\n",
      "Epoch 1877/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9309 - val_loss: 25.6041\n",
      "Epoch 1878/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7942 - val_loss: 24.9901\n",
      "Epoch 1879/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8412 - val_loss: 25.2590\n",
      "Epoch 1880/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9243 - val_loss: 26.2812\n",
      "Epoch 1881/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1617 - val_loss: 25.7709\n",
      "Epoch 1882/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0645 - val_loss: 26.1303\n",
      "Epoch 1883/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0810 - val_loss: 25.7952\n",
      "Epoch 1884/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1659 - val_loss: 26.1788\n",
      "Epoch 1885/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8796 - val_loss: 25.3277\n",
      "Epoch 1886/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9281 - val_loss: 26.1084\n",
      "Epoch 1887/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8395 - val_loss: 25.8919\n",
      "Epoch 1888/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9379 - val_loss: 25.6490\n",
      "Epoch 1889/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1454 - val_loss: 25.8443\n",
      "Epoch 1890/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9845 - val_loss: 25.7929\n",
      "Epoch 1891/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8958 - val_loss: 26.5009\n",
      "Epoch 1892/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9020 - val_loss: 25.7631\n",
      "Epoch 1893/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1330 - val_loss: 25.9424\n",
      "Epoch 1894/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1278 - val_loss: 26.1519\n",
      "Epoch 1895/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0175 - val_loss: 25.0742\n",
      "Epoch 1896/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1000 - val_loss: 25.4388\n",
      "Epoch 1897/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7720 - val_loss: 25.7177\n",
      "Epoch 1898/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8996 - val_loss: 26.2635\n",
      "Epoch 1899/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8152 - val_loss: 25.5169\n",
      "Epoch 1900/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9221 - val_loss: 26.1622\n",
      "Epoch 1901/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8513 - val_loss: 25.6456\n",
      "Epoch 1902/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0739 - val_loss: 25.4966\n",
      "Epoch 1903/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0408 - val_loss: 25.3593\n",
      "Epoch 1904/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8674 - val_loss: 26.1696\n",
      "Epoch 1905/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0010 - val_loss: 25.8341\n",
      "Epoch 1906/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8496 - val_loss: 26.7513\n",
      "Epoch 1907/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1467 - val_loss: 27.1957\n",
      "Epoch 1908/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2399 - val_loss: 25.6613\n",
      "Epoch 1909/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2673 - val_loss: 25.2390\n",
      "Epoch 1910/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9014 - val_loss: 25.8633\n",
      "Epoch 1911/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9122 - val_loss: 25.5760\n",
      "Epoch 1912/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0148 - val_loss: 25.8685\n",
      "Epoch 1913/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7602 - val_loss: 25.2907\n",
      "Epoch 1914/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8260 - val_loss: 25.4369\n",
      "Epoch 1915/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8016 - val_loss: 25.5162\n",
      "Epoch 1916/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8470 - val_loss: 25.5570\n",
      "Epoch 1917/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8664 - val_loss: 24.6239\n",
      "Epoch 1918/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7787 - val_loss: 25.3133\n",
      "Epoch 1919/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7815 - val_loss: 25.3804\n",
      "Epoch 1920/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7030 - val_loss: 25.0731\n",
      "Epoch 1921/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8356 - val_loss: 25.1553\n",
      "Epoch 1922/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7795 - val_loss: 25.7728\n",
      "Epoch 1923/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.6277 - val_loss: 25.2050\n",
      "Epoch 1924/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 22.9826 - val_loss: 25.9165\n",
      "Epoch 1925/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8760 - val_loss: 25.3848\n",
      "Epoch 1926/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8147 - val_loss: 25.3628\n",
      "Epoch 1927/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6975 - val_loss: 25.6014\n",
      "Epoch 1928/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6311 - val_loss: 25.9694\n",
      "Epoch 1929/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.6202 - val_loss: 25.5151\n",
      "Epoch 1930/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6670 - val_loss: 24.9138\n",
      "Epoch 1931/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7965 - val_loss: 25.8637\n",
      "Epoch 1932/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6295 - val_loss: 25.2685\n",
      "Epoch 1933/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7938 - val_loss: 25.6330\n",
      "Epoch 1934/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8335 - val_loss: 25.1791\n",
      "Epoch 1935/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6300 - val_loss: 26.1926\n",
      "Epoch 1936/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8357 - val_loss: 25.5227\n",
      "Epoch 1937/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9038 - val_loss: 26.2357\n",
      "Epoch 1938/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0680 - val_loss: 25.8009\n",
      "Epoch 1939/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9917 - val_loss: 25.6968\n",
      "Epoch 1940/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6880 - val_loss: 25.9275\n",
      "Epoch 1941/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7196 - val_loss: 26.0346\n",
      "Epoch 1942/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7846 - val_loss: 26.1475\n",
      "Epoch 1943/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3124 - val_loss: 25.6853\n",
      "Epoch 1944/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9916 - val_loss: 25.8581\n",
      "Epoch 1945/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7959 - val_loss: 24.9896\n",
      "Epoch 1946/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7734 - val_loss: 25.0469\n",
      "Epoch 1947/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.5657 - val_loss: 26.0707\n",
      "Epoch 1948/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7004 - val_loss: 25.6604\n",
      "Epoch 1949/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8368 - val_loss: 26.5877\n",
      "Epoch 1950/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9740 - val_loss: 26.2583\n",
      "Epoch 1951/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8368 - val_loss: 25.9500\n",
      "Epoch 1952/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6949 - val_loss: 25.4408\n",
      "Epoch 1953/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7411 - val_loss: 25.5931\n",
      "Epoch 1954/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5872 - val_loss: 25.4066\n",
      "Epoch 1955/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5689 - val_loss: 25.4454\n",
      "Epoch 1956/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6167 - val_loss: 25.3452\n",
      "Epoch 1957/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9130 - val_loss: 25.8807\n",
      "Epoch 1958/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.8932 - val_loss: 25.4725\n",
      "Epoch 1959/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.9523 - val_loss: 25.6264\n",
      "Epoch 1960/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6041 - val_loss: 24.9509\n",
      "Epoch 1961/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8994 - val_loss: 26.1587\n",
      "Epoch 1962/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8307 - val_loss: 25.9725\n",
      "Epoch 1963/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9094 - val_loss: 25.2399\n",
      "Epoch 1964/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7130 - val_loss: 25.9151\n",
      "Epoch 1965/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8755 - val_loss: 25.6370\n",
      "Epoch 1966/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8385 - val_loss: 25.2599\n",
      "Epoch 1967/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8125 - val_loss: 25.5125\n",
      "Epoch 1968/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9459 - val_loss: 25.6833\n",
      "Epoch 1969/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8769 - val_loss: 25.6555\n",
      "Epoch 1970/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0242 - val_loss: 25.8293\n",
      "Epoch 1971/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0099 - val_loss: 25.9535\n",
      "Epoch 1972/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7206 - val_loss: 25.5456\n",
      "Epoch 1973/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9731 - val_loss: 25.4456\n",
      "Epoch 1974/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7316 - val_loss: 25.5643\n",
      "Epoch 1975/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9217 - val_loss: 25.1901\n",
      "Epoch 1976/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6788 - val_loss: 25.7488\n",
      "Epoch 1977/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0221 - val_loss: 25.5156\n",
      "Epoch 1978/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0869 - val_loss: 25.4152\n",
      "Epoch 1979/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0994 - val_loss: 25.2758\n",
      "Epoch 1980/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8847 - val_loss: 24.7916\n",
      "Epoch 1981/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8300 - val_loss: 26.0654\n",
      "Epoch 1982/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9694 - val_loss: 24.8443\n",
      "Epoch 1983/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9327 - val_loss: 25.3365\n",
      "Epoch 1984/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7932 - val_loss: 25.7148\n",
      "Epoch 1985/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9687 - val_loss: 25.7389\n",
      "Epoch 1986/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0177 - val_loss: 25.4894\n",
      "Epoch 1987/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8256 - val_loss: 25.1988\n",
      "Epoch 1988/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8480 - val_loss: 25.3292\n",
      "Epoch 1989/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6475 - val_loss: 25.2592\n",
      "Epoch 1990/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.5112 - val_loss: 25.1819\n",
      "Epoch 1991/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8559 - val_loss: 25.4199\n",
      "Epoch 1992/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6467 - val_loss: 25.4659\n",
      "Epoch 1993/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8476 - val_loss: 25.9836\n",
      "Epoch 1994/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8601 - val_loss: 25.0844\n",
      "Epoch 1995/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7856 - val_loss: 26.0257\n",
      "Epoch 1996/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8842 - val_loss: 25.6636\n",
      "Epoch 1997/2000\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 22.7633 - val_loss: 25.4083\n",
      "Epoch 1998/2000\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 22.8918 - val_loss: 25.8751\n",
      "Epoch 1999/2000\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 23.0447 - val_loss: 25.7161\n",
      "Epoch 2000/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 22.9188\n",
      "Epoch 02000: saving model to saved_models/latent16/cp-2000.h5\n",
      "6/6 [==============================] - 1s 167ms/step - loss: 22.9188 - val_loss: 25.6752\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train VAE model with callbacks \"\"\"\n",
    "history = vae.fit(trainX, trainX, \n",
    "                  batch_size = batch_size, \n",
    "                  epochs = epochs, \n",
    "                  callbacks=[initial_checkpoint, checkpoint, EarlyStoppingAtMinLoss()],\n",
    "                  #callbacks=[initial_checkpoint, checkpoint],\n",
    "                  shuffle=True,\n",
    "                  validation_data=(valX, valX)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 6.0\n"
     ]
    }
   ],
   "source": [
    "# In this GPU implementation, it shows the number of batches/iterations for one epoch (and not the training samples), which is:\n",
    "print (\"Step size:\", np.ceil(trainX.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save the entire model \"\"\"\n",
    "# Save notes about hyperparameters used:\n",
    "with open(f'{SAVE_FOLDER}/notes.txt', 'w') as f:\n",
    "    f.write(f'Epochs: {epochs} \\n')\n",
    "    f.write(f'Batch size: {batch_size} \\n')\n",
    "    f.write(f'Latent dimension: {latent_dim} \\n')\n",
    "    f.write(f'Filter sizes: {filters} \\n')\n",
    "    f.write(f'img_width: {img_width} \\n')\n",
    "    f.write(f'img_height: {img_height} \\n')\n",
    "    f.write(f'num_channels: {num_channels}')\n",
    "    f.close()\n",
    "\n",
    "# Save weights for encoder model:\n",
    "encoder.save_weights(f'{SAVE_FOLDER}/ENCODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for decoder model:\n",
    "decoder.save_weights(f'{SAVE_FOLDER}/DECODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for total VAE:\n",
    "vae.save_weights(f'{SAVE_FOLDER}/VAE_WEIGHTS.h5')\n",
    "\n",
    "# Save the history at each epochs (cost of train and validation sets):\n",
    "np.save(f'{SAVE_FOLDER}/HISTORY.npy', history.history)\n",
    "\n",
    "# Save exact training, validation and testing data set (as numpy arrays):\n",
    "np.save(f'{SAVE_FOLDER}/trainX.npy', trainX)\n",
    "np.save(f'{SAVE_FOLDER}/valX.npy', valX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllX.npy', testAllX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllY.npy', testAllY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate training process\n",
    "\n",
    "Now that the training process is complete, let’s evaluate the average loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(13, 6))\n",
    "\n",
    "# summarize history for loss:\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='blue', alpha=0.4, label='validation')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Cost', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.title('Average Loss During Training', fontsize=18)\n",
    "#plt.xticks(range(0,epochs))\n",
    "plt.legend(loc='upper right', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
