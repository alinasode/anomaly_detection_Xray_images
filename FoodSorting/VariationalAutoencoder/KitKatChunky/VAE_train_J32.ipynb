{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import axis, colorbar, imshow, show, figure, subplot\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=18)\n",
    "mpl.rc('xtick', labelsize=16)\n",
    "mpl.rc('ytick', labelsize=16)\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## ----- GPU ------------------------------------\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'    # use of GPU 0 (ERDA) (use before importing torch or tensorflow/keeas)\n",
    "## ----------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization, ReLU, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.activations import relu\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K                         #contains calls for tensor manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL_TRAIN_AUG:       /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTrainAugmentations\n",
      "NORMAL_VALIDATION_AUG:  /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalValidationAugmentations\n",
      "NORMAL_TEST_AUG:        /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTestAugmentations\n",
      "ANOMALY_AUG:            /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/AnomalyAugmentations\n"
     ]
    }
   ],
   "source": [
    "from utils_vae_kitkat_paths import *\n",
    "\n",
    "# Get work directions for augmentated data set:\n",
    "print (\"NORMAL_TRAIN_AUG:      \", NORMAL_TRAIN_AUG)\n",
    "print (\"NORMAL_VALIDATION_AUG: \", NORMAL_VAL_AUG)\n",
    "print (\"NORMAL_TEST_AUG:       \", NORMAL_TEST_AUG)\n",
    "print (\"ANOMALY_AUG:           \", ANOMALY_AUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which Folder The Models Are Saved In: saved_models/latent32\n"
     ]
    }
   ],
   "source": [
    "# What latent dimension and filter size are we using?:\n",
    "latent_dim = 32\n",
    "filters    = 32\n",
    "\n",
    "# Get work direction for saving files:\n",
    "SAVE_FOLDER = f'saved_models/latent{latent_dim}'\n",
    "print (\"Which Folder The Models Are Saved In:\", SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] deleting existing files in folder...\n",
      "         done in 0.040 minutes\n"
     ]
    }
   ],
   "source": [
    "# Delete all existing files in folder where models are saved:\n",
    "print(\"[INFO] deleting existing files in folder...\")\n",
    "t0 = time()\n",
    "\n",
    "files = glob.glob(f'{SAVE_FOLDER}/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "print (\"         done in %0.3f minutes\" % ((time() - t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_VAE_AE import get_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Investigation of Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"normal\" training images is:     1400\n",
      "Number of \"normal\" validation images is:   134\n",
      "Number of \"normal\" test images is:         266\n",
      "Number of \"anomaly\" images is:             600\n"
     ]
    }
   ],
   "source": [
    "# Glob the directories and get the lists of good and bad images\n",
    "good_train_fns = [f for f in os.listdir(NORMAL_TRAIN_AUG) if not f.startswith('.')]\n",
    "good_val_fns = [f for f in os.listdir(NORMAL_VAL_AUG) if not f.startswith('.')]\n",
    "good_test_fns = [f for f in os.listdir(NORMAL_TEST_AUG) if not f.startswith('.')]\n",
    "bad_fns = [f for f in os.listdir(ANOMALY_AUG) if not f.startswith('.')]\n",
    "\n",
    "print('Number of \"normal\" training images is:     {}'.format(len(good_train_fns)))\n",
    "print('Number of \"normal\" validation images is:   {}'.format(len(good_val_fns)))\n",
    "print('Number of \"normal\" test images is:         {}'.format(len(good_test_fns)))\n",
    "print('Number of \"anomaly\" images is:             {}'.format(len(bad_fns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Procentage of normal samples (ground truth):   75.00 %\n",
      "> Procentage of anomaly samples (ground truth):  25.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of bad images vs good images:\n",
    "normal_fns = len(good_train_fns) + len(good_val_fns) + len(good_test_fns)\n",
    "anomaly_fns = len(bad_fns)\n",
    "print(f\"> Procentage of normal samples (ground truth):   {(normal_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")\n",
    "print(f\"> Procentage of anomaly samples (ground truth):  {(anomaly_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Normal Data\n",
    "\n",
    "Load normal samples in pre-splitted data sets: train, valdiation and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal training images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal training images...\")\n",
    "trainX = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TRAIN_AUG}/*.png\")]  # read as grayscale\n",
    "trainX = np.array(trainX)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "trainY = get_labels(trainX, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal validation images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal validation images...\")\n",
    "x_normal_val = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_VAL_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_val = np.array(x_normal_val)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_val = get_labels(x_normal_val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal testing images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal testing images...\")\n",
    "x_normal_test = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TEST_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_test = np.array(x_normal_test)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_test = get_labels(x_normal_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Data\n",
    "\n",
    "\n",
    "Load anomaly samples in its singular training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly images...\")\n",
    "x_anomaly = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY_AUG}/*.png\")]  # read as grayscale\n",
    "x_anomaly = np.array(x_anomaly)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_anomaly = get_labels(x_anomaly, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine normal samples and anomaly samples and split them into training, validation and test sets\n",
    "\n",
    "`label 0` == Normal Samples\n",
    "\n",
    "`label 1` == Anomaly Samples\n",
    "\n",
    "Train and Validation sets only consists of `Normal Data`, aka. `label 0`.\n",
    "\n",
    "Testing sets consists of both  `Normal Data` (aka. `label 0`) and `Anomaly Data` (aka. `label 1`)\n",
    "\n",
    "________________\n",
    "\n",
    "Due to the very sparse original (preprossed) KitKat Chunky data, the validation set will be a mix of normal testing and validation data. The normal testing set will consists of all validation and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testvalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testvalY = get_labels(testvalX, 0)\n",
    "\n",
    "# randomly split in order to make new validation set:\n",
    "NoUsageX, valX, NoUsageY, valY = train_test_split(testvalX, testvalY, test_size=0.25, random_state=4345672)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testNormalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testNormalY = get_labels(testNormalX, 0)\n",
    "\n",
    "# anomaly class (only used for testing):\n",
    "testAnomalyX, testAnomalyY = x_anomaly, y_anomaly\n",
    "\n",
    "# Combine all testing data in one array:\n",
    "testAllX = np.vstack((testNormalX, testAnomalyX))\n",
    "testAllY = np.hstack((testNormalY, testAnomalyY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Dimensions and Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      " > images: (1400, 128, 128)\n",
      " > labels: (1400,)\n",
      "\n",
      "Validation set:\n",
      " > images: (100, 128, 128)\n",
      " > labels: (100,)\n",
      "\n",
      "Test set:\n",
      " > images: (1000, 128, 128)\n",
      " > labels: (1000,)\n",
      "\t'Normal' test set:\n",
      "\t  > images: (400, 128, 128)\n",
      "\t  > labels: (400,)\n",
      "\t'Anomaly' test set:\n",
      "\t  > images: (600, 128, 128)\n",
      "\t  > labels: (600,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "print(\" > images:\", trainX.shape)\n",
    "print(\" > labels:\", trainY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Validation set:\")\n",
    "print(\" > images:\", valX.shape)\n",
    "print(\" > labels:\", valY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Test set:\")\n",
    "print(\" > images:\", testAllX.shape)\n",
    "print(\" > labels:\", testAllY.shape)\n",
    "print(\"\\t'Normal' test set:\")\n",
    "print(\"\\t  > images:\", testNormalX.shape)\n",
    "print(\"\\t  > labels:\", testNormalY.shape)\n",
    "print(\"\\t'Anomaly' test set:\")\n",
    "print(\"\\t  > images:\", testAnomalyX.shape)\n",
    "print(\"\\t  > labels:\", testAnomalyY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 73.68 %\n",
      "Procentage of validation samples: 5.26 %\n",
      "Procentage of (only normal) testing samples: 21.05 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets (only normal data):\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (only normal) testing samples: {(len(testNormalX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 56.00 %\n",
      "Procentage of validation samples: 4.00 %\n",
      "Procentage of (total) testing samples: 40.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets:\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (total) testing samples: {(len(testAllX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for (un)balanced data\n",
    "\n",
    "In an ideal world the data should be balanced. However in the real world we expect there being around ~$99 \\%$ good samples and only ~$1 \\%$ bad samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9AAAAEoCAYAAACw8Bm1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAABC9klEQVR4nO3deZhcVbWw8TdpICECXycaUFAEhLu8RAUVBxRlcCAqBFARFFFAEIGLIsokXpkFBQUVuaIioogg1wkcmAQCanACVCIuRRJBBW80CTLJkPT3xz4FRaW6u6pTPdb7e556quucfc5ZNWSnVu1pUl9fH5IkSZIkaWCTRzsASZIkSZLGAxNoSZIkSZJaYAItSZIkSVILTKAlSZIkSWqBCbQkSZIkSS0wgZYkSZIkqQUm0NIwiIgvR4RrxEkaURFxbET0RcQGddv2qrZt0+I5FkbEtcMU37URsXA4zi1J0khYZbQDkEZaRGwO7Ax8OTMXjmowkjTBRMQhwNLM/PIohyKpC43k9zzru+5kC7S60ebAMcAGw3iN/YDVh/H8ktSqr1Lqo+tG6HqHAHv1s++1QIxQHJK60+YM//e8mkPov77TBGULtDSAiOgBpmTmA+0cl5mPAI8MT1SS1LrMXAYsG+04ADLz4dGOQZKklWECra4SEcdSfpUEuCbisYaQ84BrgXOB1wBbUn5RXJ/SmvzliHgt8C7gRcDTgIeAnwMnZebchut8GXhnZk5q3Ab0AqcAbwLWAn4FHJqZP+vcM5U0lkXE64AfAO/LzE832T8P2BhYF3g+cCDwMuDplGT4N8BpmfntFq61F6Vu2zYzr63b/gzgE8D2wCRgLqU1pdk5dgP2oLTsrAPcC/wY+Ehm/qauXG3uh2c2zAOxYWbWxlZvkJkbNJz/lcB/Ay8GVgNuBT6bmec0lLuW0qr0sir22cAU4Hrg4Mz8w2Cvh6SJa6DveZm5V0RMAT5Aqc+eBfybUn98JDNvqjvPZOC9wD7AhkAfcBel3ntPZj4yWH03DE9PY4RduNVtvgV8vvr7o8Ce1e3sujKnAbsDXwDeB2S1fS9gBvAV4GDgdOA/gR9FxCvaiOFyypfg44GTgecA34+INdt/OpLGqSuAu4F3NO6IiE2AlwIXVL1ZdgGeDXyDUiedRKmLvhURbxvKxSOil9Kl+42ULt5HAg8A1wBPanLIfwHLKfXnQZT68RXAT6p4a/YE/gH8nsfr1z2BRQPEsiNwNaU+/QTwIUoPni9GxElNDnlSFfuyquyZwDbAd6teQ5K6V7/f8yJiVeAySoI9D3g/pUFjU0pdtkXdeY6mfM9bCBwBHAZ8m9LAMqUq03Z9p4nBFmh1lcz8TdWy827gyobWmNrPlKsDz2/SbXu/zLy/fkNEfA6YDxxF+QWzFTdm5oF15/gd5Yvx23hiIi9pgsrMZRFxPvDBiNg0M39Xt7uWVJ9X3Z+YmUfVHx8RnwZuAj4MXDCEEA6ntOTuk5nnVtvOiogzKEl6o9lN6r+vADdTvoQeWD2v8yPiRODvmXn+YEFUCe+ZwH3AizPzb9X2z1KS+SMj4suZ+ce6w54CnJqZH687zyLg48CrKT9SSupCg3zPez/lx7bZmXl53fazgFsoDSjbVJt3AW7NzDkNlziy7lpt1XeaOGyBllb0P83GPNd/eYyINSLiyZQWkJ8BL2nj/Kc3PL66ut+ksaCkCa2WID/WCh0Rk4C3A7dk5o2wQt0zrap7plG12kbEWkO49s7A3yk9aup9rFnhWgwRMSki1oqIp1BaWZL26r9GL6QMlflSLXmurvcwJSGeDOzUcMxyoLHbu/WopMG8ndJa/KuIeErtRhk2ciWwVUTUJoC9B1gvIrYapVg1htkCLa2o6Ri6iHgWpevk9pRxzPXaWfP59voHmfnPqvH7yW2cQ9I4l5m3RMSNwB4R8aHMXA68ktIyfHitXESsDZxISSTXbnKqXuBfbV5+I+AX1QRj9THdFRFLGwtHxPOBEyitM41dvBe0ee16G1b385vsq23bqGH73zLz3w3b/lndW49K6s9/UnoZDtTF+inAnZThId8Bro+Iv1Hmyfk+8L9OhigTaGlFK7Q+R8QalDF3TwLOAH5LmURnOaX79natnrzxC2udSf1slzRxfYVSp2wHXEVpjV4GnA+PtUhfQfni9yngl5SWkWXA3pShH8Pamywi1qfUf/+iJNEJ3E/54fAMYI3hvH4TA80obj0qqT+TKN/fDh2gzCKAzJxXNZxsD2xb3d4GfDgitsrMxcMdrMYuE2h1o3Zai2teRZkNt368IADV+BdJGooLgFOBd0TET4A3U8bt3VXtfx6wGXB8Zh5Tf2BE7LsS170d2CQieup/1IuIp7FiD5tdKEnynMy8piGGJ1NWJKg3lB45s5rs27ShjCS1or866I/ATODqqsfPgDLzPuCb1Y2IOBD4LGVFllMHuZYmMMdAqxvdV93PaOOY2hfMJ7RuVEtbrcz4P0ldLDMXAT+kzIa9B2Vpu/PqivRX9zyHktgO1Xcpy1E1zgJ+RJOy/cWwH/DUJuXvo/X69UbgDmDviHjsXNVsuYdRvpx+t8VzSRL0/z3vK5Q6q2kLdESsU/f3U5oUubHJedup7zRB2AKtbvQLStfroyNiOqUr4mBj+H5MWXLmExGxAfAXynqoe1K6Az13uIKVNOGdB8yhLOF0D2XcXc2tlLHAh0fENEr36f8A9qfUPS8c4jU/TumO+IWIeGF1jW0oS7T8o6HsDylDW74aEWcCS4CXA68H/sSK3yVuAN4VESdU8S8HLm2cxRsem438vyjLw/wiIj5PGR6zG2Upr482zMAtSYPp73vep4DXAKdGxHaUyQf/RZnI8FWUNaG3rc5xa0TcQJko9m/A0ygzez8MXFh3rZbrO00ctkCr62TmHcA+lIkk/gf4OnDAIMcspYyD+RllDehPULoXvp7Hf5GUpKH4HrCY0vp8cf0EWVX36jcAlwLvpHwB3Lr6+3tDvWBmLqGs4/wdSiv0xygze29L+bJZX/ZPwOsoX0A/RFk3dUYVx1+anP5oSkJ8EGUs99cp3Sb7i+VSypfX31NanU8BpgL7ZubRQ3yKkrpUf9/zMvMRSn36PkqddBxlZZTdKENFTq47zSeA/we8tzrHe4CfA1tm5q/ryrVV32limNTXZ9d9SZIkSZIGYwu0JEmSJEktMIGWJEmSJKkFJtCSJEmSJLXABFqSJEmSpBa4jNUQLF++vG/Zsu6efK2nZxLd/hrocX4eYNVVe/7BBJt507qu8POtGj8LxUSr76zrCj/fqvGzUPRX15lAD8GyZX0sXfrAaIcxqnp7p3X9a6DH+XmAmTPX/PNox9Bp1nWFn2/V+FkoJlp9Z11X+PlWjZ+For+6zi7ckiRJkiS1wARakiRJkqQWmEBLkiRJktQCE2hJkiRJklrgJGKSNEZExNOBI4AtgM2A1YENM3NhQ7mpwAnA24Fe4GbgiMy8rqHc5Op8+wNPBRI4PjO/OZzPQ5JaERGvB44EXgAsB/4AHJ6ZV1f7pwOnAjtT6sN5wPsz87cN52mpTpSkTrAFWpLGjo2BtwBLgOsHKHcOsB/wEWAH4C7g8ojYvKHcCcCxwJnA64AbgIurL62SNGoiYn/gu8CvgF2AXYGLgWnV/knApcBs4GDgTcCqwDXVj431Wq0TJWml2QItSWPHdZm5DkBE7Au8trFARGwGvA3YJzPPrbbNBeYDxwNzqm1rAx8ETsnM06rDr4mIjYFTgB8M83ORpKYiYgPgDOCwzDyjbtfldX/PAV4ObJeZ11THzQMWAIcD7622tVQnSlKn2AItSWNEZi5vodgc4BHgorrjHgUuBLaPiCnV5u2B1YDzG44/H3huRGy48hFL0pDsQ+my/bkByswB/lZLngEy8x5Kq/RODeVaqRMlqSNMoCVpfJkFLMjMBxq2z6ckzBvXlXsIuK1JOYBNhy1CSRrYVsDvgd0j4k8R8WhE3BYRB9WVmQXc0uTY+cD6EbFGXblW6kRJ6gi7cGtAa6y1OqtPaf4xmTlzzabbH3zoUe7714PDGZbUzWZQxkg3Wly3v3a/NDP7BinXr56eSfT2ThtSkOPNMmDqqj397m9W3/37kWX0f4Qmop6eyV3zb2KYrVvdTgU+BPyJMgb6zIhYJTM/RamjFjY5tlaHTQfuo/U6sV/dVNcNxM/3+DXY/2HNDPR/mJ+FgZlAa0CrT1mFDY78flvHLDzlDdw3TPFIGjnLlvWxdGljo87ENHPmmkOq6xYtuneYItJY1Ns7rWv+TQykvx/Q2zAZWBPYKzO/VW27uhobfVREfHplL9CObqrrBuLne/zq9P9hfhaK/uo6u3BL0viyhNLy0qjWyrK4rlxvNZPtQOUkaaT9s7q/smH7FcA6wNMYvK5bUnffSp0oSR1hAi1J48t8YMOIaOxbtSnwMI+PeZ4PTAGe1aQcwO+GLUJJGtj8QfYvr8rMarJvU+COzKx1dmu1TpSkjjCBlqTx5VLKWqi71jZExCrAbsAVmflQtfkyysy0ezQc/3bglsxcMAKxSlIz367ut2/YPhv4S2beDVwCrBcRW9d2RsRawI7VvppW60RJ6gjHQEvSGBIRb67+fGF1/7qIWAQsysy5mXlTRFwEnBERq1LWRD0A2JC6ZDkz/y8iPkkZT3gvcCPlC+V2uC6qpNH1A+Aa4OyIeApwOyUBfi2wd1XmEmAecH5EHEbpqn0UMAn4eO1ErdaJktQpJtCSNLZc3PD4rOp+LrBN9ffewEnAiUAv8Gtgdmbe2HDs0ZRZat8HPBVI4C2Z+b2ORy1JLcrMvojYGTgZOI4yhvn3wB6ZeUFVZnlE7ACcRqkHp1IS6m0z886GU7ZaJ0rSSjOBlqQxJDMbJ/1qVuZB4NDqNlC5ZZQvlCd2JjpJ6ozM/BdwUHXrr8xiYJ/qNtC5WqoTJakTHAMtSZIkSVILTKAlSZIkSWqBCbQkSZIkSS0wgZYkSZIkqQUm0JIkSZIktcAEWpIkSZKkFphAS5IkSZLUAhNoSZIkSZJaYAItSZIkSVILVmm1YES8GNgsM79Qt20n4ERgBnBeZn6onYtHxNOBI4AtgM2A1YENM3NhXZktgHcDrwTWB/4BXA98ODMXNJxvIfDMJpfaJTO/01B2P+ADwIbAQuD0zPxcO/FLkiRJkrpHOy3QxwBzag8iYn3g68BTgXuAIyJi7zavvzHwFmAJJSluZndgFvBp4HXAkcALgF9GxDOalL8c2LLhNre+QJU8nw18E5gNXAycFREHtBm/JEmSJKlLtNwCTWkh/kzd492BScDmmfnXiPghpaX43DbOeV1mrgMQEfsCr21S5mOZuah+Q0T8BFgA7Ad8pKH8PzLzhv4uGBGrACcBX83Mo6vN10TEusAJEfHFzHykjecgSZIkSeoC7bRAPxn4e93j7SkJ8F+rx5cAm7Rz8cxc3kKZRU22/RlYBKzXzvUqWwIzgfMbtn+V8hy3GsI5JUmSJEkTXDsJ9FKg1lo8BXgpcF3d/j7KGOZhFxH/CawN3Npk944R8UBEPBQRN0TEzg37Z1X3tzRsn1/db9q5SCVJkiRJE0U7XbhvBvaNiKuAXYCplPHGNRvyxBbqYVF1wf4cpQX6nIbdlwK/oHTvXgf4L+DbEbFnZtZanGdU90sajl3csL9fPT2T6O2dNoTou4evT3fp6Znsey5JkqQJr50E+gTgCuDnlLHPV2bmL+v27wD8rIOx9edM4GXAGzLzCUlwZh5c/zgivg3cAJzMil22h2zZsj6WLn2gU6cb02bOXHNIx3XL66Oit3da17/nQ/23IkmSpPGj5S7cmflTyuzXhwB7ATvW9kXEkynJ9f90NrwniohTKBOV7ZOZVwxWPjOXUWbYfnpEPK3aXEu6pzcUr7U8L0aSJEmSpAbttECTmX8A/tBk+z+B93cqqGYi4mjKmtEHZ+ZXh3CKvuq+NtZ5FnBX3f7a2OffDS1CSZIkSdJE1lYCDRARGwCvpowx/lpmLoyI1SjrQd+dmQ93NkSIiPcCJwJHZ+aZbRy3CrAbcEdm3l1tngf8A9gDuKqu+Nsprc8/6UjQkiRJkqQJpa0EOiI+BhwK9FBadOcBCykTiv0O+DBwRpvnfHP15wur+9dFxCJgUWbOjYjdq3NeBlwdES+tO/xfmfm76jxvBXYCfgDcSUnwD6J0O39r7YDMfCQi/hs4KyL+SkmitwP2obRud/wHAEmSJEnS+NdyAh0R+wOHAZ8GvkcZ8wxAZv4rIi6hjIs+o80YLm54fFZ1PxfYBphNmbRsdnWrVysDZebttYFTKeOZ7wd+CczOzPrZwsnMz0VEH/CB6jndAfxXZp6FJEmSJElNtNMCfSDw7cw8pJo0rNFvKMtGtSUzJw2yfy/KpGWDnecGSktyq9c9Gzi71fKSJEmSpO7W8izcwH8AVw6wfxHwlJULR5IkSZKksamdBPrfwJMG2P9MYOlKRSNJkiRJ0hjVTgL9c2CXZjsiYiqwJ85gLUmSJEmaoNpJoE8FtoyIrwLPq7Y9NSK2B64Fng6c1tnwJEmSJEkaG1pOoDPzKuAA4M08vn7yVynLRm0G7JeZ8zoeoSRJkiRJY0Bb60Bn5uer5ap2BZ5NWV7qj8A3MvOvwxCfJEmSJEljQlsJNEBm3g18ZhhikSS1KCJeDhwDbA6sTvkx88zM/FJdmanACcDbgV7gZuCIzLxuhMOVJEmaENoZAy1JGgMi4nmUoTSrAvsBbwR+AZwTEQfUFT2n2v8RYAfgLuDyiNh8RAOWJEmaIFpugY6Iqwcp0gc8CNwBXAF8NzP7ViI2SVJzuwM9wI6ZeV+17coqsX4H8D8RsRnwNmCfzDwXICLmAvOB44E5Ix+2JEnS+NZOC/RGwCxgm+q2eXWrPX4O8BLgPcA3gbkRMdC60ZKkoVkNeITyo2W9e3i8Xp9TlbmotjMzHwUuBLaPiCkjEKckSdKE0k4CvQ3wAGU5q3Uyc0ZmzgDWoSxfdT+wBfAU4JPAVpRug5Kkzvpydf/piFg3InojYj/gVcDp1b5ZwILMfKDh2PmUBHzjEYlUkiRpAmlnErHTgZ9k5hH1GzNzEXB4RKwHnJ6ZbwQOi4hnA28CjljxVJKkocrMWyJiG+DbwIHV5keA92TmhdXjGcCSJocvrts/oJ6eSfT2TlvJaCc2X5/u0tMz2fdckrpcOwn0dsDhA+y/Hjil7vFVwGuGEpQkqX8RsQllqMx8yrCZB4GdgM9FxL8z82uduM6yZX0sXdrYgD0xzZy55pCO65bXR0Vv7zTfc4b+70WSJoJ2l7F69iD7JtU9Xs6K4/MkSSvvo5QW5x0y85Fq248i4snApyLi65TW52c2ObbW8ry4yT5JkiQNoJ0x0FcBB0TE7o07IuKtlFaQK+s2vwBYuFLRSZKaeS7w67rkuebnwJOBtSmt0xtGRGN/002Bh4Hbhj1KSZKkCaadFuhDgRcDX4uI03j8y9fGwNMo64t+ACAiplJaPr7SuVAlSZW7gc0jYrXMfLhu+0uAf1Naly8FjgN2Bc4DiIhVgN2AKzLzoZENWZIkafxrOYHOzD9X64oeCexA+aIGpZX5AuBjmfnPquy/KWOmJUmddyZwMXBpRJxFGS4zB3grZTLHh4GbIuIi4IyIWBVYABwAbAjsMTphS5IkjW9tjYHOzMWUicQGmkxMkjSMMvN/I+L1lFUOvghMBf4EHAScXVd0b+Ak4ESgF/g1MDszbxzRgCVJkiaIdicRkySNAZn5Q+CHg5R5kDL85tARCUqSJGmCazuBjoh1gC2A6TSZhCwzHfcsSZIkSZpwWk6gI2Iy8FlgXwaevdsEWpIkSZI04bSzjNUHgf2BrwPvpKz5fCRlzN0fgV8Cr+l0gJIkSZIkjQXtJNDvBC7LzHfw+Li7X2Xm54AXAk+p7iVJkiRJmnDaSaA3Ai6r/l5e3a8KkJn3A+dSundLkiRJkjThtDOJ2IPAI9Xf9wF9wNp1++8GntHOxSPi6ZRlWLYANgNWBzbMzIUN5aYCJwBvpyzFcjNwRGZe11BucnW+/YGnAgkcn5nfbHLt/YAPUNZEXUhZO/Vz7cQvSZIkSeoe7bRA/xl4FkBmPgLcBsyu2/9q4O9tXn9j4C3AEuD6AcqdA+wHfATYAbgLuDwiNm8odwJwLHAm8DrgBuDiar3Ux1TJ89nAN6vncDFwVkQc0Gb8kiRJkqQu0U4L9NXALpTJxAC+ChwfEetSJhR7BXBam9e/LjPXAYiIfYHXNhaIiM2AtwH7ZOa51ba5wHzgeGBOtW3tKrZTMrMWxzURsTFwCvCDqtwqwEnAVzPz6Lpy6wInRMQXqx8IJEmSJEl6TDst0KcBB0bElOrxyZSW3s2AWcDngWPauXhmLh+8FHMoXccvqjvuUeBCYPu6eLYHVgPObzj+fOC5EbFh9XhLYGaTcl8Fngxs1c5zkCRJkiR1h5ZboDPzLkrX6drjZcB7q9twmgUsyMwHGrbPpyTMG1d/zwIeonQtbywHsCmwoCoHcMsA5a5Z+bAlSZIkSRNJO124R8sMyhjpRovr9tful2ZmXwvlaHLOxnL96umZRG/vtMGKdTVfn+7S0zPZ91ySJEkTXtsJdERsAmxC6e48qXF/Zn6lA3GNacuW9bF0aWOD+MQ0c+aaQzquW14fFb2907r+PR/qvxVJkiSNHy0n0BHxNOA84FXVphWSZ8rSVp1OoJcAz2yyvdZSvLiuXG9ETGpohW5WDmA6dV3Sm5STJEmSJOkx7bRAfx7YFjiDsuRUs27Vw2E+sEtETGsYB70p8DCPj3meD0yhLLV1W0M5gN/VlYMyFvquAcpJkiRJkvSYdhLo7YBPZeYHBy3ZWZcCxwG7UlrAa0tR7QZckZkPVeUuo8zWvUdVvubtwC2ZuaB6PA/4R1XuqoZyi4GfDM/TkCRJkiSNZ+0k0Pex4gzXKy0i3lz9+cLq/nURsQhYlJlzM/OmiLgIOCMiVqXMpH0AsCElCQYgM/8vIj4JHBUR9wI3UpLs7ajWiq7KPRIR/w2cFRF/pSTR2wH7AAdn5sOdfo6SJElqLiIuoyxHelJmfrhu+3TgVGBnYHVKI8j7M/O3DcdPBU6gNIb0AjcDR2TmdSMQvqQu08460N8DXj0MMVxc3d5TPT6relzfirw3cC5wIvB94BnA7My8seFcR1dl3gdcDrwceEtmfq++UGZ+jpKEv6Uq91bgvzLzs517WpIkSRpIRLwV2KzJ9kmUXoizgYOBNwGrAtdExNMbip8D7Ad8BNiBMkTv8ojYfPgil9St2mmB/gDwo4g4HfgMZW3mxiWj2paZzSYjayzzIHBodRuo3DJKAn1iC+c8Gzi7xTAlSZLUQVUL8+nA+4ELGnbPoTSEbJeZ11Tl51F6Ih4OvLfathnwNmCfzDy32jaXMufN8dT1QpSkTmi5BTozl1LGIL8X+CPwaEQsa7g9OkxxSpIkaWL5GGWemq832TcH+FsteQbIzHsordI7NZR7BLiortyjwIXA9hExZTgCl9S92lnG6nDgZODvwM8ZuVm4JUmSNIFExFbAO2jSfbsyC7ilyfb5wDsiYo3MvK8qt6BhpZZaudWAjXl8BRZJWmntdOE+GLiWMvb4keEJR5IkSRNZRKxGGUZ3WmZmP8VmAAubbF9c3U+nTHA7g+aNOrVyMwaLp6dnEr290wYrNiEsA6au2tPv/pkz11xh278fWUb/R2g86+9z39MzuWv+TQxFOwn0DOAbJs+SJElaCYdTZtU+abQDAVi2rI+lSxsbsCemmTPXZIMjv9/WMQtPeQOLFt07TBGpE5r98NGK/j73vb3TuubfxED6e13bSaB/DazfkWgkSZLUdSJifcqqKfsCUxrGKE+JiF7gXkqr8vQmp6i1KC+pu3/mAOUWN9knSUPWzjJWRwPvjogthisYSZIkTWgbAVOB8ynJb+0G8MHq7+dSxi3PanL8psAd1fhnqnIbRkRjf9NNgYeB2zoavaSu104L9J7AX4EbqmUEbqcMpajXl5nv6lRwkiRJmlBuBrZtsv0aSlJ9DiXpvQTYOyK2zsy5ABGxFrAjT1zy6lLgOGBXymoxRMQqwG7AFZn50PA8DUndqp0Eeq+6v19e3Rr1ASbQkiRJWkG1LOq1jdsjAuDPmXlt9fgSYB5wfkQcRmmZPgqYBHy87nw3RcRFwBkRsSplnegDgA2BPYbxqUjqUi0n0JnZTndvSZIkaUgyc3lE7ACcBpxF6fY9D9g2M+9sKL43ZUKyE4Feyrw9szPzxpGLWFK3aKcFWpIkSeq4zJzUZNtiYJ/qNtCxDwKHVjdJGlYm0JI0TkXE64EjgRcAy4E/AIdn5tXV/unAqcDOlCVj5gHvz8zfjkrAkiRJ41y/CXREfIkypvndmbmsejwYJxGTpBEQEfsDZ1a3EyirKmwOTKv2T6JMrrMBcDCPjx+8JiI2z8y/jHzUkiRJ49tALdB7URLoAyizbe/VwvmcREyShllEbACcARyWmWfU7bq87u85lMket8vMa6rj5lEm2DkceO9IxCpJkjSR9JtAN04a5iRikjRm7EPpsv25AcrMAf5WS54BMvOeiLgU2AkTaEmSpLaZFEvS+LMV8Htg94j4U0Q8GhG3RcRBdWVmAbc0OXY+sH5ErDESgUqSJE0kJtCSNP6sC2xCmSDsFOC1wJXAmRHxvqrMDMq450aLq/vpwx2kJEnSROMs3JI0/kwG1gT2ysxvVduursZGHxURn+7ERXp6JtHbO60Tp5qwfH26S0/PZN9zSepyJtCSNP78k9ICfWXD9iuA2cDTKK3PzVqZZ1T3zVqnn2DZsj6WLn1gJcIcP2bOXHNIx3XL66Oit3ea7zlD//ciSROBXbglafyZP8j+5VWZWU32bQrckZn3dTwqSZKkCc4EWpLGn29X99s3bJ8N/CUz7wYuAdaLiK1rOyNiLWDHap8kSZLa1G8X7oi4HTgkMy+pHn8E+FZmNpvVVZI0cn4AXAOcHRFPAW4HdqVMJrZ3VeYSYB5wfkQcRumyfRQwCfj4iEcsSZI0AQzUAr0+ZZKammOB5w1rNJKkQWVmH7AzcCFwHPA94CXAHpn55arMcmAHyjjpsyit1suAbTPzzpGPWpIkafwbaBKxvwLPbdjWN4yxSJJalJn/Ag6qbv2VWQzsU90kSZK0kgZKoL8LHB4Rs3l83dAPR8R+AxzTl5mv6lh0kiRJkiSNEQMl0EdQxsy9GngmpfV5JjDiCyBGxLXA1v3svjwzZ1frny7op8z0zFxad76pwAnA24Fe4GbgiMy8rjMRS5IkSZImmn4T6Mx8EDimuhERyymTil0wQrHVOxBYq2HblsAnWXE22ZObbLu34fE5wBuAwyiT7xwEXB4RW2bmzZ0IWJIkSZI0sQzUAt1ob+CnwxXIQDLzd43bqq7kD1Mm0al3e2be0N+5ImIz4G3APpl5brVtLmXN1OOBOZ2KW5IkSZI0cbScQGfmebW/I+LJwIbVwwWZ+c9OBzaQiJhGWbLl0mqSnHbMAR4BLqptyMxHI+JC4MiImJKZD3UuWkmSJEnSRNBOC3St9fbTwFYN268H3puZv+lgbAPZhbLE1nlN9p0cEZ8D7gfmAkdn5m/r9s+iJP0PNBw3H1gN2Lj6W5IkSZKkx7ScQEfEc4AfA1MpM3TXksxZwI7A9RHxsswcieTzHcD/AT+s2/YQcDZwBbAIeDbwIeCnEfHizLy1KjeDMjlao8V1+wfU0zOJ3t4Rn0ttXPH16S49PZN9zyVJkjThtdMCfTyl6/PLG1uaq+T6uqrMmzoX3ooiYl3KzOCfysxHa9sz8y7gPXVFr4+IyyiJ/tGUGbc7YtmyPpYubWzAnphmzlxzSMd1y+ujord3Wte/50P9tyJJkqTxY3IbZV8JfLZZN+3MvAU4i/6Xmuqkt1PibtZ9+wky805Kq/mL6jYvAaY3KV5reW53TLUkSZIkqQu0k0A/Cbh7gP13VWWG2zuBX2fmr9s4pq/u7/nAhtVEZPU2pczqfdtKxidJkiRJmoDaSaBvB3YYYP8OVZlhExFbUBLdQVufq/LrUyY8+3nd5kuBVSmzeNfKrQLsBlzhDNySJEmSpGbaGQP9FcoM1xcAJwG/r7b/J3AU8FrgyM6Gt4J3AI8CX2vcERGfoPwgMI8yiVhUcS2v4gUgM2+KiIuAMyJiVWABcABlWa49hjl+SZIkSdI41U4CfRrwAmB3Smvt8mr7ZGAS8A3gEx2Nrk6V7L4VuCwz/69JkfmURHgvYA3gn8DVwHGZmQ1l96Yk1ScCvcCvgdmZeeOwBC9JkiRJGvdaTqAzcxmwW0R8EdiZ0mILpdv2dzLzqs6H94TrPwLMHGD/l4AvtXiuB4FDq5skSZIkSYNqpwUagMy8ErhyGGKRJEmSJGnMamcSMUmSJEmSupYJtCRJkiRJLTCBliRJkiSpBSbQkiRJkiS1wARakiRJkqQWtDQLd0SsDuwKZGb+bHhDkiRJkiRp7Gm1Bfoh4AvA84cxFkmSJEmSxqyWEujMXA7cCaw1vOFIkiRJkjQ2tTMG+jxgz4iYMlzBSJIkSZI0VrU0BrryU+CNwM0RcRbwR+CBxkKZeV2HYpMkSZIkacxoJ4G+su7vTwF9DfsnVdt6VjYoSZIkSZLGmnYS6L2HLQpJkiRJksa4lhPozDxvOAORJEmSJGksa2cSMUmSJEmSulY7XbiJiGcAxwGvBdYGZmfm1RExE/gY8D+Z+YvOhylJ6k9EXAZsD5yUmR+u2z4dOBXYGVgdmAe8PzN/OxpxSpIkjXctt0BHxIbAL4E3AfOpmywsMxcBWwD7djpASVL/IuKtwGZNtk8CLgVmAwdT6u5VgWsi4ukjGqQkSdIE0U4X7pOA5cBzgD0os27X+wGwVYfikiQNomphPh04tMnuOcDLgT0z8+uZeVm1bTJw+MhFKUmSNHG0k0C/GjgrM+9kxSWsAP4M2KohSSPnY8Atmfn1JvvmAH/LzGtqGzLzHkqr9E4jFJ8kSdKE0k4CvRZw1wD7V6PNMdWSpKGJiK2AdwAH9VNkFnBLk+3zgfUjYo3hik2SJGmiaifhvZPyhaw/LwVuW7lwJEmDiYjVgLOB0zIz+yk2A1jYZPvi6n46cN9A1+npmURv77ShhtkVfH26S0/PZN9zSepy7STQ3wLeExHn8HhLdB9ARLwJ2BU4prPhSZKaOJwyq/ZJw3mRZcv6WLr0geG8xJgxc+aaQzquW14fFb2903zPGfq/F0maCNpJoE8CdgB+BlxHSZ6PjIiPAi8GbgY+0ekAJUmPi4j1gaMpqx5MiYgpdbunREQvcC+whNLK3GhGdb9kOOOUJEmaiFoeA52Z/wK2BL5IWbJqEvAaIICzgG0z89/DEaQk6TEbAVOB8ylJcO0G8MHq7+dSxjo3G3azKXBHZg7YfVuSJEkramvSryqJfh/wvoiYSUmiF2Vms1m5OyYitgGuabLrnszsrSs3HTgV2JnSvXEe8P7M/G3D+aYCJwBvB3opredHZOZ1HQ9ekjrrZmDbJtuvoSTV51Dmo7gE2Dsits7MuQARsRawI3DByIQqSZI0sQx51uzMXNTJQFr0XuAXdY8frf0REZMoy7NsABxMaYU5CrgmIjbPzL/UHXcO8AbgMOB2yiy2l0fElpl583A+AUlaGZm5FLi2cXtEAPw5M6+tHl9C+RHx/Ig4jMfrxEnAx0cmWkmSpIml7QQ6It4C7ELpRgglAf12Zn6jk4H149bMvKGffXOAlwPb1dY9jYh5wALKhDvvrbZtBrwN2Cczz622zaV0dzy+Oo8kjWuZuTwidgBOowyzmUpJqLfNzDtHNThJkqRxquUEOiKeBHwH2I7SgrG02vUi4C0RsT8wJzPv73CMrZoD/K2WPANk5j0RcSmwE1UCXZV7BLiortyjEXEhZVK0KZn50AjGLUkrLTMnNdm2GNinukmSJGkltTyJGGUW7lcBnwHWzcwZmTkDWLfati3DvKQK8LWIWBYR/4yIC6rZaGtmAbc0OWY+sH5ErFFXbkFmNq5DMR9YDdi441FLkiRJksa9drpw7wZcnJmH1G/MzLuBQyJivarMISseutLuoSyRNRf4F/B84EPAvIh4fmb+H2VploVNjl1c3U8H7qvKNVu+pVZuRpN9T9DTM4ne3mntxN91fH26S0/PZN9zSVJLIuLNwFspq7qsDdwBfAv4aGbeW1fOyWEljTntJNBr0Xwm7JqrgdevXDjNZeZNwE11m+ZGxHXAzyldsz88HNftz7JlfSxd2tiAPTHNnLnmkI7rltdHRW/vtK5/z4f6b0WSutAHKUnzh4C/UBpGjgW2jYiXVXM4ODmspDGpnQT6N8AmA+zfBPjtAPs7KjNvjIg/UMZgQ6lYpzcpOqNuf+3+mQOUW9xknyRJkjpjx4bVXOZGxGLgPGAbSqOMk8NKGpPaGQP9YWC/iNixcUdE7ATsS/klcaTV1qCeTxnf3GhT4I7MvK+u3IYR0djfdFPgYcr6qZIkSRoG/SyFWlumdL3qvunksJRW6Z3qjms6OSxwIbB9REzpYOiS1H8LdER8qcnmBcB3IiKBW6tt/wkEpfV5D8qvhsMuIraorvu/1aZLgL0jYuvMnFuVWQvYEbig7tBLgeOAXSm/dBIRq1DGb1/hDNySJEkjbuvqvvb9cqDJYd8REWtUjSOtTA47fxjildSlBurCvdcA+55d3eo9D3gu8K6VjGkFEfE1SvJ+I2X5rOdTxsH8Ffh0VewSyuQS50fEYTw+VmYS8PHauTLzpoi4CDgjIlatznsAsCHlBwBJkiSNkGoi2uOBqzLzl9VmJ4cdQ3x9Jqb+3lcnhx1Yvwl0ZrbTvXu43UKZrfFgYBpwN2W2xmMy8x8A1YQTOwCnAWcBUykJ9baZeWfD+famLLl1ImW2xl8DszPzxuF/KpIkSQKolhn9LvAo5fvZiHNy2MF1y+szXnX6fXVy2KK/17WdScRGTWaeDJzcQrnFwD7VbaByDwKHVjdJkiSNsIhYnTK0biNg64aZtZ0cVtKYNJZamSVJktQFqmF0/0tZC/r1jWs74+SwksaotlqgI+JllLX1NgGeTBlfXK8vM5/VodgkSZI0wUTEZOBrwHbADpl5Q5NiTg4raUxqOYGOiP2Az1F+zUvgjuEKSpIkSRPWZykJ70nA/RHx0rp9f6m6cjs5rKQxqZ0W6A8BNwPb1ybukiRJktr0uur+6OpW7zjgWCeHlTRWtZNArwOcavIsSZKkocrMDVos5+SwksacdiYRu5XmsyFKkiRJkjThtZNAnwQcGBHrDlcwkiRJkiSNVS134c7Mb1VLBPwuIr4LLASWNRTry8wTOhifJEmSJEljQjuzcP8HcDywFrBnP8X6ABNoSZIkSdKE084kYmcBawPvA66nLCcgSZIkSVJXaCeB3pIyC/dnhisYSZIkSZLGqnYmEbsHWDRcgUiSJEmSNJa1k0B/A3jjcAUiSZIkSdJY1k4X7rOB8yLiO8CngQWsOAs3mXlHZ0KTJEmSJGnsaCeBnk+ZZXsLYMcByvWsVESSJEmSJI1B7STQx1MSaEmSJEmSuk7LCXRmHjuMcUiSJEmSNKa1M4mYJEmSJEldq+UW6Ih4ZSvlMvO6oYcjSZIkSdLY1M4Y6GtpbQy0k4hJ0jCKiDcDb6VM6rg2cAfwLeCjmXlvXbnpwKnAzsDqwDzg/Zn525GOWZIkaSJoJ4Heu5/jnwXsBSykLHUlSRpeH6QkzR8C/gI8HzgW2DYiXpaZyyNiEnApsAFwMLAEOAq4JiI2z8y/jEbgkiRJ41k7k4id19++iDgVuLEjEUmSBrNjZi6qezw3IhYD5wHbAFcDc4CXA9tl5jUAETEPWAAcDrx3RCOWJEmaADoyiVhmLgG+SPlSJkkaRg3Jc80vqvv1qvs5wN9qyXN13D2UVumdhjdCSZKkiamTs3AvATbq4PkkSa3burq/tbqfBdzSpNx8YP2IWGNEopIkSZpA2hkD3a+ImArsCdzdifM1Of+gE+ZExAaUronNTM/MpQ3xngC8HegFbgaOcAZxSeNRRKwHHA9clZm/rDbPoMxN0WhxdT8duG+g8/b0TKK3d1qnwpyQfH26S0/PZN9zSepy7Sxj9aV+ds0AtgRmAod1IqgmBp0wp67sycAlDcff2/D4HOANlHhvBw4CLo+ILTPz5o5HL0nDpGpJ/i7wKM0nexyyZcv6WLr0gU6ecsyaOXPNIR3XLa+Pit7eab7nDP3fiyRNBO20QO/Vz/bFwB8oS6NcsNIRNdfKhDk1t2fmDf2dKCI2A94G7JOZ51bb5lK6NR5PGTcoSWNeRKxOGdO8EbB1w8zaSyitzI1m1O2XJElSG9qZhbuT46Xb0uKEOa2aAzwCXFR3/kcj4kLgyIiYkpkPDS1SSRoZEbEq8L+UoS2vabK283zgtU0O3RS4IzMH7L4tSZKkFY1aUtwBjRPm1JwcEY9GxD0RcUlEPLdh/yxgQWY29sGaD6wGbDwMsUpSx0TEZOBrwHbAzv30urkEWC8itq47bi1gR1Yc5iJJkqQWdGQSsZHWz4Q5DwFnA1cAi4BnU8ZM/zQiXpyZtUR7Bs27Li6u2z8gJ9YZnK9Pd3FinRH3WWBX4CTg/oh4ad2+v1RduS8B5gHnR8RhlHrvKGAS8PERjleSJGlCGDCBjoh2Wyn6MnNY1xftb8KczLwLeE9d0esj4jJKy/LRlBm3O8KJdQbXLa+PCifWGfFJdV5X3R9d3eodBxybmcsjYgfgNOAsYColod42M+8csUglSZImkMFaoHdo83x9Qw2kFYNMmLOCzLwzIn4MvKhu8xLgmU2K11qeFzfZJ0ljRmZu0GK5xcA+1U2SJEkracAEupWJw6rxdR+nJKl3dSiuZtcZbMKcgdQn9vOBXSJiWsM46E2Bh4HbVjpYSZIkSdKEM+RJxCLiORHxfcoSUgH8N7BJpwJruFYrE+Y0O259YCvg53WbLwVWpYwfrJVbBdgNuMIZuCVJkiRJzbQ9iVhEPAM4AdgDWAZ8GjgxM//Z4djqDTphTkR8gvKDwDzKJGJBmTBneXUcAJl5U0RcBJxRtWovAA4ANqyekyRJkiRJK2g5gY6I6ZTJag4EpgBfBz6cmQuHJ7QnGHTCHErX7AOAvYA1gH9SWsePy8xsOGZvSlJ9ItAL/BqYnZk3dj50SZIkSdJEMGgCHRFTgEOAIyjJ5pXAEZl583AGVq+VCXMy80vAl1o834PAodVNkiRJkqRBDbaM1bsorbvrAjcCR2bmj0YgLkmSJEmSxpTBWqC/QJnB+pfAN4DNImKzAcr3ZebpnQpOkiRJkqSxopUx0JMoS1S9aLCClGTbBFqSJEmSNOEMlkBvOyJRSJIkSZI0xg2YQGfm3JEKRJIkSZKksWzyaAcgSZIkSdJ4YAItSZIkSVILTKAlSZIkSWqBCbQkSZIkSS0wgZYkSZIkqQUm0JIkSZIktcAEWpIkSZKkFphAS5IkSZLUAhNoSZIkSZJaYAItSZIkSVILTKAlSZIkSWqBCbQkSZIkSS0wgZYkSZIkqQUm0JIkSZIktcAEWpIkSZKkFphAS5IkSZLUAhNoSZIkSZJaYAItSZIkSVILTKAlSZIkSWrBKqMdwGiJiGcApwOvASYBVwGHZOYdoxqYJHWQdZ2kbmBdJ2mkdGULdERMA64Gng28E9gT2AS4JiKeNJqxSVKnWNdJ6gbWdZJGUre2QO8HbAREZt4GEBG/Af4I7A98chRjk6ROsa6T1A2s6ySNmK5sgQbmADfUKlmAzFwA/ATYadSikqTOsq6T1A2s6ySNmG5NoGcBtzTZPh/YdIRjkaThYl0nqRtY10kaMd3ahXsGsKTJ9sXA9MEOXnXVnn/MnLnmnzse1Ri18JQ3tH3MzJlrDkMkGst8z3nmaAfQhHVdG6zr1Arfc2Ds1XfWdW2wrpuYOv2++p4D/dR13ZpAr6yZox2AJI0A6zpJ3cC6TlLLurUL9xKa/yLZ3y+YkjQeWddJ6gbWdZJGTLcm0PMp42UabQr8boRjkaThYl0nqRtY10kaMd2aQF8CvDQiNqptiIgNgJdX+yRpIrCuk9QNrOskjZhJfX19ox3DiIuIJwG/Bh4EPgz0AScAawLPy8z7RjE8SeoI6zpJ3cC6TtJI6soW6My8H9gO+APwVeBrwAJgOytZSROFdZ2kbmBdJ2kkdWULtCRJkiRJ7XIZq3EiIp4BnA68BpgEXAUckpl3jGpgY0RELASuzcy9RjmUjoiIpwNHAFsAmwGrAxtm5sLRjGusioi9gHPxNRr3rOsGZl3X3azrJg7ruoFZ13W3sV7XdWUX7vEmIqYBVwPPBt4J7AlsAlxTjfvRxLMx8BbK8hvXj3Is0oiwrutK1nXqOtZ1Xcm6bgKxBXp82A/YCIjMvA0gIn4D/BHYH/jkKMbWVERMAlbNzIdHO5Zx6rrMXAcgIvYFXjvK8Ugjwbqu+1jXqRtZ13Uf67oJxAR6fJgD3FCrZAEyc0FE/ATYiSFUtFXXmB8D3wOOAdYHbqV0H/pxQ9m3A4cBAdwH/BA4PDPvanK+q4HDgWcBb4mI/0fpgvFy4BDgdcADwBmZeXJEzAZOBv6DslbjezLzV3XnfW113POB/wfcXp3vjMxc1u7zHi8yc3mnz9nqazmMn43LKbOjrg/8EtgH+Bvl8/tm4FHgfOCIzHy0OnYq5fPxGmCD6hq/AA7LzN8P8FwvBZ6emc9v2L4h8CfgwMz83GCvmUacdZ113UqzrrOuGwes66zrVpp13ejVdXbhHh9mAbc02T4f2LR+Q0T0RcSXWzzvK4APAP8N7Ab0AN+LiN66872bMqPlrcAbgSOB7YG5EbFGw/m2BQ4FjgNmA7+p23ce8FtgF+A7wEcj4mPAqcDHqus/CfhORKxWd9xGwI8o/yjfUJ3nWOCkFp/jhBcRCyPi2haKtvNadvqz8UrgQMr4n3dS/iP+JmWm1HuB3YHPUz4/7647bgplGZITq5gPAKYC8yLiqQM81/8BNo+IFzdsfzdwf3VdjT3WddZ1/bKua8q6bnyyrrOu65d1XVNjqq6zBXp8mEEZM9FoMTC9Yduy6taKtYDNM3MJQETcTfkV6PXABRHRQ1lH8drM3L12UET8njJ+Yx/g03Xnmw68MDPvriv7iurPr2bmCdW2aykV7qHAf2Tmgmr7ZOC7wJbAXID6X5Oq7kPXA6sBH4yIDw3HL3rj0KO08J63+Vp2+rOxBjA7M++pyj0V+BTw88z8YFXmyoh4A7ArcFYV8z3AvnXn76H84vl34K2UCViauYzyS+z+wM+rY1cF9ga+lpn3DvZ6aVRY12FdNwDruhVZ141P1nVY1w3Aum5FY6quM4GeYDKznfd0Xu0fUuW31f361X0AawNHN1zjxxHxZ2BrnviP6Yb6SrbBD+uOfzQibgP+X62SrdS6bjyjtiEinkb5NW02sC5P/MyuDfR3va6RmRu3Uq7N17LTn415tUq2UnuvL28I8/fAE35djIi3UH41DUoXpcd20Y/MXB4RZwPHRMSh1bV3BtYBzu7vOI0f1nXdx7puRdZ1E591XfexrlvRWKvr7MI9PixhxV8kof9fMFu1uP5BZj5U/Tm17vwAd7Giu+v2M0C5msY4H+5n22PXr365vATYgdLVYzvgRTzeNWUqaskQXstOfzb6e6+bbX8slojYEbiI0p3obcBLqrgXNYm50TmULkp7Vo/fQ/ll9KZBjtPosa6zrlsp1nWAdd14YF1nXbdSrOuAUazrbIEeH+ZTxss02pQyQcNwqf1jazYm4anArxq29XX4+s+irJe3Z2aeX9tY/eNTezr9Wrb72Riq3YHbsm4dyKrLTmNFvoLM/GdEfAPYPyIup4zl2neQwzS6rOus61aWdZ113XhgXWddt7Ks60axrrMFeny4BHhpRGxU2xARG1BmQLxkGK+blDEJu9dvjIiXAc8Erh3GawNMq+4fqbv2qsAew3zdiajTr+VIfTamUcYC1duT8gtkK84CngN8EbgHuLBDcWl4WNc9fm3ruqGxrrOuGw+s6x6/tnXd0FjXjWJdZwv0+PAF4L+A70bEhym/CJ4A3ElDv/+IeBQ4LzPftbIXzcxlEfER4OyIOJ8yFf16lO4hfwS+tLLXGMStwJ+BkyJiGaWSeP8wX3PMiIg3V3++sLp/XUQsAhZl5ty6crcBf87MVw1wuo6+liP42bgM2DkiTqcsv7AFcDCwtMU4b4iImyizRX4mMx/oUFwaHtZ11nVgXWddN/FZ11nXgXXduK3rbIEeBzLzfsrYhj9Qppf/GrAA2C4z72so3kPrv+K0cu3PU34Zei5lJsWPA1cCW1dxDZvMfJgyQcDdwFeAzwLXAacM53XHkIur23uqx2dVj49rKLcKg7znw/FajtBn4wuUyns34FLKbJE7Un51bNXF1b0T6oxx1nXWddVj6zrrugnNus66rnpsXTdO67pJfX2dHt4gSWNHRPwEWJ6Zrxi0sCSNU9Z1krrBWKjr7MItacKJiCnAC4BXAy8DdhrdiCSp86zrJHWDsVbXmUBLmoieBvyUMqbmo5k5nJOySNJosa6T1A3GVF1nF25JkiRJklrgJGKSJEmSJLXABFqSJEmSpBaYQEuSJEmS1AITaEmSRkBEbBARfRHx5dGOZayIiC9Xr8kGw3iNY6trbDNc15AkdQ9n4ZYkaYgi4tnAQcC2wDOA1YF/ADcB3wLOz8yHRi/ClRcRfQCZOWm0Y5EkabSZQEuSNAQR8RHgGEpvrnnAecB9wDrANsAXgQOALUYpREmS1GEm0JIktSkiPgQcB9wJ7JqZP2tSZgfgAyMdmyRJGj4m0JIktaEar3ss8Ajw+sy8pVm5zPxeRFzZwvn+A9gHeDXwTGAt4G7gcuD4zPxLQ/lJwDuA/YFNgDWBRcDvgC9l5kV1ZZ8HHAVsCTwN+Bcl6b8OOCwzH2n1ebciInYG3gy8GFiv2vx7Suv8mZm5vJ9DJ0fEocC7gQ0o3eAvBo7JzH81uc7TgSOB11fXuQ/4CXBCZv6ixVhfARwOPB+YCSwBFgI/zMzjWjmHJKn7OImYJEnt2RtYFfhmf8lzTYvjn98IvIeS2H4d+AwlGd4X+EVErNdQ/iTgy8BTgW8AnwSuoiSSu9YKVcnzz4CdgBuqct+gJNsHAlNaiK1dpwAvqK77GeArwBrApyhJdH9OB/4bmFuV/QdwCHB1REytLxgRLwBupjyHrK5zKfBK4McR8frBgoyI2cC1wFbAj4BPAN8BHqrOK0lSU7ZAS5LUnq2q+x916HxfBU5vTLYj4rXAD4EPU8ZS1+wP/BV4TmY+0HDMU+oevhOYCuycmd9tKDcdeMKxHfKGzPxTw7UmA+cC74iIM5t1dwdeDmyemX+ujjmK0gL9RuAw4IRq+yqUHwHWALbNzLl111kX+AVwTkRsMMiPF/tRGhG2ycxfN8T7lOaHSJJkC7QkSe16WnX/lwFLtSgz/9os2cvMK4D5wPZNDnsEWNbkmH80Kftgk3JLBuhOPWSNyXO1bTmlVRmaPxeAT9WS57pjDgOWU7q317wBeBbwmfrkuTrmb8DHKS3zr2ox5GavTbPXUJIkwBZoSZJGVTWmeQ9gL2AzYDrQU1fk4YZDvgYcDPwuIr5B6fY8LzPvaSh3EfA+4DsR8b+Ubt4/aZbkdkpEPJmS+L4e2Ah4UkORxu7oNXMbN2Tm7RFxJ7BBRPRm5lLKWG6AZ0bEsU3Os0l1/5/ADwYI9WuU1u2fRcRFwDWU16YjP4pIkiYuE2hJktpzFyVB6y8ZbNcnKeN976JMHPZXHm8Z3YsysVi99wO3U8ZiH1ndHo2IHwAfyMzbADLz59VEWUdTJvbaEyAiEjguM7/eofipzttL6UK9IfBzyvjnxcCjQC8lme9v3PXf+9l+N+X5/z9gKfDkavuu/ZSvWWOgnZn5rbpZ0vehdIsnIn4FHJWZg07+JknqTibQkiS158fAdpRuwueszIkiYm3gvcAtwMsy896G/W9tPCYzlwFnAGdUx28F7E5JKmdFxKxal/DMnAfsEBFTgBcCsymt1xdExKLMvGpl4m+wLyV5Pi4zj214HltSEuj+rEOZEKzRU6v7exrud8rMS4YeKmTm94HvR8STgJcAO1DGmn8vIp6fmb9bmfNLkiYmx0BLktSecyljkN8UEZsOVLBKXAeyEeX/4iuaJM9Pr/b3KzP/LzO/lZlvAa6mjA9+TpNyD2XmTzPzI5SEHcrs3J20cXX/zSb7th7k2BX2R8RGwDOAhVX3bSiziQO8YigBNpOZ92fm1Zl5KPBRYDXgdZ06vyRpYjGBliSpDZm5kLIO9GqUFswtmpWrlkr64SCnW1jdbxURj417jog1gC/Q0FMsIqZExMubXGtVYEb18IFq28siYvUm11ynvlwHLazut2mI7fmUtagH8r6IeKyrejVz96mU7ynn1pX7LvAn4KD+lquKiC0jYtpAF4uIV1YzejcartdGkjRB2IVbkqQ2ZeZHqwTsGMpazT8FfgncR0nCXkmZ0OqXg5zn7oi4kNIF++aIuIIy3vc1wL8p6x1vXnfI6pS1jm8DfgX8mbJU1Wso47Ivycxbq7KHA9tFxPXAgiq2WZTW1SXA59t5zhHx5QF2H0gZ83wYpWv5tsAfKa/BDsC3gN0GOP4nlOd/EaWb9vaUCdV+RZlZG4DMfCQi3kgZK/796nW/mZLwPgN4EaXV/mkMnAR/GlgvIn5CSfwfpnRx347yml44wLGSpC5mAi1J0hBk5vERcTEledyWMqnXVOCflKTuY8D5LZzqXZRJwXYDDgIWAZcAH2HF7tD3A0dU13sZsDNwL6VV9gDgS3Vlz6Ikyi+hjJNehbL01lnAJ+qXjWrROwfYd0hm/q2atOyU6nrbA7+nvD5XMXAC/X5gF8r6zBtQXsNPAR/JzH/XF8zM30TEZsChlOR8b8pyV3cBN1F+1BhsKaqPVtfbAnh1dfwd1fYzMnPJIMdLkrrUpL6+vtGOQZIkSZKkMc8x0JIkSZIktcAEWpIkSZKkFphAS5IkSZLUAhNoSZIkSZJaYAItSZIkSVILTKAlSZIkSWqBCbQkSZIkSS0wgZYkSZIkqQUm0JIkSZIkteD/A2t36kPapOP7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['0: normal', '1: anomaly']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,4))\n",
    "ax[0].hist(trainY, align=\"left\"); ax[0].set_title('train', fontsize=18); ax[0].set_xticks([0,1]); ax[0].set_xticklabels(labels); ax[0].tick_params(axis='both', which='major', labelsize=16); ax[0].set_xlim(-0.5, 1.5);\n",
    "ax[1].hist(valY, align=\"left\"); ax[1].set_title('validation', fontsize=18); ax[1].set_xticks([0,1]); ax[1].set_xticklabels(labels); ax[1].tick_params(axis='both', which='major', labelsize=16); ax[1].set_xlim(-0.5, 1.5);\n",
    "ax[2].hist(testAllY, align=\"left\"); ax[2].set_title('test', fontsize=18); ax[2].set_xticks([0,1]); ax[2].set_xticklabels(labels); ax[2].tick_params(axis='both', which='major', labelsize=16); ax[2].set_xlim(-0.5, 1.5);\n",
    "\n",
    "ax[0].set_ylabel(\"Number of images\", fontsize=18)\n",
    "ax[1].set_xlabel(\"Class Labels\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of normal test samples: 40.00 %\n",
      "Procentage of total anomaly test samples: 60.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of normal test images vs total anomaly test images:\n",
    "print(f\"Procentage of normal test samples: {(len(testNormalX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of total anomaly test samples: {(len(testAnomalyX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Only normalization has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "img_width, img_height = trainX.shape[1], trainX.shape[2]      # input image dimensions\n",
    "num_channels = 1                                              # gray-channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Neural Networks learns faster with normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to be in  the [0, 1] range:\n",
    "trainX = trainX.astype('float32') / 255.\n",
    "valX = valX.astype('float32') / 255.\n",
    "testAllX = testAllX.astype('float32') / 255.\n",
    "\n",
    "# reshape data to keras inputs --> (n_samples, img_rows, img_cols, n_channels):\n",
    "trainX = trainX.reshape(trainX.shape[0], img_height, img_width, num_channels)\n",
    "valX = valX.reshape(valX.shape[0], img_height, img_width, num_channels)\n",
    "testAllX = testAllX.reshape(testAllX.shape[0], img_height, img_width, num_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import architecture of VAE model and its hyperparameters:\n",
    "from J32_VAE_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Encoder Part*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 32)   320         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 64, 64, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 32)   9248        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 32, 32, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 32)   9248        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 16, 16, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 8, 32)     9248        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 8, 8, 32)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 4, 4, 32)     9248        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 4, 4, 32)     0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "latent (Dense)                  (None, 200)          102600      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 200)          0           latent[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 32)           6432        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_log_mean (Dense)              (None, 32)           6432        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 32)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_mean[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 152,776\n",
      "Trainable params: 152,776\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Decoder Part*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               16896     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 128, 128, 1)       289       \n",
      "=================================================================\n",
      "Total params: 63,425\n",
      "Trainable params: 63,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Total VAE Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         [(None, 32), (None, 32),  152776    \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 128, 128, 1)       63425     \n",
      "=================================================================\n",
      "Total params: 216,201\n",
      "Trainable params: 216,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "batch_size  = 256\n",
    "epochs      = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at 1'st epoch (inital model)\n",
    "\n",
    "https://stackoverflow.com/questions/54323960/save-keras-model-at-specific-epochs\n",
    "\"\"\"\n",
    "\n",
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 1:\n",
    "            self.model.save_weights(f\"{SAVE_FOLDER}/cp-0001.h5\")\n",
    "            \n",
    "# create and use callback:\n",
    "initial_checkpoint = CustomSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at every k'te epochs\n",
    "\"\"\"\n",
    "# Include the epoch in the file name (uses `str.format`):\n",
    "checkpoint_path = \"saved_models/latent32/cp-{epoch:04d}.h5\"\n",
    "\n",
    "# Create a callback that saves the model's weights every k'te epochs:\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path,\n",
    "                             monitor='loss',           # name of the metrics to monitor\n",
    "                             verbose=1, \n",
    "                             #save_best_only=False,     # if False, the best model will not be overridden.\n",
    "                             save_weights_only=True,   # if True, only the weights of the models will be saved.\n",
    "                                                        # if False, the whole models will be saved.\n",
    "                             #mode='auto', \n",
    "                             period=200                  # save after every k'te epochs\n",
    "                            )\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "vae.save_weights(checkpoint_path.format(epoch=0))          # save for zero epoch (inital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: Early stopping\n",
    "https://www.tensorflow.org/guide/keras/custom_callback\n",
    "\"\"\"\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=100):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create and Compile Model\"\"\"\n",
    "# import architecture of VAE model and its hyperparameters. learning rate choosen from graph above.\n",
    "vae = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 3521.6223 - val_loss: 3420.9810\n",
      "Epoch 2/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 3194.6270 - val_loss: 2717.3582\n",
      "Epoch 3/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 1988.2480 - val_loss: 1169.2228\n",
      "Epoch 4/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 939.4931 - val_loss: 918.3871\n",
      "Epoch 5/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 835.1301 - val_loss: 895.4007\n",
      "Epoch 6/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 801.3235 - val_loss: 827.7040\n",
      "Epoch 7/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 716.3030 - val_loss: 721.0084\n",
      "Epoch 8/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 641.2645 - val_loss: 671.7783\n",
      "Epoch 9/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 604.7368 - val_loss: 648.7355\n",
      "Epoch 10/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 589.5350 - val_loss: 634.8691\n",
      "Epoch 11/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 580.6743 - val_loss: 627.1142\n",
      "Epoch 12/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 572.9740 - val_loss: 616.7845\n",
      "Epoch 13/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 565.0919 - val_loss: 615.9090\n",
      "Epoch 14/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 560.4333 - val_loss: 604.3873\n",
      "Epoch 15/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 548.1859 - val_loss: 577.4667\n",
      "Epoch 16/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 517.9431 - val_loss: 529.4374\n",
      "Epoch 17/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 455.1332 - val_loss: 457.1393\n",
      "Epoch 18/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 389.0519 - val_loss: 402.1623\n",
      "Epoch 19/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 347.7969 - val_loss: 369.3416\n",
      "Epoch 20/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 324.0471 - val_loss: 347.1794\n",
      "Epoch 21/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 307.9742 - val_loss: 330.6832\n",
      "Epoch 22/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 293.9216 - val_loss: 311.9998\n",
      "Epoch 23/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 281.7401 - val_loss: 303.2777\n",
      "Epoch 24/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 272.4149 - val_loss: 292.8648\n",
      "Epoch 25/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 263.2893 - val_loss: 282.6681\n",
      "Epoch 26/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 255.0166 - val_loss: 271.4953\n",
      "Epoch 27/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 246.9086 - val_loss: 264.2617\n",
      "Epoch 28/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 239.8144 - val_loss: 256.7054\n",
      "Epoch 29/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 232.0439 - val_loss: 245.7663\n",
      "Epoch 30/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 224.8830 - val_loss: 235.5905\n",
      "Epoch 31/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 217.2931 - val_loss: 230.9921\n",
      "Epoch 32/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 207.8235 - val_loss: 220.5174\n",
      "Epoch 33/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 199.5629 - val_loss: 215.6763\n",
      "Epoch 34/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 190.7236 - val_loss: 208.4501\n",
      "Epoch 35/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 181.8526 - val_loss: 196.2372\n",
      "Epoch 36/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 175.3696 - val_loss: 192.5311\n",
      "Epoch 37/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 169.4089 - val_loss: 185.3998\n",
      "Epoch 38/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 164.1150 - val_loss: 181.3562\n",
      "Epoch 39/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 160.5731 - val_loss: 176.3706\n",
      "Epoch 40/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 155.6662 - val_loss: 175.2947\n",
      "Epoch 41/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 154.4463 - val_loss: 178.8147\n",
      "Epoch 42/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 152.8084 - val_loss: 165.5497\n",
      "Epoch 43/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 148.3086 - val_loss: 169.9237\n",
      "Epoch 44/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 147.1676 - val_loss: 164.5202\n",
      "Epoch 45/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 144.0163 - val_loss: 163.0020\n",
      "Epoch 46/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 142.8547 - val_loss: 160.3653\n",
      "Epoch 47/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 142.8509 - val_loss: 159.6862\n",
      "Epoch 48/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 145.7507 - val_loss: 163.3033\n",
      "Epoch 49/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 142.4747 - val_loss: 155.5531\n",
      "Epoch 50/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 140.4202 - val_loss: 156.7966\n",
      "Epoch 51/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 137.7412 - val_loss: 152.9439\n",
      "Epoch 52/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 134.7595 - val_loss: 153.4520\n",
      "Epoch 53/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 133.3288 - val_loss: 151.4088\n",
      "Epoch 54/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 132.6206 - val_loss: 148.6514\n",
      "Epoch 55/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 131.4048 - val_loss: 149.2295\n",
      "Epoch 56/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 129.7609 - val_loss: 146.2195\n",
      "Epoch 57/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 128.7648 - val_loss: 146.4115\n",
      "Epoch 58/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 127.5119 - val_loss: 143.2021\n",
      "Epoch 59/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 126.1881 - val_loss: 141.7241\n",
      "Epoch 60/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 125.5808 - val_loss: 140.7241\n",
      "Epoch 61/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 123.8509 - val_loss: 143.8498\n",
      "Epoch 62/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 124.8532 - val_loss: 145.3658\n",
      "Epoch 63/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 123.2003 - val_loss: 140.4909\n",
      "Epoch 64/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 121.1511 - val_loss: 136.6240\n",
      "Epoch 65/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 120.1698 - val_loss: 134.7261\n",
      "Epoch 66/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 119.7445 - val_loss: 138.1675\n",
      "Epoch 67/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 117.9379 - val_loss: 134.7398\n",
      "Epoch 68/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 117.8164 - val_loss: 132.6535\n",
      "Epoch 69/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 115.7110 - val_loss: 134.1678\n",
      "Epoch 70/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 115.0136 - val_loss: 132.4137\n",
      "Epoch 71/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 114.9514 - val_loss: 130.1193\n",
      "Epoch 72/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 113.3815 - val_loss: 129.0747\n",
      "Epoch 73/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 113.4054 - val_loss: 137.6819\n",
      "Epoch 74/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 115.3259 - val_loss: 130.1408\n",
      "Epoch 75/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 112.9806 - val_loss: 128.7763\n",
      "Epoch 76/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 111.4035 - val_loss: 127.9668\n",
      "Epoch 77/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 111.3797 - val_loss: 128.6953\n",
      "Epoch 78/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 109.1178 - val_loss: 125.3493\n",
      "Epoch 79/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 108.3010 - val_loss: 124.2117\n",
      "Epoch 80/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 107.2378 - val_loss: 123.4605\n",
      "Epoch 81/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 107.4468 - val_loss: 123.3450\n",
      "Epoch 82/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 106.5216 - val_loss: 123.7753\n",
      "Epoch 83/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 106.0769 - val_loss: 121.5390\n",
      "Epoch 84/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 105.2955 - val_loss: 123.2009\n",
      "Epoch 85/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 104.7371 - val_loss: 122.0320\n",
      "Epoch 86/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 104.3868 - val_loss: 121.7569\n",
      "Epoch 87/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 103.8014 - val_loss: 119.9429\n",
      "Epoch 88/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 104.3984 - val_loss: 118.6830\n",
      "Epoch 89/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 103.0506 - val_loss: 119.7309\n",
      "Epoch 90/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 101.5392 - val_loss: 118.3780\n",
      "Epoch 91/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 101.9691 - val_loss: 117.3537\n",
      "Epoch 92/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 101.6634 - val_loss: 117.5044\n",
      "Epoch 93/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 100.9903 - val_loss: 116.4019\n",
      "Epoch 94/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 99.9309 - val_loss: 116.4026\n",
      "Epoch 95/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 98.7903 - val_loss: 114.7168\n",
      "Epoch 96/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 96.6471 - val_loss: 110.4850\n",
      "Epoch 97/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 93.2400 - val_loss: 103.3640\n",
      "Epoch 98/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 88.9456 - val_loss: 96.7661\n",
      "Epoch 99/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 83.0965 - val_loss: 90.6682\n",
      "Epoch 100/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 77.2482 - val_loss: 84.8001\n",
      "Epoch 101/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 72.8430 - val_loss: 77.9453\n",
      "Epoch 102/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 69.2471 - val_loss: 72.8466\n",
      "Epoch 103/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 67.0323 - val_loss: 72.9106\n",
      "Epoch 104/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 65.3060 - val_loss: 67.2594\n",
      "Epoch 105/2000\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 64.3084 - val_loss: 67.5552\n",
      "Epoch 106/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 65.1104 - val_loss: 66.7782\n",
      "Epoch 107/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 63.2647 - val_loss: 65.2741\n",
      "Epoch 108/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 62.3140 - val_loss: 62.4067\n",
      "Epoch 109/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 61.6469 - val_loss: 61.5984\n",
      "Epoch 110/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 61.4863 - val_loss: 61.8760\n",
      "Epoch 111/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 61.0352 - val_loss: 60.1466\n",
      "Epoch 112/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 60.1562 - val_loss: 60.6914\n",
      "Epoch 113/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 59.9274 - val_loss: 61.9592\n",
      "Epoch 114/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 59.6988 - val_loss: 61.0411\n",
      "Epoch 115/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 58.9446 - val_loss: 59.4985\n",
      "Epoch 116/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 58.5064 - val_loss: 61.3832\n",
      "Epoch 117/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 58.6391 - val_loss: 61.5172\n",
      "Epoch 118/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 58.7735 - val_loss: 59.7411\n",
      "Epoch 119/2000\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 57.6176 - val_loss: 59.7723\n",
      "Epoch 120/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 57.9475 - val_loss: 59.9644\n",
      "Epoch 121/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 58.0327 - val_loss: 58.9219\n",
      "Epoch 122/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 57.9280 - val_loss: 58.3941\n",
      "Epoch 123/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 57.2114 - val_loss: 57.8220\n",
      "Epoch 124/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 56.7341 - val_loss: 56.7592\n",
      "Epoch 125/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 56.4981 - val_loss: 57.0518\n",
      "Epoch 126/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 56.0813 - val_loss: 56.7081\n",
      "Epoch 127/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 56.1102 - val_loss: 57.8045\n",
      "Epoch 128/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 55.7257 - val_loss: 56.9672\n",
      "Epoch 129/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.0338 - val_loss: 61.0149\n",
      "Epoch 130/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 56.2966 - val_loss: 58.0246\n",
      "Epoch 131/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 55.5237 - val_loss: 57.1183\n",
      "Epoch 132/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 55.6660 - val_loss: 56.3666\n",
      "Epoch 133/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 55.4856 - val_loss: 56.9840\n",
      "Epoch 134/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 56.1982 - val_loss: 56.3449\n",
      "Epoch 135/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 54.7033 - val_loss: 55.7783\n",
      "Epoch 136/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 54.4705 - val_loss: 54.6232\n",
      "Epoch 137/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 54.3710 - val_loss: 55.4935\n",
      "Epoch 138/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 54.7645 - val_loss: 56.8070\n",
      "Epoch 139/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 55.1123 - val_loss: 54.3975\n",
      "Epoch 140/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 53.2555 - val_loss: 53.3566\n",
      "Epoch 141/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 53.0037 - val_loss: 53.5937\n",
      "Epoch 142/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 52.2948 - val_loss: 53.6681\n",
      "Epoch 143/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 51.9778 - val_loss: 52.6446\n",
      "Epoch 144/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 52.0504 - val_loss: 54.6898\n",
      "Epoch 145/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 51.9647 - val_loss: 51.8586\n",
      "Epoch 146/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 52.0772 - val_loss: 53.3318\n",
      "Epoch 147/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 51.8196 - val_loss: 53.4018\n",
      "Epoch 148/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 51.2116 - val_loss: 50.5833\n",
      "Epoch 149/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 50.8800 - val_loss: 51.9476\n",
      "Epoch 150/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 50.4261 - val_loss: 52.6485\n",
      "Epoch 151/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 51.4374 - val_loss: 54.9146\n",
      "Epoch 152/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 52.0884 - val_loss: 51.0380\n",
      "Epoch 153/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.7357 - val_loss: 50.0741\n",
      "Epoch 154/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 49.7890 - val_loss: 49.6329\n",
      "Epoch 155/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 49.3698 - val_loss: 50.3469\n",
      "Epoch 156/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 49.0946 - val_loss: 49.8067\n",
      "Epoch 157/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 48.7944 - val_loss: 49.4053\n",
      "Epoch 158/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 48.3945 - val_loss: 48.5050\n",
      "Epoch 159/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 48.3450 - val_loss: 50.5706\n",
      "Epoch 160/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 48.1437 - val_loss: 48.6752\n",
      "Epoch 161/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 48.1082 - val_loss: 48.7806\n",
      "Epoch 162/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 47.3182 - val_loss: 49.0783\n",
      "Epoch 163/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.4921 - val_loss: 48.0837\n",
      "Epoch 164/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.4697 - val_loss: 46.8748\n",
      "Epoch 165/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 46.3242 - val_loss: 48.5210\n",
      "Epoch 166/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 46.5849 - val_loss: 48.3466\n",
      "Epoch 167/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.0943 - val_loss: 49.0804\n",
      "Epoch 168/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 46.4495 - val_loss: 46.5904\n",
      "Epoch 169/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 46.1526 - val_loss: 46.9620\n",
      "Epoch 170/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 46.1125 - val_loss: 46.4974\n",
      "Epoch 171/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.5611 - val_loss: 49.9461\n",
      "Epoch 172/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.8684 - val_loss: 47.3048\n",
      "Epoch 173/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 46.0308 - val_loss: 45.1154\n",
      "Epoch 174/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 44.8869 - val_loss: 45.6712\n",
      "Epoch 175/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 44.0596 - val_loss: 44.6149\n",
      "Epoch 176/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 44.3782 - val_loss: 46.1839\n",
      "Epoch 177/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 44.0985 - val_loss: 45.6028\n",
      "Epoch 178/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 44.5208 - val_loss: 46.9774\n",
      "Epoch 179/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 43.9858 - val_loss: 45.1508\n",
      "Epoch 180/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 44.3128 - val_loss: 46.3246\n",
      "Epoch 181/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 44.0558 - val_loss: 44.8725\n",
      "Epoch 182/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 43.5686 - val_loss: 43.9682\n",
      "Epoch 183/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 43.8674 - val_loss: 46.4931\n",
      "Epoch 184/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 43.3327 - val_loss: 45.5188\n",
      "Epoch 185/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 43.0286 - val_loss: 44.4452\n",
      "Epoch 186/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 42.6374 - val_loss: 43.5952\n",
      "Epoch 187/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 42.0391 - val_loss: 42.9537\n",
      "Epoch 188/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 42.5492 - val_loss: 45.2049\n",
      "Epoch 189/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 42.5136 - val_loss: 44.3102\n",
      "Epoch 190/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 42.5212 - val_loss: 43.0000\n",
      "Epoch 191/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 42.0707 - val_loss: 44.5206\n",
      "Epoch 192/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 42.2907 - val_loss: 44.2825\n",
      "Epoch 193/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 42.9245 - val_loss: 43.0296\n",
      "Epoch 194/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 41.8426 - val_loss: 43.9958\n",
      "Epoch 195/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 41.8090 - val_loss: 42.7496\n",
      "Epoch 196/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 41.8631 - val_loss: 44.0947\n",
      "Epoch 197/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 42.0234 - val_loss: 42.2953\n",
      "Epoch 198/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 41.7991 - val_loss: 44.5522\n",
      "Epoch 199/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 41.8205 - val_loss: 44.9470\n",
      "Epoch 200/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 42.0963\n",
      "Epoch 00200: saving model to saved_models/latent32/cp-0200.h5\n",
      "6/6 [==============================] - 1s 170ms/step - loss: 42.0963 - val_loss: 44.5830\n",
      "Epoch 201/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 41.8750 - val_loss: 42.8926\n",
      "Epoch 202/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 41.5073 - val_loss: 42.9034\n",
      "Epoch 203/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 41.5764 - val_loss: 42.8888\n",
      "Epoch 204/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 40.9837 - val_loss: 41.8703\n",
      "Epoch 205/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 41.6616 - val_loss: 42.6091\n",
      "Epoch 206/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 41.0439 - val_loss: 42.2033\n",
      "Epoch 207/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 40.5760 - val_loss: 42.4047\n",
      "Epoch 208/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 40.3712 - val_loss: 42.5262\n",
      "Epoch 209/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 40.6175 - val_loss: 41.7997\n",
      "Epoch 210/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 40.1436 - val_loss: 41.0459\n",
      "Epoch 211/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 39.5503 - val_loss: 41.1362\n",
      "Epoch 212/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 39.2096 - val_loss: 42.8087\n",
      "Epoch 213/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 40.3563 - val_loss: 41.2747\n",
      "Epoch 214/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 40.1738 - val_loss: 40.9222\n",
      "Epoch 215/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.9153 - val_loss: 40.5006\n",
      "Epoch 216/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.8791 - val_loss: 41.3816\n",
      "Epoch 217/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.6299 - val_loss: 42.2047\n",
      "Epoch 218/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.3833 - val_loss: 41.8140\n",
      "Epoch 219/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.4233 - val_loss: 40.3759\n",
      "Epoch 220/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 38.9925 - val_loss: 41.9900\n",
      "Epoch 221/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.2029 - val_loss: 41.0959\n",
      "Epoch 222/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.4563 - val_loss: 40.1179\n",
      "Epoch 223/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.4593 - val_loss: 40.8536\n",
      "Epoch 224/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.6259 - val_loss: 40.9998\n",
      "Epoch 225/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 38.9133 - val_loss: 40.7658\n",
      "Epoch 226/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 38.4314 - val_loss: 40.1540\n",
      "Epoch 227/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 38.6939 - val_loss: 40.4696\n",
      "Epoch 228/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 38.3781 - val_loss: 40.2587\n",
      "Epoch 229/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 38.4664 - val_loss: 40.0705\n",
      "Epoch 230/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 38.0274 - val_loss: 39.5387\n",
      "Epoch 231/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 38.1844 - val_loss: 40.1721\n",
      "Epoch 232/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 38.0499 - val_loss: 39.5396\n",
      "Epoch 233/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 37.8882 - val_loss: 39.3785\n",
      "Epoch 234/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 37.8394 - val_loss: 38.9885\n",
      "Epoch 235/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.8887 - val_loss: 39.0704\n",
      "Epoch 236/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 38.3660 - val_loss: 39.5876\n",
      "Epoch 237/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 38.1689 - val_loss: 40.6193\n",
      "Epoch 238/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 38.2292 - val_loss: 39.6476\n",
      "Epoch 239/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 38.4062 - val_loss: 39.1310\n",
      "Epoch 240/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 38.3662 - val_loss: 39.7127\n",
      "Epoch 241/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 37.5031 - val_loss: 40.9854\n",
      "Epoch 242/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.9722 - val_loss: 40.3574\n",
      "Epoch 243/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.9892 - val_loss: 38.9873\n",
      "Epoch 244/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.5730 - val_loss: 39.4151\n",
      "Epoch 245/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.5407 - val_loss: 38.2844\n",
      "Epoch 246/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 37.7510 - val_loss: 39.2541\n",
      "Epoch 247/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 37.5484 - val_loss: 37.9754\n",
      "Epoch 248/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 36.7863 - val_loss: 38.5510\n",
      "Epoch 249/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.2441 - val_loss: 37.6149\n",
      "Epoch 250/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.8458 - val_loss: 38.6702\n",
      "Epoch 251/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.9514 - val_loss: 37.7317\n",
      "Epoch 252/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.8305 - val_loss: 37.5030\n",
      "Epoch 253/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 36.7575 - val_loss: 38.3352\n",
      "Epoch 254/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 36.6130 - val_loss: 38.5667\n",
      "Epoch 255/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.9129 - val_loss: 39.8949\n",
      "Epoch 256/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.3405 - val_loss: 39.9787\n",
      "Epoch 257/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.0895 - val_loss: 38.4524\n",
      "Epoch 258/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.7955 - val_loss: 39.8170\n",
      "Epoch 259/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 37.4386 - val_loss: 38.6965\n",
      "Epoch 260/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.3216 - val_loss: 40.5757\n",
      "Epoch 261/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.8663 - val_loss: 38.2127\n",
      "Epoch 262/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 36.5496 - val_loss: 38.2449\n",
      "Epoch 263/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 36.1185 - val_loss: 37.4925\n",
      "Epoch 264/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 36.0863 - val_loss: 37.2911\n",
      "Epoch 265/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.1261 - val_loss: 37.7117\n",
      "Epoch 266/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 36.0838 - val_loss: 37.6306\n",
      "Epoch 267/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.4457 - val_loss: 40.5704\n",
      "Epoch 268/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.7589 - val_loss: 38.1907\n",
      "Epoch 269/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.0346 - val_loss: 39.1570\n",
      "Epoch 270/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.9115 - val_loss: 37.9112\n",
      "Epoch 271/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 36.0518 - val_loss: 38.2925\n",
      "Epoch 272/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.3053 - val_loss: 38.3979\n",
      "Epoch 273/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.1267 - val_loss: 37.7442\n",
      "Epoch 274/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 35.8158 - val_loss: 37.7627\n",
      "Epoch 275/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 35.6213 - val_loss: 37.6464\n",
      "Epoch 276/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.9651 - val_loss: 37.3500\n",
      "Epoch 277/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.7467 - val_loss: 37.3916\n",
      "Epoch 278/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 35.4781 - val_loss: 37.1619\n",
      "Epoch 279/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.6041 - val_loss: 37.3677\n",
      "Epoch 280/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 35.2019 - val_loss: 37.1389\n",
      "Epoch 281/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 35.1794 - val_loss: 37.8799\n",
      "Epoch 282/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.6537 - val_loss: 36.7944\n",
      "Epoch 283/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.4016 - val_loss: 38.3194\n",
      "Epoch 284/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.6853 - val_loss: 37.1373\n",
      "Epoch 285/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.7844 - val_loss: 38.9838\n",
      "Epoch 286/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.0178 - val_loss: 38.1028\n",
      "Epoch 287/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.6945 - val_loss: 38.0595\n",
      "Epoch 288/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.5161 - val_loss: 37.0304\n",
      "Epoch 289/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.3815 - val_loss: 36.2583\n",
      "Epoch 290/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.3214 - val_loss: 37.9778\n",
      "Epoch 291/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 34.9912 - val_loss: 37.5293\n",
      "Epoch 292/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.0702 - val_loss: 36.6494\n",
      "Epoch 293/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.0951 - val_loss: 37.0535\n",
      "Epoch 294/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 34.9073 - val_loss: 37.9323\n",
      "Epoch 295/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.1922 - val_loss: 36.5883\n",
      "Epoch 296/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 34.7548 - val_loss: 36.3084\n",
      "Epoch 297/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 34.6320 - val_loss: 36.2691\n",
      "Epoch 298/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.6529 - val_loss: 36.8263\n",
      "Epoch 299/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.8378 - val_loss: 37.6759\n",
      "Epoch 300/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.8597 - val_loss: 36.5588\n",
      "Epoch 301/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.7856 - val_loss: 36.2571\n",
      "Epoch 302/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 34.3175 - val_loss: 36.5208\n",
      "Epoch 303/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 34.2966 - val_loss: 35.7147\n",
      "Epoch 304/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.6046 - val_loss: 36.8954\n",
      "Epoch 305/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 34.2863 - val_loss: 37.6435\n",
      "Epoch 306/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 34.2509 - val_loss: 36.5840\n",
      "Epoch 307/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 34.1575 - val_loss: 36.0419\n",
      "Epoch 308/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.1770 - val_loss: 37.2948\n",
      "Epoch 309/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.7209 - val_loss: 36.3177\n",
      "Epoch 310/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.6478 - val_loss: 36.6279\n",
      "Epoch 311/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.4401 - val_loss: 36.5473\n",
      "Epoch 312/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.7216 - val_loss: 35.9098\n",
      "Epoch 313/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.3779 - val_loss: 35.7806\n",
      "Epoch 314/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.4940 - val_loss: 36.8862\n",
      "Epoch 315/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.5862 - val_loss: 36.4535\n",
      "Epoch 316/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 33.7964 - val_loss: 35.7120\n",
      "Epoch 317/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.0235 - val_loss: 35.9236\n",
      "Epoch 318/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.2826 - val_loss: 36.0410\n",
      "Epoch 319/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.4927 - val_loss: 34.7474\n",
      "Epoch 320/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.9246 - val_loss: 35.6780\n",
      "Epoch 321/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.2711 - val_loss: 35.6297\n",
      "Epoch 322/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.8383 - val_loss: 35.8084\n",
      "Epoch 323/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.1185 - val_loss: 36.1850\n",
      "Epoch 324/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 33.7365 - val_loss: 35.4982\n",
      "Epoch 325/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 33.5255 - val_loss: 34.8005\n",
      "Epoch 326/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.7614 - val_loss: 35.2740\n",
      "Epoch 327/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 33.3570 - val_loss: 35.5574\n",
      "Epoch 328/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.6241 - val_loss: 34.9525\n",
      "Epoch 329/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.6089 - val_loss: 36.4931\n",
      "Epoch 330/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 33.1939 - val_loss: 34.8093\n",
      "Epoch 331/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.2616 - val_loss: 34.8416\n",
      "Epoch 332/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 33.1511 - val_loss: 35.5541\n",
      "Epoch 333/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.1872 - val_loss: 36.3203\n",
      "Epoch 334/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.7013 - val_loss: 35.0551\n",
      "Epoch 335/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.6550 - val_loss: 34.9797\n",
      "Epoch 336/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.6959 - val_loss: 35.3547\n",
      "Epoch 337/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.4101 - val_loss: 36.4906\n",
      "Epoch 338/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.7680 - val_loss: 34.8552\n",
      "Epoch 339/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 32.9063 - val_loss: 35.1067\n",
      "Epoch 340/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.0541 - val_loss: 34.0124\n",
      "Epoch 341/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 32.7550 - val_loss: 34.4737\n",
      "Epoch 342/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 32.8939 - val_loss: 36.2146\n",
      "Epoch 343/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.7636 - val_loss: 34.2785\n",
      "Epoch 344/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 32.6353 - val_loss: 34.2634\n",
      "Epoch 345/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 32.5838 - val_loss: 35.1218\n",
      "Epoch 346/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.8218 - val_loss: 34.6198\n",
      "Epoch 347/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 32.7124 - val_loss: 34.5446\n",
      "Epoch 348/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.8568 - val_loss: 35.2032\n",
      "Epoch 349/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.6623 - val_loss: 34.9934\n",
      "Epoch 350/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.0513 - val_loss: 35.8319\n",
      "Epoch 351/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.0144 - val_loss: 35.5056\n",
      "Epoch 352/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 32.5404 - val_loss: 34.5867\n",
      "Epoch 353/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 32.3738 - val_loss: 34.7462\n",
      "Epoch 354/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.6375 - val_loss: 34.0203\n",
      "Epoch 355/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.4844 - val_loss: 34.3141\n",
      "Epoch 356/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.9840 - val_loss: 33.8145\n",
      "Epoch 357/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 32.9185 - val_loss: 34.8779\n",
      "Epoch 358/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.6166 - val_loss: 34.9205\n",
      "Epoch 359/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.8519 - val_loss: 34.2705\n",
      "Epoch 360/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.5778 - val_loss: 35.1472\n",
      "Epoch 361/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.9718 - val_loss: 36.0112\n",
      "Epoch 362/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.8272 - val_loss: 34.3368\n",
      "Epoch 363/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 32.1862 - val_loss: 34.0944\n",
      "Epoch 364/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.1919 - val_loss: 34.5929\n",
      "Epoch 365/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.7066 - val_loss: 33.9301\n",
      "Epoch 366/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.5193 - val_loss: 34.4688\n",
      "Epoch 367/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.2728 - val_loss: 34.9879\n",
      "Epoch 368/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.2481 - val_loss: 33.5513\n",
      "Epoch 369/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.4032 - val_loss: 34.8708\n",
      "Epoch 370/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 32.7990 - val_loss: 35.3861\n",
      "Epoch 371/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.6496 - val_loss: 33.6052\n",
      "Epoch 372/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 32.1448 - val_loss: 34.1592\n",
      "Epoch 373/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 32.1255 - val_loss: 34.6957\n",
      "Epoch 374/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.4966 - val_loss: 35.2053\n",
      "Epoch 375/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.2072 - val_loss: 33.8423\n",
      "Epoch 376/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.2269 - val_loss: 34.7165\n",
      "Epoch 377/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.3668 - val_loss: 33.7308\n",
      "Epoch 378/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 31.9834 - val_loss: 34.8618\n",
      "Epoch 379/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.9978 - val_loss: 33.9251\n",
      "Epoch 380/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.9894 - val_loss: 34.2222\n",
      "Epoch 381/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.0282 - val_loss: 33.8410\n",
      "Epoch 382/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.9936 - val_loss: 33.9619\n",
      "Epoch 383/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 31.9220 - val_loss: 33.5599\n",
      "Epoch 384/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.0423 - val_loss: 33.8870\n",
      "Epoch 385/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 31.7648 - val_loss: 33.7579\n",
      "Epoch 386/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.9108 - val_loss: 34.5487\n",
      "Epoch 387/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.3093 - val_loss: 34.2063\n",
      "Epoch 388/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.1229 - val_loss: 34.6151\n",
      "Epoch 389/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 31.6708 - val_loss: 33.8077\n",
      "Epoch 390/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 31.6609 - val_loss: 32.8711\n",
      "Epoch 391/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 31.4143 - val_loss: 32.8874\n",
      "Epoch 392/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.4480 - val_loss: 33.6818\n",
      "Epoch 393/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.0609 - val_loss: 33.4199\n",
      "Epoch 394/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.7907 - val_loss: 33.8502\n",
      "Epoch 395/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.7151 - val_loss: 33.2278\n",
      "Epoch 396/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.8483 - val_loss: 33.6284\n",
      "Epoch 397/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.8329 - val_loss: 33.3578\n",
      "Epoch 398/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 31.2301 - val_loss: 32.9975\n",
      "Epoch 399/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 31.1396 - val_loss: 33.8335\n",
      "Epoch 400/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 31.3472\n",
      "Epoch 00400: saving model to saved_models/latent32/cp-0400.h5\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 31.3472 - val_loss: 33.3089\n",
      "Epoch 401/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 31.1315 - val_loss: 33.0388\n",
      "Epoch 402/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 31.1687 - val_loss: 33.6404\n",
      "Epoch 403/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.7941 - val_loss: 33.1774\n",
      "Epoch 404/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.5570 - val_loss: 33.7608\n",
      "Epoch 405/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.6367 - val_loss: 33.2554\n",
      "Epoch 406/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.5453 - val_loss: 33.7603\n",
      "Epoch 407/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.7989 - val_loss: 33.8555\n",
      "Epoch 408/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.8239 - val_loss: 34.9500\n",
      "Epoch 409/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.1590 - val_loss: 34.3767\n",
      "Epoch 410/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.4209 - val_loss: 35.8815\n",
      "Epoch 411/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.0694 - val_loss: 34.5809\n",
      "Epoch 412/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.4472 - val_loss: 33.2320\n",
      "Epoch 413/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 31.1042 - val_loss: 33.3381\n",
      "Epoch 414/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.1205 - val_loss: 33.4998\n",
      "Epoch 415/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.1092 - val_loss: 33.1715\n",
      "Epoch 416/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 30.9256 - val_loss: 32.2944\n",
      "Epoch 417/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.0292 - val_loss: 32.8832\n",
      "Epoch 418/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.3288 - val_loss: 33.1470\n",
      "Epoch 419/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 30.6838 - val_loss: 33.0119\n",
      "Epoch 420/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.0172 - val_loss: 32.8635\n",
      "Epoch 421/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.0945 - val_loss: 33.8090\n",
      "Epoch 422/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.1647 - val_loss: 33.1600\n",
      "Epoch 423/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.2405 - val_loss: 32.9239\n",
      "Epoch 424/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.2119 - val_loss: 32.5501\n",
      "Epoch 425/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 31.0972 - val_loss: 33.8844\n",
      "Epoch 426/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 31.1071 - val_loss: 32.4452\n",
      "Epoch 427/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.1663 - val_loss: 33.1669\n",
      "Epoch 428/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.0525 - val_loss: 33.2903\n",
      "Epoch 429/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.9623 - val_loss: 32.5264\n",
      "Epoch 430/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 30.6698 - val_loss: 32.8597\n",
      "Epoch 431/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.7640 - val_loss: 33.1323\n",
      "Epoch 432/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.9288 - val_loss: 32.8256\n",
      "Epoch 433/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.6991 - val_loss: 33.4321\n",
      "Epoch 434/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.1157 - val_loss: 33.6264\n",
      "Epoch 435/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.9898 - val_loss: 33.7732\n",
      "Epoch 436/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.4481 - val_loss: 34.9249\n",
      "Epoch 437/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.4462 - val_loss: 34.8352\n",
      "Epoch 438/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.2904 - val_loss: 34.7238\n",
      "Epoch 439/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.0679 - val_loss: 33.2423\n",
      "Epoch 440/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.2809 - val_loss: 33.5567\n",
      "Epoch 441/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.4277 - val_loss: 33.8221\n",
      "Epoch 442/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.1171 - val_loss: 32.6882\n",
      "Epoch 443/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.8988 - val_loss: 32.6521\n",
      "Epoch 444/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 30.4119 - val_loss: 32.3966\n",
      "Epoch 445/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.4732 - val_loss: 32.0489\n",
      "Epoch 446/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.6981 - val_loss: 33.1516\n",
      "Epoch 447/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.1091 - val_loss: 33.8307\n",
      "Epoch 448/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 30.9262 - val_loss: 33.1741\n",
      "Epoch 449/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.9054 - val_loss: 31.9654\n",
      "Epoch 450/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.5037 - val_loss: 32.1638\n",
      "Epoch 451/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 30.2439 - val_loss: 32.6226\n",
      "Epoch 452/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.5796 - val_loss: 32.2676\n",
      "Epoch 453/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.3457 - val_loss: 31.8004\n",
      "Epoch 454/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.4068 - val_loss: 31.9477\n",
      "Epoch 455/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 30.1294 - val_loss: 32.4862\n",
      "Epoch 456/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 30.0301 - val_loss: 31.8970\n",
      "Epoch 457/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 29.9787 - val_loss: 32.6497\n",
      "Epoch 458/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.1080 - val_loss: 32.7073\n",
      "Epoch 459/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.0142 - val_loss: 32.6299\n",
      "Epoch 460/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.2357 - val_loss: 32.2353\n",
      "Epoch 461/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.3659 - val_loss: 32.1104\n",
      "Epoch 462/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.5709 - val_loss: 32.0146\n",
      "Epoch 463/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.2965 - val_loss: 32.2029\n",
      "Epoch 464/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.1939 - val_loss: 32.4514\n",
      "Epoch 465/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.2543 - val_loss: 31.2683\n",
      "Epoch 466/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.0034 - val_loss: 32.2065\n",
      "Epoch 467/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.0825 - val_loss: 32.0593\n",
      "Epoch 468/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.9861 - val_loss: 31.5533\n",
      "Epoch 469/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 29.8746 - val_loss: 32.2228\n",
      "Epoch 470/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.0790 - val_loss: 31.7130\n",
      "Epoch 471/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.9978 - val_loss: 32.5529\n",
      "Epoch 472/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.3163 - val_loss: 32.0234\n",
      "Epoch 473/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.9222 - val_loss: 31.8359\n",
      "Epoch 474/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.2051 - val_loss: 33.1633\n",
      "Epoch 475/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.0192 - val_loss: 32.1515\n",
      "Epoch 476/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.0098 - val_loss: 31.9781\n",
      "Epoch 477/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 29.7328 - val_loss: 31.5805\n",
      "Epoch 478/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.9577 - val_loss: 32.7183\n",
      "Epoch 479/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.8872 - val_loss: 31.5916\n",
      "Epoch 480/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.9190 - val_loss: 32.0929\n",
      "Epoch 481/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.8547 - val_loss: 32.5964\n",
      "Epoch 482/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.9910 - val_loss: 32.2386\n",
      "Epoch 483/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.9145 - val_loss: 31.1998\n",
      "Epoch 484/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.9473 - val_loss: 31.2342\n",
      "Epoch 485/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.9513 - val_loss: 32.0391\n",
      "Epoch 486/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.8814 - val_loss: 31.8597\n",
      "Epoch 487/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.7414 - val_loss: 31.7053\n",
      "Epoch 488/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.7974 - val_loss: 31.4846\n",
      "Epoch 489/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 29.4062 - val_loss: 31.8587\n",
      "Epoch 490/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.4170 - val_loss: 31.2772\n",
      "Epoch 491/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.4761 - val_loss: 31.9300\n",
      "Epoch 492/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.4714 - val_loss: 31.4479\n",
      "Epoch 493/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 29.3784 - val_loss: 31.2224\n",
      "Epoch 494/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 29.3559 - val_loss: 30.9164\n",
      "Epoch 495/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 29.1217 - val_loss: 31.3494\n",
      "Epoch 496/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 29.4669 - val_loss: 31.2463\n",
      "Epoch 497/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.7087 - val_loss: 31.5727\n",
      "Epoch 498/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.4539 - val_loss: 31.8635\n",
      "Epoch 499/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.5775 - val_loss: 31.4079\n",
      "Epoch 500/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.7576 - val_loss: 32.3633\n",
      "Epoch 501/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.6351 - val_loss: 32.2867\n",
      "Epoch 502/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.8642 - val_loss: 32.4230\n",
      "Epoch 503/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.9275 - val_loss: 31.8894\n",
      "Epoch 504/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.7944 - val_loss: 31.2718\n",
      "Epoch 505/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.5856 - val_loss: 32.2636\n",
      "Epoch 506/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.2850 - val_loss: 30.8831\n",
      "Epoch 507/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.2276 - val_loss: 30.8800\n",
      "Epoch 508/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.5097 - val_loss: 32.5911\n",
      "Epoch 509/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.8206 - val_loss: 31.3767\n",
      "Epoch 510/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.1559 - val_loss: 31.4089\n",
      "Epoch 511/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2654 - val_loss: 31.0547\n",
      "Epoch 512/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.4446 - val_loss: 31.2073\n",
      "Epoch 513/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.3907 - val_loss: 32.7122\n",
      "Epoch 514/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.4864 - val_loss: 30.8280\n",
      "Epoch 515/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.6147 - val_loss: 31.0625\n",
      "Epoch 516/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.5687 - val_loss: 31.9568\n",
      "Epoch 517/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.4436 - val_loss: 30.9322\n",
      "Epoch 518/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.2227 - val_loss: 32.3465\n",
      "Epoch 519/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.2543 - val_loss: 31.6457\n",
      "Epoch 520/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 29.0314 - val_loss: 31.2020\n",
      "Epoch 521/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.2738 - val_loss: 31.4915\n",
      "Epoch 522/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.2195 - val_loss: 31.6930\n",
      "Epoch 523/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.0790 - val_loss: 30.8481\n",
      "Epoch 524/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.1140 - val_loss: 30.2596\n",
      "Epoch 525/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 28.9067 - val_loss: 30.9680\n",
      "Epoch 526/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 28.8234 - val_loss: 31.1895\n",
      "Epoch 527/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.0072 - val_loss: 30.7420\n",
      "Epoch 528/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.0357 - val_loss: 30.8403\n",
      "Epoch 529/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 29.1648 - val_loss: 31.3321\n",
      "Epoch 530/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.9658 - val_loss: 30.9738\n",
      "Epoch 531/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.7637 - val_loss: 31.1425\n",
      "Epoch 532/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.9719 - val_loss: 30.5427\n",
      "Epoch 533/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.8484 - val_loss: 30.6237\n",
      "Epoch 534/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 28.7343 - val_loss: 30.9475\n",
      "Epoch 535/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.0219 - val_loss: 31.1112\n",
      "Epoch 536/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.9622 - val_loss: 30.5445\n",
      "Epoch 537/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 28.7175 - val_loss: 30.9416\n",
      "Epoch 538/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.0410 - val_loss: 30.5720\n",
      "Epoch 539/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 28.7045 - val_loss: 31.4305\n",
      "Epoch 540/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.3841 - val_loss: 31.0927\n",
      "Epoch 541/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.9025 - val_loss: 30.8692\n",
      "Epoch 542/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.9266 - val_loss: 31.5689\n",
      "Epoch 543/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.2814 - val_loss: 32.0110\n",
      "Epoch 544/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.8080 - val_loss: 31.9019\n",
      "Epoch 545/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.7962 - val_loss: 31.6884\n",
      "Epoch 546/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.5604 - val_loss: 31.1515\n",
      "Epoch 547/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.1297 - val_loss: 30.3806\n",
      "Epoch 548/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.0649 - val_loss: 30.9365\n",
      "Epoch 549/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.0048 - val_loss: 30.5180\n",
      "Epoch 550/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 28.8631 - val_loss: 31.0066\n",
      "Epoch 551/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.0897 - val_loss: 32.1770\n",
      "Epoch 552/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.9016 - val_loss: 31.6029\n",
      "Epoch 553/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.7846 - val_loss: 31.6117\n",
      "Epoch 554/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.1480 - val_loss: 31.4270\n",
      "Epoch 555/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.0414 - val_loss: 30.7858\n",
      "Epoch 556/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.0500 - val_loss: 30.5881\n",
      "Epoch 557/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 28.4959 - val_loss: 30.1386\n",
      "Epoch 558/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.7784 - val_loss: 30.1165\n",
      "Epoch 559/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.8322 - val_loss: 31.1592\n",
      "Epoch 560/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.1285 - val_loss: 31.0511\n",
      "Epoch 561/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.1395 - val_loss: 31.1695\n",
      "Epoch 562/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.3701 - val_loss: 29.9011\n",
      "Epoch 563/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.6957 - val_loss: 31.8079\n",
      "Epoch 564/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.5979 - val_loss: 30.8546\n",
      "Epoch 565/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 28.2314 - val_loss: 30.0849\n",
      "Epoch 566/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.4260 - val_loss: 31.0476\n",
      "Epoch 567/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.5545 - val_loss: 31.5378\n",
      "Epoch 568/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.7674 - val_loss: 30.9345\n",
      "Epoch 569/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.0660 - val_loss: 30.7296\n",
      "Epoch 570/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.8071 - val_loss: 31.3556\n",
      "Epoch 571/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.6751 - val_loss: 31.1918\n",
      "Epoch 572/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.9508 - val_loss: 30.3423\n",
      "Epoch 573/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.0318 - val_loss: 30.5808\n",
      "Epoch 574/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9455 - val_loss: 30.4972\n",
      "Epoch 575/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.5303 - val_loss: 31.6290\n",
      "Epoch 576/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.6881 - val_loss: 30.7042\n",
      "Epoch 577/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.4410 - val_loss: 30.4503\n",
      "Epoch 578/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.4497 - val_loss: 30.2932\n",
      "Epoch 579/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 28.2143 - val_loss: 30.4218\n",
      "Epoch 580/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2950 - val_loss: 29.8130\n",
      "Epoch 581/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.2549 - val_loss: 30.8732\n",
      "Epoch 582/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 28.1394 - val_loss: 31.2278\n",
      "Epoch 583/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.4071 - val_loss: 30.7472\n",
      "Epoch 584/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.3430 - val_loss: 30.1707\n",
      "Epoch 585/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.4431 - val_loss: 30.5009\n",
      "Epoch 586/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 28.0736 - val_loss: 30.1508\n",
      "Epoch 587/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1447 - val_loss: 30.1287\n",
      "Epoch 588/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 28.0284 - val_loss: 30.7618\n",
      "Epoch 589/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.2286 - val_loss: 29.7788\n",
      "Epoch 590/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.3421 - val_loss: 31.2553\n",
      "Epoch 591/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.5456 - val_loss: 31.1992\n",
      "Epoch 592/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.4287 - val_loss: 30.2612\n",
      "Epoch 593/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.1725 - val_loss: 30.5693\n",
      "Epoch 594/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.2081 - val_loss: 30.5611\n",
      "Epoch 595/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.1518 - val_loss: 30.1115\n",
      "Epoch 596/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.9408 - val_loss: 30.2119\n",
      "Epoch 597/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.3403 - val_loss: 30.7063\n",
      "Epoch 598/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.7241 - val_loss: 30.0808\n",
      "Epoch 599/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.1699 - val_loss: 30.3439\n",
      "Epoch 600/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 28.1588\n",
      "Epoch 00600: saving model to saved_models/latent32/cp-0600.h5\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 28.1588 - val_loss: 30.5256\n",
      "Epoch 601/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.9460 - val_loss: 30.2128\n",
      "Epoch 602/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.9872 - val_loss: 30.3635\n",
      "Epoch 603/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.9557 - val_loss: 30.0183\n",
      "Epoch 604/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.0437 - val_loss: 30.1559\n",
      "Epoch 605/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.2932 - val_loss: 31.2896\n",
      "Epoch 606/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.5304 - val_loss: 29.3337\n",
      "Epoch 607/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.4783 - val_loss: 29.6489\n",
      "Epoch 608/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.1043 - val_loss: 30.5488\n",
      "Epoch 609/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.0157 - val_loss: 30.9785\n",
      "Epoch 610/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 28.1484 - val_loss: 29.6015\n",
      "Epoch 611/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.9294 - val_loss: 30.5694\n",
      "Epoch 612/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.2841 - val_loss: 29.6631\n",
      "Epoch 613/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.9440 - val_loss: 30.1022\n",
      "Epoch 614/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.9440 - val_loss: 30.2265\n",
      "Epoch 615/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.8942 - val_loss: 29.7717\n",
      "Epoch 616/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.9950 - val_loss: 30.2213\n",
      "Epoch 617/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.0947 - val_loss: 30.8921\n",
      "Epoch 618/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.1827 - val_loss: 30.4737\n",
      "Epoch 619/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.1935 - val_loss: 30.5885\n",
      "Epoch 620/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.7764 - val_loss: 30.0345\n",
      "Epoch 621/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.1479 - val_loss: 30.8874\n",
      "Epoch 622/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.0159 - val_loss: 29.7943\n",
      "Epoch 623/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.0037 - val_loss: 29.7275\n",
      "Epoch 624/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.4374 - val_loss: 30.2074\n",
      "Epoch 625/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.5430 - val_loss: 29.9732\n",
      "Epoch 626/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.7995 - val_loss: 29.3158\n",
      "Epoch 627/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.7241 - val_loss: 29.9771\n",
      "Epoch 628/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.0357 - val_loss: 29.1805\n",
      "Epoch 629/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.8524 - val_loss: 30.2594\n",
      "Epoch 630/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.9596 - val_loss: 29.9104\n",
      "Epoch 631/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.9407 - val_loss: 29.7583\n",
      "Epoch 632/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.8875 - val_loss: 29.4508\n",
      "Epoch 633/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.7375 - val_loss: 29.6869\n",
      "Epoch 634/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.4698 - val_loss: 29.5120\n",
      "Epoch 635/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.5590 - val_loss: 29.5074\n",
      "Epoch 636/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.5754 - val_loss: 29.4669\n",
      "Epoch 637/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.5920 - val_loss: 29.5596\n",
      "Epoch 638/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.6954 - val_loss: 30.2971\n",
      "Epoch 639/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.6341 - val_loss: 30.5013\n",
      "Epoch 640/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.0536 - val_loss: 30.2661\n",
      "Epoch 641/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.9011 - val_loss: 29.4626\n",
      "Epoch 642/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.7090 - val_loss: 29.3930\n",
      "Epoch 643/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.6214 - val_loss: 29.6615\n",
      "Epoch 644/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.4024 - val_loss: 29.3030\n",
      "Epoch 645/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.3857 - val_loss: 29.4689\n",
      "Epoch 646/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.2411 - val_loss: 29.3537\n",
      "Epoch 647/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4214 - val_loss: 29.8751\n",
      "Epoch 648/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4556 - val_loss: 29.5919\n",
      "Epoch 649/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4147 - val_loss: 29.4849\n",
      "Epoch 650/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4585 - val_loss: 30.5154\n",
      "Epoch 651/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.5065 - val_loss: 29.3334\n",
      "Epoch 652/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.3866 - val_loss: 29.4666\n",
      "Epoch 653/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.5976 - val_loss: 29.3395\n",
      "Epoch 654/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.2463 - val_loss: 29.7277\n",
      "Epoch 655/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.1574 - val_loss: 29.9282\n",
      "Epoch 656/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.5624 - val_loss: 29.5059\n",
      "Epoch 657/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.3719 - val_loss: 29.7469\n",
      "Epoch 658/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4359 - val_loss: 29.3780\n",
      "Epoch 659/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.1513 - val_loss: 28.7863\n",
      "Epoch 660/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1859 - val_loss: 28.9605\n",
      "Epoch 661/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.3157 - val_loss: 28.9338\n",
      "Epoch 662/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4163 - val_loss: 29.4140\n",
      "Epoch 663/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.2202 - val_loss: 29.8135\n",
      "Epoch 664/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.3073 - val_loss: 30.1994\n",
      "Epoch 665/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.3446 - val_loss: 29.7860\n",
      "Epoch 666/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.5802 - val_loss: 29.8489\n",
      "Epoch 667/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.8769 - val_loss: 30.4088\n",
      "Epoch 668/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7455 - val_loss: 29.1072\n",
      "Epoch 669/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.5918 - val_loss: 29.6090\n",
      "Epoch 670/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5322 - val_loss: 29.7709\n",
      "Epoch 671/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4069 - val_loss: 28.8590\n",
      "Epoch 672/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.3766 - val_loss: 29.3334\n",
      "Epoch 673/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4141 - val_loss: 30.3628\n",
      "Epoch 674/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.1197 - val_loss: 29.0921\n",
      "Epoch 675/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1308 - val_loss: 29.1167\n",
      "Epoch 676/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2818 - val_loss: 29.4879\n",
      "Epoch 677/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.2588 - val_loss: 28.6518\n",
      "Epoch 678/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.0777 - val_loss: 29.0145\n",
      "Epoch 679/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.8748 - val_loss: 28.6727\n",
      "Epoch 680/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.0337 - val_loss: 29.6057\n",
      "Epoch 681/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4070 - val_loss: 29.3986\n",
      "Epoch 682/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.6506 - val_loss: 29.2943\n",
      "Epoch 683/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.5698 - val_loss: 29.2614\n",
      "Epoch 684/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4385 - val_loss: 29.2262\n",
      "Epoch 685/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.0875 - val_loss: 28.9316\n",
      "Epoch 686/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.0627 - val_loss: 29.2344\n",
      "Epoch 687/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.3691 - val_loss: 29.7865\n",
      "Epoch 688/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.2050 - val_loss: 31.0911\n",
      "Epoch 689/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.7379 - val_loss: 29.4536\n",
      "Epoch 690/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.6862 - val_loss: 30.3774\n",
      "Epoch 691/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4134 - val_loss: 29.4390\n",
      "Epoch 692/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.3270 - val_loss: 29.9075\n",
      "Epoch 693/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.5770 - val_loss: 29.3541\n",
      "Epoch 694/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4699 - val_loss: 29.4508\n",
      "Epoch 695/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.3154 - val_loss: 29.6743\n",
      "Epoch 696/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.2141 - val_loss: 29.4936\n",
      "Epoch 697/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1275 - val_loss: 29.0057\n",
      "Epoch 698/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1514 - val_loss: 29.6757\n",
      "Epoch 699/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1984 - val_loss: 29.8285\n",
      "Epoch 700/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 26.7981 - val_loss: 29.1012\n",
      "Epoch 701/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9635 - val_loss: 29.3505\n",
      "Epoch 702/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1703 - val_loss: 29.5503\n",
      "Epoch 703/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9707 - val_loss: 29.3303\n",
      "Epoch 704/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9798 - val_loss: 29.3136\n",
      "Epoch 705/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.2409 - val_loss: 29.1683\n",
      "Epoch 706/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.2001 - val_loss: 29.3349\n",
      "Epoch 707/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.6141 - val_loss: 29.5882\n",
      "Epoch 708/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1347 - val_loss: 29.1969\n",
      "Epoch 709/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.2795 - val_loss: 29.2447\n",
      "Epoch 710/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.3116 - val_loss: 29.2818\n",
      "Epoch 711/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5105 - val_loss: 29.2182\n",
      "Epoch 712/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1740 - val_loss: 29.8629\n",
      "Epoch 713/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2106 - val_loss: 29.3244\n",
      "Epoch 714/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.3166 - val_loss: 28.9399\n",
      "Epoch 715/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1622 - val_loss: 29.8103\n",
      "Epoch 716/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1911 - val_loss: 29.1906\n",
      "Epoch 717/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1739 - val_loss: 28.6489\n",
      "Epoch 718/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9648 - val_loss: 29.1009\n",
      "Epoch 719/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.8820 - val_loss: 29.0277\n",
      "Epoch 720/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 26.7676 - val_loss: 29.2276\n",
      "Epoch 721/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.8447 - val_loss: 29.1960\n",
      "Epoch 722/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.8576 - val_loss: 28.6470\n",
      "Epoch 723/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.6648 - val_loss: 28.7240\n",
      "Epoch 724/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7768 - val_loss: 28.8879\n",
      "Epoch 725/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0540 - val_loss: 29.1337\n",
      "Epoch 726/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.8792 - val_loss: 29.3544\n",
      "Epoch 727/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.0281 - val_loss: 29.3809\n",
      "Epoch 728/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1398 - val_loss: 30.7527\n",
      "Epoch 729/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.3758 - val_loss: 29.6277\n",
      "Epoch 730/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.3561 - val_loss: 30.5771\n",
      "Epoch 731/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.0838 - val_loss: 28.6411\n",
      "Epoch 732/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.8652 - val_loss: 29.3606\n",
      "Epoch 733/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1268 - val_loss: 30.2027\n",
      "Epoch 734/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.5390 - val_loss: 29.8685\n",
      "Epoch 735/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.3231 - val_loss: 29.4346\n",
      "Epoch 736/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.0741 - val_loss: 29.4070\n",
      "Epoch 737/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.8064 - val_loss: 28.6875\n",
      "Epoch 738/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.5628 - val_loss: 28.3778\n",
      "Epoch 739/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9383 - val_loss: 29.3924\n",
      "Epoch 740/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9380 - val_loss: 29.1447\n",
      "Epoch 741/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1896 - val_loss: 30.6085\n",
      "Epoch 742/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.0411 - val_loss: 31.2960\n",
      "Epoch 743/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.0190 - val_loss: 29.8900\n",
      "Epoch 744/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.9828 - val_loss: 31.9116\n",
      "Epoch 745/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.5682 - val_loss: 28.8677\n",
      "Epoch 746/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.2887 - val_loss: 29.5423\n",
      "Epoch 747/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.5405 - val_loss: 30.3362\n",
      "Epoch 748/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.7604 - val_loss: 30.0592\n",
      "Epoch 749/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.7028 - val_loss: 30.8249\n",
      "Epoch 750/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.5279 - val_loss: 29.9557\n",
      "Epoch 751/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1842 - val_loss: 30.0936\n",
      "Epoch 752/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0538 - val_loss: 28.9908\n",
      "Epoch 753/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9195 - val_loss: 29.6010\n",
      "Epoch 754/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.8997 - val_loss: 29.5597\n",
      "Epoch 755/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0920 - val_loss: 29.0246\n",
      "Epoch 756/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.0592 - val_loss: 29.8414\n",
      "Epoch 757/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.0739 - val_loss: 28.9682\n",
      "Epoch 758/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9763 - val_loss: 29.7488\n",
      "Epoch 759/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.8587 - val_loss: 29.2089\n",
      "Epoch 760/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9901 - val_loss: 28.1026\n",
      "Epoch 761/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7635 - val_loss: 29.1431\n",
      "Epoch 762/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6154 - val_loss: 28.9450\n",
      "Epoch 763/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6111 - val_loss: 30.0147\n",
      "Epoch 764/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.5572 - val_loss: 28.4513\n",
      "Epoch 765/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6146 - val_loss: 28.2236\n",
      "Epoch 766/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7795 - val_loss: 28.6175\n",
      "Epoch 767/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6397 - val_loss: 29.0214\n",
      "Epoch 768/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7050 - val_loss: 28.9866\n",
      "Epoch 769/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6302 - val_loss: 28.5884\n",
      "Epoch 770/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5982 - val_loss: 29.0082\n",
      "Epoch 771/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9159 - val_loss: 29.3286\n",
      "Epoch 772/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.5061 - val_loss: 29.0591\n",
      "Epoch 773/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.5057 - val_loss: 28.8473\n",
      "Epoch 774/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.3075 - val_loss: 28.5881\n",
      "Epoch 775/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8583 - val_loss: 28.8863\n",
      "Epoch 776/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7418 - val_loss: 29.2175\n",
      "Epoch 777/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.8623 - val_loss: 29.3463\n",
      "Epoch 778/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6537 - val_loss: 28.9440\n",
      "Epoch 779/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6903 - val_loss: 28.8275\n",
      "Epoch 780/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6969 - val_loss: 28.9570\n",
      "Epoch 781/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7784 - val_loss: 29.2274\n",
      "Epoch 782/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7838 - val_loss: 29.0953\n",
      "Epoch 783/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7568 - val_loss: 28.6337\n",
      "Epoch 784/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.8291 - val_loss: 28.7071\n",
      "Epoch 785/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7197 - val_loss: 29.0106\n",
      "Epoch 786/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7415 - val_loss: 29.0178\n",
      "Epoch 787/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6925 - val_loss: 28.4057\n",
      "Epoch 788/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6322 - val_loss: 28.6876\n",
      "Epoch 789/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4752 - val_loss: 28.2889\n",
      "Epoch 790/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5083 - val_loss: 28.8625\n",
      "Epoch 791/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6566 - val_loss: 29.4549\n",
      "Epoch 792/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4239 - val_loss: 29.7617\n",
      "Epoch 793/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7033 - val_loss: 29.0392\n",
      "Epoch 794/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1751 - val_loss: 28.6175\n",
      "Epoch 795/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3597 - val_loss: 29.7920\n",
      "Epoch 796/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.3170 - val_loss: 29.6142\n",
      "Epoch 797/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9894 - val_loss: 28.2700\n",
      "Epoch 798/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6140 - val_loss: 28.8670\n",
      "Epoch 799/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4557 - val_loss: 28.2077\n",
      "Epoch 800/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 26.6175\n",
      "Epoch 00800: saving model to saved_models/latent32/cp-0800.h5\n",
      "6/6 [==============================] - 1s 161ms/step - loss: 26.6175 - val_loss: 29.8958\n",
      "Epoch 801/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.1265 - val_loss: 29.4765\n",
      "Epoch 802/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6792 - val_loss: 28.1812\n",
      "Epoch 803/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7000 - val_loss: 29.0063\n",
      "Epoch 804/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5678 - val_loss: 28.6728\n",
      "Epoch 805/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6174 - val_loss: 28.9145\n",
      "Epoch 806/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6142 - val_loss: 28.9378\n",
      "Epoch 807/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5885 - val_loss: 28.2178\n",
      "Epoch 808/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4613 - val_loss: 28.5356\n",
      "Epoch 809/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5728 - val_loss: 28.9029\n",
      "Epoch 810/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4871 - val_loss: 28.2811\n",
      "Epoch 811/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7325 - val_loss: 28.9323\n",
      "Epoch 812/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6394 - val_loss: 29.0360\n",
      "Epoch 813/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5053 - val_loss: 28.6023\n",
      "Epoch 814/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5514 - val_loss: 28.4991\n",
      "Epoch 815/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.4095 - val_loss: 28.3372\n",
      "Epoch 816/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4888 - val_loss: 28.6054\n",
      "Epoch 817/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.5930 - val_loss: 29.5336\n",
      "Epoch 818/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.8435 - val_loss: 30.4176\n",
      "Epoch 819/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6592 - val_loss: 28.7632\n",
      "Epoch 820/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.8697 - val_loss: 29.2255\n",
      "Epoch 821/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5935 - val_loss: 29.0524\n",
      "Epoch 822/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7071 - val_loss: 28.2880\n",
      "Epoch 823/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.7619 - val_loss: 29.8521\n",
      "Epoch 824/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4936 - val_loss: 29.0087\n",
      "Epoch 825/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7776 - val_loss: 28.5656\n",
      "Epoch 826/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.1704 - val_loss: 28.5798\n",
      "Epoch 827/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5066 - val_loss: 28.2315\n",
      "Epoch 828/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4876 - val_loss: 28.7865\n",
      "Epoch 829/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7023 - val_loss: 28.8786\n",
      "Epoch 830/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7040 - val_loss: 28.2418\n",
      "Epoch 831/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5238 - val_loss: 27.9846\n",
      "Epoch 832/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5119 - val_loss: 29.3135\n",
      "Epoch 833/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4023 - val_loss: 29.0039\n",
      "Epoch 834/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5588 - val_loss: 28.5388\n",
      "Epoch 835/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2157 - val_loss: 29.3177\n",
      "Epoch 836/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1943 - val_loss: 28.9328\n",
      "Epoch 837/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5490 - val_loss: 28.5350\n",
      "Epoch 838/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3794 - val_loss: 28.4233\n",
      "Epoch 839/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4429 - val_loss: 28.8966\n",
      "Epoch 840/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3460 - val_loss: 28.5118\n",
      "Epoch 841/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.0498 - val_loss: 28.4675\n",
      "Epoch 842/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2137 - val_loss: 28.3490\n",
      "Epoch 843/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0594 - val_loss: 28.2885\n",
      "Epoch 844/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.0011 - val_loss: 28.4720\n",
      "Epoch 845/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.9959 - val_loss: 28.1448\n",
      "Epoch 846/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3516 - val_loss: 28.2109\n",
      "Epoch 847/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4441 - val_loss: 28.8710\n",
      "Epoch 848/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2080 - val_loss: 28.4641\n",
      "Epoch 849/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1940 - val_loss: 29.0330\n",
      "Epoch 850/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5102 - val_loss: 28.6295\n",
      "Epoch 851/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6101 - val_loss: 29.4624\n",
      "Epoch 852/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9855 - val_loss: 29.6533\n",
      "Epoch 853/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9138 - val_loss: 28.4719\n",
      "Epoch 854/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.8301 - val_loss: 29.4956\n",
      "Epoch 855/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6417 - val_loss: 28.6169\n",
      "Epoch 856/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6976 - val_loss: 28.8517\n",
      "Epoch 857/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3802 - val_loss: 28.3224\n",
      "Epoch 858/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3898 - val_loss: 28.2421\n",
      "Epoch 859/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2823 - val_loss: 28.9448\n",
      "Epoch 860/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5083 - val_loss: 29.1085\n",
      "Epoch 861/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3497 - val_loss: 28.5518\n",
      "Epoch 862/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3681 - val_loss: 28.2679\n",
      "Epoch 863/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2023 - val_loss: 28.5904\n",
      "Epoch 864/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2396 - val_loss: 28.5880\n",
      "Epoch 865/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3093 - val_loss: 28.7075\n",
      "Epoch 866/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5203 - val_loss: 28.7879\n",
      "Epoch 867/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3167 - val_loss: 28.4105\n",
      "Epoch 868/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3724 - val_loss: 28.1521\n",
      "Epoch 869/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1659 - val_loss: 27.8031\n",
      "Epoch 870/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0293 - val_loss: 28.1641\n",
      "Epoch 871/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1738 - val_loss: 28.4517\n",
      "Epoch 872/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1464 - val_loss: 27.8892\n",
      "Epoch 873/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3093 - val_loss: 29.4757\n",
      "Epoch 874/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3753 - val_loss: 27.8037\n",
      "Epoch 875/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 25.9747 - val_loss: 28.6063\n",
      "Epoch 876/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2275 - val_loss: 29.2861\n",
      "Epoch 877/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4774 - val_loss: 28.8508\n",
      "Epoch 878/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3139 - val_loss: 28.6214\n",
      "Epoch 879/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3789 - val_loss: 28.2635\n",
      "Epoch 880/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0733 - val_loss: 27.9503\n",
      "Epoch 881/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1656 - val_loss: 27.4980\n",
      "Epoch 882/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0621 - val_loss: 28.2073\n",
      "Epoch 883/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2351 - val_loss: 28.1290\n",
      "Epoch 884/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.9726 - val_loss: 28.9744\n",
      "Epoch 885/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9910 - val_loss: 28.1913\n",
      "Epoch 886/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1027 - val_loss: 28.3804\n",
      "Epoch 887/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2918 - val_loss: 28.2980\n",
      "Epoch 888/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2454 - val_loss: 28.7821\n",
      "Epoch 889/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2470 - val_loss: 27.5617\n",
      "Epoch 890/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9856 - val_loss: 27.9664\n",
      "Epoch 891/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2672 - val_loss: 28.0144\n",
      "Epoch 892/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.9460 - val_loss: 28.2669\n",
      "Epoch 893/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9693 - val_loss: 27.3523\n",
      "Epoch 894/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.8425 - val_loss: 27.6950\n",
      "Epoch 895/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.8365 - val_loss: 27.6687\n",
      "Epoch 896/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9682 - val_loss: 27.9904\n",
      "Epoch 897/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0112 - val_loss: 27.9896\n",
      "Epoch 898/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2391 - val_loss: 28.3887\n",
      "Epoch 899/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1253 - val_loss: 29.0409\n",
      "Epoch 900/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2756 - val_loss: 28.5593\n",
      "Epoch 901/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9090 - val_loss: 28.2575\n",
      "Epoch 902/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0882 - val_loss: 28.9725\n",
      "Epoch 903/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0206 - val_loss: 28.8796\n",
      "Epoch 904/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2894 - val_loss: 28.7550\n",
      "Epoch 905/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3414 - val_loss: 28.5876\n",
      "Epoch 906/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0137 - val_loss: 28.1356\n",
      "Epoch 907/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9570 - val_loss: 27.5202\n",
      "Epoch 908/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9379 - val_loss: 28.7124\n",
      "Epoch 909/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8929 - val_loss: 29.1506\n",
      "Epoch 910/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0637 - val_loss: 28.5109\n",
      "Epoch 911/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8405 - val_loss: 28.5736\n",
      "Epoch 912/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9528 - val_loss: 28.3275\n",
      "Epoch 913/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1238 - val_loss: 28.8913\n",
      "Epoch 914/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0366 - val_loss: 28.2957\n",
      "Epoch 915/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 25.7435 - val_loss: 28.5500\n",
      "Epoch 916/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8576 - val_loss: 27.5979\n",
      "Epoch 917/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0504 - val_loss: 28.4047\n",
      "Epoch 918/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3390 - val_loss: 28.2475\n",
      "Epoch 919/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1066 - val_loss: 28.5331\n",
      "Epoch 920/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3119 - val_loss: 27.9934\n",
      "Epoch 921/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2167 - val_loss: 28.3950\n",
      "Epoch 922/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9426 - val_loss: 29.1674\n",
      "Epoch 923/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0966 - val_loss: 28.6929\n",
      "Epoch 924/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0278 - val_loss: 28.4581\n",
      "Epoch 925/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3808 - val_loss: 28.4535\n",
      "Epoch 926/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4544 - val_loss: 28.8062\n",
      "Epoch 927/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5024 - val_loss: 28.1973\n",
      "Epoch 928/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3837 - val_loss: 29.7459\n",
      "Epoch 929/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5156 - val_loss: 29.6643\n",
      "Epoch 930/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6485 - val_loss: 29.4605\n",
      "Epoch 931/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4084 - val_loss: 28.7014\n",
      "Epoch 932/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3722 - val_loss: 28.3389\n",
      "Epoch 933/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4756 - val_loss: 29.4798\n",
      "Epoch 934/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1132 - val_loss: 28.6168\n",
      "Epoch 935/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0696 - val_loss: 28.4809\n",
      "Epoch 936/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0283 - val_loss: 28.3176\n",
      "Epoch 937/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1146 - val_loss: 28.7992\n",
      "Epoch 938/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0249 - val_loss: 28.9881\n",
      "Epoch 939/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8547 - val_loss: 28.0627\n",
      "Epoch 940/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0003 - val_loss: 28.3210\n",
      "Epoch 941/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8081 - val_loss: 28.0661\n",
      "Epoch 942/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.7065 - val_loss: 28.1737\n",
      "Epoch 943/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7899 - val_loss: 28.1975\n",
      "Epoch 944/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0427 - val_loss: 28.3727\n",
      "Epoch 945/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0046 - val_loss: 28.4085\n",
      "Epoch 946/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8332 - val_loss: 27.7557\n",
      "Epoch 947/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8279 - val_loss: 28.2289\n",
      "Epoch 948/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7647 - val_loss: 28.2878\n",
      "Epoch 949/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8729 - val_loss: 28.2183\n",
      "Epoch 950/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8328 - val_loss: 28.3427\n",
      "Epoch 951/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9332 - val_loss: 28.2529\n",
      "Epoch 952/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8955 - val_loss: 27.5036\n",
      "Epoch 953/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9474 - val_loss: 27.4669\n",
      "Epoch 954/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8416 - val_loss: 28.8631\n",
      "Epoch 955/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1982 - val_loss: 27.8652\n",
      "Epoch 956/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0926 - val_loss: 28.2902\n",
      "Epoch 957/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8158 - val_loss: 28.1347\n",
      "Epoch 958/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8788 - val_loss: 28.2690\n",
      "Epoch 959/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1362 - val_loss: 28.8019\n",
      "Epoch 960/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1458 - val_loss: 28.3866\n",
      "Epoch 961/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1627 - val_loss: 27.2000\n",
      "Epoch 962/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9367 - val_loss: 28.4635\n",
      "Epoch 963/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9817 - val_loss: 28.1871\n",
      "Epoch 964/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9678 - val_loss: 27.6699\n",
      "Epoch 965/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7576 - val_loss: 28.0514\n",
      "Epoch 966/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7244 - val_loss: 27.8510\n",
      "Epoch 967/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.6933 - val_loss: 27.5147\n",
      "Epoch 968/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.6523 - val_loss: 27.5936\n",
      "Epoch 969/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8895 - val_loss: 28.4216\n",
      "Epoch 970/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6581 - val_loss: 27.9967\n",
      "Epoch 971/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9722 - val_loss: 28.2013\n",
      "Epoch 972/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7584 - val_loss: 28.2722\n",
      "Epoch 973/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.6095 - val_loss: 28.5643\n",
      "Epoch 974/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7622 - val_loss: 28.0860\n",
      "Epoch 975/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7354 - val_loss: 28.2139\n",
      "Epoch 976/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1298 - val_loss: 29.8483\n",
      "Epoch 977/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5032 - val_loss: 28.1637\n",
      "Epoch 978/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0489 - val_loss: 27.7095\n",
      "Epoch 979/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9140 - val_loss: 28.3022\n",
      "Epoch 980/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9412 - val_loss: 28.1625\n",
      "Epoch 981/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2599 - val_loss: 28.5736\n",
      "Epoch 982/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8919 - val_loss: 28.0200\n",
      "Epoch 983/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9346 - val_loss: 28.4077\n",
      "Epoch 984/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9921 - val_loss: 29.2716\n",
      "Epoch 985/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2088 - val_loss: 28.2709\n",
      "Epoch 986/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4595 - val_loss: 28.6962\n",
      "Epoch 987/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2756 - val_loss: 28.6864\n",
      "Epoch 988/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8126 - val_loss: 27.9935\n",
      "Epoch 989/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8811 - val_loss: 27.9203\n",
      "Epoch 990/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6792 - val_loss: 28.8651\n",
      "Epoch 991/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.5759 - val_loss: 27.6223\n",
      "Epoch 992/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7663 - val_loss: 27.7787\n",
      "Epoch 993/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7528 - val_loss: 28.6136\n",
      "Epoch 994/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6364 - val_loss: 28.1709\n",
      "Epoch 995/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8448 - val_loss: 27.6177\n",
      "Epoch 996/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7783 - val_loss: 27.9872\n",
      "Epoch 997/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7861 - val_loss: 28.1390\n",
      "Epoch 998/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8901 - val_loss: 28.1139\n",
      "Epoch 999/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8766 - val_loss: 28.6064\n",
      "Epoch 1000/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 25.8849\n",
      "Epoch 01000: saving model to saved_models/latent32/cp-1000.h5\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 25.8849 - val_loss: 27.9900\n",
      "Epoch 1001/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7726 - val_loss: 28.8278\n",
      "Epoch 1002/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7922 - val_loss: 28.3572\n",
      "Epoch 1003/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6945 - val_loss: 27.8642\n",
      "Epoch 1004/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6395 - val_loss: 29.2665\n",
      "Epoch 1005/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8605 - val_loss: 28.1345\n",
      "Epoch 1006/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9007 - val_loss: 27.6712\n",
      "Epoch 1007/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0121 - val_loss: 29.3460\n",
      "Epoch 1008/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6454 - val_loss: 29.2942\n",
      "Epoch 1009/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5658 - val_loss: 29.4504\n",
      "Epoch 1010/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4460 - val_loss: 28.3942\n",
      "Epoch 1011/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1998 - val_loss: 28.0495\n",
      "Epoch 1012/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8328 - val_loss: 27.9670\n",
      "Epoch 1013/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6427 - val_loss: 28.5178\n",
      "Epoch 1014/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7215 - val_loss: 28.2961\n",
      "Epoch 1015/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6977 - val_loss: 28.1808\n",
      "Epoch 1016/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7782 - val_loss: 27.1105\n",
      "Epoch 1017/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.5334 - val_loss: 28.4244\n",
      "Epoch 1018/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6594 - val_loss: 27.7692\n",
      "Epoch 1019/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.3394 - val_loss: 28.2911\n",
      "Epoch 1020/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6870 - val_loss: 28.0239\n",
      "Epoch 1021/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0102 - val_loss: 28.0746\n",
      "Epoch 1022/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8260 - val_loss: 29.0553\n",
      "Epoch 1023/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.0342 - val_loss: 28.8981\n",
      "Epoch 1024/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6929 - val_loss: 27.4327\n",
      "Epoch 1025/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6421 - val_loss: 27.7313\n",
      "Epoch 1026/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8313 - val_loss: 27.9875\n",
      "Epoch 1027/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0988 - val_loss: 28.6830\n",
      "Epoch 1028/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2073 - val_loss: 29.1142\n",
      "Epoch 1029/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7685 - val_loss: 27.5626\n",
      "Epoch 1030/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4861 - val_loss: 27.9336\n",
      "Epoch 1031/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4439 - val_loss: 27.6307\n",
      "Epoch 1032/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4251 - val_loss: 27.2361\n",
      "Epoch 1033/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4190 - val_loss: 27.8749\n",
      "Epoch 1034/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.3149 - val_loss: 27.7865\n",
      "Epoch 1035/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5642 - val_loss: 27.7416\n",
      "Epoch 1036/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.3123 - val_loss: 28.2458\n",
      "Epoch 1037/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4543 - val_loss: 28.1544\n",
      "Epoch 1038/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3654 - val_loss: 28.0336\n",
      "Epoch 1039/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8774 - val_loss: 27.6468\n",
      "Epoch 1040/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7906 - val_loss: 27.6734\n",
      "Epoch 1041/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5776 - val_loss: 27.9827\n",
      "Epoch 1042/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8923 - val_loss: 28.9985\n",
      "Epoch 1043/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8669 - val_loss: 28.1081\n",
      "Epoch 1044/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4423 - val_loss: 28.0250\n",
      "Epoch 1045/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6610 - val_loss: 28.3585\n",
      "Epoch 1046/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.6467 - val_loss: 29.5837\n",
      "Epoch 1047/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9248 - val_loss: 28.3142\n",
      "Epoch 1048/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9438 - val_loss: 28.4018\n",
      "Epoch 1049/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7786 - val_loss: 28.0656\n",
      "Epoch 1050/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0887 - val_loss: 29.2000\n",
      "Epoch 1051/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3107 - val_loss: 28.8970\n",
      "Epoch 1052/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0434 - val_loss: 27.7393\n",
      "Epoch 1053/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7942 - val_loss: 27.9438\n",
      "Epoch 1054/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7252 - val_loss: 28.0069\n",
      "Epoch 1055/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7274 - val_loss: 28.7998\n",
      "Epoch 1056/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0356 - val_loss: 28.2850\n",
      "Epoch 1057/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9896 - val_loss: 28.1160\n",
      "Epoch 1058/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8264 - val_loss: 28.4034\n",
      "Epoch 1059/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3181 - val_loss: 28.2502\n",
      "Epoch 1060/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0347 - val_loss: 28.8136\n",
      "Epoch 1061/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9576 - val_loss: 27.8876\n",
      "Epoch 1062/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6138 - val_loss: 27.2404\n",
      "Epoch 1063/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4092 - val_loss: 27.4788\n",
      "Epoch 1064/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.2448 - val_loss: 28.4035\n",
      "Epoch 1065/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6344 - val_loss: 27.4828\n",
      "Epoch 1066/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5128 - val_loss: 27.7633\n",
      "Epoch 1067/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5086 - val_loss: 28.0707\n",
      "Epoch 1068/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8703 - val_loss: 27.8525\n",
      "Epoch 1069/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0442 - val_loss: 27.9794\n",
      "Epoch 1070/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9107 - val_loss: 27.9961\n",
      "Epoch 1071/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9143 - val_loss: 27.8095\n",
      "Epoch 1072/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8585 - val_loss: 28.1629\n",
      "Epoch 1073/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3563 - val_loss: 27.9935\n",
      "Epoch 1074/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6579 - val_loss: 28.7180\n",
      "Epoch 1075/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4309 - val_loss: 28.3302\n",
      "Epoch 1076/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5289 - val_loss: 27.7112\n",
      "Epoch 1077/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3455 - val_loss: 28.2237\n",
      "Epoch 1078/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5386 - val_loss: 27.1353\n",
      "Epoch 1079/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2528 - val_loss: 27.6574\n",
      "Epoch 1080/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4151 - val_loss: 27.6580\n",
      "Epoch 1081/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.2028 - val_loss: 27.1792\n",
      "Epoch 1082/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5224 - val_loss: 28.5728\n",
      "Epoch 1083/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8724 - val_loss: 28.0593\n",
      "Epoch 1084/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6087 - val_loss: 28.4180\n",
      "Epoch 1085/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9633 - val_loss: 27.8090\n",
      "Epoch 1086/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4958 - val_loss: 28.0140\n",
      "Epoch 1087/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4824 - val_loss: 27.8987\n",
      "Epoch 1088/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4239 - val_loss: 28.2079\n",
      "Epoch 1089/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.3991 - val_loss: 28.0449\n",
      "Epoch 1090/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3771 - val_loss: 27.6508\n",
      "Epoch 1091/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2354 - val_loss: 27.6657\n",
      "Epoch 1092/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3438 - val_loss: 27.7464\n",
      "Epoch 1093/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2916 - val_loss: 27.8155\n",
      "Epoch 1094/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2886 - val_loss: 28.5018\n",
      "Epoch 1095/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3445 - val_loss: 27.5989\n",
      "Epoch 1096/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3605 - val_loss: 28.0280\n",
      "Epoch 1097/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5760 - val_loss: 27.7766\n",
      "Epoch 1098/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5440 - val_loss: 28.4466\n",
      "Epoch 1099/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6967 - val_loss: 28.4768\n",
      "Epoch 1100/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2539 - val_loss: 27.5228\n",
      "Epoch 1101/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8870 - val_loss: 27.9855\n",
      "Epoch 1102/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8364 - val_loss: 27.3824\n",
      "Epoch 1103/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6364 - val_loss: 26.9387\n",
      "Epoch 1104/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4639 - val_loss: 27.7499\n",
      "Epoch 1105/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5810 - val_loss: 27.9185\n",
      "Epoch 1106/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6602 - val_loss: 28.1395\n",
      "Epoch 1107/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6461 - val_loss: 28.5654\n",
      "Epoch 1108/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5228 - val_loss: 27.7677\n",
      "Epoch 1109/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 25.1081 - val_loss: 27.7940\n",
      "Epoch 1110/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1494 - val_loss: 27.5365\n",
      "Epoch 1111/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.0635 - val_loss: 27.4922\n",
      "Epoch 1112/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1390 - val_loss: 27.5827\n",
      "Epoch 1113/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0892 - val_loss: 27.1063\n",
      "Epoch 1114/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 24.8721 - val_loss: 27.6815\n",
      "Epoch 1115/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1273 - val_loss: 27.7143\n",
      "Epoch 1116/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2434 - val_loss: 27.3934\n",
      "Epoch 1117/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0425 - val_loss: 27.9231\n",
      "Epoch 1118/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6063 - val_loss: 27.5233\n",
      "Epoch 1119/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4523 - val_loss: 27.8455\n",
      "Epoch 1120/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2251 - val_loss: 27.9910\n",
      "Epoch 1121/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3882 - val_loss: 27.7193\n",
      "Epoch 1122/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3526 - val_loss: 27.7980\n",
      "Epoch 1123/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6491 - val_loss: 27.9291\n",
      "Epoch 1124/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5602 - val_loss: 27.4275\n",
      "Epoch 1125/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4419 - val_loss: 28.1346\n",
      "Epoch 1126/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4651 - val_loss: 27.2093\n",
      "Epoch 1127/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2182 - val_loss: 27.0887\n",
      "Epoch 1128/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1371 - val_loss: 27.8610\n",
      "Epoch 1129/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1782 - val_loss: 28.7228\n",
      "Epoch 1130/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5378 - val_loss: 28.1321\n",
      "Epoch 1131/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3865 - val_loss: 28.6509\n",
      "Epoch 1132/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7149 - val_loss: 28.0935\n",
      "Epoch 1133/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4175 - val_loss: 28.2264\n",
      "Epoch 1134/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4289 - val_loss: 27.1718\n",
      "Epoch 1135/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2590 - val_loss: 28.5553\n",
      "Epoch 1136/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6106 - val_loss: 29.0723\n",
      "Epoch 1137/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6581 - val_loss: 27.6779\n",
      "Epoch 1138/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6413 - val_loss: 27.8179\n",
      "Epoch 1139/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5531 - val_loss: 27.6412\n",
      "Epoch 1140/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4666 - val_loss: 28.1836\n",
      "Epoch 1141/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3636 - val_loss: 27.7200\n",
      "Epoch 1142/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3544 - val_loss: 27.1053\n",
      "Epoch 1143/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2642 - val_loss: 27.3471\n",
      "Epoch 1144/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1624 - val_loss: 27.1990\n",
      "Epoch 1145/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0441 - val_loss: 27.1115\n",
      "Epoch 1146/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1935 - val_loss: 28.6002\n",
      "Epoch 1147/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.4449 - val_loss: 27.0205\n",
      "Epoch 1148/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1454 - val_loss: 27.7032\n",
      "Epoch 1149/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3518 - val_loss: 27.4954\n",
      "Epoch 1150/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3768 - val_loss: 27.6853\n",
      "Epoch 1151/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5231 - val_loss: 27.6826\n",
      "Epoch 1152/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5003 - val_loss: 27.4064\n",
      "Epoch 1153/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3509 - val_loss: 27.1588\n",
      "Epoch 1154/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1654 - val_loss: 27.0421\n",
      "Epoch 1155/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3170 - val_loss: 27.8063\n",
      "Epoch 1156/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2424 - val_loss: 28.4933\n",
      "Epoch 1157/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2774 - val_loss: 27.5176\n",
      "Epoch 1158/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1100 - val_loss: 27.4449\n",
      "Epoch 1159/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1256 - val_loss: 27.9592\n",
      "Epoch 1160/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2072 - val_loss: 27.1826\n",
      "Epoch 1161/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2994 - val_loss: 27.3689\n",
      "Epoch 1162/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3331 - val_loss: 27.8362\n",
      "Epoch 1163/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2467 - val_loss: 27.1623\n",
      "Epoch 1164/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3816 - val_loss: 27.5971\n",
      "Epoch 1165/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5208 - val_loss: 27.7007\n",
      "Epoch 1166/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7366 - val_loss: 27.4140\n",
      "Epoch 1167/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3065 - val_loss: 27.8834\n",
      "Epoch 1168/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2852 - val_loss: 27.7118\n",
      "Epoch 1169/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5655 - val_loss: 28.6758\n",
      "Epoch 1170/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5238 - val_loss: 27.7361\n",
      "Epoch 1171/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2274 - val_loss: 27.7313\n",
      "Epoch 1172/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1060 - val_loss: 27.2962\n",
      "Epoch 1173/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1277 - val_loss: 28.3721\n",
      "Epoch 1174/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1928 - val_loss: 27.9718\n",
      "Epoch 1175/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5683 - val_loss: 27.6470\n",
      "Epoch 1176/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.3562 - val_loss: 28.2405\n",
      "Epoch 1177/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1310 - val_loss: 27.4781\n",
      "Epoch 1178/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1883 - val_loss: 27.8644\n",
      "Epoch 1179/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2624 - val_loss: 28.3773\n",
      "Epoch 1180/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4044 - val_loss: 28.4776\n",
      "Epoch 1181/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2252 - val_loss: 27.6742\n",
      "Epoch 1182/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1815 - val_loss: 27.4632\n",
      "Epoch 1183/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0816 - val_loss: 27.2136\n",
      "Epoch 1184/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4433 - val_loss: 28.0226\n",
      "Epoch 1185/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4737 - val_loss: 28.2903\n",
      "Epoch 1186/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4813 - val_loss: 28.5051\n",
      "Epoch 1187/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0951 - val_loss: 27.5237\n",
      "Epoch 1188/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9338 - val_loss: 28.1659\n",
      "Epoch 1189/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9693 - val_loss: 26.9632\n",
      "Epoch 1190/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1466 - val_loss: 27.3259\n",
      "Epoch 1191/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1649 - val_loss: 27.0834\n",
      "Epoch 1192/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2304 - val_loss: 27.1359\n",
      "Epoch 1193/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1183 - val_loss: 28.0222\n",
      "Epoch 1194/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3125 - val_loss: 27.3539\n",
      "Epoch 1195/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0275 - val_loss: 27.3821\n",
      "Epoch 1196/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0838 - val_loss: 27.4819\n",
      "Epoch 1197/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3078 - val_loss: 28.7243\n",
      "Epoch 1198/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4970 - val_loss: 27.8551\n",
      "Epoch 1199/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5763 - val_loss: 27.6195\n",
      "Epoch 1200/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 25.2998\n",
      "Epoch 01200: saving model to saved_models/latent32/cp-1200.h5\n",
      "6/6 [==============================] - 1s 164ms/step - loss: 25.2998 - val_loss: 27.7869\n",
      "Epoch 1201/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2806 - val_loss: 27.4694\n",
      "Epoch 1202/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4817 - val_loss: 27.8950\n",
      "Epoch 1203/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1033 - val_loss: 27.2461\n",
      "Epoch 1204/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9175 - val_loss: 27.7844\n",
      "Epoch 1205/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9730 - val_loss: 27.1160\n",
      "Epoch 1206/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9838 - val_loss: 27.4827\n",
      "Epoch 1207/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0717 - val_loss: 27.6565\n",
      "Epoch 1208/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3704 - val_loss: 27.9476\n",
      "Epoch 1209/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1335 - val_loss: 26.8941\n",
      "Epoch 1210/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9028 - val_loss: 27.7740\n",
      "Epoch 1211/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0081 - val_loss: 27.3496\n",
      "Epoch 1212/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8943 - val_loss: 27.3851\n",
      "Epoch 1213/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 24.7625 - val_loss: 27.3913\n",
      "Epoch 1214/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1580 - val_loss: 27.1552\n",
      "Epoch 1215/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9570 - val_loss: 26.9057\n",
      "Epoch 1216/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1247 - val_loss: 27.5730\n",
      "Epoch 1217/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0910 - val_loss: 27.5135\n",
      "Epoch 1218/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.0026 - val_loss: 26.8400\n",
      "Epoch 1219/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8388 - val_loss: 28.2126\n",
      "Epoch 1220/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9212 - val_loss: 26.9147\n",
      "Epoch 1221/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7810 - val_loss: 27.7196\n",
      "Epoch 1222/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9705 - val_loss: 27.3881\n",
      "Epoch 1223/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9831 - val_loss: 27.4476\n",
      "Epoch 1224/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0492 - val_loss: 27.9096\n",
      "Epoch 1225/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7648 - val_loss: 27.7795\n",
      "Epoch 1226/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0351 - val_loss: 26.7867\n",
      "Epoch 1227/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1649 - val_loss: 27.6056\n",
      "Epoch 1228/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0623 - val_loss: 27.4079\n",
      "Epoch 1229/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.8233 - val_loss: 27.3864\n",
      "Epoch 1230/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7874 - val_loss: 27.1711\n",
      "Epoch 1231/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.7125 - val_loss: 27.6114\n",
      "Epoch 1232/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9853 - val_loss: 27.2921\n",
      "Epoch 1233/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1184 - val_loss: 27.8300\n",
      "Epoch 1234/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4234 - val_loss: 27.5846\n",
      "Epoch 1235/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2152 - val_loss: 27.7160\n",
      "Epoch 1236/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0595 - val_loss: 28.0394\n",
      "Epoch 1237/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3187 - val_loss: 27.1213\n",
      "Epoch 1238/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2178 - val_loss: 27.9269\n",
      "Epoch 1239/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9921 - val_loss: 27.0684\n",
      "Epoch 1240/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1862 - val_loss: 27.4378\n",
      "Epoch 1241/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1078 - val_loss: 27.3906\n",
      "Epoch 1242/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1636 - val_loss: 27.8656\n",
      "Epoch 1243/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2856 - val_loss: 27.0962\n",
      "Epoch 1244/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.1434 - val_loss: 27.6257\n",
      "Epoch 1245/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9799 - val_loss: 28.2085\n",
      "Epoch 1246/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1182 - val_loss: 27.2366\n",
      "Epoch 1247/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1528 - val_loss: 27.6269\n",
      "Epoch 1248/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9245 - val_loss: 26.3398\n",
      "Epoch 1249/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0493 - val_loss: 26.9059\n",
      "Epoch 1250/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1454 - val_loss: 27.6863\n",
      "Epoch 1251/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2187 - val_loss: 27.2922\n",
      "Epoch 1252/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2996 - val_loss: 27.7676\n",
      "Epoch 1253/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9080 - val_loss: 27.5715\n",
      "Epoch 1254/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8205 - val_loss: 27.4421\n",
      "Epoch 1255/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7550 - val_loss: 27.4091\n",
      "Epoch 1256/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9043 - val_loss: 27.7338\n",
      "Epoch 1257/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1195 - val_loss: 27.9086\n",
      "Epoch 1258/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0444 - val_loss: 27.0469\n",
      "Epoch 1259/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8829 - val_loss: 27.6845\n",
      "Epoch 1260/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0502 - val_loss: 27.6548\n",
      "Epoch 1261/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1173 - val_loss: 28.1971\n",
      "Epoch 1262/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1715 - val_loss: 27.5195\n",
      "Epoch 1263/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9018 - val_loss: 27.8601\n",
      "Epoch 1264/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0395 - val_loss: 27.6861\n",
      "Epoch 1265/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1643 - val_loss: 27.3954\n",
      "Epoch 1266/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0178 - val_loss: 27.6082\n",
      "Epoch 1267/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0784 - val_loss: 27.3749\n",
      "Epoch 1268/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9589 - val_loss: 27.6810\n",
      "Epoch 1269/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7872 - val_loss: 27.9612\n",
      "Epoch 1270/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9197 - val_loss: 27.6991\n",
      "Epoch 1271/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9131 - val_loss: 27.1517\n",
      "Epoch 1272/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8900 - val_loss: 27.8387\n",
      "Epoch 1273/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8883 - val_loss: 27.5417\n",
      "Epoch 1274/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0752 - val_loss: 27.9360\n",
      "Epoch 1275/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1352 - val_loss: 27.5705\n",
      "Epoch 1276/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1703 - val_loss: 27.3129\n",
      "Epoch 1277/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7640 - val_loss: 27.6708\n",
      "Epoch 1278/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8891 - val_loss: 27.6174\n",
      "Epoch 1279/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3468 - val_loss: 27.0294\n",
      "Epoch 1280/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3126 - val_loss: 28.7562\n",
      "Epoch 1281/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1475 - val_loss: 28.5199\n",
      "Epoch 1282/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2324 - val_loss: 28.2351\n",
      "Epoch 1283/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1316 - val_loss: 27.5157\n",
      "Epoch 1284/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9537 - val_loss: 28.5826\n",
      "Epoch 1285/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7881 - val_loss: 27.4503\n",
      "Epoch 1286/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0039 - val_loss: 27.4914\n",
      "Epoch 1287/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2629 - val_loss: 27.1784\n",
      "Epoch 1288/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0215 - val_loss: 27.4217\n",
      "Epoch 1289/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9603 - val_loss: 27.5392\n",
      "Epoch 1290/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1193 - val_loss: 28.0767\n",
      "Epoch 1291/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0924 - val_loss: 27.1113\n",
      "Epoch 1292/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9351 - val_loss: 28.0719\n",
      "Epoch 1293/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9200 - val_loss: 27.7269\n",
      "Epoch 1294/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8421 - val_loss: 26.9974\n",
      "Epoch 1295/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8193 - val_loss: 27.3962\n",
      "Epoch 1296/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7322 - val_loss: 27.3834\n",
      "Epoch 1297/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8520 - val_loss: 28.3151\n",
      "Epoch 1298/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2665 - val_loss: 27.6678\n",
      "Epoch 1299/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2124 - val_loss: 27.5649\n",
      "Epoch 1300/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1536 - val_loss: 27.1005\n",
      "Epoch 1301/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0783 - val_loss: 28.3232\n",
      "Epoch 1302/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2270 - val_loss: 28.1836\n",
      "Epoch 1303/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0653 - val_loss: 27.8073\n",
      "Epoch 1304/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7527 - val_loss: 27.5809\n",
      "Epoch 1305/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7221 - val_loss: 27.2756\n",
      "Epoch 1306/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7529 - val_loss: 27.5493\n",
      "Epoch 1307/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9726 - val_loss: 27.2606\n",
      "Epoch 1308/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7882 - val_loss: 27.5494\n",
      "Epoch 1309/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7493 - val_loss: 27.2113\n",
      "Epoch 1310/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8201 - val_loss: 27.1685\n",
      "Epoch 1311/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7797 - val_loss: 27.3646\n",
      "Epoch 1312/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8522 - val_loss: 27.0502\n",
      "Epoch 1313/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0324 - val_loss: 28.7245\n",
      "Epoch 1314/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2725 - val_loss: 27.4243\n",
      "Epoch 1315/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1138 - val_loss: 27.4626\n",
      "Epoch 1316/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1492 - val_loss: 27.2378\n",
      "Epoch 1317/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0154 - val_loss: 27.1794\n",
      "Epoch 1318/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9911 - val_loss: 27.4192\n",
      "Epoch 1319/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9435 - val_loss: 27.4823\n",
      "Epoch 1320/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7928 - val_loss: 27.3456\n",
      "Epoch 1321/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.6997 - val_loss: 27.7936\n",
      "Epoch 1322/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0219 - val_loss: 26.9404\n",
      "Epoch 1323/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.7524 - val_loss: 27.1448\n",
      "Epoch 1324/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 24.6642 - val_loss: 26.8628\n",
      "Epoch 1325/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.6613 - val_loss: 27.1462\n",
      "Epoch 1326/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7569 - val_loss: 27.4306\n",
      "Epoch 1327/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7167 - val_loss: 27.0697\n",
      "Epoch 1328/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6824 - val_loss: 26.7800\n",
      "Epoch 1329/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6713 - val_loss: 26.9510\n",
      "Epoch 1330/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.4536 - val_loss: 26.5406\n",
      "Epoch 1331/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7871 - val_loss: 27.1775\n",
      "Epoch 1332/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8195 - val_loss: 28.0295\n",
      "Epoch 1333/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8958 - val_loss: 27.1716\n",
      "Epoch 1334/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9288 - val_loss: 28.1787\n",
      "Epoch 1335/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9773 - val_loss: 28.3639\n",
      "Epoch 1336/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3919 - val_loss: 27.3417\n",
      "Epoch 1337/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2913 - val_loss: 27.2830\n",
      "Epoch 1338/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0389 - val_loss: 26.6927\n",
      "Epoch 1339/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8321 - val_loss: 27.0248\n",
      "Epoch 1340/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6419 - val_loss: 27.2072\n",
      "Epoch 1341/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0492 - val_loss: 27.4454\n",
      "Epoch 1342/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7074 - val_loss: 27.0204\n",
      "Epoch 1343/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8655 - val_loss: 27.3067\n",
      "Epoch 1344/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0065 - val_loss: 27.3362\n",
      "Epoch 1345/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2170 - val_loss: 29.0921\n",
      "Epoch 1346/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5406 - val_loss: 27.3902\n",
      "Epoch 1347/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0628 - val_loss: 26.9357\n",
      "Epoch 1348/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0208 - val_loss: 27.2282\n",
      "Epoch 1349/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9833 - val_loss: 28.2977\n",
      "Epoch 1350/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2465 - val_loss: 26.9840\n",
      "Epoch 1351/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9190 - val_loss: 27.3250\n",
      "Epoch 1352/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7411 - val_loss: 27.2871\n",
      "Epoch 1353/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8069 - val_loss: 27.4150\n",
      "Epoch 1354/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6617 - val_loss: 26.6897\n",
      "Epoch 1355/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5606 - val_loss: 26.9280\n",
      "Epoch 1356/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7225 - val_loss: 27.8577\n",
      "Epoch 1357/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8897 - val_loss: 27.6708\n",
      "Epoch 1358/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7409 - val_loss: 27.5283\n",
      "Epoch 1359/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6722 - val_loss: 26.6005\n",
      "Epoch 1360/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7045 - val_loss: 26.9376\n",
      "Epoch 1361/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.8697 - val_loss: 27.9661\n",
      "Epoch 1362/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7277 - val_loss: 27.4022\n",
      "Epoch 1363/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7650 - val_loss: 27.5536\n",
      "Epoch 1364/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7893 - val_loss: 27.2367\n",
      "Epoch 1365/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7552 - val_loss: 27.1682\n",
      "Epoch 1366/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7551 - val_loss: 27.5232\n",
      "Epoch 1367/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8315 - val_loss: 27.9709\n",
      "Epoch 1368/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.8424 - val_loss: 26.9493\n",
      "Epoch 1369/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.5469 - val_loss: 28.0627\n",
      "Epoch 1370/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5104 - val_loss: 27.1667\n",
      "Epoch 1371/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5877 - val_loss: 27.3867\n",
      "Epoch 1372/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.6801 - val_loss: 26.9626\n",
      "Epoch 1373/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.8082 - val_loss: 27.6322\n",
      "Epoch 1374/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0561 - val_loss: 27.2898\n",
      "Epoch 1375/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1079 - val_loss: 27.3279\n",
      "Epoch 1376/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7300 - val_loss: 27.0618\n",
      "Epoch 1377/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.5603 - val_loss: 26.8685\n",
      "Epoch 1378/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8573 - val_loss: 27.6981\n",
      "Epoch 1379/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5958 - val_loss: 27.8633\n",
      "Epoch 1380/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7864 - val_loss: 27.5763\n",
      "Epoch 1381/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7752 - val_loss: 27.1366\n",
      "Epoch 1382/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0194 - val_loss: 27.7808\n",
      "Epoch 1383/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9097 - val_loss: 27.5869\n",
      "Epoch 1384/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9130 - val_loss: 26.9578\n",
      "Epoch 1385/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8250 - val_loss: 26.9142\n",
      "Epoch 1386/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.4425 - val_loss: 27.4381\n",
      "Epoch 1387/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6007 - val_loss: 27.4419\n",
      "Epoch 1388/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7868 - val_loss: 27.4154\n",
      "Epoch 1389/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6293 - val_loss: 27.1812\n",
      "Epoch 1390/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6668 - val_loss: 27.7671\n",
      "Epoch 1391/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9028 - val_loss: 26.7142\n",
      "Epoch 1392/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7968 - val_loss: 27.5567\n",
      "Epoch 1393/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9616 - val_loss: 27.9896\n",
      "Epoch 1394/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1206 - val_loss: 28.3413\n",
      "Epoch 1395/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2252 - val_loss: 28.1471\n",
      "Epoch 1396/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0296 - val_loss: 27.3503\n",
      "Epoch 1397/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8848 - val_loss: 27.3439\n",
      "Epoch 1398/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6634 - val_loss: 27.2857\n",
      "Epoch 1399/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5068 - val_loss: 27.0934\n",
      "Epoch 1400/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 24.6512\n",
      "Epoch 01400: saving model to saved_models/latent32/cp-1400.h5\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 24.6512 - val_loss: 27.0835\n",
      "Epoch 1401/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6692 - val_loss: 27.4807\n",
      "Epoch 1402/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8124 - val_loss: 27.3509\n",
      "Epoch 1403/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9034 - val_loss: 27.6657\n",
      "Epoch 1404/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0513 - val_loss: 27.3262\n",
      "Epoch 1405/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8859 - val_loss: 27.2627\n",
      "Epoch 1406/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9855 - val_loss: 27.3842\n",
      "Epoch 1407/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8671 - val_loss: 26.8624\n",
      "Epoch 1408/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9172 - val_loss: 27.2482\n",
      "Epoch 1409/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6180 - val_loss: 27.0078\n",
      "Epoch 1410/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5153 - val_loss: 27.2179\n",
      "Epoch 1411/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4848 - val_loss: 26.5034\n",
      "Epoch 1412/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6177 - val_loss: 27.2579\n",
      "Epoch 1413/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5120 - val_loss: 27.2652\n",
      "Epoch 1414/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.6419 - val_loss: 26.9922\n",
      "Epoch 1415/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6503 - val_loss: 27.1997\n",
      "Epoch 1416/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5830 - val_loss: 26.8417\n",
      "Epoch 1417/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6048 - val_loss: 26.7165\n",
      "Epoch 1418/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5808 - val_loss: 27.7023\n",
      "Epoch 1419/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5896 - val_loss: 26.9668\n",
      "Epoch 1420/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7087 - val_loss: 26.9378\n",
      "Epoch 1421/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6828 - val_loss: 27.2797\n",
      "Epoch 1422/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.4212 - val_loss: 26.6978\n",
      "Epoch 1423/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5301 - val_loss: 27.3245\n",
      "Epoch 1424/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4401 - val_loss: 27.3747\n",
      "Epoch 1425/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6100 - val_loss: 27.4277\n",
      "Epoch 1426/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.7955 - val_loss: 27.4672\n",
      "Epoch 1427/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7003 - val_loss: 27.5907\n",
      "Epoch 1428/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5938 - val_loss: 26.8696\n",
      "Epoch 1429/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7077 - val_loss: 27.1555\n",
      "Epoch 1430/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4297 - val_loss: 27.1727\n",
      "Epoch 1431/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5103 - val_loss: 27.2015\n",
      "Epoch 1432/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7954 - val_loss: 27.6074\n",
      "Epoch 1433/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0949 - val_loss: 27.1134\n",
      "Epoch 1434/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7188 - val_loss: 27.6946\n",
      "Epoch 1435/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8021 - val_loss: 27.3251\n",
      "Epoch 1436/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.4127 - val_loss: 27.0573\n",
      "Epoch 1437/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4416 - val_loss: 27.0771\n",
      "Epoch 1438/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.3128 - val_loss: 27.9755\n",
      "Epoch 1439/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6097 - val_loss: 27.0897\n",
      "Epoch 1440/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.5853 - val_loss: 27.5323\n",
      "Epoch 1441/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7436 - val_loss: 27.2358\n",
      "Epoch 1442/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6373 - val_loss: 27.6242\n",
      "Epoch 1443/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5214 - val_loss: 27.4231\n",
      "Epoch 1444/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5835 - val_loss: 26.9239\n",
      "Epoch 1445/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9069 - val_loss: 26.5126\n",
      "Epoch 1446/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4237 - val_loss: 27.0549\n",
      "Epoch 1447/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4595 - val_loss: 26.9673\n",
      "Epoch 1448/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.3069 - val_loss: 26.2682\n",
      "Epoch 1449/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4949 - val_loss: 27.7167\n",
      "Epoch 1450/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6035 - val_loss: 27.2686\n",
      "Epoch 1451/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5652 - val_loss: 26.4863\n",
      "Epoch 1452/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5465 - val_loss: 27.5081\n",
      "Epoch 1453/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8014 - val_loss: 27.9390\n",
      "Epoch 1454/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2656 - val_loss: 27.1809\n",
      "Epoch 1455/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9610 - val_loss: 27.3083\n",
      "Epoch 1456/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5648 - val_loss: 27.6442\n",
      "Epoch 1457/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8529 - val_loss: 27.6657\n",
      "Epoch 1458/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9056 - val_loss: 27.9079\n",
      "Epoch 1459/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9568 - val_loss: 27.7011\n",
      "Epoch 1460/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1883 - val_loss: 27.5057\n",
      "Epoch 1461/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7554 - val_loss: 26.7323\n",
      "Epoch 1462/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5501 - val_loss: 27.9126\n",
      "Epoch 1463/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5639 - val_loss: 27.7601\n",
      "Epoch 1464/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6830 - val_loss: 27.5230\n",
      "Epoch 1465/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7759 - val_loss: 26.8879\n",
      "Epoch 1466/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7556 - val_loss: 26.6857\n",
      "Epoch 1467/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.6037 - val_loss: 27.4541\n",
      "Epoch 1468/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7654 - val_loss: 27.0839\n",
      "Epoch 1469/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4551 - val_loss: 27.1230\n",
      "Epoch 1470/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4796 - val_loss: 27.3779\n",
      "Epoch 1471/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.3135 - val_loss: 27.3490\n",
      "Epoch 1472/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6324 - val_loss: 26.8443\n",
      "Epoch 1473/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4837 - val_loss: 26.8388\n",
      "Epoch 1474/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7732 - val_loss: 26.0298\n",
      "Epoch 1475/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4345 - val_loss: 27.0988\n",
      "Epoch 1476/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3287 - val_loss: 27.5552\n",
      "Epoch 1477/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5420 - val_loss: 26.8326\n",
      "Epoch 1478/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4251 - val_loss: 26.7183\n",
      "Epoch 1479/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3603 - val_loss: 26.5476\n",
      "Epoch 1480/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3352 - val_loss: 26.7524\n",
      "Epoch 1481/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4794 - val_loss: 27.1527\n",
      "Epoch 1482/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5672 - val_loss: 27.1979\n",
      "Epoch 1483/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6207 - val_loss: 27.2505\n",
      "Epoch 1484/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7246 - val_loss: 26.6349\n",
      "Epoch 1485/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9231 - val_loss: 27.3009\n",
      "Epoch 1486/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0652 - val_loss: 27.8594\n",
      "Epoch 1487/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8075 - val_loss: 26.7070\n",
      "Epoch 1488/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7157 - val_loss: 27.0179\n",
      "Epoch 1489/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6268 - val_loss: 27.0587\n",
      "Epoch 1490/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6309 - val_loss: 27.5122\n",
      "Epoch 1491/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6482 - val_loss: 27.4351\n",
      "Epoch 1492/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6735 - val_loss: 26.5976\n",
      "Epoch 1493/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3241 - val_loss: 27.5187\n",
      "Epoch 1494/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8913 - val_loss: 27.5666\n",
      "Epoch 1495/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7795 - val_loss: 27.2781\n",
      "Epoch 1496/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6115 - val_loss: 27.2701\n",
      "Epoch 1497/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4623 - val_loss: 27.4885\n",
      "Epoch 1498/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4397 - val_loss: 27.2371\n",
      "Epoch 1499/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6200 - val_loss: 26.8265\n",
      "Epoch 1500/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7930 - val_loss: 26.9158\n",
      "Epoch 1501/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6437 - val_loss: 26.8233\n",
      "Epoch 1502/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.5194 - val_loss: 27.0835\n",
      "Epoch 1503/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6648 - val_loss: 26.5536\n",
      "Epoch 1504/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5030 - val_loss: 26.2963\n",
      "Epoch 1505/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5329 - val_loss: 26.3684\n",
      "Epoch 1506/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5967 - val_loss: 27.6342\n",
      "Epoch 1507/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3702 - val_loss: 26.2944\n",
      "Epoch 1508/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3178 - val_loss: 26.8419\n",
      "Epoch 1509/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 24.2755 - val_loss: 26.8140\n",
      "Epoch 1510/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3945 - val_loss: 27.0243\n",
      "Epoch 1511/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4367 - val_loss: 27.0788\n",
      "Epoch 1512/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3191 - val_loss: 26.5049\n",
      "Epoch 1513/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3714 - val_loss: 27.0714\n",
      "Epoch 1514/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.2289 - val_loss: 26.5952\n",
      "Epoch 1515/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2376 - val_loss: 27.2643\n",
      "Epoch 1516/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5500 - val_loss: 27.6134\n",
      "Epoch 1517/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7104 - val_loss: 27.6682\n",
      "Epoch 1518/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4810 - val_loss: 27.6532\n",
      "Epoch 1519/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5709 - val_loss: 27.4324\n",
      "Epoch 1520/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5634 - val_loss: 26.9808\n",
      "Epoch 1521/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5160 - val_loss: 26.8306\n",
      "Epoch 1522/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3760 - val_loss: 26.8393\n",
      "Epoch 1523/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3004 - val_loss: 26.7888\n",
      "Epoch 1524/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4437 - val_loss: 26.9608\n",
      "Epoch 1525/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2381 - val_loss: 27.0574\n",
      "Epoch 1526/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2403 - val_loss: 26.5677\n",
      "Epoch 1527/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.2282 - val_loss: 27.2332\n",
      "Epoch 1528/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4016 - val_loss: 26.6523\n",
      "Epoch 1529/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4061 - val_loss: 27.7711\n",
      "Epoch 1530/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7667 - val_loss: 27.3276\n",
      "Epoch 1531/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6522 - val_loss: 27.7845\n",
      "Epoch 1532/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6409 - val_loss: 27.1799\n",
      "Epoch 1533/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8667 - val_loss: 27.5345\n",
      "Epoch 1534/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6686 - val_loss: 27.1655\n",
      "Epoch 1535/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7798 - val_loss: 27.3252\n",
      "Epoch 1536/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7100 - val_loss: 26.9233\n",
      "Epoch 1537/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4192 - val_loss: 27.1184\n",
      "Epoch 1538/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5511 - val_loss: 26.9207\n",
      "Epoch 1539/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3892 - val_loss: 27.2555\n",
      "Epoch 1540/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3182 - val_loss: 26.5046\n",
      "Epoch 1541/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4868 - val_loss: 27.1974\n",
      "Epoch 1542/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5143 - val_loss: 26.4128\n",
      "Epoch 1543/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3967 - val_loss: 26.7677\n",
      "Epoch 1544/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3724 - val_loss: 27.5099\n",
      "Epoch 1545/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3741 - val_loss: 27.0765\n",
      "Epoch 1546/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3917 - val_loss: 26.7454\n",
      "Epoch 1547/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4454 - val_loss: 26.9630\n",
      "Epoch 1548/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4742 - val_loss: 27.6418\n",
      "Epoch 1549/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5237 - val_loss: 27.4193\n",
      "Epoch 1550/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2774 - val_loss: 27.3825\n",
      "Epoch 1551/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 24.4634 - val_loss: 26.9892\n",
      "Epoch 1552/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7111 - val_loss: 27.0184\n",
      "Epoch 1553/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5443 - val_loss: 26.7719\n",
      "Epoch 1554/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6782 - val_loss: 27.6021\n",
      "Epoch 1555/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6392 - val_loss: 27.2856\n",
      "Epoch 1556/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6122 - val_loss: 27.0258\n",
      "Epoch 1557/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6936 - val_loss: 26.6227\n",
      "Epoch 1558/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2959 - val_loss: 26.7279\n",
      "Epoch 1559/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3928 - val_loss: 27.4508\n",
      "Epoch 1560/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5913 - val_loss: 27.4986\n",
      "Epoch 1561/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5857 - val_loss: 28.5219\n",
      "Epoch 1562/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6285 - val_loss: 27.7919\n",
      "Epoch 1563/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5186 - val_loss: 26.3120\n",
      "Epoch 1564/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 24.2132 - val_loss: 26.4833\n",
      "Epoch 1565/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.2772 - val_loss: 27.1256\n",
      "Epoch 1566/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.1707 - val_loss: 26.9852\n",
      "Epoch 1567/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.1502 - val_loss: 26.7312\n",
      "Epoch 1568/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1518 - val_loss: 26.5218\n",
      "Epoch 1569/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.0889 - val_loss: 26.5227\n",
      "Epoch 1570/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1232 - val_loss: 26.9197\n",
      "Epoch 1571/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4832 - val_loss: 27.3988\n",
      "Epoch 1572/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5696 - val_loss: 27.2898\n",
      "Epoch 1573/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4576 - val_loss: 26.5181\n",
      "Epoch 1574/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5850 - val_loss: 27.0882\n",
      "Epoch 1575/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5003 - val_loss: 27.0095\n",
      "Epoch 1576/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3419 - val_loss: 26.6779\n",
      "Epoch 1577/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2343 - val_loss: 26.4292\n",
      "Epoch 1578/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3200 - val_loss: 27.4830\n",
      "Epoch 1579/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4134 - val_loss: 26.7210\n",
      "Epoch 1580/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3031 - val_loss: 27.4181\n",
      "Epoch 1581/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1907 - val_loss: 26.7588\n",
      "Epoch 1582/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1598 - val_loss: 27.1610\n",
      "Epoch 1583/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5103 - val_loss: 26.9412\n",
      "Epoch 1584/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4155 - val_loss: 27.4582\n",
      "Epoch 1585/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2417 - val_loss: 26.6489\n",
      "Epoch 1586/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1029 - val_loss: 26.3977\n",
      "Epoch 1587/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3014 - val_loss: 26.9312\n",
      "Epoch 1588/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0973 - val_loss: 26.9650\n",
      "Epoch 1589/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2639 - val_loss: 26.9823\n",
      "Epoch 1590/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2162 - val_loss: 26.9776\n",
      "Epoch 1591/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2245 - val_loss: 26.9227\n",
      "Epoch 1592/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3136 - val_loss: 26.8041\n",
      "Epoch 1593/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2032 - val_loss: 26.8321\n",
      "Epoch 1594/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2267 - val_loss: 27.0269\n",
      "Epoch 1595/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1461 - val_loss: 26.7625\n",
      "Epoch 1596/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.2068 - val_loss: 27.2649\n",
      "Epoch 1597/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1913 - val_loss: 27.0142\n",
      "Epoch 1598/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1602 - val_loss: 26.9743\n",
      "Epoch 1599/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2230 - val_loss: 27.1373\n",
      "Epoch 1600/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 24.4373\n",
      "Epoch 01600: saving model to saved_models/latent32/cp-1600.h5\n",
      "6/6 [==============================] - 1s 163ms/step - loss: 24.4373 - val_loss: 26.6392\n",
      "Epoch 1601/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2818 - val_loss: 26.2050\n",
      "Epoch 1602/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1582 - val_loss: 26.9345\n",
      "Epoch 1603/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4105 - val_loss: 27.2394\n",
      "Epoch 1604/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3301 - val_loss: 27.2990\n",
      "Epoch 1605/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3642 - val_loss: 27.0690\n",
      "Epoch 1606/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3388 - val_loss: 27.1595\n",
      "Epoch 1607/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3990 - val_loss: 27.4788\n",
      "Epoch 1608/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3109 - val_loss: 27.7222\n",
      "Epoch 1609/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5149 - val_loss: 27.7366\n",
      "Epoch 1610/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3636 - val_loss: 26.9563\n",
      "Epoch 1611/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3699 - val_loss: 27.0249\n",
      "Epoch 1612/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4750 - val_loss: 26.8282\n",
      "Epoch 1613/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3652 - val_loss: 27.3724\n",
      "Epoch 1614/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1614 - val_loss: 26.9205\n",
      "Epoch 1615/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2455 - val_loss: 26.6378\n",
      "Epoch 1616/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2147 - val_loss: 27.2748\n",
      "Epoch 1617/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3518 - val_loss: 27.8060\n",
      "Epoch 1618/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5161 - val_loss: 26.5163\n",
      "Epoch 1619/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2185 - val_loss: 26.9014\n",
      "Epoch 1620/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.0162 - val_loss: 26.3851\n",
      "Epoch 1621/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0937 - val_loss: 27.0205\n",
      "Epoch 1622/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1922 - val_loss: 27.0354\n",
      "Epoch 1623/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3130 - val_loss: 26.4633\n",
      "Epoch 1624/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2269 - val_loss: 26.4841\n",
      "Epoch 1625/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1768 - val_loss: 26.5047\n",
      "Epoch 1626/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1816 - val_loss: 26.8413\n",
      "Epoch 1627/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1507 - val_loss: 27.1279\n",
      "Epoch 1628/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2997 - val_loss: 26.9481\n",
      "Epoch 1629/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2994 - val_loss: 26.5350\n",
      "Epoch 1630/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1081 - val_loss: 27.5527\n",
      "Epoch 1631/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1979 - val_loss: 27.1502\n",
      "Epoch 1632/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1496 - val_loss: 27.1028\n",
      "Epoch 1633/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2832 - val_loss: 26.8176\n",
      "Epoch 1634/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2230 - val_loss: 27.8470\n",
      "Epoch 1635/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4995 - val_loss: 26.8343\n",
      "Epoch 1636/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.9732 - val_loss: 26.3884\n",
      "Epoch 1637/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2268 - val_loss: 27.4603\n",
      "Epoch 1638/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.3001 - val_loss: 26.7310\n",
      "Epoch 1639/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4056 - val_loss: 26.6218\n",
      "Epoch 1640/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4410 - val_loss: 27.9056\n",
      "Epoch 1641/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4988 - val_loss: 26.8135\n",
      "Epoch 1642/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3429 - val_loss: 27.1753\n",
      "Epoch 1643/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2582 - val_loss: 27.5240\n",
      "Epoch 1644/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2553 - val_loss: 26.5623\n",
      "Epoch 1645/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4775 - val_loss: 26.7123\n",
      "Epoch 1646/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1085 - val_loss: 26.4418\n",
      "Epoch 1647/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1118 - val_loss: 26.9647\n",
      "Epoch 1648/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3736 - val_loss: 27.2135\n",
      "Epoch 1649/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3562 - val_loss: 26.7621\n",
      "Epoch 1650/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1717 - val_loss: 26.6554\n",
      "Epoch 1651/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1591 - val_loss: 26.5617\n",
      "Epoch 1652/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9872 - val_loss: 26.6549\n",
      "Epoch 1653/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1127 - val_loss: 26.1895\n",
      "Epoch 1654/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2743 - val_loss: 26.9432\n",
      "Epoch 1655/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5833 - val_loss: 27.0236\n",
      "Epoch 1656/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4600 - val_loss: 26.9375\n",
      "Epoch 1657/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2371 - val_loss: 26.5431\n",
      "Epoch 1658/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3703 - val_loss: 26.2727\n",
      "Epoch 1659/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4956 - val_loss: 27.4220\n",
      "Epoch 1660/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4430 - val_loss: 27.1583\n",
      "Epoch 1661/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2984 - val_loss: 26.9272\n",
      "Epoch 1662/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.2084 - val_loss: 26.7684\n",
      "Epoch 1663/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3233 - val_loss: 27.5513\n",
      "Epoch 1664/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7064 - val_loss: 26.8886\n",
      "Epoch 1665/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4386 - val_loss: 26.8118\n",
      "Epoch 1666/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4725 - val_loss: 26.0528\n",
      "Epoch 1667/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4170 - val_loss: 26.9696\n",
      "Epoch 1668/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4020 - val_loss: 26.7495\n",
      "Epoch 1669/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2466 - val_loss: 27.0703\n",
      "Epoch 1670/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4453 - val_loss: 27.3305\n",
      "Epoch 1671/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4033 - val_loss: 26.9990\n",
      "Epoch 1672/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0676 - val_loss: 26.4830\n",
      "Epoch 1673/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.1612 - val_loss: 26.5242\n",
      "Epoch 1674/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1682 - val_loss: 26.2666\n",
      "Epoch 1675/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.9576 - val_loss: 26.6571\n",
      "Epoch 1676/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.8907 - val_loss: 27.0317\n",
      "Epoch 1677/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2122 - val_loss: 26.7011\n",
      "Epoch 1678/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0549 - val_loss: 27.3993\n",
      "Epoch 1679/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1264 - val_loss: 26.7702\n",
      "Epoch 1680/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3894 - val_loss: 26.5318\n",
      "Epoch 1681/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0525 - val_loss: 26.7772\n",
      "Epoch 1682/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2094 - val_loss: 26.6625\n",
      "Epoch 1683/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1124 - val_loss: 26.5567\n",
      "Epoch 1684/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2226 - val_loss: 27.0995\n",
      "Epoch 1685/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4871 - val_loss: 27.1963\n",
      "Epoch 1686/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5096 - val_loss: 26.5106\n",
      "Epoch 1687/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1546 - val_loss: 27.4660\n",
      "Epoch 1688/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1633 - val_loss: 27.2816\n",
      "Epoch 1689/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2720 - val_loss: 26.5544\n",
      "Epoch 1690/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0811 - val_loss: 26.8944\n",
      "Epoch 1691/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0618 - val_loss: 26.3651\n",
      "Epoch 1692/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2606 - val_loss: 26.8106\n",
      "Epoch 1693/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2830 - val_loss: 26.4440\n",
      "Epoch 1694/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2427 - val_loss: 27.3227\n",
      "Epoch 1695/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2515 - val_loss: 26.5854\n",
      "Epoch 1696/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9610 - val_loss: 27.1238\n",
      "Epoch 1697/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0944 - val_loss: 27.3721\n",
      "Epoch 1698/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9687 - val_loss: 26.5438\n",
      "Epoch 1699/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1311 - val_loss: 26.6152\n",
      "Epoch 1700/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2113 - val_loss: 26.2962\n",
      "Epoch 1701/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1055 - val_loss: 26.5807\n",
      "Epoch 1702/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1734 - val_loss: 27.0062\n",
      "Epoch 1703/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9275 - val_loss: 27.1037\n",
      "Epoch 1704/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0336 - val_loss: 27.0470\n",
      "Epoch 1705/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0747 - val_loss: 27.2818\n",
      "Epoch 1706/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9780 - val_loss: 26.5331\n",
      "Epoch 1707/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0978 - val_loss: 26.6138\n",
      "Epoch 1708/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2585 - val_loss: 26.6744\n",
      "Epoch 1709/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0702 - val_loss: 26.9421\n",
      "Epoch 1710/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0697 - val_loss: 27.7114\n",
      "Epoch 1711/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0695 - val_loss: 26.5325\n",
      "Epoch 1712/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1892 - val_loss: 27.1911\n",
      "Epoch 1713/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9572 - val_loss: 27.8277\n",
      "Epoch 1714/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7506 - val_loss: 28.1387\n",
      "Epoch 1715/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7658 - val_loss: 26.9308\n",
      "Epoch 1716/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4322 - val_loss: 26.9290\n",
      "Epoch 1717/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2379 - val_loss: 26.5011\n",
      "Epoch 1718/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2600 - val_loss: 26.9044\n",
      "Epoch 1719/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2296 - val_loss: 27.3961\n",
      "Epoch 1720/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5607 - val_loss: 26.5408\n",
      "Epoch 1721/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.1742 - val_loss: 27.0567\n",
      "Epoch 1722/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9965 - val_loss: 26.6460\n",
      "Epoch 1723/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9362 - val_loss: 26.7379\n",
      "Epoch 1724/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9703 - val_loss: 27.3379\n",
      "Epoch 1725/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9386 - val_loss: 26.6031\n",
      "Epoch 1726/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9081 - val_loss: 26.7589\n",
      "Epoch 1727/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2273 - val_loss: 28.0068\n",
      "Epoch 1728/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2086 - val_loss: 26.8636\n",
      "Epoch 1729/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2117 - val_loss: 26.6305\n",
      "Epoch 1730/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1913 - val_loss: 26.5897\n",
      "Epoch 1731/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2607 - val_loss: 27.0452\n",
      "Epoch 1732/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3123 - val_loss: 26.5741\n",
      "Epoch 1733/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1857 - val_loss: 26.7587\n",
      "Epoch 1734/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2894 - val_loss: 27.1625\n",
      "Epoch 1735/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3912 - val_loss: 26.8619\n",
      "Epoch 1736/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2260 - val_loss: 26.8054\n",
      "Epoch 1737/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2258 - val_loss: 26.5918\n",
      "Epoch 1738/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1186 - val_loss: 26.8068\n",
      "Epoch 1739/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1219 - val_loss: 26.8221\n",
      "Epoch 1740/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3434 - val_loss: 25.8520\n",
      "Epoch 1741/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9221 - val_loss: 26.7996\n",
      "Epoch 1742/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0141 - val_loss: 26.4225\n",
      "Epoch 1743/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0247 - val_loss: 26.7916\n",
      "Epoch 1744/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.8482 - val_loss: 26.4338\n",
      "Epoch 1745/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1108 - val_loss: 26.4151\n",
      "Epoch 1746/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9179 - val_loss: 27.0225\n",
      "Epoch 1747/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8941 - val_loss: 26.4373\n",
      "Epoch 1748/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.8090 - val_loss: 26.4900\n",
      "Epoch 1749/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9370 - val_loss: 27.0205\n",
      "Epoch 1750/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9006 - val_loss: 26.1400\n",
      "Epoch 1751/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.7635 - val_loss: 26.3492\n",
      "Epoch 1752/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9836 - val_loss: 26.9594\n",
      "Epoch 1753/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9391 - val_loss: 25.9819\n",
      "Epoch 1754/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4310 - val_loss: 27.6228\n",
      "Epoch 1755/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3291 - val_loss: 26.9382\n",
      "Epoch 1756/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0078 - val_loss: 27.7509\n",
      "Epoch 1757/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2108 - val_loss: 27.5110\n",
      "Epoch 1758/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2335 - val_loss: 25.9821\n",
      "Epoch 1759/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1198 - val_loss: 26.6047\n",
      "Epoch 1760/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9756 - val_loss: 25.8181\n",
      "Epoch 1761/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9588 - val_loss: 26.3985\n",
      "Epoch 1762/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0869 - val_loss: 27.4825\n",
      "Epoch 1763/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0841 - val_loss: 26.1587\n",
      "Epoch 1764/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1476 - val_loss: 26.9357\n",
      "Epoch 1765/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1584 - val_loss: 26.1264\n",
      "Epoch 1766/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0885 - val_loss: 27.0484\n",
      "Epoch 1767/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1765 - val_loss: 26.4727\n",
      "Epoch 1768/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.2447 - val_loss: 27.1574\n",
      "Epoch 1769/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0469 - val_loss: 26.2996\n",
      "Epoch 1770/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0122 - val_loss: 26.6124\n",
      "Epoch 1771/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9389 - val_loss: 26.0672\n",
      "Epoch 1772/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0632 - val_loss: 27.0095\n",
      "Epoch 1773/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9868 - val_loss: 26.9156\n",
      "Epoch 1774/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8857 - val_loss: 26.5260\n",
      "Epoch 1775/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8370 - val_loss: 27.2415\n",
      "Epoch 1776/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9376 - val_loss: 26.5386\n",
      "Epoch 1777/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0505 - val_loss: 27.1721\n",
      "Epoch 1778/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9628 - val_loss: 26.8426\n",
      "Epoch 1779/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2570 - val_loss: 26.9092\n",
      "Epoch 1780/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1824 - val_loss: 26.9171\n",
      "Epoch 1781/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9179 - val_loss: 27.1341\n",
      "Epoch 1782/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.1487 - val_loss: 26.6228\n",
      "Epoch 1783/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0768 - val_loss: 27.0629\n",
      "Epoch 1784/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2542 - val_loss: 26.7949\n",
      "Epoch 1785/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1914 - val_loss: 26.5467\n",
      "Epoch 1786/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0453 - val_loss: 27.3828\n",
      "Epoch 1787/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1062 - val_loss: 26.6186\n",
      "Epoch 1788/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1507 - val_loss: 26.0181\n",
      "Epoch 1789/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2298 - val_loss: 27.2763\n",
      "Epoch 1790/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2767 - val_loss: 27.6978\n",
      "Epoch 1791/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1633 - val_loss: 27.2922\n",
      "Epoch 1792/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2059 - val_loss: 26.4889\n",
      "Epoch 1793/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0220 - val_loss: 26.5926\n",
      "Epoch 1794/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9454 - val_loss: 26.6254\n",
      "Epoch 1795/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9135 - val_loss: 26.8748\n",
      "Epoch 1796/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0428 - val_loss: 27.0697\n",
      "Epoch 1797/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8503 - val_loss: 27.0118\n",
      "Epoch 1798/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9316 - val_loss: 26.0821\n",
      "Epoch 1799/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 24.0157 - val_loss: 26.2292\n",
      "Epoch 1800/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 23.9399\n",
      "Epoch 01800: saving model to saved_models/latent32/cp-1800.h5\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 23.9399 - val_loss: 26.6038\n",
      "Epoch 1801/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9158 - val_loss: 26.4430\n",
      "Epoch 1802/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0419 - val_loss: 26.8858\n",
      "Epoch 1803/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8062 - val_loss: 26.8747\n",
      "Epoch 1804/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8476 - val_loss: 26.0900\n",
      "Epoch 1805/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0592 - val_loss: 26.2835\n",
      "Epoch 1806/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2276 - val_loss: 26.8222\n",
      "Epoch 1807/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1745 - val_loss: 26.6557\n",
      "Epoch 1808/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4066 - val_loss: 27.3586\n",
      "Epoch 1809/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3864 - val_loss: 26.7283\n",
      "Epoch 1810/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1208 - val_loss: 26.7760\n",
      "Epoch 1811/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0712 - val_loss: 26.7951\n",
      "Epoch 1812/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0052 - val_loss: 26.7136\n",
      "Epoch 1813/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1386 - val_loss: 26.3080\n",
      "Epoch 1814/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0916 - val_loss: 27.1868\n",
      "Epoch 1815/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.2952 - val_loss: 25.9339\n",
      "Epoch 1816/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0336 - val_loss: 27.1159\n",
      "Epoch 1817/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1477 - val_loss: 26.3355\n",
      "Epoch 1818/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0284 - val_loss: 26.0983\n",
      "Epoch 1819/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8618 - val_loss: 26.2606\n",
      "Epoch 1820/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8838 - val_loss: 26.3752\n",
      "Epoch 1821/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8107 - val_loss: 26.8545\n",
      "Epoch 1822/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1702 - val_loss: 26.4760\n",
      "Epoch 1823/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1334 - val_loss: 26.8491\n",
      "Epoch 1824/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2147 - val_loss: 26.6343\n",
      "Epoch 1825/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1716 - val_loss: 26.7704\n",
      "Epoch 1826/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8730 - val_loss: 26.5608\n",
      "Epoch 1827/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0121 - val_loss: 26.2071\n",
      "Epoch 1828/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9571 - val_loss: 27.1165\n",
      "Epoch 1829/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8756 - val_loss: 26.5560\n",
      "Epoch 1830/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8966 - val_loss: 26.4293\n",
      "Epoch 1831/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8975 - val_loss: 26.0621\n",
      "Epoch 1832/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0696 - val_loss: 26.6636\n",
      "Epoch 1833/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9664 - val_loss: 26.4782\n",
      "Epoch 1834/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9202 - val_loss: 26.9013\n",
      "Epoch 1835/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0228 - val_loss: 27.5868\n",
      "Epoch 1836/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4503 - val_loss: 26.9198\n",
      "Epoch 1837/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2746 - val_loss: 26.4308\n",
      "Epoch 1838/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9468 - val_loss: 26.7018\n",
      "Epoch 1839/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9256 - val_loss: 26.2984\n",
      "Epoch 1840/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8722 - val_loss: 27.8905\n",
      "Epoch 1841/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9357 - val_loss: 26.9191\n",
      "Epoch 1842/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0386 - val_loss: 26.8878\n",
      "Epoch 1843/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9308 - val_loss: 26.0054\n",
      "Epoch 1844/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.6126 - val_loss: 26.5789\n",
      "Epoch 1845/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7940 - val_loss: 27.2169\n",
      "Epoch 1846/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8587 - val_loss: 27.0825\n",
      "Epoch 1847/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9938 - val_loss: 26.5018\n",
      "Epoch 1848/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9571 - val_loss: 26.6778\n",
      "Epoch 1849/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0068 - val_loss: 27.2143\n",
      "Epoch 1850/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8831 - val_loss: 26.6673\n",
      "Epoch 1851/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8723 - val_loss: 26.6386\n",
      "Epoch 1852/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8060 - val_loss: 26.0581\n",
      "Epoch 1853/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8898 - val_loss: 26.6496\n",
      "Epoch 1854/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8545 - val_loss: 26.9872\n",
      "Epoch 1855/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9032 - val_loss: 26.6382\n",
      "Epoch 1856/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9272 - val_loss: 26.8204\n",
      "Epoch 1857/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9797 - val_loss: 25.9896\n",
      "Epoch 1858/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8376 - val_loss: 25.9415\n",
      "Epoch 1859/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0063 - val_loss: 26.3123\n",
      "Epoch 1860/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0541 - val_loss: 27.3878\n",
      "Epoch 1861/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9649 - val_loss: 27.1793\n",
      "Epoch 1862/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9769 - val_loss: 26.4923\n",
      "Epoch 1863/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0207 - val_loss: 26.1588\n",
      "Epoch 1864/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9078 - val_loss: 27.1194\n",
      "Epoch 1865/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9679 - val_loss: 26.4135\n",
      "Epoch 1866/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1110 - val_loss: 27.3010\n",
      "Epoch 1867/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2243 - val_loss: 26.3372\n",
      "Epoch 1868/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9549 - val_loss: 26.8565\n",
      "Epoch 1869/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1134 - val_loss: 27.0809\n",
      "Epoch 1870/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0172 - val_loss: 26.9559\n",
      "Epoch 1871/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7419 - val_loss: 26.1384\n",
      "Epoch 1872/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8645 - val_loss: 26.2113\n",
      "Epoch 1873/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7437 - val_loss: 26.3559\n",
      "Epoch 1874/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7654 - val_loss: 26.9694\n",
      "Epoch 1875/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7767 - val_loss: 26.3925\n",
      "Epoch 1876/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7227 - val_loss: 26.6931\n",
      "Epoch 1877/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6255 - val_loss: 26.7044\n",
      "Epoch 1878/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6910 - val_loss: 26.8342\n",
      "Epoch 1879/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7599 - val_loss: 26.0284\n",
      "Epoch 1880/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.6097 - val_loss: 26.9796\n",
      "Epoch 1881/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8041 - val_loss: 25.9301\n",
      "Epoch 1882/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9572 - val_loss: 26.5065\n",
      "Epoch 1883/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8670 - val_loss: 26.4141\n",
      "Epoch 1884/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8828 - val_loss: 26.7559\n",
      "Epoch 1885/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0303 - val_loss: 26.9497\n",
      "Epoch 1886/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1971 - val_loss: 26.9783\n",
      "Epoch 1887/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1817 - val_loss: 26.9786\n",
      "Epoch 1888/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1704 - val_loss: 27.1665\n",
      "Epoch 1889/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9200 - val_loss: 26.8000\n",
      "Epoch 1890/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9198 - val_loss: 26.1702\n",
      "Epoch 1891/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9467 - val_loss: 27.7714\n",
      "Epoch 1892/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1394 - val_loss: 26.9516\n",
      "Epoch 1893/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0060 - val_loss: 26.8007\n",
      "Epoch 1894/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8186 - val_loss: 25.7411\n",
      "Epoch 1895/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.9234 - val_loss: 26.9846\n",
      "Epoch 1896/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7909 - val_loss: 26.6368\n",
      "Epoch 1897/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.0266 - val_loss: 27.2404\n",
      "Epoch 1898/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2294 - val_loss: 26.3428\n",
      "Epoch 1899/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8640 - val_loss: 26.3629\n",
      "Epoch 1900/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2970 - val_loss: 26.7282\n",
      "Epoch 1901/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1172 - val_loss: 26.0533\n",
      "Epoch 1902/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1001 - val_loss: 26.8556\n",
      "Epoch 1903/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0457 - val_loss: 26.2845\n",
      "Epoch 1904/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1214 - val_loss: 26.4874\n",
      "Epoch 1905/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0770 - val_loss: 25.6927\n",
      "Epoch 1906/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1106 - val_loss: 27.0191\n",
      "Epoch 1907/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9209 - val_loss: 26.7462\n",
      "Epoch 1908/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1028 - val_loss: 27.0402\n",
      "Epoch 1909/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1241 - val_loss: 27.0544\n",
      "Epoch 1910/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1236 - val_loss: 27.3695\n",
      "Epoch 1911/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1708 - val_loss: 26.9769\n",
      "Epoch 1912/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9502 - val_loss: 26.1764\n",
      "Epoch 1913/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7246 - val_loss: 25.8454\n",
      "Epoch 1914/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6708 - val_loss: 26.1320\n",
      "Epoch 1915/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.5750 - val_loss: 26.0497\n",
      "Epoch 1916/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7653 - val_loss: 26.9680\n",
      "Epoch 1917/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1908 - val_loss: 26.5688\n",
      "Epoch 1918/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9682 - val_loss: 26.6363\n",
      "Epoch 1919/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9876 - val_loss: 27.0765\n",
      "Epoch 1920/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1788 - val_loss: 26.0209\n",
      "Epoch 1921/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9873 - val_loss: 26.2599\n",
      "Epoch 1922/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8048 - val_loss: 26.4156\n",
      "Epoch 1923/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6384 - val_loss: 26.5359\n",
      "Epoch 1924/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7333 - val_loss: 26.5100\n",
      "Epoch 1925/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.5064 - val_loss: 26.7571\n",
      "Epoch 1926/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7073 - val_loss: 26.1958\n",
      "Epoch 1927/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6452 - val_loss: 25.9742\n",
      "Epoch 1928/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6018 - val_loss: 25.7898\n",
      "Epoch 1929/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9641 - val_loss: 26.6623\n",
      "Epoch 1930/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7038 - val_loss: 26.3544\n",
      "Epoch 1931/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9895 - val_loss: 27.3363\n",
      "Epoch 1932/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1501 - val_loss: 27.3400\n",
      "Epoch 1933/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9832 - val_loss: 26.8904\n",
      "Epoch 1934/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1659 - val_loss: 26.6714\n",
      "Epoch 1935/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0911 - val_loss: 27.0891\n",
      "Epoch 1936/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0825 - val_loss: 27.0185\n",
      "Epoch 1937/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1279 - val_loss: 27.5626\n",
      "Epoch 1938/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0381 - val_loss: 26.6110\n",
      "Epoch 1939/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7908 - val_loss: 27.8459\n",
      "Epoch 1940/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2020 - val_loss: 26.5350\n",
      "Epoch 1941/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0105 - val_loss: 26.5608\n",
      "Epoch 1942/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2028 - val_loss: 26.9158\n",
      "Epoch 1943/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8012 - val_loss: 26.8225\n",
      "Epoch 1944/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7216 - val_loss: 26.1076\n",
      "Epoch 1945/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8520 - val_loss: 25.8421\n",
      "Epoch 1946/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7464 - val_loss: 26.4838\n",
      "Epoch 1947/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8641 - val_loss: 26.3953\n",
      "Epoch 1948/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8184 - val_loss: 26.1246\n",
      "Epoch 1949/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6064 - val_loss: 26.3757\n",
      "Epoch 1950/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7529 - val_loss: 26.4166\n",
      "Epoch 1951/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8771 - val_loss: 26.5719\n",
      "Epoch 1952/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9152 - val_loss: 26.3183\n",
      "Epoch 1953/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8269 - val_loss: 26.4006\n",
      "Epoch 1954/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5421 - val_loss: 26.3224\n",
      "Epoch 1955/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6835 - val_loss: 26.6363\n",
      "Epoch 1956/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8846 - val_loss: 26.2856\n",
      "Epoch 1957/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6979 - val_loss: 26.8774\n",
      "Epoch 1958/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5904 - val_loss: 25.9850\n",
      "Epoch 1959/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7714 - val_loss: 27.2231\n",
      "Epoch 1960/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0603 - val_loss: 27.3996\n",
      "Epoch 1961/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9144 - val_loss: 26.4044\n",
      "Epoch 1962/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7763 - val_loss: 26.8399\n",
      "Epoch 1963/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9149 - val_loss: 25.7822\n",
      "Epoch 1964/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7608 - val_loss: 27.1196\n",
      "Epoch 1965/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8198 - val_loss: 26.4490\n",
      "Epoch 1966/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.6148 - val_loss: 25.7717\n",
      "Epoch 1967/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6416 - val_loss: 26.9066\n",
      "Epoch 1968/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6703 - val_loss: 25.9014\n",
      "Epoch 1969/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5963 - val_loss: 26.5148\n",
      "Epoch 1970/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8036 - val_loss: 26.8825\n",
      "Epoch 1971/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 23.9310 - val_loss: 26.1385\n",
      "Epoch 1972/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7048 - val_loss: 26.1016\n",
      "Epoch 1973/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8248 - val_loss: 26.1876\n",
      "Epoch 1974/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.0133 - val_loss: 26.8101\n",
      "Epoch 1975/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9719 - val_loss: 26.1890\n",
      "Epoch 1976/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8843 - val_loss: 26.8077\n",
      "Epoch 1977/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7680 - val_loss: 26.8910\n",
      "Epoch 1978/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5555 - val_loss: 26.3501\n",
      "Epoch 1979/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5963 - val_loss: 26.1637\n",
      "Epoch 1980/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6286 - val_loss: 25.9658\n",
      "Epoch 1981/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5248 - val_loss: 27.0720\n",
      "Epoch 1982/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7949 - val_loss: 26.7181\n",
      "Epoch 1983/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9284 - val_loss: 26.9429\n",
      "Epoch 1984/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5835 - val_loss: 25.9516\n",
      "Epoch 1985/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7065 - val_loss: 26.4649\n",
      "Epoch 1986/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5726 - val_loss: 26.7979\n",
      "Epoch 1987/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8772 - val_loss: 26.8921\n",
      "Epoch 1988/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8304 - val_loss: 26.1269\n",
      "Epoch 1989/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6061 - val_loss: 26.9768\n",
      "Epoch 1990/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8666 - val_loss: 27.2075\n",
      "Epoch 1991/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7287 - val_loss: 26.0926\n",
      "Epoch 1992/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8058 - val_loss: 26.4190\n",
      "Epoch 1993/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5927 - val_loss: 27.0332\n",
      "Epoch 1994/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.9708 - val_loss: 26.7181\n",
      "Epoch 1995/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7719 - val_loss: 26.1431\n",
      "Epoch 1996/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8394 - val_loss: 26.6710\n",
      "Epoch 1997/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.7916 - val_loss: 26.9734\n",
      "Epoch 1998/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.6368 - val_loss: 26.4812\n",
      "Epoch 1999/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.7165 - val_loss: 26.5971\n",
      "Epoch 2000/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 23.7493\n",
      "Epoch 02000: saving model to saved_models/latent32/cp-2000.h5\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 23.7493 - val_loss: 25.9586\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train VAE model with callbacks \"\"\"\n",
    "history = vae.fit(trainX, trainX, \n",
    "                  batch_size = batch_size, \n",
    "                  epochs = epochs, \n",
    "                  callbacks=[initial_checkpoint, checkpoint, EarlyStoppingAtMinLoss()],\n",
    "                  #callbacks=[initial_checkpoint, checkpoint],\n",
    "                  shuffle=True,\n",
    "                  validation_data=(valX, valX)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 6.0\n"
     ]
    }
   ],
   "source": [
    "# In this GPU implementation, it shows the number of batches/iterations for one epoch (and not the training samples), which is:\n",
    "print (\"Step size:\", np.ceil(trainX.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save the entire model \"\"\"\n",
    "# Save notes about hyperparameters used:\n",
    "with open(f'{SAVE_FOLDER}/notes.txt', 'w') as f:\n",
    "    f.write(f'Epochs: {epochs} \\n')\n",
    "    f.write(f'Batch size: {batch_size} \\n')\n",
    "    f.write(f'Latent dimension: {latent_dim} \\n')\n",
    "    f.write(f'Filter sizes: {filters} \\n')\n",
    "    f.write(f'img_width: {img_width} \\n')\n",
    "    f.write(f'img_height: {img_height} \\n')\n",
    "    f.write(f'num_channels: {num_channels}')\n",
    "    f.close()\n",
    "\n",
    "# Save weights for encoder model:\n",
    "encoder.save_weights(f'{SAVE_FOLDER}/ENCODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for decoder model:\n",
    "decoder.save_weights(f'{SAVE_FOLDER}/DECODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for total VAE:\n",
    "vae.save_weights(f'{SAVE_FOLDER}/VAE_WEIGHTS.h5')\n",
    "\n",
    "# Save the history at each epochs (cost of train and validation sets):\n",
    "np.save(f'{SAVE_FOLDER}/HISTORY.npy', history.history)\n",
    "\n",
    "# Save exact training, validation and testing data set (as numpy arrays):\n",
    "np.save(f'{SAVE_FOLDER}/trainX.npy', trainX)\n",
    "np.save(f'{SAVE_FOLDER}/valX.npy', valX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllX.npy', testAllX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllY.npy', testAllY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate training process\n",
    "\n",
    "Now that the training process is complete, let’s evaluate the average loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(13, 6))\n",
    "\n",
    "# summarize history for loss:\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='blue', alpha=0.4, label='validation')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Cost', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.title('Average Loss During Training', fontsize=18)\n",
    "#plt.xticks(range(0,epochs))\n",
    "plt.legend(loc='upper right', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
