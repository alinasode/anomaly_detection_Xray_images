{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import axis, colorbar, imshow, show, figure, subplot\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=18)\n",
    "mpl.rc('xtick', labelsize=16)\n",
    "mpl.rc('ytick', labelsize=16)\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## ----- GPU ------------------------------------\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'    # use of GPU 0 (ERDA) (use before importing torch or tensorflow/keeas)\n",
    "## ----------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization, ReLU, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.activations import relu\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K                         #contains calls for tensor manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL_TRAIN_AUG:       /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTrainAugmentations\n",
      "NORMAL_VALIDATION_AUG:  /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalValidationAugmentations\n",
      "NORMAL_TEST_AUG:        /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTestAugmentations\n",
      "ANOMALY_AUG:            /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/AnomalyAugmentations\n"
     ]
    }
   ],
   "source": [
    "from utils_vae_kitkat_paths import *\n",
    "\n",
    "# Get work directions for augmentated data set:\n",
    "print (\"NORMAL_TRAIN_AUG:      \", NORMAL_TRAIN_AUG)\n",
    "print (\"NORMAL_VALIDATION_AUG: \", NORMAL_VAL_AUG)\n",
    "print (\"NORMAL_TEST_AUG:       \", NORMAL_TEST_AUG)\n",
    "print (\"ANOMALY_AUG:           \", ANOMALY_AUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which Folder The Models Are Saved In: saved_models/latent64\n"
     ]
    }
   ],
   "source": [
    "# What latent dimension and filter size are we using?:\n",
    "latent_dim = 64\n",
    "filters    = 32\n",
    "\n",
    "# Get work direction for saving files:\n",
    "SAVE_FOLDER = f'saved_models/latent{latent_dim}'\n",
    "print (\"Which Folder The Models Are Saved In:\", SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] deleting existing files in folder...\n",
      "         done in 0.000 minutes\n"
     ]
    }
   ],
   "source": [
    "# Delete all existing files in folder where models are saved:\n",
    "print(\"[INFO] deleting existing files in folder...\")\n",
    "t0 = time()\n",
    "\n",
    "files = glob.glob(f'{SAVE_FOLDER}/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "print (\"         done in %0.3f minutes\" % ((time() - t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_VAE_AE import get_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Investigation of Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"normal\" training images is:     1400\n",
      "Number of \"normal\" validation images is:   134\n",
      "Number of \"normal\" test images is:         266\n",
      "Number of \"anomaly\" images is:             600\n"
     ]
    }
   ],
   "source": [
    "# Glob the directories and get the lists of good and bad images\n",
    "good_train_fns = [f for f in os.listdir(NORMAL_TRAIN_AUG) if not f.startswith('.')]\n",
    "good_val_fns = [f for f in os.listdir(NORMAL_VAL_AUG) if not f.startswith('.')]\n",
    "good_test_fns = [f for f in os.listdir(NORMAL_TEST_AUG) if not f.startswith('.')]\n",
    "bad_fns = [f for f in os.listdir(ANOMALY_AUG) if not f.startswith('.')]\n",
    "\n",
    "print('Number of \"normal\" training images is:     {}'.format(len(good_train_fns)))\n",
    "print('Number of \"normal\" validation images is:   {}'.format(len(good_val_fns)))\n",
    "print('Number of \"normal\" test images is:         {}'.format(len(good_test_fns)))\n",
    "print('Number of \"anomaly\" images is:             {}'.format(len(bad_fns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Procentage of normal samples (ground truth):   75.00 %\n",
      "> Procentage of anomaly samples (ground truth):  25.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of bad images vs good images:\n",
    "normal_fns = len(good_train_fns) + len(good_val_fns) + len(good_test_fns)\n",
    "anomaly_fns = len(bad_fns)\n",
    "print(f\"> Procentage of normal samples (ground truth):   {(normal_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")\n",
    "print(f\"> Procentage of anomaly samples (ground truth):  {(anomaly_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Normal Data\n",
    "\n",
    "Load normal samples in pre-splitted data sets: train, valdiation and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal training images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal training images...\")\n",
    "trainX = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TRAIN_AUG}/*.png\")]  # read as grayscale\n",
    "trainX = np.array(trainX)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "trainY = get_labels(trainX, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal validation images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal validation images...\")\n",
    "x_normal_val = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_VAL_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_val = np.array(x_normal_val)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_val = get_labels(x_normal_val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal testing images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal testing images...\")\n",
    "x_normal_test = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TEST_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_test = np.array(x_normal_test)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_test = get_labels(x_normal_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Data\n",
    "\n",
    "\n",
    "Load anomaly samples in its singular training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly images...\")\n",
    "x_anomaly = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY_AUG}/*.png\")]  # read as grayscale\n",
    "x_anomaly = np.array(x_anomaly)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_anomaly = get_labels(x_anomaly, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine normal samples and anomaly samples and split them into training, validation and test sets\n",
    "\n",
    "`label 0` == Normal Samples\n",
    "\n",
    "`label 1` == Anomaly Samples\n",
    "\n",
    "Train and Validation sets only consists of `Normal Data`, aka. `label 0`.\n",
    "\n",
    "Testing sets consists of both  `Normal Data` (aka. `label 0`) and `Anomaly Data` (aka. `label 1`)\n",
    "\n",
    "________________\n",
    "\n",
    "Due to the very sparse original (preprossed) KitKat Chunky data, the validation set will be a mix of normal testing and validation data. The normal testing set will consists of all validation and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testvalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testvalY = get_labels(testvalX, 0)\n",
    "\n",
    "# randomly split in order to make new validation set:\n",
    "NoUsageX, valX, NoUsageY, valY = train_test_split(testvalX, testvalY, test_size=0.25, random_state=4345672)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testNormalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testNormalY = get_labels(testNormalX, 0)\n",
    "\n",
    "# anomaly class (only used for testing):\n",
    "testAnomalyX, testAnomalyY = x_anomaly, y_anomaly\n",
    "\n",
    "# Combine all testing data in one array:\n",
    "testAllX = np.vstack((testNormalX, testAnomalyX))\n",
    "testAllY = np.hstack((testNormalY, testAnomalyY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Dimensions and Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      " > images: (1400, 128, 128)\n",
      " > labels: (1400,)\n",
      "\n",
      "Validation set:\n",
      " > images: (100, 128, 128)\n",
      " > labels: (100,)\n",
      "\n",
      "Test set:\n",
      " > images: (1000, 128, 128)\n",
      " > labels: (1000,)\n",
      "\t'Normal' test set:\n",
      "\t  > images: (400, 128, 128)\n",
      "\t  > labels: (400,)\n",
      "\t'Anomaly' test set:\n",
      "\t  > images: (600, 128, 128)\n",
      "\t  > labels: (600,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "print(\" > images:\", trainX.shape)\n",
    "print(\" > labels:\", trainY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Validation set:\")\n",
    "print(\" > images:\", valX.shape)\n",
    "print(\" > labels:\", valY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Test set:\")\n",
    "print(\" > images:\", testAllX.shape)\n",
    "print(\" > labels:\", testAllY.shape)\n",
    "print(\"\\t'Normal' test set:\")\n",
    "print(\"\\t  > images:\", testNormalX.shape)\n",
    "print(\"\\t  > labels:\", testNormalY.shape)\n",
    "print(\"\\t'Anomaly' test set:\")\n",
    "print(\"\\t  > images:\", testAnomalyX.shape)\n",
    "print(\"\\t  > labels:\", testAnomalyY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 73.68 %\n",
      "Procentage of validation samples: 5.26 %\n",
      "Procentage of (only normal) testing samples: 21.05 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets (only normal data):\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (only normal) testing samples: {(len(testNormalX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 56.00 %\n",
      "Procentage of validation samples: 4.00 %\n",
      "Procentage of (total) testing samples: 40.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets:\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (total) testing samples: {(len(testAllX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for (un)balanced data\n",
    "\n",
    "In an ideal world the data should be balanced. However in the real world we expect there being around ~$99 \\%$ good samples and only ~$1 \\%$ bad samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['0: normal', '1: anomaly']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,4))\n",
    "ax[0].hist(trainY, align=\"left\"); ax[0].set_title('train', fontsize=18); ax[0].set_xticks([0,1]); ax[0].set_xticklabels(labels); ax[0].tick_params(axis='both', which='major', labelsize=16); ax[0].set_xlim(-0.5, 1.5);\n",
    "ax[1].hist(valY, align=\"left\"); ax[1].set_title('validation', fontsize=18); ax[1].set_xticks([0,1]); ax[1].set_xticklabels(labels); ax[1].tick_params(axis='both', which='major', labelsize=16); ax[1].set_xlim(-0.5, 1.5);\n",
    "ax[2].hist(testAllY, align=\"left\"); ax[2].set_title('test', fontsize=18); ax[2].set_xticks([0,1]); ax[2].set_xticklabels(labels); ax[2].tick_params(axis='both', which='major', labelsize=16); ax[2].set_xlim(-0.5, 1.5);\n",
    "\n",
    "ax[0].set_ylabel(\"Number of images\", fontsize=18)\n",
    "ax[1].set_xlabel(\"Class Labels\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of normal test samples: 40.00 %\n",
      "Procentage of total anomaly test samples: 60.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of normal test images vs total anomaly test images:\n",
    "print(f\"Procentage of normal test samples: {(len(testNormalX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of total anomaly test samples: {(len(testAnomalyX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Only normalization has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "img_width, img_height = trainX.shape[1], trainX.shape[2]      # input image dimensions\n",
    "num_channels = 1                                              # gray-channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Neural Networks learns faster with normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to be in  the [0, 1] range:\n",
    "trainX = trainX.astype('float32') / 255.\n",
    "valX = valX.astype('float32') / 255.\n",
    "testAllX = testAllX.astype('float32') / 255.\n",
    "\n",
    "# reshape data to keras inputs --> (n_samples, img_rows, img_cols, n_channels):\n",
    "trainX = trainX.reshape(trainX.shape[0], img_height, img_width, num_channels)\n",
    "valX = valX.reshape(valX.shape[0], img_height, img_width, num_channels)\n",
    "testAllX = testAllX.reshape(testAllX.shape[0], img_height, img_width, num_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import architecture of VAE model and its hyperparameters:\n",
    "from J64_VAE_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Encoder Part*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 32)   320         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 64, 64, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 32)   9248        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 32, 32, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 32)   9248        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 16, 16, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 8, 32)     9248        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 8, 8, 32)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 4, 4, 32)     9248        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 4, 4, 32)     0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "latent (Dense)                  (None, 200)          102600      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 200)          0           latent[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 64)           12864       leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_log_mean (Dense)              (None, 64)           12864       leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 64)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_mean[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 165,640\n",
      "Trainable params: 165,640\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Decoder Part*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               33280     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 128, 128, 1)       289       \n",
      "=================================================================\n",
      "Total params: 79,809\n",
      "Trainable params: 79,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Total VAE Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         [(None, 64), (None, 64),  165640    \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 128, 128, 1)       79809     \n",
      "=================================================================\n",
      "Total params: 245,449\n",
      "Trainable params: 245,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "batch_size  = 256\n",
    "epochs      = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at 1'st epoch (inital model)\n",
    "\n",
    "https://stackoverflow.com/questions/54323960/save-keras-model-at-specific-epochs\n",
    "\"\"\"\n",
    "\n",
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 1:\n",
    "            self.model.save_weights(f\"{SAVE_FOLDER}/cp-0001.h5\")\n",
    "            \n",
    "# create and use callback:\n",
    "initial_checkpoint = CustomSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at every k'te epochs\n",
    "\"\"\"\n",
    "# Include the epoch in the file name (uses `str.format`):\n",
    "checkpoint_path = \"saved_models/latent64/cp-{epoch:04d}.h5\"\n",
    "\n",
    "# Create a callback that saves the model's weights every k'te epochs:\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path,\n",
    "                             monitor='loss',           # name of the metrics to monitor\n",
    "                             verbose=1, \n",
    "                             #save_best_only=False,     # if False, the best model will not be overridden.\n",
    "                             save_weights_only=True,   # if True, only the weights of the models will be saved.\n",
    "                                                        # if False, the whole models will be saved.\n",
    "                             #mode='auto', \n",
    "                             period=200                  # save after every k'te epochs\n",
    "                            )\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "vae.save_weights(checkpoint_path.format(epoch=0))          # save for zero epoch (inital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: Early stopping\n",
    "https://www.tensorflow.org/guide/keras/custom_callback\n",
    "\"\"\"\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=100):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create and Compile Model\"\"\"\n",
    "# import architecture of VAE model and its hyperparameters. learning rate choosen from graph above.\n",
    "vae = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 3518.2034 - val_loss: 3409.5554\n",
      "Epoch 2/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 3165.9592 - val_loss: 2637.7698\n",
      "Epoch 3/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 1862.2002 - val_loss: 1076.0852\n",
      "Epoch 4/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 887.3513 - val_loss: 894.4470\n",
      "Epoch 5/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 810.1148 - val_loss: 844.6154\n",
      "Epoch 6/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 728.0083 - val_loss: 729.2238\n",
      "Epoch 7/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 645.7774 - val_loss: 676.6975\n",
      "Epoch 8/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 610.3078 - val_loss: 653.6676\n",
      "Epoch 9/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 592.5826 - val_loss: 641.3908\n",
      "Epoch 10/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 582.1772 - val_loss: 630.3510\n",
      "Epoch 11/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 577.5897 - val_loss: 627.6819\n",
      "Epoch 12/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 571.6337 - val_loss: 616.4478\n",
      "Epoch 13/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 566.9121 - val_loss: 615.5936\n",
      "Epoch 14/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 563.5923 - val_loss: 614.1313\n",
      "Epoch 15/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 557.4366 - val_loss: 605.8425\n",
      "Epoch 16/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 553.8369 - val_loss: 600.9537\n",
      "Epoch 17/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 549.7569 - val_loss: 596.2590\n",
      "Epoch 18/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 536.4759 - val_loss: 572.9954\n",
      "Epoch 19/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 544.9080 - val_loss: 595.4925\n",
      "Epoch 20/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 543.2214 - val_loss: 576.3299\n",
      "Epoch 21/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 495.6772 - val_loss: 479.8390\n",
      "Epoch 22/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 398.1881 - val_loss: 399.5046\n",
      "Epoch 23/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 352.5061 - val_loss: 368.9259\n",
      "Epoch 24/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 323.2462 - val_loss: 344.4013\n",
      "Epoch 25/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 305.1321 - val_loss: 327.2119\n",
      "Epoch 26/2000\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 292.3534 - val_loss: 310.6151\n",
      "Epoch 27/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 279.3817 - val_loss: 297.1057\n",
      "Epoch 28/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 270.7708 - val_loss: 287.9429\n",
      "Epoch 29/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 263.2213 - val_loss: 275.8109\n",
      "Epoch 30/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 254.3248 - val_loss: 271.2789\n",
      "Epoch 31/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 249.6126 - val_loss: 263.9078\n",
      "Epoch 32/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 244.4227 - val_loss: 255.2209\n",
      "Epoch 33/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 238.4651 - val_loss: 255.8569\n",
      "Epoch 34/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 235.1974 - val_loss: 248.0632\n",
      "Epoch 35/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 230.0520 - val_loss: 243.5199\n",
      "Epoch 36/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 227.5080 - val_loss: 238.1377\n",
      "Epoch 37/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 224.1318 - val_loss: 237.1129\n",
      "Epoch 38/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 219.7031 - val_loss: 229.8767\n",
      "Epoch 39/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 214.3337 - val_loss: 227.2578\n",
      "Epoch 40/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 210.1303 - val_loss: 222.5441\n",
      "Epoch 41/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 201.6837 - val_loss: 210.9819\n",
      "Epoch 42/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 193.3665 - val_loss: 203.5274\n",
      "Epoch 43/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 183.9154 - val_loss: 196.7041\n",
      "Epoch 44/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 177.2410 - val_loss: 194.4698\n",
      "Epoch 45/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 173.1252 - val_loss: 194.6124\n",
      "Epoch 46/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 171.6016 - val_loss: 191.5077\n",
      "Epoch 47/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 168.9675 - val_loss: 187.9601\n",
      "Epoch 48/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 166.3916 - val_loss: 185.6306\n",
      "Epoch 49/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 164.7515 - val_loss: 182.8897\n",
      "Epoch 50/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 162.4399 - val_loss: 178.4333\n",
      "Epoch 51/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 159.8962 - val_loss: 178.6683\n",
      "Epoch 52/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 160.1690 - val_loss: 177.9208\n",
      "Epoch 53/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 158.0588 - val_loss: 173.4283\n",
      "Epoch 54/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 155.6188 - val_loss: 173.4358\n",
      "Epoch 55/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 154.9621 - val_loss: 173.5360\n",
      "Epoch 56/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 155.8122 - val_loss: 173.3742\n",
      "Epoch 57/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 154.2338 - val_loss: 178.6953\n",
      "Epoch 58/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 153.1982 - val_loss: 169.7450\n",
      "Epoch 59/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 149.4673 - val_loss: 163.8414\n",
      "Epoch 60/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 149.1249 - val_loss: 165.2633\n",
      "Epoch 61/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 147.0513 - val_loss: 170.3515\n",
      "Epoch 62/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 149.4956 - val_loss: 159.7778\n",
      "Epoch 63/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 146.7303 - val_loss: 163.3038\n",
      "Epoch 64/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 146.7831 - val_loss: 160.4644\n",
      "Epoch 65/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 145.6113 - val_loss: 158.6731\n",
      "Epoch 66/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 142.3970 - val_loss: 159.9811\n",
      "Epoch 67/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 142.8662 - val_loss: 158.4950\n",
      "Epoch 68/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 140.8356 - val_loss: 159.6761\n",
      "Epoch 69/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 140.4064 - val_loss: 156.6731\n",
      "Epoch 70/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 138.7190 - val_loss: 153.5422\n",
      "Epoch 71/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 135.9469 - val_loss: 155.1177\n",
      "Epoch 72/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 135.1091 - val_loss: 150.4200\n",
      "Epoch 73/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 136.0329 - val_loss: 157.4463\n",
      "Epoch 74/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 135.0090 - val_loss: 152.4071\n",
      "Epoch 75/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 133.4114 - val_loss: 148.5229\n",
      "Epoch 76/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 132.1609 - val_loss: 146.2428\n",
      "Epoch 77/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 131.1539 - val_loss: 144.6960\n",
      "Epoch 78/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 130.5344 - val_loss: 145.7996\n",
      "Epoch 79/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 130.5865 - val_loss: 144.2946\n",
      "Epoch 80/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 128.0670 - val_loss: 145.4927\n",
      "Epoch 81/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 126.8831 - val_loss: 141.8191\n",
      "Epoch 82/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 126.0306 - val_loss: 140.2564\n",
      "Epoch 83/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 125.1105 - val_loss: 143.1590\n",
      "Epoch 84/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 126.6022 - val_loss: 137.2671\n",
      "Epoch 85/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 123.2991 - val_loss: 138.9060\n",
      "Epoch 86/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 123.0618 - val_loss: 137.5356\n",
      "Epoch 87/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 121.7733 - val_loss: 138.1103\n",
      "Epoch 88/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 122.6139 - val_loss: 137.4076\n",
      "Epoch 89/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 121.0467 - val_loss: 135.8566\n",
      "Epoch 90/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 118.1130 - val_loss: 132.6984\n",
      "Epoch 91/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 117.0769 - val_loss: 132.1064\n",
      "Epoch 92/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 115.5707 - val_loss: 129.4044\n",
      "Epoch 93/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 114.1838 - val_loss: 130.7819\n",
      "Epoch 94/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 114.1367 - val_loss: 128.7695\n",
      "Epoch 95/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 113.2543 - val_loss: 127.9630\n",
      "Epoch 96/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 112.4752 - val_loss: 126.9128\n",
      "Epoch 97/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 110.9284 - val_loss: 127.4389\n",
      "Epoch 98/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 110.4472 - val_loss: 126.1515\n",
      "Epoch 99/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 109.9725 - val_loss: 127.6163\n",
      "Epoch 100/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 110.7795 - val_loss: 130.1205\n",
      "Epoch 101/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 111.2183 - val_loss: 124.5620\n",
      "Epoch 102/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 108.6512 - val_loss: 125.1051\n",
      "Epoch 103/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 107.6442 - val_loss: 124.9682\n",
      "Epoch 104/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 106.4501 - val_loss: 123.3768\n",
      "Epoch 105/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 105.8076 - val_loss: 121.1388\n",
      "Epoch 106/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 104.6299 - val_loss: 117.9366\n",
      "Epoch 107/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 104.1946 - val_loss: 119.4407\n",
      "Epoch 108/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 102.9999 - val_loss: 115.8994\n",
      "Epoch 109/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 102.7681 - val_loss: 118.4894\n",
      "Epoch 110/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 102.3470 - val_loss: 117.9300\n",
      "Epoch 111/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 102.6926 - val_loss: 117.1446\n",
      "Epoch 112/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 101.0798 - val_loss: 115.8849\n",
      "Epoch 113/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 100.6689 - val_loss: 115.5404\n",
      "Epoch 114/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 99.5774 - val_loss: 113.5201\n",
      "Epoch 115/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 99.4183 - val_loss: 112.5653\n",
      "Epoch 116/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 98.5364 - val_loss: 118.0010\n",
      "Epoch 117/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 98.2475 - val_loss: 112.0095\n",
      "Epoch 118/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 98.2629 - val_loss: 110.6728\n",
      "Epoch 119/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 98.2010 - val_loss: 114.1889\n",
      "Epoch 120/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 97.4412 - val_loss: 111.2440\n",
      "Epoch 121/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 96.3038 - val_loss: 112.1321\n",
      "Epoch 122/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 95.3927 - val_loss: 110.5273\n",
      "Epoch 123/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 95.0118 - val_loss: 110.2329\n",
      "Epoch 124/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 95.2417 - val_loss: 109.7161\n",
      "Epoch 125/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 95.6727 - val_loss: 109.4090\n",
      "Epoch 126/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 93.9006 - val_loss: 107.8495\n",
      "Epoch 127/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 93.1466 - val_loss: 105.0307\n",
      "Epoch 128/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 92.7383 - val_loss: 105.8438\n",
      "Epoch 129/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 92.2981 - val_loss: 106.5395\n",
      "Epoch 130/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 91.3083 - val_loss: 105.0011\n",
      "Epoch 131/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 91.9745 - val_loss: 106.9834\n",
      "Epoch 132/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 90.5572 - val_loss: 104.3497\n",
      "Epoch 133/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 89.2024 - val_loss: 104.3715\n",
      "Epoch 134/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 89.1887 - val_loss: 103.6615\n",
      "Epoch 135/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 89.7834 - val_loss: 104.1513\n",
      "Epoch 136/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 89.4257 - val_loss: 107.5024\n",
      "Epoch 137/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 88.5283 - val_loss: 100.8080\n",
      "Epoch 138/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 88.0826 - val_loss: 100.8321\n",
      "Epoch 139/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 87.8929 - val_loss: 101.7158\n",
      "Epoch 140/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 86.9815 - val_loss: 99.5384\n",
      "Epoch 141/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 86.4972 - val_loss: 100.8327\n",
      "Epoch 142/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 86.2151 - val_loss: 100.0841\n",
      "Epoch 143/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 85.5036 - val_loss: 100.5943\n",
      "Epoch 144/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 85.5520 - val_loss: 99.1722\n",
      "Epoch 145/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 84.6847 - val_loss: 102.1320\n",
      "Epoch 146/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 84.3574 - val_loss: 99.2597\n",
      "Epoch 147/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 84.1326 - val_loss: 99.7590\n",
      "Epoch 148/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 84.3103 - val_loss: 96.5516\n",
      "Epoch 149/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 83.1528 - val_loss: 97.6337\n",
      "Epoch 150/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 83.8253 - val_loss: 97.8619\n",
      "Epoch 151/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 83.1514 - val_loss: 95.1674\n",
      "Epoch 152/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 82.4481 - val_loss: 96.5908\n",
      "Epoch 153/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 81.7138 - val_loss: 95.1841\n",
      "Epoch 154/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 81.2797 - val_loss: 94.8861\n",
      "Epoch 155/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 81.0389 - val_loss: 91.9410\n",
      "Epoch 156/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 77.9588 - val_loss: 89.7087\n",
      "Epoch 157/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 74.8763 - val_loss: 83.1773\n",
      "Epoch 158/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 70.0008 - val_loss: 76.4204\n",
      "Epoch 159/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 65.3794 - val_loss: 68.7744\n",
      "Epoch 160/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 60.5783 - val_loss: 65.9881\n",
      "Epoch 161/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 58.0557 - val_loss: 60.4086\n",
      "Epoch 162/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 56.6189 - val_loss: 57.7138\n",
      "Epoch 163/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 55.8344 - val_loss: 56.0827\n",
      "Epoch 164/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 55.6395 - val_loss: 55.5978\n",
      "Epoch 165/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.9400 - val_loss: 57.3167\n",
      "Epoch 166/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 56.2102 - val_loss: 56.6739\n",
      "Epoch 167/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 55.4938 - val_loss: 55.4204\n",
      "Epoch 168/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 55.5000 - val_loss: 56.3269\n",
      "Epoch 169/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.7848 - val_loss: 57.3840\n",
      "Epoch 170/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 55.5519 - val_loss: 56.6922\n",
      "Epoch 171/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 54.5017 - val_loss: 56.3509\n",
      "Epoch 172/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.5773 - val_loss: 55.3684\n",
      "Epoch 173/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.9585 - val_loss: 53.4837\n",
      "Epoch 174/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.0642 - val_loss: 53.9915\n",
      "Epoch 175/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 53.0097 - val_loss: 54.4503\n",
      "Epoch 176/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.2234 - val_loss: 53.9697\n",
      "Epoch 177/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.3300 - val_loss: 56.0198\n",
      "Epoch 178/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.1683 - val_loss: 54.8496\n",
      "Epoch 179/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 52.7160 - val_loss: 52.3358\n",
      "Epoch 180/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 51.8699 - val_loss: 53.9102\n",
      "Epoch 181/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 51.6539 - val_loss: 54.0945\n",
      "Epoch 182/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.5526 - val_loss: 52.6437\n",
      "Epoch 183/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.9149 - val_loss: 51.7733\n",
      "Epoch 184/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 50.8121 - val_loss: 55.2478\n",
      "Epoch 185/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.2776 - val_loss: 52.0410\n",
      "Epoch 186/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 50.6425 - val_loss: 52.2484\n",
      "Epoch 187/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 50.2274 - val_loss: 51.2177\n",
      "Epoch 188/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 50.1329 - val_loss: 50.2019\n",
      "Epoch 189/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 49.6893 - val_loss: 51.2674\n",
      "Epoch 190/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.9725 - val_loss: 51.3353\n",
      "Epoch 191/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 49.1079 - val_loss: 52.3054\n",
      "Epoch 192/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 49.7621 - val_loss: 49.4214\n",
      "Epoch 193/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 48.8015 - val_loss: 49.6126\n",
      "Epoch 194/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.1687 - val_loss: 50.0760\n",
      "Epoch 195/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.7098 - val_loss: 51.4055\n",
      "Epoch 196/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.5612 - val_loss: 49.0227\n",
      "Epoch 197/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 48.6505 - val_loss: 51.2695\n",
      "Epoch 198/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.9529 - val_loss: 51.9446\n",
      "Epoch 199/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.2796 - val_loss: 50.4281\n",
      "Epoch 200/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 48.0154\n",
      "Epoch 00200: saving model to saved_models/latent64/cp-0200.h5\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 48.0154 - val_loss: 47.8845\n",
      "Epoch 201/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 47.3907 - val_loss: 47.9672\n",
      "Epoch 202/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 46.9676 - val_loss: 47.1152\n",
      "Epoch 203/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 46.5897 - val_loss: 47.2423\n",
      "Epoch 204/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 45.9515 - val_loss: 47.5369\n",
      "Epoch 205/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 45.5189 - val_loss: 47.9653\n",
      "Epoch 206/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.9617 - val_loss: 46.6485\n",
      "Epoch 207/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.7689 - val_loss: 46.7511\n",
      "Epoch 208/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.8631 - val_loss: 47.1652\n",
      "Epoch 209/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 44.9976 - val_loss: 46.4030\n",
      "Epoch 210/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 44.7549 - val_loss: 45.2134\n",
      "Epoch 211/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.8000 - val_loss: 45.6502\n",
      "Epoch 212/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 44.7548 - val_loss: 46.6428\n",
      "Epoch 213/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 44.2946 - val_loss: 44.5518\n",
      "Epoch 214/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 43.6913 - val_loss: 44.3613\n",
      "Epoch 215/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 43.3417 - val_loss: 44.4905\n",
      "Epoch 216/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.7245 - val_loss: 45.3244\n",
      "Epoch 217/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.4138 - val_loss: 44.2364\n",
      "Epoch 218/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 43.0131 - val_loss: 44.8803\n",
      "Epoch 219/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.0376 - val_loss: 46.3391\n",
      "Epoch 220/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.3271 - val_loss: 47.5680\n",
      "Epoch 221/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.4721 - val_loss: 44.8030\n",
      "Epoch 222/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.0525 - val_loss: 45.1011\n",
      "Epoch 223/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 42.8556 - val_loss: 45.2878\n",
      "Epoch 224/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 43.2048 - val_loss: 44.3154\n",
      "Epoch 225/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 42.9178 - val_loss: 44.1364\n",
      "Epoch 226/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 42.2938 - val_loss: 43.7939\n",
      "Epoch 227/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 42.1635 - val_loss: 43.5869\n",
      "Epoch 228/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 42.6501 - val_loss: 42.5745\n",
      "Epoch 229/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 42.1467 - val_loss: 45.1319\n",
      "Epoch 230/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 42.8226 - val_loss: 44.6793\n",
      "Epoch 231/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 42.7694 - val_loss: 44.7042\n",
      "Epoch 232/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 42.0567 - val_loss: 43.7112\n",
      "Epoch 233/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 41.5846 - val_loss: 43.4654\n",
      "Epoch 234/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 41.1237 - val_loss: 43.4143\n",
      "Epoch 235/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 40.9251 - val_loss: 42.7816\n",
      "Epoch 236/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 40.9069 - val_loss: 42.4988\n",
      "Epoch 237/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.9094 - val_loss: 43.9782\n",
      "Epoch 238/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 41.1569 - val_loss: 43.0511\n",
      "Epoch 239/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 40.5711 - val_loss: 42.6073\n",
      "Epoch 240/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 40.2726 - val_loss: 42.6118\n",
      "Epoch 241/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.9258 - val_loss: 42.7414\n",
      "Epoch 242/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 40.5162 - val_loss: 44.4305\n",
      "Epoch 243/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 41.1676 - val_loss: 43.6668\n",
      "Epoch 244/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.7453 - val_loss: 41.6105\n",
      "Epoch 245/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.9936 - val_loss: 42.7762\n",
      "Epoch 246/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.2887 - val_loss: 41.5679\n",
      "Epoch 247/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.0564 - val_loss: 42.5102\n",
      "Epoch 248/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.3800 - val_loss: 41.7099\n",
      "Epoch 249/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 40.4891 - val_loss: 42.2729\n",
      "Epoch 250/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.2540 - val_loss: 42.9331\n",
      "Epoch 251/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 40.7276 - val_loss: 41.5778\n",
      "Epoch 252/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.2255 - val_loss: 41.6324\n",
      "Epoch 253/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 39.6451 - val_loss: 40.8238\n",
      "Epoch 254/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 39.3370 - val_loss: 41.4375\n",
      "Epoch 255/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 39.2118 - val_loss: 41.2260\n",
      "Epoch 256/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 39.1265 - val_loss: 41.5820\n",
      "Epoch 257/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.1131 - val_loss: 40.7852\n",
      "Epoch 258/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 39.2667 - val_loss: 40.5179\n",
      "Epoch 259/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 39.2076 - val_loss: 39.8260\n",
      "Epoch 260/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 38.8682 - val_loss: 41.2140\n",
      "Epoch 261/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 38.7701 - val_loss: 41.4215\n",
      "Epoch 262/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.0530 - val_loss: 41.1628\n",
      "Epoch 263/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 38.3903 - val_loss: 40.7106\n",
      "Epoch 264/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 38.2001 - val_loss: 40.3687\n",
      "Epoch 265/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.7745 - val_loss: 41.3997\n",
      "Epoch 266/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 38.6587 - val_loss: 40.4853\n",
      "Epoch 267/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.7426 - val_loss: 42.6237\n",
      "Epoch 268/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 39.2645 - val_loss: 40.4316\n",
      "Epoch 269/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 38.5825 - val_loss: 40.0501\n",
      "Epoch 270/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.4597 - val_loss: 40.3942\n",
      "Epoch 271/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.2756 - val_loss: 39.2489\n",
      "Epoch 272/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.3083 - val_loss: 41.0054\n",
      "Epoch 273/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 38.4668 - val_loss: 39.8464\n",
      "Epoch 274/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 37.6600 - val_loss: 40.1428\n",
      "Epoch 275/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.7311 - val_loss: 40.9665\n",
      "Epoch 276/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.7584 - val_loss: 40.2541\n",
      "Epoch 277/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.1568 - val_loss: 38.6903\n",
      "Epoch 278/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.1485 - val_loss: 40.5509\n",
      "Epoch 279/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.6656 - val_loss: 38.7268\n",
      "Epoch 280/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 37.3312 - val_loss: 37.4438\n",
      "Epoch 281/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.3413 - val_loss: 39.0864\n",
      "Epoch 282/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 37.1989 - val_loss: 39.4764\n",
      "Epoch 283/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 37.0118 - val_loss: 39.6856\n",
      "Epoch 284/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 37.0114 - val_loss: 38.7724\n",
      "Epoch 285/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.8862 - val_loss: 39.0810\n",
      "Epoch 286/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.0404 - val_loss: 39.0343\n",
      "Epoch 287/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.5600 - val_loss: 39.7862\n",
      "Epoch 288/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.0336 - val_loss: 38.7240\n",
      "Epoch 289/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 36.8183 - val_loss: 39.0971\n",
      "Epoch 290/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.2355 - val_loss: 39.9455\n",
      "Epoch 291/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.6592 - val_loss: 40.1008\n",
      "Epoch 292/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.3975 - val_loss: 41.7346\n",
      "Epoch 293/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.3917 - val_loss: 39.3118\n",
      "Epoch 294/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.1331 - val_loss: 38.5120\n",
      "Epoch 295/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.9278 - val_loss: 38.9580\n",
      "Epoch 296/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.1313 - val_loss: 40.5964\n",
      "Epoch 297/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.4480 - val_loss: 38.0092\n",
      "Epoch 298/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.8946 - val_loss: 38.6121\n",
      "Epoch 299/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 36.5092 - val_loss: 38.7975\n",
      "Epoch 300/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.8479 - val_loss: 38.9464\n",
      "Epoch 301/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.7677 - val_loss: 38.2688\n",
      "Epoch 302/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 36.3779 - val_loss: 38.2601\n",
      "Epoch 303/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 36.3589 - val_loss: 39.0784\n",
      "Epoch 304/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.9547 - val_loss: 37.9716\n",
      "Epoch 305/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.6621 - val_loss: 38.1722\n",
      "Epoch 306/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.7527 - val_loss: 38.9607\n",
      "Epoch 307/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.7426 - val_loss: 38.6560\n",
      "Epoch 308/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 36.1932 - val_loss: 37.0116\n",
      "Epoch 309/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.5805 - val_loss: 37.6966\n",
      "Epoch 310/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.5801 - val_loss: 37.7404\n",
      "Epoch 311/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.5101 - val_loss: 37.3203\n",
      "Epoch 312/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.7033 - val_loss: 38.1319\n",
      "Epoch 313/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.7509 - val_loss: 37.9671\n",
      "Epoch 314/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.1016 - val_loss: 38.7576\n",
      "Epoch 315/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.0696 - val_loss: 37.6984\n",
      "Epoch 316/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.0969 - val_loss: 36.7198\n",
      "Epoch 317/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.7489 - val_loss: 37.3449\n",
      "Epoch 318/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.5488 - val_loss: 38.4193\n",
      "Epoch 319/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.0686 - val_loss: 38.5460\n",
      "Epoch 320/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.8300 - val_loss: 38.6572\n",
      "Epoch 321/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.5806 - val_loss: 37.7757\n",
      "Epoch 322/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 35.4878 - val_loss: 37.4675\n",
      "Epoch 323/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.4156 - val_loss: 37.8265\n",
      "Epoch 324/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.1052 - val_loss: 37.0164\n",
      "Epoch 325/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.1679 - val_loss: 37.0631\n",
      "Epoch 326/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 34.9076 - val_loss: 36.7558\n",
      "Epoch 327/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.9399 - val_loss: 36.6652\n",
      "Epoch 328/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.8737 - val_loss: 37.5573\n",
      "Epoch 329/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.9616 - val_loss: 37.2407\n",
      "Epoch 330/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.5512 - val_loss: 37.1297\n",
      "Epoch 331/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.3209 - val_loss: 37.7728\n",
      "Epoch 332/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.2086 - val_loss: 37.5806\n",
      "Epoch 333/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.9442 - val_loss: 37.6212\n",
      "Epoch 334/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.1142 - val_loss: 38.2020\n",
      "Epoch 335/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.1317 - val_loss: 36.0890\n",
      "Epoch 336/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 34.6961 - val_loss: 37.3431\n",
      "Epoch 337/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.7946 - val_loss: 36.3273\n",
      "Epoch 338/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.7000 - val_loss: 36.6054\n",
      "Epoch 339/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.8781 - val_loss: 37.7123\n",
      "Epoch 340/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 34.5759 - val_loss: 36.1565\n",
      "Epoch 341/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.6824 - val_loss: 36.9439\n",
      "Epoch 342/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.7257 - val_loss: 36.9881\n",
      "Epoch 343/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.6535 - val_loss: 36.1766\n",
      "Epoch 344/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.5520 - val_loss: 36.2651\n",
      "Epoch 345/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.7980 - val_loss: 39.5619\n",
      "Epoch 346/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.4743 - val_loss: 39.0736\n",
      "Epoch 347/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.5897 - val_loss: 37.6092\n",
      "Epoch 348/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.5029 - val_loss: 36.1250\n",
      "Epoch 349/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.9240 - val_loss: 37.8198\n",
      "Epoch 350/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.1189 - val_loss: 35.8514\n",
      "Epoch 351/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 34.2140 - val_loss: 36.5475\n",
      "Epoch 352/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.3302 - val_loss: 35.6905\n",
      "Epoch 353/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 33.8651 - val_loss: 35.9389\n",
      "Epoch 354/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.0434 - val_loss: 35.0798\n",
      "Epoch 355/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.8667 - val_loss: 36.2669\n",
      "Epoch 356/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.2219 - val_loss: 36.2799\n",
      "Epoch 357/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.4160 - val_loss: 37.1240\n",
      "Epoch 358/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.3267 - val_loss: 36.0830\n",
      "Epoch 359/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.4163 - val_loss: 35.8958\n",
      "Epoch 360/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.1866 - val_loss: 36.5548\n",
      "Epoch 361/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.0347 - val_loss: 36.3205\n",
      "Epoch 362/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.8845 - val_loss: 34.8080\n",
      "Epoch 363/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 33.7127 - val_loss: 35.8870\n",
      "Epoch 364/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.5873 - val_loss: 34.6901\n",
      "Epoch 365/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.5619 - val_loss: 35.3192\n",
      "Epoch 366/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.7595 - val_loss: 35.5859\n",
      "Epoch 367/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.3701 - val_loss: 35.9903\n",
      "Epoch 368/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.4619 - val_loss: 35.1975\n",
      "Epoch 369/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.5434 - val_loss: 35.2802\n",
      "Epoch 370/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.4090 - val_loss: 36.5915\n",
      "Epoch 371/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.4573 - val_loss: 35.9181\n",
      "Epoch 372/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.7172 - val_loss: 35.9776\n",
      "Epoch 373/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.7118 - val_loss: 35.5448\n",
      "Epoch 374/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 33.2433 - val_loss: 36.2441\n",
      "Epoch 375/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.7710 - val_loss: 35.6670\n",
      "Epoch 376/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.5550 - val_loss: 35.2038\n",
      "Epoch 377/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.3431 - val_loss: 37.1274\n",
      "Epoch 378/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.6827 - val_loss: 35.3498\n",
      "Epoch 379/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 33.1128 - val_loss: 36.3755\n",
      "Epoch 380/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.2436 - val_loss: 35.1175\n",
      "Epoch 381/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.1348 - val_loss: 34.9936\n",
      "Epoch 382/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 32.8551 - val_loss: 34.8743\n",
      "Epoch 383/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.8612 - val_loss: 34.8825\n",
      "Epoch 384/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 32.7981 - val_loss: 34.5665\n",
      "Epoch 385/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.1356 - val_loss: 35.7509\n",
      "Epoch 386/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.4513 - val_loss: 35.4918\n",
      "Epoch 387/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.2736 - val_loss: 34.9052\n",
      "Epoch 388/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.2241 - val_loss: 36.4297\n",
      "Epoch 389/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.0915 - val_loss: 34.7078\n",
      "Epoch 390/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.2281 - val_loss: 36.6482\n",
      "Epoch 391/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.1982 - val_loss: 36.8162\n",
      "Epoch 392/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.6211 - val_loss: 34.8691\n",
      "Epoch 393/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.4844 - val_loss: 34.9835\n",
      "Epoch 394/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.0827 - val_loss: 34.5486\n",
      "Epoch 395/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.8204 - val_loss: 33.9957\n",
      "Epoch 396/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 32.6895 - val_loss: 34.8131\n",
      "Epoch 397/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.7939 - val_loss: 35.9599\n",
      "Epoch 398/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.0856 - val_loss: 35.5936\n",
      "Epoch 399/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.8328 - val_loss: 34.4082\n",
      "Epoch 400/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 32.5872\n",
      "Epoch 00400: saving model to saved_models/latent64/cp-0400.h5\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 32.5872 - val_loss: 34.7307\n",
      "Epoch 401/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.7293 - val_loss: 34.7429\n",
      "Epoch 402/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.7526 - val_loss: 35.1005\n",
      "Epoch 403/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.0641 - val_loss: 33.7682\n",
      "Epoch 404/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.7992 - val_loss: 34.6731\n",
      "Epoch 405/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.5899 - val_loss: 34.4213\n",
      "Epoch 406/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 32.5354 - val_loss: 34.5219\n",
      "Epoch 407/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.6275 - val_loss: 34.6447\n",
      "Epoch 408/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.5430 - val_loss: 34.4399\n",
      "Epoch 409/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.3707 - val_loss: 34.1014\n",
      "Epoch 410/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.5877 - val_loss: 35.4093\n",
      "Epoch 411/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.5843 - val_loss: 33.8282\n",
      "Epoch 412/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.5028 - val_loss: 33.7689\n",
      "Epoch 413/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.5538 - val_loss: 34.4474\n",
      "Epoch 414/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.5943 - val_loss: 33.8925\n",
      "Epoch 415/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.3436 - val_loss: 35.0282\n",
      "Epoch 416/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 31.9650 - val_loss: 34.8061\n",
      "Epoch 417/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 31.8734 - val_loss: 34.0422\n",
      "Epoch 418/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.2006 - val_loss: 33.3773\n",
      "Epoch 419/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 31.6799 - val_loss: 33.9376\n",
      "Epoch 420/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.0108 - val_loss: 34.2080\n",
      "Epoch 421/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.9964 - val_loss: 33.6058\n",
      "Epoch 422/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.9172 - val_loss: 34.3671\n",
      "Epoch 423/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.7669 - val_loss: 35.0735\n",
      "Epoch 424/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.4726 - val_loss: 35.4003\n",
      "Epoch 425/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.8286 - val_loss: 35.6549\n",
      "Epoch 426/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.0980 - val_loss: 35.1603\n",
      "Epoch 427/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.9068 - val_loss: 34.0378\n",
      "Epoch 428/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.4784 - val_loss: 33.6380\n",
      "Epoch 429/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.1112 - val_loss: 33.5598\n",
      "Epoch 430/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.7274 - val_loss: 34.2894\n",
      "Epoch 431/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.8805 - val_loss: 34.1899\n",
      "Epoch 432/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 31.6331 - val_loss: 34.4071\n",
      "Epoch 433/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.0462 - val_loss: 33.7254\n",
      "Epoch 434/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.6483 - val_loss: 34.0363\n",
      "Epoch 435/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.0370 - val_loss: 33.9502\n",
      "Epoch 436/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 31.4398 - val_loss: 34.1941\n",
      "Epoch 437/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.7871 - val_loss: 33.7395\n",
      "Epoch 438/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.4930 - val_loss: 32.9099\n",
      "Epoch 439/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.6258 - val_loss: 33.5507\n",
      "Epoch 440/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.4160 - val_loss: 33.9795\n",
      "Epoch 441/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.5355 - val_loss: 33.1316\n",
      "Epoch 442/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.9917 - val_loss: 34.9400\n",
      "Epoch 443/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.9876 - val_loss: 32.7766\n",
      "Epoch 444/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.7182 - val_loss: 34.0961\n",
      "Epoch 445/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.9040 - val_loss: 33.0235\n",
      "Epoch 446/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.7976 - val_loss: 33.9839\n",
      "Epoch 447/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.3122 - val_loss: 33.4069\n",
      "Epoch 448/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.5085 - val_loss: 33.4171\n",
      "Epoch 449/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.3494 - val_loss: 33.0943\n",
      "Epoch 450/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 31.1996 - val_loss: 32.5449\n",
      "Epoch 451/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 31.0661 - val_loss: 32.9288\n",
      "Epoch 452/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.2559 - val_loss: 32.0395\n",
      "Epoch 453/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.2304 - val_loss: 33.0996\n",
      "Epoch 454/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.2105 - val_loss: 33.1822\n",
      "Epoch 455/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.9877 - val_loss: 32.4744\n",
      "Epoch 456/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 30.8610 - val_loss: 33.2110\n",
      "Epoch 457/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.1935 - val_loss: 33.2456\n",
      "Epoch 458/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.2531 - val_loss: 33.7691\n",
      "Epoch 459/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.7437 - val_loss: 32.8086\n",
      "Epoch 460/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.4197 - val_loss: 33.5638\n",
      "Epoch 461/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.1719 - val_loss: 33.0739\n",
      "Epoch 462/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.5860 - val_loss: 33.7586\n",
      "Epoch 463/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.4060 - val_loss: 33.2558\n",
      "Epoch 464/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.5541 - val_loss: 33.9796\n",
      "Epoch 465/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.2031 - val_loss: 33.2411\n",
      "Epoch 466/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.1340 - val_loss: 33.9552\n",
      "Epoch 467/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.4589 - val_loss: 32.9908\n",
      "Epoch 468/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.4577 - val_loss: 33.1669\n",
      "Epoch 469/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.5483 - val_loss: 35.1697\n",
      "Epoch 470/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.5647 - val_loss: 33.5957\n",
      "Epoch 471/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.2239 - val_loss: 34.5247\n",
      "Epoch 472/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.1682 - val_loss: 33.4987\n",
      "Epoch 473/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.1860 - val_loss: 33.7025\n",
      "Epoch 474/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.1416 - val_loss: 32.1358\n",
      "Epoch 475/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.9802 - val_loss: 33.0413\n",
      "Epoch 476/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.0236 - val_loss: 32.8879\n",
      "Epoch 477/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.1486 - val_loss: 33.7955\n",
      "Epoch 478/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.4238 - val_loss: 33.9593\n",
      "Epoch 479/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.6717 - val_loss: 33.2123\n",
      "Epoch 480/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.3341 - val_loss: 33.1444\n",
      "Epoch 481/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.1220 - val_loss: 33.2943\n",
      "Epoch 482/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.0855 - val_loss: 33.8717\n",
      "Epoch 483/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.1702 - val_loss: 33.8237\n",
      "Epoch 484/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.7630 - val_loss: 33.8837\n",
      "Epoch 485/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.8641 - val_loss: 32.4205\n",
      "Epoch 486/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.7345 - val_loss: 32.6636\n",
      "Epoch 487/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 30.4161 - val_loss: 32.7241\n",
      "Epoch 488/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.6595 - val_loss: 33.2605\n",
      "Epoch 489/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.6581 - val_loss: 32.2106\n",
      "Epoch 490/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 30.3893 - val_loss: 31.8482\n",
      "Epoch 491/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 30.3398 - val_loss: 31.7835\n",
      "Epoch 492/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.5759 - val_loss: 32.9685\n",
      "Epoch 493/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.4891 - val_loss: 32.7810\n",
      "Epoch 494/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.5716 - val_loss: 33.7338\n",
      "Epoch 495/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.9083 - val_loss: 32.9503\n",
      "Epoch 496/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.6327 - val_loss: 31.9689\n",
      "Epoch 497/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.4636 - val_loss: 32.7301\n",
      "Epoch 498/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.2703 - val_loss: 32.8606\n",
      "Epoch 499/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.3899 - val_loss: 33.0673\n",
      "Epoch 500/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.5178 - val_loss: 32.3834\n",
      "Epoch 501/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.4806 - val_loss: 32.7366\n",
      "Epoch 502/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.4408 - val_loss: 32.9299\n",
      "Epoch 503/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.4356 - val_loss: 32.6865\n",
      "Epoch 504/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.2577 - val_loss: 32.4449\n",
      "Epoch 505/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.4732 - val_loss: 31.9462\n",
      "Epoch 506/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 30.1951 - val_loss: 32.0206\n",
      "Epoch 507/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.4610 - val_loss: 31.9311\n",
      "Epoch 508/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.2710 - val_loss: 32.6019\n",
      "Epoch 509/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.6670 - val_loss: 32.8277\n",
      "Epoch 510/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.5617 - val_loss: 32.2624\n",
      "Epoch 511/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.7286 - val_loss: 32.7285\n",
      "Epoch 512/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.3805 - val_loss: 33.0534\n",
      "Epoch 513/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.7001 - val_loss: 32.6441\n",
      "Epoch 514/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.3434 - val_loss: 32.3506\n",
      "Epoch 515/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.5830 - val_loss: 32.0543\n",
      "Epoch 516/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.2924 - val_loss: 32.9959\n",
      "Epoch 517/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.4137 - val_loss: 32.6180\n",
      "Epoch 518/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.3665 - val_loss: 32.4696\n",
      "Epoch 519/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.5566 - val_loss: 32.0480\n",
      "Epoch 520/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.2340 - val_loss: 32.0551\n",
      "Epoch 521/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.5501 - val_loss: 33.6138\n",
      "Epoch 522/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.9940 - val_loss: 33.8237\n",
      "Epoch 523/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.8023 - val_loss: 33.2256\n",
      "Epoch 524/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.5558 - val_loss: 32.3655\n",
      "Epoch 525/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.1024 - val_loss: 31.1841\n",
      "Epoch 526/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.2119 - val_loss: 32.3006\n",
      "Epoch 527/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 30.0397 - val_loss: 31.4083\n",
      "Epoch 528/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.0522 - val_loss: 31.4980\n",
      "Epoch 529/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 29.8571 - val_loss: 32.3272\n",
      "Epoch 530/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.1879 - val_loss: 31.9724\n",
      "Epoch 531/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.9433 - val_loss: 32.4702\n",
      "Epoch 532/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.8889 - val_loss: 32.1611\n",
      "Epoch 533/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.2529 - val_loss: 32.3591\n",
      "Epoch 534/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.1283 - val_loss: 31.9248\n",
      "Epoch 535/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.3067 - val_loss: 32.6309\n",
      "Epoch 536/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.3078 - val_loss: 32.2966\n",
      "Epoch 537/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.0560 - val_loss: 33.2265\n",
      "Epoch 538/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.3470 - val_loss: 32.1408\n",
      "Epoch 539/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.1066 - val_loss: 32.9696\n",
      "Epoch 540/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.3038 - val_loss: 32.4149\n",
      "Epoch 541/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.9162 - val_loss: 32.3414\n",
      "Epoch 542/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.0424 - val_loss: 31.6783\n",
      "Epoch 543/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.9072 - val_loss: 31.7640\n",
      "Epoch 544/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.0856 - val_loss: 32.4039\n",
      "Epoch 545/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.0287 - val_loss: 31.3696\n",
      "Epoch 546/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 29.6911 - val_loss: 32.4015\n",
      "Epoch 547/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.9204 - val_loss: 31.8468\n",
      "Epoch 548/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.2975 - val_loss: 32.7407\n",
      "Epoch 549/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.8430 - val_loss: 32.3267\n",
      "Epoch 550/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.1062 - val_loss: 32.1197\n",
      "Epoch 551/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.9774 - val_loss: 32.3218\n",
      "Epoch 552/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.5420 - val_loss: 32.3342\n",
      "Epoch 553/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.1359 - val_loss: 31.9589\n",
      "Epoch 554/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.9447 - val_loss: 32.1589\n",
      "Epoch 555/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.9759 - val_loss: 31.2084\n",
      "Epoch 556/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 29.6716 - val_loss: 31.2255\n",
      "Epoch 557/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.7875 - val_loss: 31.9293\n",
      "Epoch 558/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.7637 - val_loss: 32.0588\n",
      "Epoch 559/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.6795 - val_loss: 32.2654\n",
      "Epoch 560/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.9734 - val_loss: 32.8391\n",
      "Epoch 561/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.0335 - val_loss: 32.0479\n",
      "Epoch 562/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.8443 - val_loss: 31.4560\n",
      "Epoch 563/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.8837 - val_loss: 31.4529\n",
      "Epoch 564/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.0325 - val_loss: 32.0122\n",
      "Epoch 565/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.0429 - val_loss: 32.0662\n",
      "Epoch 566/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.2488 - val_loss: 32.2433\n",
      "Epoch 567/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.9310 - val_loss: 31.6924\n",
      "Epoch 568/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.4010 - val_loss: 31.5851\n",
      "Epoch 569/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.4497 - val_loss: 31.8395\n",
      "Epoch 570/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.5703 - val_loss: 31.1615\n",
      "Epoch 571/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.4126 - val_loss: 30.9941\n",
      "Epoch 572/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.7809 - val_loss: 31.4532\n",
      "Epoch 573/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 29.3903 - val_loss: 32.3811\n",
      "Epoch 574/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.5273 - val_loss: 31.8391\n",
      "Epoch 575/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.4219 - val_loss: 31.1796\n",
      "Epoch 576/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.5931 - val_loss: 31.7759\n",
      "Epoch 577/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.6106 - val_loss: 31.0215\n",
      "Epoch 578/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.5839 - val_loss: 31.2973\n",
      "Epoch 579/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.3152 - val_loss: 32.2106\n",
      "Epoch 580/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.4128 - val_loss: 32.8350\n",
      "Epoch 581/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.0336 - val_loss: 33.3768\n",
      "Epoch 582/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.8730 - val_loss: 32.1651\n",
      "Epoch 583/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.9073 - val_loss: 32.2650\n",
      "Epoch 584/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.9012 - val_loss: 31.3613\n",
      "Epoch 585/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.5762 - val_loss: 31.2393\n",
      "Epoch 586/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.8203 - val_loss: 32.3233\n",
      "Epoch 587/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.1136 - val_loss: 31.5807\n",
      "Epoch 588/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.8509 - val_loss: 31.4982\n",
      "Epoch 589/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.7205 - val_loss: 30.9308\n",
      "Epoch 590/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.4303 - val_loss: 31.9893\n",
      "Epoch 591/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 29.3127 - val_loss: 31.4960\n",
      "Epoch 592/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 29.2760 - val_loss: 32.2867\n",
      "Epoch 593/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.6460 - val_loss: 33.5853\n",
      "Epoch 594/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.0253 - val_loss: 32.6392\n",
      "Epoch 595/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.7367 - val_loss: 32.5042\n",
      "Epoch 596/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.8809 - val_loss: 32.1186\n",
      "Epoch 597/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.6254 - val_loss: 31.5408\n",
      "Epoch 598/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3963 - val_loss: 31.2757\n",
      "Epoch 599/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2767 - val_loss: 31.3771\n",
      "Epoch 600/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 29.1133\n",
      "Epoch 00600: saving model to saved_models/latent64/cp-0600.h5\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 29.1133 - val_loss: 31.4195\n",
      "Epoch 601/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2870 - val_loss: 32.1044\n",
      "Epoch 602/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3996 - val_loss: 32.0196\n",
      "Epoch 603/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2395 - val_loss: 31.9365\n",
      "Epoch 604/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.5778 - val_loss: 31.5137\n",
      "Epoch 605/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.5237 - val_loss: 31.0122\n",
      "Epoch 606/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.5122 - val_loss: 31.0779\n",
      "Epoch 607/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3976 - val_loss: 31.0074\n",
      "Epoch 608/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2210 - val_loss: 31.9007\n",
      "Epoch 609/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.5567 - val_loss: 32.2683\n",
      "Epoch 610/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 29.5186 - val_loss: 31.5100\n",
      "Epoch 611/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.1214 - val_loss: 31.0471\n",
      "Epoch 612/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2962 - val_loss: 30.6696\n",
      "Epoch 613/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.7061 - val_loss: 31.2951\n",
      "Epoch 614/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.2297 - val_loss: 30.8685\n",
      "Epoch 615/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.1871 - val_loss: 31.1786\n",
      "Epoch 616/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 29.0970 - val_loss: 31.0429\n",
      "Epoch 617/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2353 - val_loss: 31.3846\n",
      "Epoch 618/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.3806 - val_loss: 31.6687\n",
      "Epoch 619/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.0446 - val_loss: 31.6030\n",
      "Epoch 620/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.2105 - val_loss: 31.2192\n",
      "Epoch 621/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.0555 - val_loss: 31.0029\n",
      "Epoch 622/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.1934 - val_loss: 31.0234\n",
      "Epoch 623/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.9706 - val_loss: 31.0035\n",
      "Epoch 624/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9764 - val_loss: 31.4016\n",
      "Epoch 625/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.1213 - val_loss: 32.0611\n",
      "Epoch 626/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.9162 - val_loss: 31.2177\n",
      "Epoch 627/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9932 - val_loss: 31.5835\n",
      "Epoch 628/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.4237 - val_loss: 31.3520\n",
      "Epoch 629/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.1338 - val_loss: 31.1510\n",
      "Epoch 630/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9985 - val_loss: 32.3784\n",
      "Epoch 631/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.3459 - val_loss: 32.3299\n",
      "Epoch 632/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.0050 - val_loss: 30.1939\n",
      "Epoch 633/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.9529 - val_loss: 31.0288\n",
      "Epoch 634/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2205 - val_loss: 32.1473\n",
      "Epoch 635/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.7280 - val_loss: 32.6420\n",
      "Epoch 636/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.7884 - val_loss: 29.9242\n",
      "Epoch 637/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.2806 - val_loss: 32.2382\n",
      "Epoch 638/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3389 - val_loss: 31.2941\n",
      "Epoch 639/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.1506 - val_loss: 32.1107\n",
      "Epoch 640/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.0390 - val_loss: 31.7568\n",
      "Epoch 641/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2189 - val_loss: 31.9654\n",
      "Epoch 642/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.1270 - val_loss: 30.7294\n",
      "Epoch 643/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 28.6287 - val_loss: 30.6842\n",
      "Epoch 644/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.6588 - val_loss: 32.0717\n",
      "Epoch 645/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.1583 - val_loss: 31.5840\n",
      "Epoch 646/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2255 - val_loss: 31.1438\n",
      "Epoch 647/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.0093 - val_loss: 31.1785\n",
      "Epoch 648/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.1511 - val_loss: 31.5554\n",
      "Epoch 649/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7895 - val_loss: 31.4175\n",
      "Epoch 650/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9741 - val_loss: 30.8762\n",
      "Epoch 651/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2348 - val_loss: 31.3155\n",
      "Epoch 652/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3011 - val_loss: 31.6681\n",
      "Epoch 653/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.1637 - val_loss: 31.6337\n",
      "Epoch 654/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.1116 - val_loss: 30.5957\n",
      "Epoch 655/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2174 - val_loss: 30.9932\n",
      "Epoch 656/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9618 - val_loss: 30.2319\n",
      "Epoch 657/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7092 - val_loss: 30.1014\n",
      "Epoch 658/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.5946 - val_loss: 30.9529\n",
      "Epoch 659/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.5758 - val_loss: 30.0617\n",
      "Epoch 660/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.5962 - val_loss: 30.6187\n",
      "Epoch 661/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.4372 - val_loss: 31.4357\n",
      "Epoch 662/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.6208 - val_loss: 30.7656\n",
      "Epoch 663/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9236 - val_loss: 31.6025\n",
      "Epoch 664/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9220 - val_loss: 30.6311\n",
      "Epoch 665/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9252 - val_loss: 30.7283\n",
      "Epoch 666/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.4977 - val_loss: 30.3743\n",
      "Epoch 667/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 28.3296 - val_loss: 30.4883\n",
      "Epoch 668/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.2694 - val_loss: 31.1125\n",
      "Epoch 669/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.8303 - val_loss: 30.3560\n",
      "Epoch 670/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.6647 - val_loss: 30.4094\n",
      "Epoch 671/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.6177 - val_loss: 30.6054\n",
      "Epoch 672/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.8710 - val_loss: 31.6208\n",
      "Epoch 673/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9886 - val_loss: 31.3736\n",
      "Epoch 674/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.5273 - val_loss: 31.4447\n",
      "Epoch 675/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7782 - val_loss: 30.8676\n",
      "Epoch 676/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.7021 - val_loss: 30.5346\n",
      "Epoch 677/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.0757 - val_loss: 30.9133\n",
      "Epoch 678/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7925 - val_loss: 30.7606\n",
      "Epoch 679/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.6668 - val_loss: 30.9741\n",
      "Epoch 680/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7042 - val_loss: 30.9264\n",
      "Epoch 681/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.5990 - val_loss: 30.8819\n",
      "Epoch 682/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.4877 - val_loss: 30.6884\n",
      "Epoch 683/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.4518 - val_loss: 31.0675\n",
      "Epoch 684/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.8068 - val_loss: 31.6769\n",
      "Epoch 685/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.5161 - val_loss: 30.6341\n",
      "Epoch 686/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.5911 - val_loss: 30.8176\n",
      "Epoch 687/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.8450 - val_loss: 30.5479\n",
      "Epoch 688/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.7903 - val_loss: 30.5469\n",
      "Epoch 689/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.6898 - val_loss: 31.3686\n",
      "Epoch 690/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.3871 - val_loss: 30.7565\n",
      "Epoch 691/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.3947 - val_loss: 30.4827\n",
      "Epoch 692/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.5216 - val_loss: 30.2131\n",
      "Epoch 693/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.7588 - val_loss: 31.4004\n",
      "Epoch 694/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.4213 - val_loss: 29.9300\n",
      "Epoch 695/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.5605 - val_loss: 30.2703\n",
      "Epoch 696/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.3107 - val_loss: 30.2805\n",
      "Epoch 697/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.3151 - val_loss: 30.0468\n",
      "Epoch 698/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 28.1757 - val_loss: 30.9489\n",
      "Epoch 699/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.5360 - val_loss: 30.4899\n",
      "Epoch 700/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7523 - val_loss: 30.6610\n",
      "Epoch 701/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.5584 - val_loss: 30.7249\n",
      "Epoch 702/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.4254 - val_loss: 30.7487\n",
      "Epoch 703/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.5902 - val_loss: 31.3829\n",
      "Epoch 704/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7542 - val_loss: 30.8120\n",
      "Epoch 705/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.6630 - val_loss: 30.6178\n",
      "Epoch 706/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.6605 - val_loss: 30.7309\n",
      "Epoch 707/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.3928 - val_loss: 30.9145\n",
      "Epoch 708/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.6195 - val_loss: 31.5343\n",
      "Epoch 709/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7404 - val_loss: 30.5959\n",
      "Epoch 710/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.7461 - val_loss: 30.5989\n",
      "Epoch 711/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.6718 - val_loss: 30.2174\n",
      "Epoch 712/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.4228 - val_loss: 31.3972\n",
      "Epoch 713/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.5138 - val_loss: 31.1879\n",
      "Epoch 714/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.4403 - val_loss: 31.2426\n",
      "Epoch 715/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.3806 - val_loss: 30.6545\n",
      "Epoch 716/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7434 - val_loss: 31.0747\n",
      "Epoch 717/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 28.5728 - val_loss: 30.3695\n",
      "Epoch 718/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.4218 - val_loss: 30.5586\n",
      "Epoch 719/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.3537 - val_loss: 30.7922\n",
      "Epoch 720/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.5204 - val_loss: 31.3141\n",
      "Epoch 721/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.4800 - val_loss: 31.2403\n",
      "Epoch 722/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.6317 - val_loss: 30.8616\n",
      "Epoch 723/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.4404 - val_loss: 30.2506\n",
      "Epoch 724/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.4604 - val_loss: 30.6899\n",
      "Epoch 725/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.3201 - val_loss: 31.1350\n",
      "Epoch 726/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.5705 - val_loss: 31.5176\n",
      "Epoch 727/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.3304 - val_loss: 31.2192\n",
      "Epoch 728/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.9639 - val_loss: 31.1963\n",
      "Epoch 729/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.7218 - val_loss: 30.7677\n",
      "Epoch 730/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.7291 - val_loss: 31.2418\n",
      "Epoch 731/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.6878 - val_loss: 31.4204\n",
      "Epoch 732/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.4885 - val_loss: 30.5239\n",
      "Epoch 733/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.5930 - val_loss: 30.7176\n",
      "Epoch 734/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2637 - val_loss: 30.0812\n",
      "Epoch 735/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1574 - val_loss: 30.9751\n",
      "Epoch 736/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 28.4985 - val_loss: 30.7167\n",
      "Epoch 737/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.2317 - val_loss: 30.0516\n",
      "Epoch 738/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0581 - val_loss: 29.8426\n",
      "Epoch 739/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.8686 - val_loss: 30.0037\n",
      "Epoch 740/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1069 - val_loss: 31.1997\n",
      "Epoch 741/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.3635 - val_loss: 32.2949\n",
      "Epoch 742/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.3027 - val_loss: 31.1108\n",
      "Epoch 743/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.3579 - val_loss: 31.6256\n",
      "Epoch 744/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.1247 - val_loss: 30.7245\n",
      "Epoch 745/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.5617 - val_loss: 32.2436\n",
      "Epoch 746/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.5226 - val_loss: 30.6317\n",
      "Epoch 747/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.0744 - val_loss: 30.7754\n",
      "Epoch 748/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.0407 - val_loss: 31.3488\n",
      "Epoch 749/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1618 - val_loss: 30.5136\n",
      "Epoch 750/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2571 - val_loss: 30.1842\n",
      "Epoch 751/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1144 - val_loss: 29.6809\n",
      "Epoch 752/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9116 - val_loss: 30.7624\n",
      "Epoch 753/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2658 - val_loss: 29.8836\n",
      "Epoch 754/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.4424 - val_loss: 30.5161\n",
      "Epoch 755/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.0708 - val_loss: 29.7379\n",
      "Epoch 756/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.9937 - val_loss: 30.0438\n",
      "Epoch 757/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9088 - val_loss: 29.4655\n",
      "Epoch 758/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.7094 - val_loss: 30.4628\n",
      "Epoch 759/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7988 - val_loss: 30.4550\n",
      "Epoch 760/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.6778 - val_loss: 29.8194\n",
      "Epoch 761/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.6135 - val_loss: 30.1929\n",
      "Epoch 762/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9826 - val_loss: 30.2386\n",
      "Epoch 763/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.9724 - val_loss: 30.9290\n",
      "Epoch 764/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.1617 - val_loss: 29.9354\n",
      "Epoch 765/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8314 - val_loss: 29.9716\n",
      "Epoch 766/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8468 - val_loss: 30.0937\n",
      "Epoch 767/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7971 - val_loss: 30.5070\n",
      "Epoch 768/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8751 - val_loss: 30.3086\n",
      "Epoch 769/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9773 - val_loss: 30.3401\n",
      "Epoch 770/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6625 - val_loss: 30.0974\n",
      "Epoch 771/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7598 - val_loss: 30.3141\n",
      "Epoch 772/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.8714 - val_loss: 31.8617\n",
      "Epoch 773/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2880 - val_loss: 32.7800\n",
      "Epoch 774/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.4474 - val_loss: 31.2963\n",
      "Epoch 775/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.3246 - val_loss: 31.5840\n",
      "Epoch 776/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.8318 - val_loss: 32.9184\n",
      "Epoch 777/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.8667 - val_loss: 32.3617\n",
      "Epoch 778/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.9986 - val_loss: 29.9370\n",
      "Epoch 779/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2562 - val_loss: 30.1737\n",
      "Epoch 780/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.4137 - val_loss: 30.1641\n",
      "Epoch 781/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2213 - val_loss: 29.8384\n",
      "Epoch 782/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9243 - val_loss: 30.3752\n",
      "Epoch 783/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0171 - val_loss: 29.7130\n",
      "Epoch 784/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0142 - val_loss: 29.6861\n",
      "Epoch 785/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0678 - val_loss: 30.3065\n",
      "Epoch 786/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2369 - val_loss: 30.5825\n",
      "Epoch 787/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6983 - val_loss: 30.8083\n",
      "Epoch 788/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.6050 - val_loss: 28.9076\n",
      "Epoch 789/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.2733 - val_loss: 29.7535\n",
      "Epoch 790/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5323 - val_loss: 30.6218\n",
      "Epoch 791/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.6241 - val_loss: 31.1687\n",
      "Epoch 792/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0865 - val_loss: 31.2049\n",
      "Epoch 793/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0416 - val_loss: 30.5704\n",
      "Epoch 794/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0251 - val_loss: 29.7281\n",
      "Epoch 795/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.2487 - val_loss: 29.5120\n",
      "Epoch 796/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0114 - val_loss: 29.9043\n",
      "Epoch 797/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6836 - val_loss: 29.8965\n",
      "Epoch 798/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6022 - val_loss: 30.5200\n",
      "Epoch 799/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5665 - val_loss: 30.5008\n",
      "Epoch 800/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 27.6548\n",
      "Epoch 00800: saving model to saved_models/latent64/cp-0800.h5\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 27.6548 - val_loss: 30.6747\n",
      "Epoch 801/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6367 - val_loss: 29.5284\n",
      "Epoch 802/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7829 - val_loss: 30.5028\n",
      "Epoch 803/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.6241 - val_loss: 29.8082\n",
      "Epoch 804/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7539 - val_loss: 29.5291\n",
      "Epoch 805/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.5894 - val_loss: 30.1564\n",
      "Epoch 806/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.5804 - val_loss: 29.6098\n",
      "Epoch 807/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.7819 - val_loss: 29.0492\n",
      "Epoch 808/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.9899 - val_loss: 30.8319\n",
      "Epoch 809/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7457 - val_loss: 29.6200\n",
      "Epoch 810/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4131 - val_loss: 29.8188\n",
      "Epoch 811/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4641 - val_loss: 29.5849\n",
      "Epoch 812/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.5005 - val_loss: 29.6158\n",
      "Epoch 813/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4762 - val_loss: 29.3844\n",
      "Epoch 814/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.3105 - val_loss: 30.0828\n",
      "Epoch 815/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.8152 - val_loss: 30.8261\n",
      "Epoch 816/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8382 - val_loss: 30.1196\n",
      "Epoch 817/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.9258 - val_loss: 30.5126\n",
      "Epoch 818/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7906 - val_loss: 30.1033\n",
      "Epoch 819/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7229 - val_loss: 29.7599\n",
      "Epoch 820/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6719 - val_loss: 29.6989\n",
      "Epoch 821/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4800 - val_loss: 30.2179\n",
      "Epoch 822/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4011 - val_loss: 30.4327\n",
      "Epoch 823/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7693 - val_loss: 29.3251\n",
      "Epoch 824/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3505 - val_loss: 30.5760\n",
      "Epoch 825/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6498 - val_loss: 29.4131\n",
      "Epoch 826/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7857 - val_loss: 30.1265\n",
      "Epoch 827/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.8330 - val_loss: 30.1251\n",
      "Epoch 828/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9871 - val_loss: 30.0609\n",
      "Epoch 829/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5481 - val_loss: 30.3937\n",
      "Epoch 830/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7370 - val_loss: 29.9202\n",
      "Epoch 831/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7315 - val_loss: 29.7405\n",
      "Epoch 832/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9155 - val_loss: 30.2995\n",
      "Epoch 833/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1962 - val_loss: 30.9223\n",
      "Epoch 834/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9835 - val_loss: 30.5270\n",
      "Epoch 835/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8498 - val_loss: 30.7397\n",
      "Epoch 836/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.8294 - val_loss: 29.2760\n",
      "Epoch 837/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6467 - val_loss: 29.1945\n",
      "Epoch 838/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4560 - val_loss: 29.4594\n",
      "Epoch 839/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8615 - val_loss: 29.7497\n",
      "Epoch 840/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8152 - val_loss: 29.8382\n",
      "Epoch 841/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3921 - val_loss: 29.6430\n",
      "Epoch 842/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2873 - val_loss: 30.5084\n",
      "Epoch 843/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7694 - val_loss: 30.1131\n",
      "Epoch 844/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4263 - val_loss: 29.4770\n",
      "Epoch 845/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2835 - val_loss: 29.4234\n",
      "Epoch 846/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.2366 - val_loss: 29.3776\n",
      "Epoch 847/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 27.1109 - val_loss: 29.4673\n",
      "Epoch 848/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2343 - val_loss: 29.2631\n",
      "Epoch 849/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.2325 - val_loss: 29.0400\n",
      "Epoch 850/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1150 - val_loss: 29.7633\n",
      "Epoch 851/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2945 - val_loss: 30.0127\n",
      "Epoch 852/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3799 - val_loss: 29.2960\n",
      "Epoch 853/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0016 - val_loss: 29.5532\n",
      "Epoch 854/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4684 - val_loss: 28.9634\n",
      "Epoch 855/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2949 - val_loss: 29.5729\n",
      "Epoch 856/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3735 - val_loss: 29.2594\n",
      "Epoch 857/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3270 - val_loss: 28.6962\n",
      "Epoch 858/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2736 - val_loss: 29.6156\n",
      "Epoch 859/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1205 - val_loss: 29.0637\n",
      "Epoch 860/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1486 - val_loss: 29.5721\n",
      "Epoch 861/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3434 - val_loss: 29.7304\n",
      "Epoch 862/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5397 - val_loss: 30.1437\n",
      "Epoch 863/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5925 - val_loss: 29.7559\n",
      "Epoch 864/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5676 - val_loss: 29.3579\n",
      "Epoch 865/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5394 - val_loss: 30.1434\n",
      "Epoch 866/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8091 - val_loss: 29.9937\n",
      "Epoch 867/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3800 - val_loss: 29.2656\n",
      "Epoch 868/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.2862 - val_loss: 29.5085\n",
      "Epoch 869/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1345 - val_loss: 29.9309\n",
      "Epoch 870/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3661 - val_loss: 30.2823\n",
      "Epoch 871/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7194 - val_loss: 30.0634\n",
      "Epoch 872/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8251 - val_loss: 28.9658\n",
      "Epoch 873/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.5139 - val_loss: 29.2159\n",
      "Epoch 874/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6610 - val_loss: 29.4250\n",
      "Epoch 875/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7095 - val_loss: 30.0009\n",
      "Epoch 876/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.7491 - val_loss: 29.2552\n",
      "Epoch 877/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5274 - val_loss: 30.4082\n",
      "Epoch 878/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1747 - val_loss: 28.8964\n",
      "Epoch 879/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.0952 - val_loss: 29.6595\n",
      "Epoch 880/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.0815 - val_loss: 29.0101\n",
      "Epoch 881/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.3527 - val_loss: 29.4774\n",
      "Epoch 882/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.6443 - val_loss: 30.2930\n",
      "Epoch 883/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6592 - val_loss: 30.2964\n",
      "Epoch 884/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7787 - val_loss: 29.7747\n",
      "Epoch 885/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6168 - val_loss: 29.9695\n",
      "Epoch 886/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4772 - val_loss: 29.3057\n",
      "Epoch 887/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4738 - val_loss: 29.2028\n",
      "Epoch 888/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7625 - val_loss: 31.7267\n",
      "Epoch 889/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0043 - val_loss: 30.5645\n",
      "Epoch 890/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.5817 - val_loss: 30.2711\n",
      "Epoch 891/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.0811 - val_loss: 29.5552\n",
      "Epoch 892/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1140 - val_loss: 29.3977\n",
      "Epoch 893/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.0688 - val_loss: 30.2147\n",
      "Epoch 894/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2776 - val_loss: 29.2113\n",
      "Epoch 895/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.9300 - val_loss: 29.7481\n",
      "Epoch 896/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.0640 - val_loss: 29.1596\n",
      "Epoch 897/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1612 - val_loss: 28.9363\n",
      "Epoch 898/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1095 - val_loss: 28.8847\n",
      "Epoch 899/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3565 - val_loss: 29.3783\n",
      "Epoch 900/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4815 - val_loss: 29.0654\n",
      "Epoch 901/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1768 - val_loss: 28.8540\n",
      "Epoch 902/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.2487 - val_loss: 28.4731\n",
      "Epoch 903/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4181 - val_loss: 29.4936\n",
      "Epoch 904/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5398 - val_loss: 29.4370\n",
      "Epoch 905/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2480 - val_loss: 30.1855\n",
      "Epoch 906/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4339 - val_loss: 29.0942\n",
      "Epoch 907/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.3057 - val_loss: 29.8190\n",
      "Epoch 908/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1496 - val_loss: 29.1126\n",
      "Epoch 909/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.3542 - val_loss: 29.2608\n",
      "Epoch 910/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.2339 - val_loss: 30.1216\n",
      "Epoch 911/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4641 - val_loss: 29.7237\n",
      "Epoch 912/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.2300 - val_loss: 29.0512\n",
      "Epoch 913/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1632 - val_loss: 29.3212\n",
      "Epoch 914/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9683 - val_loss: 29.1321\n",
      "Epoch 915/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9964 - val_loss: 29.2547\n",
      "Epoch 916/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0721 - val_loss: 29.8257\n",
      "Epoch 917/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4022 - val_loss: 29.1792\n",
      "Epoch 918/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1957 - val_loss: 29.2078\n",
      "Epoch 919/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.8975 - val_loss: 29.6339\n",
      "Epoch 920/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1074 - val_loss: 30.0103\n",
      "Epoch 921/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4445 - val_loss: 29.8417\n",
      "Epoch 922/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3498 - val_loss: 29.2341\n",
      "Epoch 923/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2372 - val_loss: 30.6325\n",
      "Epoch 924/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3401 - val_loss: 30.9925\n",
      "Epoch 925/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5721 - val_loss: 31.6974\n",
      "Epoch 926/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1656 - val_loss: 28.2459\n",
      "Epoch 927/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.8225 - val_loss: 29.0590\n",
      "Epoch 928/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9282 - val_loss: 29.3774\n",
      "Epoch 929/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1498 - val_loss: 29.0088\n",
      "Epoch 930/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1466 - val_loss: 29.3479\n",
      "Epoch 931/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9645 - val_loss: 28.7944\n",
      "Epoch 932/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7646 - val_loss: 29.1405\n",
      "Epoch 933/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.7201 - val_loss: 29.0114\n",
      "Epoch 934/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.8875 - val_loss: 29.0775\n",
      "Epoch 935/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.0680 - val_loss: 28.9347\n",
      "Epoch 936/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1976 - val_loss: 29.0921\n",
      "Epoch 937/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1248 - val_loss: 29.7352\n",
      "Epoch 938/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1788 - val_loss: 29.3760\n",
      "Epoch 939/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0252 - val_loss: 29.3246\n",
      "Epoch 940/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.0402 - val_loss: 29.4074\n",
      "Epoch 941/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.0389 - val_loss: 28.7434\n",
      "Epoch 942/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.0853 - val_loss: 29.1361\n",
      "Epoch 943/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0005 - val_loss: 28.7417\n",
      "Epoch 944/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8739 - val_loss: 29.3023\n",
      "Epoch 945/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7963 - val_loss: 29.4220\n",
      "Epoch 946/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0983 - val_loss: 28.8105\n",
      "Epoch 947/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.8386 - val_loss: 29.1986\n",
      "Epoch 948/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.8017 - val_loss: 29.6498\n",
      "Epoch 949/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9757 - val_loss: 29.4029\n",
      "Epoch 950/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0671 - val_loss: 29.5804\n",
      "Epoch 951/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0151 - val_loss: 29.5340\n",
      "Epoch 952/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9685 - val_loss: 29.1912\n",
      "Epoch 953/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9250 - val_loss: 29.0746\n",
      "Epoch 954/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7538 - val_loss: 28.9664\n",
      "Epoch 955/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7790 - val_loss: 28.4940\n",
      "Epoch 956/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8406 - val_loss: 29.3654\n",
      "Epoch 957/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7470 - val_loss: 29.4653\n",
      "Epoch 958/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.7015 - val_loss: 29.2509\n",
      "Epoch 959/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.8661 - val_loss: 29.7062\n",
      "Epoch 960/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1857 - val_loss: 29.9871\n",
      "Epoch 961/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3158 - val_loss: 29.1187\n",
      "Epoch 962/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9847 - val_loss: 28.6383\n",
      "Epoch 963/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7719 - val_loss: 29.7547\n",
      "Epoch 964/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8428 - val_loss: 29.7986\n",
      "Epoch 965/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2473 - val_loss: 30.5324\n",
      "Epoch 966/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4144 - val_loss: 30.7360\n",
      "Epoch 967/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.6049 - val_loss: 29.2818\n",
      "Epoch 968/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9080 - val_loss: 29.0720\n",
      "Epoch 969/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8716 - val_loss: 28.8988\n",
      "Epoch 970/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9648 - val_loss: 28.9712\n",
      "Epoch 971/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0107 - val_loss: 30.0007\n",
      "Epoch 972/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3662 - val_loss: 29.7794\n",
      "Epoch 973/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6498 - val_loss: 29.9480\n",
      "Epoch 974/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4855 - val_loss: 29.2754\n",
      "Epoch 975/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2831 - val_loss: 29.8051\n",
      "Epoch 976/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.5168 - val_loss: 30.1447\n",
      "Epoch 977/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3146 - val_loss: 28.6509\n",
      "Epoch 978/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1139 - val_loss: 29.3511\n",
      "Epoch 979/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.6915 - val_loss: 29.9723\n",
      "Epoch 980/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0862 - val_loss: 28.5423\n",
      "Epoch 981/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7417 - val_loss: 30.1562\n",
      "Epoch 982/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8569 - val_loss: 29.6641\n",
      "Epoch 983/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1780 - val_loss: 30.1102\n",
      "Epoch 984/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1726 - val_loss: 28.9940\n",
      "Epoch 985/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1052 - val_loss: 29.2250\n",
      "Epoch 986/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4507 - val_loss: 30.1420\n",
      "Epoch 987/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0061 - val_loss: 28.3167\n",
      "Epoch 988/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9213 - val_loss: 28.2653\n",
      "Epoch 989/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.5210 - val_loss: 29.1239\n",
      "Epoch 990/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5707 - val_loss: 29.3933\n",
      "Epoch 991/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8005 - val_loss: 28.4462\n",
      "Epoch 992/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7567 - val_loss: 28.6167\n",
      "Epoch 993/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8610 - val_loss: 29.4257\n",
      "Epoch 994/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8048 - val_loss: 28.5805\n",
      "Epoch 995/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5979 - val_loss: 29.2745\n",
      "Epoch 996/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0138 - val_loss: 28.7737\n",
      "Epoch 997/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5949 - val_loss: 29.5811\n",
      "Epoch 998/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5483 - val_loss: 29.4666\n",
      "Epoch 999/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5516 - val_loss: 28.8777\n",
      "Epoch 1000/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 26.5173\n",
      "Epoch 01000: saving model to saved_models/latent64/cp-1000.h5\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 26.5173 - val_loss: 29.4660\n",
      "Epoch 1001/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6066 - val_loss: 28.4199\n",
      "Epoch 1002/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6107 - val_loss: 29.5241\n",
      "Epoch 1003/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5105 - val_loss: 28.8788\n",
      "Epoch 1004/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5782 - val_loss: 29.5447\n",
      "Epoch 1005/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7395 - val_loss: 29.0639\n",
      "Epoch 1006/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.0486 - val_loss: 29.7629\n",
      "Epoch 1007/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.3829 - val_loss: 30.0460\n",
      "Epoch 1008/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1408 - val_loss: 29.0780\n",
      "Epoch 1009/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.0626 - val_loss: 29.8816\n",
      "Epoch 1010/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1877 - val_loss: 30.0164\n",
      "Epoch 1011/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3799 - val_loss: 30.9612\n",
      "Epoch 1012/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3895 - val_loss: 30.5521\n",
      "Epoch 1013/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9612 - val_loss: 28.7034\n",
      "Epoch 1014/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.3039 - val_loss: 29.9248\n",
      "Epoch 1015/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9895 - val_loss: 28.4818\n",
      "Epoch 1016/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6618 - val_loss: 29.4411\n",
      "Epoch 1017/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7383 - val_loss: 28.7985\n",
      "Epoch 1018/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6311 - val_loss: 28.8875\n",
      "Epoch 1019/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.3868 - val_loss: 28.7538\n",
      "Epoch 1020/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4543 - val_loss: 28.5958\n",
      "Epoch 1021/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4259 - val_loss: 29.1304\n",
      "Epoch 1022/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4648 - val_loss: 28.6595\n",
      "Epoch 1023/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5916 - val_loss: 28.7661\n",
      "Epoch 1024/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7497 - val_loss: 29.9094\n",
      "Epoch 1025/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3207 - val_loss: 29.4567\n",
      "Epoch 1026/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0031 - val_loss: 29.2509\n",
      "Epoch 1027/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8284 - val_loss: 29.1368\n",
      "Epoch 1028/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6653 - val_loss: 28.5904\n",
      "Epoch 1029/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6223 - val_loss: 29.2580\n",
      "Epoch 1030/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7902 - val_loss: 28.8887\n",
      "Epoch 1031/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5985 - val_loss: 28.5764\n",
      "Epoch 1032/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5268 - val_loss: 29.0788\n",
      "Epoch 1033/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5762 - val_loss: 29.3349\n",
      "Epoch 1034/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5533 - val_loss: 29.0889\n",
      "Epoch 1035/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4908 - val_loss: 29.7075\n",
      "Epoch 1036/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4247 - val_loss: 28.5592\n",
      "Epoch 1037/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5352 - val_loss: 28.0951\n",
      "Epoch 1038/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6141 - val_loss: 29.6025\n",
      "Epoch 1039/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7084 - val_loss: 28.7375\n",
      "Epoch 1040/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6395 - val_loss: 30.4205\n",
      "Epoch 1041/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7641 - val_loss: 27.8264\n",
      "Epoch 1042/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7524 - val_loss: 28.7544\n",
      "Epoch 1043/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8518 - val_loss: 29.0534\n",
      "Epoch 1044/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4444 - val_loss: 29.4312\n",
      "Epoch 1045/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3918 - val_loss: 28.3814\n",
      "Epoch 1046/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.1712 - val_loss: 28.5978\n",
      "Epoch 1047/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1288 - val_loss: 28.8602\n",
      "Epoch 1048/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4423 - val_loss: 28.1902\n",
      "Epoch 1049/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.1065 - val_loss: 28.9370\n",
      "Epoch 1050/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4172 - val_loss: 28.5768\n",
      "Epoch 1051/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2665 - val_loss: 29.0730\n",
      "Epoch 1052/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4043 - val_loss: 28.7474\n",
      "Epoch 1053/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4572 - val_loss: 29.0138\n",
      "Epoch 1054/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5352 - val_loss: 28.5375\n",
      "Epoch 1055/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2373 - val_loss: 28.8307\n",
      "Epoch 1056/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3119 - val_loss: 29.1828\n",
      "Epoch 1057/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6007 - val_loss: 28.7508\n",
      "Epoch 1058/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6138 - val_loss: 28.6551\n",
      "Epoch 1059/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1783 - val_loss: 28.9388\n",
      "Epoch 1060/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3337 - val_loss: 28.9206\n",
      "Epoch 1061/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4268 - val_loss: 28.3574\n",
      "Epoch 1062/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2836 - val_loss: 28.6155\n",
      "Epoch 1063/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3536 - val_loss: 28.3934\n",
      "Epoch 1064/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7262 - val_loss: 28.3948\n",
      "Epoch 1065/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2547 - val_loss: 28.3338\n",
      "Epoch 1066/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3797 - val_loss: 28.0366\n",
      "Epoch 1067/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.0546 - val_loss: 29.0173\n",
      "Epoch 1068/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1188 - val_loss: 28.1973\n",
      "Epoch 1069/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0088 - val_loss: 29.2227\n",
      "Epoch 1070/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4221 - val_loss: 28.0379\n",
      "Epoch 1071/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2323 - val_loss: 28.2817\n",
      "Epoch 1072/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2167 - val_loss: 29.1983\n",
      "Epoch 1073/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5833 - val_loss: 29.2046\n",
      "Epoch 1074/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3367 - val_loss: 28.2026\n",
      "Epoch 1075/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2058 - val_loss: 28.2607\n",
      "Epoch 1076/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4217 - val_loss: 29.4106\n",
      "Epoch 1077/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4845 - val_loss: 27.8256\n",
      "Epoch 1078/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3171 - val_loss: 27.8121\n",
      "Epoch 1079/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2397 - val_loss: 28.5005\n",
      "Epoch 1080/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0472 - val_loss: 29.0135\n",
      "Epoch 1081/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2270 - val_loss: 28.3569\n",
      "Epoch 1082/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2480 - val_loss: 28.5951\n",
      "Epoch 1083/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0588 - val_loss: 28.9795\n",
      "Epoch 1084/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4242 - val_loss: 28.1569\n",
      "Epoch 1085/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1269 - val_loss: 27.9895\n",
      "Epoch 1086/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3265 - val_loss: 28.5104\n",
      "Epoch 1087/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.0066 - val_loss: 28.2924\n",
      "Epoch 1088/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1401 - val_loss: 28.3976\n",
      "Epoch 1089/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2611 - val_loss: 27.8122\n",
      "Epoch 1090/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0757 - val_loss: 27.9746\n",
      "Epoch 1091/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0913 - val_loss: 28.6408\n",
      "Epoch 1092/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1124 - val_loss: 28.7446\n",
      "Epoch 1093/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0294 - val_loss: 28.0586\n",
      "Epoch 1094/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1337 - val_loss: 29.1425\n",
      "Epoch 1095/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4867 - val_loss: 29.0403\n",
      "Epoch 1096/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2722 - val_loss: 28.5632\n",
      "Epoch 1097/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6422 - val_loss: 28.6962\n",
      "Epoch 1098/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3894 - val_loss: 28.9887\n",
      "Epoch 1099/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2664 - val_loss: 29.2935\n",
      "Epoch 1100/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5762 - val_loss: 28.3260\n",
      "Epoch 1101/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1644 - val_loss: 29.4802\n",
      "Epoch 1102/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2512 - val_loss: 28.8312\n",
      "Epoch 1103/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.9873 - val_loss: 28.2752\n",
      "Epoch 1104/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3297 - val_loss: 28.6809\n",
      "Epoch 1105/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4805 - val_loss: 28.4841\n",
      "Epoch 1106/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0921 - val_loss: 28.2615\n",
      "Epoch 1107/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2388 - val_loss: 28.7984\n",
      "Epoch 1108/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3408 - val_loss: 28.9870\n",
      "Epoch 1109/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0090 - val_loss: 29.7949\n",
      "Epoch 1110/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0272 - val_loss: 28.4564\n",
      "Epoch 1111/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1870 - val_loss: 28.3747\n",
      "Epoch 1112/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.9047 - val_loss: 28.6591\n",
      "Epoch 1113/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0250 - val_loss: 27.9792\n",
      "Epoch 1114/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8085 - val_loss: 27.8123\n",
      "Epoch 1115/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7836 - val_loss: 28.6126\n",
      "Epoch 1116/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9366 - val_loss: 28.9880\n",
      "Epoch 1117/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1082 - val_loss: 27.9024\n",
      "Epoch 1118/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9579 - val_loss: 28.4949\n",
      "Epoch 1119/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9282 - val_loss: 28.0946\n",
      "Epoch 1120/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9989 - val_loss: 29.0606\n",
      "Epoch 1121/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1666 - val_loss: 28.1453\n",
      "Epoch 1122/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9014 - val_loss: 28.8338\n",
      "Epoch 1123/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9170 - val_loss: 27.8473\n",
      "Epoch 1124/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0173 - val_loss: 27.5545\n",
      "Epoch 1125/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6160 - val_loss: 28.3715\n",
      "Epoch 1126/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0307 - val_loss: 28.1193\n",
      "Epoch 1127/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7020 - val_loss: 27.5576\n",
      "Epoch 1128/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7629 - val_loss: 28.2051\n",
      "Epoch 1129/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8388 - val_loss: 28.5828\n",
      "Epoch 1130/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9657 - val_loss: 29.3004\n",
      "Epoch 1131/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9845 - val_loss: 28.5641\n",
      "Epoch 1132/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0534 - val_loss: 28.8598\n",
      "Epoch 1133/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0169 - val_loss: 28.2735\n",
      "Epoch 1134/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0507 - val_loss: 28.0349\n",
      "Epoch 1135/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9670 - val_loss: 28.2542\n",
      "Epoch 1136/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7974 - val_loss: 27.7266\n",
      "Epoch 1137/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8640 - val_loss: 28.3008\n",
      "Epoch 1138/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2078 - val_loss: 28.2589\n",
      "Epoch 1139/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9023 - val_loss: 29.0239\n",
      "Epoch 1140/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7867 - val_loss: 28.3782\n",
      "Epoch 1141/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1162 - val_loss: 28.2062\n",
      "Epoch 1142/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2303 - val_loss: 28.2642\n",
      "Epoch 1143/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5146 - val_loss: 28.7515\n",
      "Epoch 1144/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3422 - val_loss: 28.9919\n",
      "Epoch 1145/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3385 - val_loss: 28.9101\n",
      "Epoch 1146/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7151 - val_loss: 28.4340\n",
      "Epoch 1147/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8282 - val_loss: 28.1209\n",
      "Epoch 1148/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6982 - val_loss: 28.4713\n",
      "Epoch 1149/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8229 - val_loss: 28.6248\n",
      "Epoch 1150/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.5842 - val_loss: 28.9233\n",
      "Epoch 1151/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6517 - val_loss: 28.6102\n",
      "Epoch 1152/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7082 - val_loss: 28.1029\n",
      "Epoch 1153/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7896 - val_loss: 27.3953\n",
      "Epoch 1154/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6155 - val_loss: 27.7833\n",
      "Epoch 1155/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5145 - val_loss: 27.2960\n",
      "Epoch 1156/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3477 - val_loss: 27.5316\n",
      "Epoch 1157/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4006 - val_loss: 28.4172\n",
      "Epoch 1158/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8536 - val_loss: 27.4333\n",
      "Epoch 1159/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8452 - val_loss: 28.3210\n",
      "Epoch 1160/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7666 - val_loss: 28.0871\n",
      "Epoch 1161/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9024 - val_loss: 28.0763\n",
      "Epoch 1162/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1146 - val_loss: 29.1963\n",
      "Epoch 1163/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0504 - val_loss: 28.1117\n",
      "Epoch 1164/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9954 - val_loss: 28.1280\n",
      "Epoch 1165/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1450 - val_loss: 27.9337\n",
      "Epoch 1166/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7883 - val_loss: 27.7183\n",
      "Epoch 1167/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6393 - val_loss: 27.2068\n",
      "Epoch 1168/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5737 - val_loss: 27.8770\n",
      "Epoch 1169/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4316 - val_loss: 27.9626\n",
      "Epoch 1170/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5737 - val_loss: 27.5401\n",
      "Epoch 1171/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6697 - val_loss: 27.7953\n",
      "Epoch 1172/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6092 - val_loss: 27.7633\n",
      "Epoch 1173/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6026 - val_loss: 27.3431\n",
      "Epoch 1174/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.3400 - val_loss: 28.1009\n",
      "Epoch 1175/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6819 - val_loss: 27.8500\n",
      "Epoch 1176/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8700 - val_loss: 29.2051\n",
      "Epoch 1177/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9699 - val_loss: 28.3529\n",
      "Epoch 1178/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9058 - val_loss: 27.9258\n",
      "Epoch 1179/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9534 - val_loss: 27.7013\n",
      "Epoch 1180/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6669 - val_loss: 27.9361\n",
      "Epoch 1181/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6373 - val_loss: 27.2914\n",
      "Epoch 1182/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3673 - val_loss: 27.7333\n",
      "Epoch 1183/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3818 - val_loss: 27.3819\n",
      "Epoch 1184/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.2740 - val_loss: 28.9710\n",
      "Epoch 1185/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5841 - val_loss: 27.4160\n",
      "Epoch 1186/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4656 - val_loss: 27.9688\n",
      "Epoch 1187/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4468 - val_loss: 28.4444\n",
      "Epoch 1188/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6193 - val_loss: 28.0138\n",
      "Epoch 1189/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9189 - val_loss: 27.6863\n",
      "Epoch 1190/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5267 - val_loss: 28.6828\n",
      "Epoch 1191/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8439 - val_loss: 28.3198\n",
      "Epoch 1192/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4492 - val_loss: 27.8183\n",
      "Epoch 1193/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3227 - val_loss: 27.9937\n",
      "Epoch 1194/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3713 - val_loss: 27.4208\n",
      "Epoch 1195/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3925 - val_loss: 27.6275\n",
      "Epoch 1196/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1234 - val_loss: 27.9928\n",
      "Epoch 1197/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1727 - val_loss: 28.0171\n",
      "Epoch 1198/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4455 - val_loss: 27.9239\n",
      "Epoch 1199/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4320 - val_loss: 27.7068\n",
      "Epoch 1200/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 25.1688\n",
      "Epoch 01200: saving model to saved_models/latent64/cp-1200.h5\n",
      "6/6 [==============================] - 1s 151ms/step - loss: 25.1688 - val_loss: 27.3349\n",
      "Epoch 1201/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2561 - val_loss: 28.0451\n",
      "Epoch 1202/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4165 - val_loss: 27.4434\n",
      "Epoch 1203/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2860 - val_loss: 27.9842\n",
      "Epoch 1204/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4823 - val_loss: 28.5237\n",
      "Epoch 1205/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3043 - val_loss: 27.5071\n",
      "Epoch 1206/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6359 - val_loss: 28.9626\n",
      "Epoch 1207/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8657 - val_loss: 27.8042\n",
      "Epoch 1208/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6324 - val_loss: 27.7898\n",
      "Epoch 1209/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4914 - val_loss: 27.9670\n",
      "Epoch 1210/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1812 - val_loss: 28.3901\n",
      "Epoch 1211/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3680 - val_loss: 27.4729\n",
      "Epoch 1212/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4217 - val_loss: 27.8664\n",
      "Epoch 1213/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2826 - val_loss: 27.5310\n",
      "Epoch 1214/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9024 - val_loss: 27.9097\n",
      "Epoch 1215/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6997 - val_loss: 27.7049\n",
      "Epoch 1216/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6409 - val_loss: 27.3360\n",
      "Epoch 1217/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4253 - val_loss: 28.4934\n",
      "Epoch 1218/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9879 - val_loss: 28.5458\n",
      "Epoch 1219/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8260 - val_loss: 27.8164\n",
      "Epoch 1220/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6579 - val_loss: 27.2580\n",
      "Epoch 1221/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8054 - val_loss: 28.1166\n",
      "Epoch 1222/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5472 - val_loss: 27.7601\n",
      "Epoch 1223/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6749 - val_loss: 27.9364\n",
      "Epoch 1224/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8148 - val_loss: 27.1146\n",
      "Epoch 1225/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4424 - val_loss: 27.6548\n",
      "Epoch 1226/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2273 - val_loss: 27.7124\n",
      "Epoch 1227/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1395 - val_loss: 28.6422\n",
      "Epoch 1228/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1883 - val_loss: 26.9379\n",
      "Epoch 1229/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1804 - val_loss: 27.2284\n",
      "Epoch 1230/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2517 - val_loss: 27.1163\n",
      "Epoch 1231/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.9091 - val_loss: 27.2778\n",
      "Epoch 1232/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.8081 - val_loss: 27.3087\n",
      "Epoch 1233/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9264 - val_loss: 27.1445\n",
      "Epoch 1234/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1663 - val_loss: 27.1044\n",
      "Epoch 1235/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0954 - val_loss: 27.8535\n",
      "Epoch 1236/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3071 - val_loss: 26.9876\n",
      "Epoch 1237/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3076 - val_loss: 27.6668\n",
      "Epoch 1238/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5215 - val_loss: 27.0800\n",
      "Epoch 1239/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2869 - val_loss: 28.1317\n",
      "Epoch 1240/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2529 - val_loss: 27.8139\n",
      "Epoch 1241/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2529 - val_loss: 26.9068\n",
      "Epoch 1242/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2234 - val_loss: 27.9703\n",
      "Epoch 1243/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3303 - val_loss: 27.4055\n",
      "Epoch 1244/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2640 - val_loss: 27.9645\n",
      "Epoch 1245/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1067 - val_loss: 27.0023\n",
      "Epoch 1246/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2095 - val_loss: 27.7413\n",
      "Epoch 1247/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6676 - val_loss: 28.1937\n",
      "Epoch 1248/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5328 - val_loss: 28.4073\n",
      "Epoch 1249/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3384 - val_loss: 27.7150\n",
      "Epoch 1250/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5142 - val_loss: 28.6738\n",
      "Epoch 1251/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3874 - val_loss: 27.4923\n",
      "Epoch 1252/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1541 - val_loss: 27.1688\n",
      "Epoch 1253/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1329 - val_loss: 27.2409\n",
      "Epoch 1254/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1944 - val_loss: 27.5032\n",
      "Epoch 1255/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.8003 - val_loss: 26.9580\n",
      "Epoch 1256/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1030 - val_loss: 27.7166\n",
      "Epoch 1257/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6452 - val_loss: 27.3658\n",
      "Epoch 1258/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3228 - val_loss: 28.0815\n",
      "Epoch 1259/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3245 - val_loss: 27.6258\n",
      "Epoch 1260/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0184 - val_loss: 27.0496\n",
      "Epoch 1261/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1983 - val_loss: 27.1283\n",
      "Epoch 1262/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0720 - val_loss: 27.2210\n",
      "Epoch 1263/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2464 - val_loss: 27.7433\n",
      "Epoch 1264/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0190 - val_loss: 27.0695\n",
      "Epoch 1265/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1951 - val_loss: 27.7257\n",
      "Epoch 1266/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6190 - val_loss: 27.5707\n",
      "Epoch 1267/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5177 - val_loss: 27.3755\n",
      "Epoch 1268/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2597 - val_loss: 28.1103\n",
      "Epoch 1269/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4062 - val_loss: 27.7483\n",
      "Epoch 1270/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3799 - val_loss: 27.6181\n",
      "Epoch 1271/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1743 - val_loss: 28.2083\n",
      "Epoch 1272/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4045 - val_loss: 27.9291\n",
      "Epoch 1273/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0991 - val_loss: 27.7067\n",
      "Epoch 1274/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1317 - val_loss: 26.8891\n",
      "Epoch 1275/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9407 - val_loss: 27.6800\n",
      "Epoch 1276/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2473 - val_loss: 27.4063\n",
      "Epoch 1277/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9551 - val_loss: 27.0385\n",
      "Epoch 1278/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8521 - val_loss: 27.3465\n",
      "Epoch 1279/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0391 - val_loss: 27.2424\n",
      "Epoch 1280/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9101 - val_loss: 26.5897\n",
      "Epoch 1281/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9342 - val_loss: 27.1486\n",
      "Epoch 1282/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8717 - val_loss: 27.3048\n",
      "Epoch 1283/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.7835 - val_loss: 27.1747\n",
      "Epoch 1284/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8508 - val_loss: 27.3382\n",
      "Epoch 1285/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0824 - val_loss: 27.7144\n",
      "Epoch 1286/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1836 - val_loss: 27.5947\n",
      "Epoch 1287/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1317 - val_loss: 27.8237\n",
      "Epoch 1288/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0213 - val_loss: 27.4402\n",
      "Epoch 1289/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7960 - val_loss: 27.5512\n",
      "Epoch 1290/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.7043 - val_loss: 27.5526\n",
      "Epoch 1291/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9043 - val_loss: 27.5794\n",
      "Epoch 1292/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8799 - val_loss: 27.4225\n",
      "Epoch 1293/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8646 - val_loss: 26.5911\n",
      "Epoch 1294/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1314 - val_loss: 28.0709\n",
      "Epoch 1295/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1062 - val_loss: 26.7389\n",
      "Epoch 1296/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9831 - val_loss: 27.3475\n",
      "Epoch 1297/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0590 - val_loss: 27.9205\n",
      "Epoch 1298/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5689 - val_loss: 28.3995\n",
      "Epoch 1299/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4496 - val_loss: 28.1034\n",
      "Epoch 1300/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0674 - val_loss: 27.6280\n",
      "Epoch 1301/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0701 - val_loss: 27.1308\n",
      "Epoch 1302/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0289 - val_loss: 26.7155\n",
      "Epoch 1303/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7685 - val_loss: 27.7535\n",
      "Epoch 1304/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7428 - val_loss: 27.8796\n",
      "Epoch 1305/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8924 - val_loss: 27.5570\n",
      "Epoch 1306/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3479 - val_loss: 27.8697\n",
      "Epoch 1307/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1427 - val_loss: 26.8538\n",
      "Epoch 1308/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9059 - val_loss: 27.6097\n",
      "Epoch 1309/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7464 - val_loss: 27.1965\n",
      "Epoch 1310/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2203 - val_loss: 27.1966\n",
      "Epoch 1311/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0093 - val_loss: 26.7587\n",
      "Epoch 1312/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1152 - val_loss: 27.1382\n",
      "Epoch 1313/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1080 - val_loss: 27.2328\n",
      "Epoch 1314/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8720 - val_loss: 26.9595\n",
      "Epoch 1315/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9440 - val_loss: 27.3827\n",
      "Epoch 1316/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8683 - val_loss: 26.7046\n",
      "Epoch 1317/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.6715 - val_loss: 27.1219\n",
      "Epoch 1318/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8507 - val_loss: 26.9723\n",
      "Epoch 1319/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7952 - val_loss: 26.6280\n",
      "Epoch 1320/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6166 - val_loss: 28.4099\n",
      "Epoch 1321/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9601 - val_loss: 26.6800\n",
      "Epoch 1322/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0181 - val_loss: 26.7237\n",
      "Epoch 1323/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9901 - val_loss: 26.4861\n",
      "Epoch 1324/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9380 - val_loss: 26.9235\n",
      "Epoch 1325/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7131 - val_loss: 27.5921\n",
      "Epoch 1326/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1750 - val_loss: 27.2817\n",
      "Epoch 1327/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7448 - val_loss: 28.0631\n",
      "Epoch 1328/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7555 - val_loss: 27.3781\n",
      "Epoch 1329/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9670 - val_loss: 26.7218\n",
      "Epoch 1330/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7290 - val_loss: 27.3910\n",
      "Epoch 1331/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0165 - val_loss: 28.9617\n",
      "Epoch 1332/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1515 - val_loss: 27.4182\n",
      "Epoch 1333/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6545 - val_loss: 27.5494\n",
      "Epoch 1334/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1963 - val_loss: 26.5614\n",
      "Epoch 1335/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9354 - val_loss: 26.8396\n",
      "Epoch 1336/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0069 - val_loss: 27.3379\n",
      "Epoch 1337/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8102 - val_loss: 27.0394\n",
      "Epoch 1338/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6613 - val_loss: 27.1189\n",
      "Epoch 1339/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6610 - val_loss: 26.5952\n",
      "Epoch 1340/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6593 - val_loss: 27.4089\n",
      "Epoch 1341/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0050 - val_loss: 27.0026\n",
      "Epoch 1342/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8015 - val_loss: 26.8434\n",
      "Epoch 1343/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0021 - val_loss: 27.5096\n",
      "Epoch 1344/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1252 - val_loss: 27.5215\n",
      "Epoch 1345/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8761 - val_loss: 27.8722\n",
      "Epoch 1346/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1238 - val_loss: 28.3244\n",
      "Epoch 1347/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9350 - val_loss: 27.7593\n",
      "Epoch 1348/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3848 - val_loss: 28.0267\n",
      "Epoch 1349/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1270 - val_loss: 27.1504\n",
      "Epoch 1350/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8376 - val_loss: 26.9826\n",
      "Epoch 1351/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7022 - val_loss: 27.2729\n",
      "Epoch 1352/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0059 - val_loss: 27.0211\n",
      "Epoch 1353/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8455 - val_loss: 27.4576\n",
      "Epoch 1354/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7853 - val_loss: 26.9782\n",
      "Epoch 1355/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7559 - val_loss: 27.2266\n",
      "Epoch 1356/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9796 - val_loss: 27.4594\n",
      "Epoch 1357/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.5803 - val_loss: 26.1943\n",
      "Epoch 1358/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0162 - val_loss: 27.2583\n",
      "Epoch 1359/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0131 - val_loss: 28.6789\n",
      "Epoch 1360/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2856 - val_loss: 27.8425\n",
      "Epoch 1361/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9659 - val_loss: 27.7643\n",
      "Epoch 1362/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9358 - val_loss: 27.5252\n",
      "Epoch 1363/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8068 - val_loss: 26.9410\n",
      "Epoch 1364/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5909 - val_loss: 26.7570\n",
      "Epoch 1365/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6843 - val_loss: 27.8239\n",
      "Epoch 1366/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8818 - val_loss: 27.4954\n",
      "Epoch 1367/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6076 - val_loss: 26.8296\n",
      "Epoch 1368/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6228 - val_loss: 26.5250\n",
      "Epoch 1369/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8404 - val_loss: 26.5747\n",
      "Epoch 1370/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7398 - val_loss: 27.4336\n",
      "Epoch 1371/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7516 - val_loss: 27.0569\n",
      "Epoch 1372/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6485 - val_loss: 26.7314\n",
      "Epoch 1373/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7055 - val_loss: 27.4245\n",
      "Epoch 1374/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5393 - val_loss: 27.1888\n",
      "Epoch 1375/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7534 - val_loss: 26.4001\n",
      "Epoch 1376/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5846 - val_loss: 26.7841\n",
      "Epoch 1377/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5988 - val_loss: 26.9590\n",
      "Epoch 1378/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8097 - val_loss: 26.6642\n",
      "Epoch 1379/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7776 - val_loss: 26.6134\n",
      "Epoch 1380/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6725 - val_loss: 26.0086\n",
      "Epoch 1381/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.5331 - val_loss: 27.3479\n",
      "Epoch 1382/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6696 - val_loss: 26.6985\n",
      "Epoch 1383/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7337 - val_loss: 26.7516\n",
      "Epoch 1384/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6630 - val_loss: 27.1863\n",
      "Epoch 1385/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8821 - val_loss: 26.7764\n",
      "Epoch 1386/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6695 - val_loss: 27.0351\n",
      "Epoch 1387/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6365 - val_loss: 26.6655\n",
      "Epoch 1388/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8040 - val_loss: 26.8451\n",
      "Epoch 1389/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7728 - val_loss: 27.2114\n",
      "Epoch 1390/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6511 - val_loss: 26.9148\n",
      "Epoch 1391/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7068 - val_loss: 27.3563\n",
      "Epoch 1392/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9661 - val_loss: 27.1992\n",
      "Epoch 1393/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0110 - val_loss: 27.3413\n",
      "Epoch 1394/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9299 - val_loss: 27.3493\n",
      "Epoch 1395/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6918 - val_loss: 27.2206\n",
      "Epoch 1396/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6493 - val_loss: 28.1398\n",
      "Epoch 1397/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8696 - val_loss: 27.5411\n",
      "Epoch 1398/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6553 - val_loss: 27.1098\n",
      "Epoch 1399/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7082 - val_loss: 27.1909\n",
      "Epoch 1400/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 24.7871\n",
      "Epoch 01400: saving model to saved_models/latent64/cp-1400.h5\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 24.7871 - val_loss: 27.4152\n",
      "Epoch 1401/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0350 - val_loss: 28.2767\n",
      "Epoch 1402/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8224 - val_loss: 26.9900\n",
      "Epoch 1403/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9013 - val_loss: 27.7532\n",
      "Epoch 1404/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0520 - val_loss: 27.3130\n",
      "Epoch 1405/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9077 - val_loss: 26.9493\n",
      "Epoch 1406/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9403 - val_loss: 26.7779\n",
      "Epoch 1407/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.5223 - val_loss: 26.2406\n",
      "Epoch 1408/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4018 - val_loss: 27.2248\n",
      "Epoch 1409/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5237 - val_loss: 26.4780\n",
      "Epoch 1410/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5845 - val_loss: 27.3709\n",
      "Epoch 1411/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6684 - val_loss: 28.0680\n",
      "Epoch 1412/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9247 - val_loss: 27.7180\n",
      "Epoch 1413/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8487 - val_loss: 27.8878\n",
      "Epoch 1414/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7163 - val_loss: 28.0089\n",
      "Epoch 1415/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5673 - val_loss: 27.1487\n",
      "Epoch 1416/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5238 - val_loss: 26.7779\n",
      "Epoch 1417/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7365 - val_loss: 27.9688\n",
      "Epoch 1418/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6745 - val_loss: 27.5423\n",
      "Epoch 1419/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5432 - val_loss: 26.7791\n",
      "Epoch 1420/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4897 - val_loss: 27.0917\n",
      "Epoch 1421/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4987 - val_loss: 26.8504\n",
      "Epoch 1422/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4753 - val_loss: 27.2527\n",
      "Epoch 1423/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5498 - val_loss: 26.7999\n",
      "Epoch 1424/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4595 - val_loss: 26.6714\n",
      "Epoch 1425/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4903 - val_loss: 27.0896\n",
      "Epoch 1426/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5529 - val_loss: 26.8754\n",
      "Epoch 1427/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 24.3962 - val_loss: 27.1010\n",
      "Epoch 1428/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4718 - val_loss: 27.1030\n",
      "Epoch 1429/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6271 - val_loss: 26.6187\n",
      "Epoch 1430/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3582 - val_loss: 27.0058\n",
      "Epoch 1431/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4902 - val_loss: 27.8927\n",
      "Epoch 1432/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4864 - val_loss: 26.6613\n",
      "Epoch 1433/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5070 - val_loss: 27.2950\n",
      "Epoch 1434/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4740 - val_loss: 27.0678\n",
      "Epoch 1435/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.5339 - val_loss: 26.7407\n",
      "Epoch 1436/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6408 - val_loss: 27.0217\n",
      "Epoch 1437/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6160 - val_loss: 27.0752\n",
      "Epoch 1438/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5553 - val_loss: 26.9977\n",
      "Epoch 1439/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4647 - val_loss: 27.1697\n",
      "Epoch 1440/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4121 - val_loss: 26.7104\n",
      "Epoch 1441/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5031 - val_loss: 27.6318\n",
      "Epoch 1442/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4556 - val_loss: 27.2397\n",
      "Epoch 1443/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4332 - val_loss: 26.6520\n",
      "Epoch 1444/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5034 - val_loss: 26.6238\n",
      "Epoch 1445/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3890 - val_loss: 26.8886\n",
      "Epoch 1446/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.3006 - val_loss: 26.6388\n",
      "Epoch 1447/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3096 - val_loss: 26.5969\n",
      "Epoch 1448/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3622 - val_loss: 26.7538\n",
      "Epoch 1449/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5865 - val_loss: 26.8369\n",
      "Epoch 1450/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3217 - val_loss: 26.7613\n",
      "Epoch 1451/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4048 - val_loss: 26.6866\n",
      "Epoch 1452/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4242 - val_loss: 26.4597\n",
      "Epoch 1453/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4427 - val_loss: 26.4824\n",
      "Epoch 1454/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4052 - val_loss: 28.2356\n",
      "Epoch 1455/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4178 - val_loss: 26.4900\n",
      "Epoch 1456/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.2754 - val_loss: 26.3597\n",
      "Epoch 1457/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1777 - val_loss: 26.4582\n",
      "Epoch 1458/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4509 - val_loss: 26.3722\n",
      "Epoch 1459/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3710 - val_loss: 26.8176\n",
      "Epoch 1460/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5100 - val_loss: 26.3040\n",
      "Epoch 1461/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3841 - val_loss: 27.4413\n",
      "Epoch 1462/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4318 - val_loss: 26.6662\n",
      "Epoch 1463/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3963 - val_loss: 27.6662\n",
      "Epoch 1464/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5248 - val_loss: 26.8445\n",
      "Epoch 1465/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2546 - val_loss: 27.1059\n",
      "Epoch 1466/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3350 - val_loss: 26.6594\n",
      "Epoch 1467/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4691 - val_loss: 26.7763\n",
      "Epoch 1468/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6967 - val_loss: 26.9491\n",
      "Epoch 1469/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9550 - val_loss: 27.5939\n",
      "Epoch 1470/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5434 - val_loss: 27.1374\n",
      "Epoch 1471/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4241 - val_loss: 27.1217\n",
      "Epoch 1472/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3090 - val_loss: 26.8202\n",
      "Epoch 1473/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6032 - val_loss: 26.9929\n",
      "Epoch 1474/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4303 - val_loss: 26.4269\n",
      "Epoch 1475/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4513 - val_loss: 26.3855\n",
      "Epoch 1476/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3374 - val_loss: 26.4042\n",
      "Epoch 1477/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4229 - val_loss: 27.2652\n",
      "Epoch 1478/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4497 - val_loss: 27.2630\n",
      "Epoch 1479/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4556 - val_loss: 27.1199\n",
      "Epoch 1480/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5060 - val_loss: 27.4589\n",
      "Epoch 1481/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7409 - val_loss: 27.1182\n",
      "Epoch 1482/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6773 - val_loss: 26.8686\n",
      "Epoch 1483/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4302 - val_loss: 26.9225\n",
      "Epoch 1484/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.4516 - val_loss: 27.2304\n",
      "Epoch 1485/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5905 - val_loss: 26.9714\n",
      "Epoch 1486/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4658 - val_loss: 27.5593\n",
      "Epoch 1487/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6518 - val_loss: 28.0599\n",
      "Epoch 1488/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0106 - val_loss: 27.0567\n",
      "Epoch 1489/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6126 - val_loss: 27.0918\n",
      "Epoch 1490/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6616 - val_loss: 26.4256\n",
      "Epoch 1491/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6811 - val_loss: 26.5617\n",
      "Epoch 1492/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3511 - val_loss: 27.3245\n",
      "Epoch 1493/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6693 - val_loss: 27.4864\n",
      "Epoch 1494/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8108 - val_loss: 26.2946\n",
      "Epoch 1495/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6263 - val_loss: 26.7028\n",
      "Epoch 1496/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5929 - val_loss: 26.4325\n",
      "Epoch 1497/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4158 - val_loss: 27.9587\n",
      "Epoch 1498/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4708 - val_loss: 26.9442\n",
      "Epoch 1499/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6058 - val_loss: 26.6556\n",
      "Epoch 1500/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5013 - val_loss: 26.9364\n",
      "Epoch 1501/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3054 - val_loss: 26.4343\n",
      "Epoch 1502/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1274 - val_loss: 26.2872\n",
      "Epoch 1503/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1935 - val_loss: 26.8588\n",
      "Epoch 1504/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1556 - val_loss: 26.6263\n",
      "Epoch 1505/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4126 - val_loss: 26.3605\n",
      "Epoch 1506/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2319 - val_loss: 26.8589\n",
      "Epoch 1507/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3848 - val_loss: 26.2257\n",
      "Epoch 1508/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4231 - val_loss: 26.4806\n",
      "Epoch 1509/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5641 - val_loss: 27.5397\n",
      "Epoch 1510/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4173 - val_loss: 26.8080\n",
      "Epoch 1511/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6344 - val_loss: 27.0939\n",
      "Epoch 1512/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8970 - val_loss: 27.3171\n",
      "Epoch 1513/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3483 - val_loss: 27.4630\n",
      "Epoch 1514/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4987 - val_loss: 27.0800\n",
      "Epoch 1515/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1462 - val_loss: 26.7646\n",
      "Epoch 1516/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1674 - val_loss: 26.1888\n",
      "Epoch 1517/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3868 - val_loss: 26.3175\n",
      "Epoch 1518/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4228 - val_loss: 26.9097\n",
      "Epoch 1519/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3809 - val_loss: 27.0440\n",
      "Epoch 1520/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1444 - val_loss: 26.7874\n",
      "Epoch 1521/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3695 - val_loss: 27.3359\n",
      "Epoch 1522/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.1158 - val_loss: 26.7292\n",
      "Epoch 1523/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3259 - val_loss: 27.1545\n",
      "Epoch 1524/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2202 - val_loss: 26.6915\n",
      "Epoch 1525/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.0860 - val_loss: 26.3707\n",
      "Epoch 1526/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1195 - val_loss: 26.9485\n",
      "Epoch 1527/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4171 - val_loss: 26.6236\n",
      "Epoch 1528/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6615 - val_loss: 27.2909\n",
      "Epoch 1529/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6213 - val_loss: 26.9532\n",
      "Epoch 1530/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3623 - val_loss: 26.6878\n",
      "Epoch 1531/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4249 - val_loss: 26.5511\n",
      "Epoch 1532/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6948 - val_loss: 27.1696\n",
      "Epoch 1533/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4473 - val_loss: 27.2506\n",
      "Epoch 1534/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5066 - val_loss: 27.2471\n",
      "Epoch 1535/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3780 - val_loss: 26.9526\n",
      "Epoch 1536/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6396 - val_loss: 26.5237\n",
      "Epoch 1537/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4712 - val_loss: 27.3791\n",
      "Epoch 1538/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3983 - val_loss: 26.8162\n",
      "Epoch 1539/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5106 - val_loss: 26.9491\n",
      "Epoch 1540/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4231 - val_loss: 26.6903\n",
      "Epoch 1541/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1963 - val_loss: 26.2559\n",
      "Epoch 1542/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4425 - val_loss: 26.5863\n",
      "Epoch 1543/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4845 - val_loss: 27.0294\n",
      "Epoch 1544/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2927 - val_loss: 26.8865\n",
      "Epoch 1545/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2027 - val_loss: 26.3218\n",
      "Epoch 1546/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1700 - val_loss: 27.3527\n",
      "Epoch 1547/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5525 - val_loss: 27.1719\n",
      "Epoch 1548/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6165 - val_loss: 26.8133\n",
      "Epoch 1549/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5831 - val_loss: 27.4100\n",
      "Epoch 1550/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6875 - val_loss: 26.3955\n",
      "Epoch 1551/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5310 - val_loss: 27.1734\n",
      "Epoch 1552/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8602 - val_loss: 26.3548\n",
      "Epoch 1553/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3894 - val_loss: 27.4589\n",
      "Epoch 1554/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5084 - val_loss: 26.4642\n",
      "Epoch 1555/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3032 - val_loss: 26.6971\n",
      "Epoch 1556/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3685 - val_loss: 26.1657\n",
      "Epoch 1557/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4404 - val_loss: 26.8742\n",
      "Epoch 1558/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2710 - val_loss: 26.6411\n",
      "Epoch 1559/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3509 - val_loss: 26.6092\n",
      "Epoch 1560/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0746 - val_loss: 27.0982\n",
      "Epoch 1561/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4305 - val_loss: 26.9002\n",
      "Epoch 1562/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4705 - val_loss: 27.1998\n",
      "Epoch 1563/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3495 - val_loss: 26.1101\n",
      "Epoch 1564/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.0011 - val_loss: 26.9572\n",
      "Epoch 1565/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2313 - val_loss: 27.2117\n",
      "Epoch 1566/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2291 - val_loss: 26.2727\n",
      "Epoch 1567/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2257 - val_loss: 26.0177\n",
      "Epoch 1568/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0384 - val_loss: 27.4056\n",
      "Epoch 1569/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0673 - val_loss: 25.9642\n",
      "Epoch 1570/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9369 - val_loss: 27.0622\n",
      "Epoch 1571/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3430 - val_loss: 26.9130\n",
      "Epoch 1572/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4006 - val_loss: 25.9510\n",
      "Epoch 1573/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3730 - val_loss: 26.5184\n",
      "Epoch 1574/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2058 - val_loss: 26.8253\n",
      "Epoch 1575/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4376 - val_loss: 27.3736\n",
      "Epoch 1576/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4729 - val_loss: 26.8182\n",
      "Epoch 1577/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3818 - val_loss: 26.3810\n",
      "Epoch 1578/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3301 - val_loss: 26.2235\n",
      "Epoch 1579/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1804 - val_loss: 26.8568\n",
      "Epoch 1580/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1930 - val_loss: 26.4920\n",
      "Epoch 1581/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9923 - val_loss: 26.4353\n",
      "Epoch 1582/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9664 - val_loss: 26.7653\n",
      "Epoch 1583/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1036 - val_loss: 26.9751\n",
      "Epoch 1584/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9487 - val_loss: 26.0534\n",
      "Epoch 1585/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7949 - val_loss: 27.5139\n",
      "Epoch 1586/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1593 - val_loss: 26.2578\n",
      "Epoch 1587/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1940 - val_loss: 26.5150\n",
      "Epoch 1588/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2854 - val_loss: 26.2160\n",
      "Epoch 1589/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1693 - val_loss: 26.5629\n",
      "Epoch 1590/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9996 - val_loss: 26.0258\n",
      "Epoch 1591/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8633 - val_loss: 26.5468\n",
      "Epoch 1592/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0039 - val_loss: 26.1844\n",
      "Epoch 1593/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8990 - val_loss: 25.7349\n",
      "Epoch 1594/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9862 - val_loss: 26.8268\n",
      "Epoch 1595/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3817 - val_loss: 27.2662\n",
      "Epoch 1596/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5797 - val_loss: 26.5147\n",
      "Epoch 1597/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1625 - val_loss: 26.8776\n",
      "Epoch 1598/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1555 - val_loss: 27.1576\n",
      "Epoch 1599/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9340 - val_loss: 26.5263\n",
      "Epoch 1600/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 24.3827\n",
      "Epoch 01600: saving model to saved_models/latent64/cp-1600.h5\n",
      "6/6 [==============================] - 1s 168ms/step - loss: 24.3827 - val_loss: 26.9010\n",
      "Epoch 1601/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1875 - val_loss: 26.5982\n",
      "Epoch 1602/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1255 - val_loss: 26.4678\n",
      "Epoch 1603/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1035 - val_loss: 26.4891\n",
      "Epoch 1604/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0116 - val_loss: 27.0006\n",
      "Epoch 1605/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3266 - val_loss: 26.4933\n",
      "Epoch 1606/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0926 - val_loss: 26.9090\n",
      "Epoch 1607/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9149 - val_loss: 26.4464\n",
      "Epoch 1608/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2113 - val_loss: 26.4776\n",
      "Epoch 1609/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1371 - val_loss: 25.9147\n",
      "Epoch 1610/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8453 - val_loss: 25.9518\n",
      "Epoch 1611/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0385 - val_loss: 26.0883\n",
      "Epoch 1612/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9535 - val_loss: 26.2439\n",
      "Epoch 1613/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8521 - val_loss: 26.2260\n",
      "Epoch 1614/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0119 - val_loss: 27.2035\n",
      "Epoch 1615/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0338 - val_loss: 26.0386\n",
      "Epoch 1616/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0420 - val_loss: 26.2700\n",
      "Epoch 1617/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9909 - val_loss: 26.1088\n",
      "Epoch 1618/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.7793 - val_loss: 26.4834\n",
      "Epoch 1619/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8430 - val_loss: 26.1719\n",
      "Epoch 1620/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8989 - val_loss: 26.0549\n",
      "Epoch 1621/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2644 - val_loss: 26.4092\n",
      "Epoch 1622/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3725 - val_loss: 25.9260\n",
      "Epoch 1623/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1658 - val_loss: 26.3821\n",
      "Epoch 1624/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3861 - val_loss: 26.1692\n",
      "Epoch 1625/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1021 - val_loss: 26.3719\n",
      "Epoch 1626/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0742 - val_loss: 26.4514\n",
      "Epoch 1627/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1028 - val_loss: 26.1694\n",
      "Epoch 1628/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8766 - val_loss: 26.5364\n",
      "Epoch 1629/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9563 - val_loss: 26.4259\n",
      "Epoch 1630/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0699 - val_loss: 26.9122\n",
      "Epoch 1631/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3909 - val_loss: 26.9753\n",
      "Epoch 1632/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3123 - val_loss: 26.8865\n",
      "Epoch 1633/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5377 - val_loss: 26.9595\n",
      "Epoch 1634/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2331 - val_loss: 26.3147\n",
      "Epoch 1635/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2299 - val_loss: 26.6781\n",
      "Epoch 1636/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0916 - val_loss: 26.4554\n",
      "Epoch 1637/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0416 - val_loss: 26.4201\n",
      "Epoch 1638/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0674 - val_loss: 26.3163\n",
      "Epoch 1639/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2862 - val_loss: 26.5940\n",
      "Epoch 1640/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3654 - val_loss: 26.7415\n",
      "Epoch 1641/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2024 - val_loss: 26.9023\n",
      "Epoch 1642/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4001 - val_loss: 26.8480\n",
      "Epoch 1643/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1870 - val_loss: 26.4478\n",
      "Epoch 1644/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1938 - val_loss: 27.0399\n",
      "Epoch 1645/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8837 - val_loss: 26.2786\n",
      "Epoch 1646/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7299 - val_loss: 26.8864\n",
      "Epoch 1647/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0661 - val_loss: 26.2953\n",
      "Epoch 1648/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8738 - val_loss: 26.5312\n",
      "Epoch 1649/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.7169 - val_loss: 26.5665\n",
      "Epoch 1650/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7297 - val_loss: 25.8386\n",
      "Epoch 1651/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9622 - val_loss: 25.7121\n",
      "Epoch 1652/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1392 - val_loss: 26.5553\n",
      "Epoch 1653/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9018 - val_loss: 26.6694\n",
      "Epoch 1654/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9217 - val_loss: 26.3114\n",
      "Epoch 1655/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8500 - val_loss: 26.5830\n",
      "Epoch 1656/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7508 - val_loss: 27.0498\n",
      "Epoch 1657/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2185 - val_loss: 26.5905\n",
      "Epoch 1658/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2638 - val_loss: 27.4513\n",
      "Epoch 1659/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5166 - val_loss: 27.1975\n",
      "Epoch 1660/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2289 - val_loss: 26.4124\n",
      "Epoch 1661/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1525 - val_loss: 26.9136\n",
      "Epoch 1662/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0679 - val_loss: 27.3441\n",
      "Epoch 1663/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0459 - val_loss: 26.9630\n",
      "Epoch 1664/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1899 - val_loss: 26.6278\n",
      "Epoch 1665/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9292 - val_loss: 26.5956\n",
      "Epoch 1666/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0491 - val_loss: 26.2879\n",
      "Epoch 1667/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7543 - val_loss: 26.6180\n",
      "Epoch 1668/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8209 - val_loss: 26.0641\n",
      "Epoch 1669/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7842 - val_loss: 26.5221\n",
      "Epoch 1670/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.6783 - val_loss: 26.4366\n",
      "Epoch 1671/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7032 - val_loss: 25.8149\n",
      "Epoch 1672/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7700 - val_loss: 26.7371\n",
      "Epoch 1673/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9947 - val_loss: 26.7374\n",
      "Epoch 1674/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0531 - val_loss: 26.5886\n",
      "Epoch 1675/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9875 - val_loss: 26.5197\n",
      "Epoch 1676/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3643 - val_loss: 26.4257\n",
      "Epoch 1677/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2307 - val_loss: 26.0883\n",
      "Epoch 1678/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9224 - val_loss: 25.5449\n",
      "Epoch 1679/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8481 - val_loss: 25.9237\n",
      "Epoch 1680/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8760 - val_loss: 26.7325\n",
      "Epoch 1681/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7626 - val_loss: 26.5048\n",
      "Epoch 1682/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1027 - val_loss: 26.1091\n",
      "Epoch 1683/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9203 - val_loss: 25.9686\n",
      "Epoch 1684/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8016 - val_loss: 26.4413\n",
      "Epoch 1685/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7137 - val_loss: 26.8022\n",
      "Epoch 1686/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0088 - val_loss: 26.9747\n",
      "Epoch 1687/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0818 - val_loss: 27.1907\n",
      "Epoch 1688/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1198 - val_loss: 27.1105\n",
      "Epoch 1689/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9236 - val_loss: 26.6226\n",
      "Epoch 1690/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9225 - val_loss: 26.1263\n",
      "Epoch 1691/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.8784 - val_loss: 26.3005\n",
      "Epoch 1692/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8753 - val_loss: 25.8894\n",
      "Epoch 1693/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7636 - val_loss: 26.4750\n",
      "Epoch 1694/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9415 - val_loss: 26.0586\n",
      "Epoch 1695/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8272 - val_loss: 26.7274\n",
      "Epoch 1696/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7973 - val_loss: 26.5709\n",
      "Epoch 1697/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7008 - val_loss: 26.5884\n",
      "Epoch 1698/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.6620 - val_loss: 26.8358\n",
      "Epoch 1699/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9332 - val_loss: 26.0231\n",
      "Epoch 1700/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1601 - val_loss: 26.8601\n",
      "Epoch 1701/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0234 - val_loss: 26.2657\n",
      "Epoch 1702/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7327 - val_loss: 26.9764\n",
      "Epoch 1703/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8701 - val_loss: 27.1255\n",
      "Epoch 1704/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9909 - val_loss: 25.8729\n",
      "Epoch 1705/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8524 - val_loss: 26.6984\n",
      "Epoch 1706/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7968 - val_loss: 26.4119\n",
      "Epoch 1707/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8354 - val_loss: 26.9541\n",
      "Epoch 1708/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0478 - val_loss: 26.8737\n",
      "Epoch 1709/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8105 - val_loss: 26.2627\n",
      "Epoch 1710/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8456 - val_loss: 27.0437\n",
      "Epoch 1711/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2737 - val_loss: 26.8328\n",
      "Epoch 1712/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9650 - val_loss: 26.7132\n",
      "Epoch 1713/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0482 - val_loss: 26.6545\n",
      "Epoch 1714/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1008 - val_loss: 26.1360\n",
      "Epoch 1715/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1418 - val_loss: 26.6507\n",
      "Epoch 1716/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0838 - val_loss: 26.3184\n",
      "Epoch 1717/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7134 - val_loss: 26.4099\n",
      "Epoch 1718/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8802 - val_loss: 26.6359\n",
      "Epoch 1719/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7579 - val_loss: 26.1918\n",
      "Epoch 1720/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8757 - val_loss: 26.6940\n",
      "Epoch 1721/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9860 - val_loss: 26.1031\n",
      "Epoch 1722/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9278 - val_loss: 26.7717\n",
      "Epoch 1723/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1168 - val_loss: 26.0093\n",
      "Epoch 1724/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9376 - val_loss: 26.5869\n",
      "Epoch 1725/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6478 - val_loss: 26.1834\n",
      "Epoch 1726/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8318 - val_loss: 26.4017\n",
      "Epoch 1727/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0058 - val_loss: 26.7875\n",
      "Epoch 1728/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8196 - val_loss: 26.0995\n",
      "Epoch 1729/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9209 - val_loss: 26.2729\n",
      "Epoch 1730/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9382 - val_loss: 26.6427\n",
      "Epoch 1731/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9120 - val_loss: 26.4211\n",
      "Epoch 1732/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7044 - val_loss: 26.5409\n",
      "Epoch 1733/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7213 - val_loss: 26.4026\n",
      "Epoch 1734/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8852 - val_loss: 25.8457\n",
      "Epoch 1735/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7482 - val_loss: 26.5414\n",
      "Epoch 1736/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8458 - val_loss: 25.8756\n",
      "Epoch 1737/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9218 - val_loss: 26.6680\n",
      "Epoch 1738/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1430 - val_loss: 26.8304\n",
      "Epoch 1739/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0504 - val_loss: 26.1049\n",
      "Epoch 1740/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8572 - val_loss: 26.9175\n",
      "Epoch 1741/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7522 - val_loss: 25.7783\n",
      "Epoch 1742/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7084 - val_loss: 26.2186\n",
      "Epoch 1743/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8174 - val_loss: 26.0152\n",
      "Epoch 1744/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9042 - val_loss: 26.9081\n",
      "Epoch 1745/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9799 - val_loss: 26.3685\n",
      "Epoch 1746/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9337 - val_loss: 26.6360\n",
      "Epoch 1747/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9152 - val_loss: 25.9121\n",
      "Epoch 1748/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0480 - val_loss: 27.5653\n",
      "Epoch 1749/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7564 - val_loss: 27.6215\n",
      "Epoch 1750/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7396 - val_loss: 27.7462\n",
      "Epoch 1751/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5091 - val_loss: 28.2864\n",
      "Epoch 1752/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4679 - val_loss: 26.6071\n",
      "Epoch 1753/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2108 - val_loss: 26.9019\n",
      "Epoch 1754/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1533 - val_loss: 26.9225\n",
      "Epoch 1755/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4573 - val_loss: 26.3856\n",
      "Epoch 1756/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0977 - val_loss: 26.2391\n",
      "Epoch 1757/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1781 - val_loss: 26.8479\n",
      "Epoch 1758/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2748 - val_loss: 26.3077\n",
      "Epoch 1759/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9678 - val_loss: 25.7329\n",
      "Epoch 1760/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8007 - val_loss: 27.2928\n",
      "Epoch 1761/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8274 - val_loss: 26.3778\n",
      "Epoch 1762/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.4789 - val_loss: 26.0415\n",
      "Epoch 1763/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6808 - val_loss: 26.0754\n",
      "Epoch 1764/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5083 - val_loss: 26.6128\n",
      "Epoch 1765/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7257 - val_loss: 26.8554\n",
      "Epoch 1766/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8050 - val_loss: 26.2470\n",
      "Epoch 1767/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6633 - val_loss: 26.0907\n",
      "Epoch 1768/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8894 - val_loss: 26.7141\n",
      "Epoch 1769/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0546 - val_loss: 26.1641\n",
      "Epoch 1770/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0355 - val_loss: 26.6102\n",
      "Epoch 1771/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0220 - val_loss: 26.1951\n",
      "Epoch 1772/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8092 - val_loss: 26.0674\n",
      "Epoch 1773/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6278 - val_loss: 25.8197\n",
      "Epoch 1774/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.3638 - val_loss: 26.4575\n",
      "Epoch 1775/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7670 - val_loss: 26.2940\n",
      "Epoch 1776/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9946 - val_loss: 26.4642\n",
      "Epoch 1777/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0348 - val_loss: 26.1913\n",
      "Epoch 1778/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7134 - val_loss: 26.4814\n",
      "Epoch 1779/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9783 - val_loss: 27.1093\n",
      "Epoch 1780/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0883 - val_loss: 26.5667\n",
      "Epoch 1781/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8752 - val_loss: 26.5067\n",
      "Epoch 1782/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8750 - val_loss: 26.8506\n",
      "Epoch 1783/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6892 - val_loss: 26.1129\n",
      "Epoch 1784/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8209 - val_loss: 26.3615\n",
      "Epoch 1785/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7604 - val_loss: 26.5667\n",
      "Epoch 1786/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5295 - val_loss: 26.4981\n",
      "Epoch 1787/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7599 - val_loss: 25.6292\n",
      "Epoch 1788/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6715 - val_loss: 25.8891\n",
      "Epoch 1789/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7114 - val_loss: 26.0176\n",
      "Epoch 1790/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7734 - val_loss: 26.5512\n",
      "Epoch 1791/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7108 - val_loss: 26.2921\n",
      "Epoch 1792/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4744 - val_loss: 26.3055\n",
      "Epoch 1793/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5723 - val_loss: 26.4259\n",
      "Epoch 1794/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4956 - val_loss: 26.7410\n",
      "Epoch 1795/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6446 - val_loss: 26.2092\n",
      "Epoch 1796/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5480 - val_loss: 26.0724\n",
      "Epoch 1797/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6804 - val_loss: 26.1476\n",
      "Epoch 1798/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7893 - val_loss: 26.5747\n",
      "Epoch 1799/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5220 - val_loss: 26.0657\n",
      "Epoch 1800/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 23.5521\n",
      "Epoch 01800: saving model to saved_models/latent64/cp-1800.h5\n",
      "6/6 [==============================] - 1s 141ms/step - loss: 23.5521 - val_loss: 26.6388\n",
      "Epoch 1801/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6126 - val_loss: 26.3069\n",
      "Epoch 1802/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6123 - val_loss: 26.8415\n",
      "Epoch 1803/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7390 - val_loss: 26.2647\n",
      "Epoch 1804/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6332 - val_loss: 26.2269\n",
      "Epoch 1805/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7369 - val_loss: 26.0218\n",
      "Epoch 1806/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8217 - val_loss: 26.3031\n",
      "Epoch 1807/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6990 - val_loss: 26.2923\n",
      "Epoch 1808/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7318 - val_loss: 25.5497\n",
      "Epoch 1809/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5889 - val_loss: 25.9902\n",
      "Epoch 1810/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6041 - val_loss: 26.0327\n",
      "Epoch 1811/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7779 - val_loss: 26.6238\n",
      "Epoch 1812/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6839 - val_loss: 25.8799\n",
      "Epoch 1813/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.4371 - val_loss: 26.2061\n",
      "Epoch 1814/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5769 - val_loss: 26.0454\n",
      "Epoch 1815/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5481 - val_loss: 26.1682\n",
      "Epoch 1816/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.4542 - val_loss: 26.8102\n",
      "Epoch 1817/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7915 - val_loss: 26.4816\n",
      "Epoch 1818/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5219 - val_loss: 26.1439\n",
      "Epoch 1819/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8758 - val_loss: 26.9709\n",
      "Epoch 1820/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5209 - val_loss: 25.9828\n",
      "Epoch 1821/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4880 - val_loss: 25.6315\n",
      "Epoch 1822/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6996 - val_loss: 25.8164\n",
      "Epoch 1823/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4575 - val_loss: 26.5677\n",
      "Epoch 1824/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5944 - val_loss: 25.7844\n",
      "Epoch 1825/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5175 - val_loss: 25.6079\n",
      "Epoch 1826/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6492 - val_loss: 26.0504\n",
      "Epoch 1827/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7881 - val_loss: 27.0093\n",
      "Epoch 1828/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7287 - val_loss: 27.2174\n",
      "Epoch 1829/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7029 - val_loss: 26.7749\n",
      "Epoch 1830/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6636 - val_loss: 26.7656\n",
      "Epoch 1831/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9887 - val_loss: 26.7055\n",
      "Epoch 1832/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1095 - val_loss: 26.2947\n",
      "Epoch 1833/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8971 - val_loss: 25.9085\n",
      "Epoch 1834/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.3292 - val_loss: 26.5216\n",
      "Epoch 1835/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4710 - val_loss: 25.4990\n",
      "Epoch 1836/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7407 - val_loss: 26.3759\n",
      "Epoch 1837/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5461 - val_loss: 26.0796\n",
      "Epoch 1838/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4373 - val_loss: 26.2959\n",
      "Epoch 1839/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5853 - val_loss: 26.1979\n",
      "Epoch 1840/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5043 - val_loss: 27.0959\n",
      "Epoch 1841/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7772 - val_loss: 26.4242\n",
      "Epoch 1842/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7771 - val_loss: 27.2052\n",
      "Epoch 1843/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8120 - val_loss: 26.6122\n",
      "Epoch 1844/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9052 - val_loss: 26.7060\n",
      "Epoch 1845/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1342 - val_loss: 26.9204\n",
      "Epoch 1846/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8605 - val_loss: 25.8962\n",
      "Epoch 1847/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9096 - val_loss: 26.5788\n",
      "Epoch 1848/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7053 - val_loss: 25.2127\n",
      "Epoch 1849/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9746 - val_loss: 26.8487\n",
      "Epoch 1850/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9327 - val_loss: 25.9282\n",
      "Epoch 1851/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8962 - val_loss: 26.7749\n",
      "Epoch 1852/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9815 - val_loss: 26.4140\n",
      "Epoch 1853/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8074 - val_loss: 26.4462\n",
      "Epoch 1854/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8543 - val_loss: 25.9029\n",
      "Epoch 1855/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6330 - val_loss: 26.5154\n",
      "Epoch 1856/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6524 - val_loss: 25.8116\n",
      "Epoch 1857/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6172 - val_loss: 26.2793\n",
      "Epoch 1858/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6132 - val_loss: 25.9886\n",
      "Epoch 1859/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7963 - val_loss: 25.8694\n",
      "Epoch 1860/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5850 - val_loss: 25.9336\n",
      "Epoch 1861/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6037 - val_loss: 26.3065\n",
      "Epoch 1862/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8323 - val_loss: 27.0949\n",
      "Epoch 1863/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8371 - val_loss: 26.3821\n",
      "Epoch 1864/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8023 - val_loss: 25.9182\n",
      "Epoch 1865/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7945 - val_loss: 25.8018\n",
      "Epoch 1866/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6304 - val_loss: 25.8978\n",
      "Epoch 1867/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5950 - val_loss: 25.9004\n",
      "Epoch 1868/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5850 - val_loss: 26.1196\n",
      "Epoch 1869/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5768 - val_loss: 26.5109\n",
      "Epoch 1870/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5425 - val_loss: 26.6031\n",
      "Epoch 1871/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6445 - val_loss: 26.0693\n",
      "Epoch 1872/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7993 - val_loss: 25.9364\n",
      "Epoch 1873/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6952 - val_loss: 25.6795\n",
      "Epoch 1874/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5540 - val_loss: 26.4983\n",
      "Epoch 1875/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5131 - val_loss: 25.6490\n",
      "Epoch 1876/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6175 - val_loss: 26.4796\n",
      "Epoch 1877/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3852 - val_loss: 25.6932\n",
      "Epoch 1878/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.4709 - val_loss: 26.0715\n",
      "Epoch 1879/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6369 - val_loss: 26.1348\n",
      "Epoch 1880/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4993 - val_loss: 26.4579\n",
      "Epoch 1881/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5859 - val_loss: 26.9115\n",
      "Epoch 1882/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9223 - val_loss: 26.4948\n",
      "Epoch 1883/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8691 - val_loss: 25.7692\n",
      "Epoch 1884/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7941 - val_loss: 26.1984\n",
      "Epoch 1885/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4853 - val_loss: 26.0126\n",
      "Epoch 1886/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3362 - val_loss: 25.5085\n",
      "Epoch 1887/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.2484 - val_loss: 25.5583\n",
      "Epoch 1888/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4334 - val_loss: 25.8724\n",
      "Epoch 1889/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5124 - val_loss: 25.5677\n",
      "Epoch 1890/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5626 - val_loss: 26.1903\n",
      "Epoch 1891/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.3464 - val_loss: 25.8952\n",
      "Epoch 1892/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6176 - val_loss: 27.7032\n",
      "Epoch 1893/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.7933 - val_loss: 26.2760\n",
      "Epoch 1894/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8981 - val_loss: 26.1460\n",
      "Epoch 1895/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7316 - val_loss: 25.7547\n",
      "Epoch 1896/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4440 - val_loss: 25.6100\n",
      "Epoch 1897/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2978 - val_loss: 26.1123\n",
      "Epoch 1898/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3128 - val_loss: 25.8396\n",
      "Epoch 1899/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3774 - val_loss: 25.7569\n",
      "Epoch 1900/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4385 - val_loss: 26.6294\n",
      "Epoch 1901/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4734 - val_loss: 26.0343\n",
      "Epoch 1902/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5102 - val_loss: 26.4528\n",
      "Epoch 1903/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4593 - val_loss: 26.0177\n",
      "Epoch 1904/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5078 - val_loss: 26.1958\n",
      "Epoch 1905/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6567 - val_loss: 26.7317\n",
      "Epoch 1906/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.4878 - val_loss: 25.7924\n",
      "Epoch 1907/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6632 - val_loss: 26.2590\n",
      "Epoch 1908/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6379 - val_loss: 25.7445\n",
      "Epoch 1909/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5945 - val_loss: 26.2413\n",
      "Epoch 1910/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5206 - val_loss: 25.7110\n",
      "Epoch 1911/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5454 - val_loss: 25.7446\n",
      "Epoch 1912/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6547 - val_loss: 26.0722\n",
      "Epoch 1913/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6239 - val_loss: 26.0679\n",
      "Epoch 1914/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8024 - val_loss: 26.1386\n",
      "Epoch 1915/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7124 - val_loss: 25.9065\n",
      "Epoch 1916/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4644 - val_loss: 26.2904\n",
      "Epoch 1917/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5919 - val_loss: 26.6598\n",
      "Epoch 1918/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6632 - val_loss: 25.9335\n",
      "Epoch 1919/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5469 - val_loss: 26.1567\n",
      "Epoch 1920/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6806 - val_loss: 26.1326\n",
      "Epoch 1921/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8702 - val_loss: 25.9682\n",
      "Epoch 1922/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5708 - val_loss: 26.0525\n",
      "Epoch 1923/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5041 - val_loss: 26.2812\n",
      "Epoch 1924/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6740 - val_loss: 26.0187\n",
      "Epoch 1925/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5313 - val_loss: 26.3941\n",
      "Epoch 1926/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4306 - val_loss: 26.8663\n",
      "Epoch 1927/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6098 - val_loss: 26.2940\n",
      "Epoch 1928/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3907 - val_loss: 25.3387\n",
      "Epoch 1929/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5312 - val_loss: 25.3642\n",
      "Epoch 1930/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3806 - val_loss: 25.6555\n",
      "Epoch 1931/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.3367 - val_loss: 26.6541\n",
      "Epoch 1932/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3565 - val_loss: 25.7982\n",
      "Epoch 1933/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3129 - val_loss: 25.6843\n",
      "Epoch 1934/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3818 - val_loss: 26.0149\n",
      "Epoch 1935/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.2323 - val_loss: 25.5108\n",
      "Epoch 1936/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3187 - val_loss: 25.8797\n",
      "Epoch 1937/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5077 - val_loss: 25.7919\n",
      "Epoch 1938/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4296 - val_loss: 26.0734\n",
      "Epoch 1939/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4551 - val_loss: 26.2422\n",
      "Epoch 1940/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5416 - val_loss: 26.1826\n",
      "Epoch 1941/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2850 - val_loss: 25.9297\n",
      "Epoch 1942/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.3955 - val_loss: 26.1899\n",
      "Epoch 1943/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4527 - val_loss: 26.3264\n",
      "Epoch 1944/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.4939 - val_loss: 26.1247\n",
      "Epoch 1945/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5738 - val_loss: 25.9442\n",
      "Epoch 1946/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.2024 - val_loss: 26.2506\n",
      "Epoch 1947/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6543 - val_loss: 26.2376\n",
      "Epoch 1948/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6943 - val_loss: 26.5345\n",
      "Epoch 1949/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4891 - val_loss: 26.6570\n",
      "Epoch 1950/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1533 - val_loss: 27.4926\n",
      "Epoch 1951/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5585 - val_loss: 26.9384\n",
      "Epoch 1952/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2059 - val_loss: 27.2454\n",
      "Epoch 1953/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6662 - val_loss: 26.7777\n",
      "Epoch 1954/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3822 - val_loss: 25.7026\n",
      "Epoch 1955/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5165 - val_loss: 25.5460\n",
      "Epoch 1956/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3901 - val_loss: 26.0530\n",
      "Epoch 1957/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3264 - val_loss: 25.5097\n",
      "Epoch 1958/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.3630 - val_loss: 26.6515\n",
      "Epoch 1959/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.3018 - val_loss: 25.8000\n",
      "Epoch 1960/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4087 - val_loss: 26.0537\n",
      "Epoch 1961/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3450 - val_loss: 26.3809\n",
      "Epoch 1962/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3902 - val_loss: 26.2351\n",
      "Epoch 1963/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.4722 - val_loss: 25.7804\n",
      "Epoch 1964/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6821 - val_loss: 26.1101\n",
      "Epoch 1965/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7417 - val_loss: 25.4976\n",
      "Epoch 1966/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6453 - val_loss: 26.2056\n",
      "Epoch 1967/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7093 - val_loss: 26.0385\n",
      "Epoch 1968/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8966 - val_loss: 26.1889\n",
      "Epoch 1969/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6714 - val_loss: 26.3545\n",
      "Epoch 1970/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4291 - val_loss: 26.3643\n",
      "Epoch 1971/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4402 - val_loss: 26.7757\n",
      "Epoch 1972/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4596 - val_loss: 26.2734\n",
      "Epoch 1973/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5179 - val_loss: 26.1902\n",
      "Epoch 1974/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3644 - val_loss: 25.9445\n",
      "Epoch 1975/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2851 - val_loss: 26.4631\n",
      "Epoch 1976/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.4401 - val_loss: 26.0647\n",
      "Epoch 1977/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.4484 - val_loss: 26.1102\n",
      "Epoch 1978/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2548 - val_loss: 25.3176\n",
      "Epoch 1979/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3155 - val_loss: 26.0713\n",
      "Epoch 1980/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.4948 - val_loss: 25.8334\n",
      "Epoch 1981/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5220 - val_loss: 25.9910\n",
      "Epoch 1982/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2155 - val_loss: 25.5952\n",
      "Epoch 1983/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4920 - val_loss: 25.6069\n",
      "Epoch 1984/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5266 - val_loss: 25.6967\n",
      "Epoch 1985/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2772 - val_loss: 26.5490\n",
      "Epoch 1986/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3565 - val_loss: 26.1719\n",
      "Epoch 1987/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2734 - val_loss: 25.9583\n",
      "Epoch 1988/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3359 - val_loss: 25.7588\n",
      "Epoch 1989/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3922 - val_loss: 26.0454\n",
      "Epoch 1990/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3544 - val_loss: 26.5060\n",
      "Epoch 1991/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3611 - val_loss: 26.3045\n",
      "Epoch 1992/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3791 - val_loss: 26.2353\n",
      "Epoch 1993/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5077 - val_loss: 25.6931\n",
      "Epoch 1994/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4350 - val_loss: 25.3657\n",
      "Epoch 1995/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.2518 - val_loss: 25.8750\n",
      "Epoch 1996/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3355 - val_loss: 25.5462\n",
      "Epoch 1997/2000\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 23.3404 - val_loss: 26.2449\n",
      "Epoch 1998/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.4349 - val_loss: 25.4915\n",
      "Epoch 1999/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.6304 - val_loss: 26.1684\n",
      "Epoch 2000/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 23.4736\n",
      "Epoch 02000: saving model to saved_models/latent64/cp-2000.h5\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 23.4736 - val_loss: 26.6064\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train VAE model with callbacks \"\"\"\n",
    "history = vae.fit(trainX, trainX, \n",
    "                  batch_size = batch_size, \n",
    "                  epochs = epochs, \n",
    "                  callbacks=[initial_checkpoint, checkpoint, EarlyStoppingAtMinLoss()],\n",
    "                  #callbacks=[initial_checkpoint, checkpoint],\n",
    "                  shuffle=True,\n",
    "                  validation_data=(valX, valX)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 6.0\n"
     ]
    }
   ],
   "source": [
    "# In this GPU implementation, it shows the number of batches/iterations for one epoch (and not the training samples), which is:\n",
    "print (\"Step size:\", np.ceil(trainX.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save the entire model \"\"\"\n",
    "# Save notes about hyperparameters used:\n",
    "with open(f'{SAVE_FOLDER}/notes.txt', 'w') as f:\n",
    "    f.write(f'Epochs: {epochs} \\n')\n",
    "    f.write(f'Batch size: {batch_size} \\n')\n",
    "    f.write(f'Latent dimension: {latent_dim} \\n')\n",
    "    f.write(f'Filter sizes: {filters} \\n')\n",
    "    f.write(f'img_width: {img_width} \\n')\n",
    "    f.write(f'img_height: {img_height} \\n')\n",
    "    f.write(f'num_channels: {num_channels}')\n",
    "    f.close()\n",
    "\n",
    "# Save weights for encoder model:\n",
    "encoder.save_weights(f'{SAVE_FOLDER}/ENCODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for decoder model:\n",
    "decoder.save_weights(f'{SAVE_FOLDER}/DECODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for total VAE:\n",
    "vae.save_weights(f'{SAVE_FOLDER}/VAE_WEIGHTS.h5')\n",
    "\n",
    "# Save the history at each epochs (cost of train and validation sets):\n",
    "np.save(f'{SAVE_FOLDER}/HISTORY.npy', history.history)\n",
    "\n",
    "# Save exact training, validation and testing data set (as numpy arrays):\n",
    "np.save(f'{SAVE_FOLDER}/trainX.npy', trainX)\n",
    "np.save(f'{SAVE_FOLDER}/valX.npy', valX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllX.npy', testAllX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllY.npy', testAllY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate training process\n",
    "\n",
    "Now that the training process is complete, let’s evaluate the average loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(13, 6))\n",
    "\n",
    "# summarize history for loss:\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='blue', alpha=0.4, label='validation')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Cost', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.title('Average Loss During Training', fontsize=18)\n",
    "#plt.xticks(range(0,epochs))\n",
    "plt.legend(loc='upper right', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
