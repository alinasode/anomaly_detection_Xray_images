{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import axis, colorbar, imshow, show, figure, subplot\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=18)\n",
    "mpl.rc('xtick', labelsize=16)\n",
    "mpl.rc('ytick', labelsize=16)\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## ----- GPU ------------------------------------\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'    # use of GPU 0 (ERDA) (use before importing torch or tensorflow/keeas)\n",
    "## ----------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization, ReLU, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.activations import relu\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K                         #contains calls for tensor manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL_TRAIN_AUG:       /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/NormalTrainAugmentations\n",
      "NORMAL_VALIDATION_AUG:  /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/NormalValidationAugmentations\n",
      "NORMAL_TEST_AUG:        /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/NormalTestAugmentations\n",
      "ANOMALY1_AUG:           /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/Anomaly1Augmentations\n",
      "ANOMALY2_AUG:           /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/Anomaly2Augmentations\n"
     ]
    }
   ],
   "source": [
    "from utils_vae_potato_paths import *\n",
    "\n",
    "# Get work directions for augmentated data set:\n",
    "print (\"NORMAL_TRAIN_AUG:      \", NORMAL_TRAIN_AUG)\n",
    "print (\"NORMAL_VALIDATION_AUG: \", NORMAL_VAL_AUG)\n",
    "print (\"NORMAL_TEST_AUG:       \", NORMAL_TEST_AUG)\n",
    "print (\"ANOMALY1_AUG:          \", ANOMALY1_AUG)\n",
    "print (\"ANOMALY2_AUG:          \", ANOMALY2_AUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which Folder The Models Are Saved In: saved_models/latent128\n"
     ]
    }
   ],
   "source": [
    "# What latent dimension and filter size are we using?:\n",
    "latent_dim = 128\n",
    "filters    = 32\n",
    "\n",
    "# Get work direction for saving files:\n",
    "SAVE_FOLDER = f'saved_models/latent{latent_dim}'\n",
    "print (\"Which Folder The Models Are Saved In:\", SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] deleting existing files in folder...\n",
      "         done in 0.041 minutes\n"
     ]
    }
   ],
   "source": [
    "# Delete all existing files in folder where models are saved:\n",
    "print(\"[INFO] deleting existing files in folder...\")\n",
    "t0 = time()\n",
    "\n",
    "files = glob.glob(f'{SAVE_FOLDER}/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "print (\"         done in %0.3f minutes\" % ((time() - t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_VAE_AE import get_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Investigation of Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"normal\" training images is:     1639\n",
      "Number of \"normal\" validation images is:   80\n",
      "Number of \"normal\" test images is:         320\n",
      "Total number of \"anomaly\" images is:       680\n"
     ]
    }
   ],
   "source": [
    "# Glob the directories and get the lists of good and bad images\n",
    "good_train_fns = [f for f in os.listdir(NORMAL_TRAIN_AUG) if not f.startswith('.')]\n",
    "good_val_fns = [f for f in os.listdir(NORMAL_VAL_AUG) if not f.startswith('.')]\n",
    "good_test_fns = [f for f in os.listdir(NORMAL_TEST_AUG) if not f.startswith('.')]\n",
    "bad1_fns = [f for f in os.listdir(ANOMALY1_AUG) if not f.startswith('.')]\n",
    "bad2_fns = [f for f in os.listdir(ANOMALY2_AUG) if not f.startswith('.')]\n",
    "\n",
    "print('Number of \"normal\" training images is:     {}'.format(len(good_train_fns)))\n",
    "print('Number of \"normal\" validation images is:   {}'.format(len(good_val_fns)))\n",
    "print('Number of \"normal\" test images is:         {}'.format(len(good_test_fns)))\n",
    "print('Total number of \"anomaly\" images is:       {}'.format(len(bad1_fns) + len(bad2_fns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Procentage of normal samples (ground truth):   74.99 %\n",
      "> Procentage of anomaly samples (ground truth):  25.01 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of bad images vs good images:\n",
    "normal_fns = len(good_train_fns) + len(good_val_fns) + len(good_test_fns)\n",
    "anomaly_fns = len(bad1_fns + bad2_fns)\n",
    "print(f\"> Procentage of normal samples (ground truth):   {(normal_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")\n",
    "print(f\"> Procentage of anomaly samples (ground truth):  {(anomaly_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Normal Data\n",
    "\n",
    "Load normal samples in pre-splitted data sets: train, valdiation and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal training images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal training images...\")\n",
    "trainX = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TRAIN_AUG}/*.png\")]  # read as grayscale\n",
    "trainX = np.array(trainX)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "trainY = get_labels(trainX, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal validation images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal validation images...\")\n",
    "x_normal_val = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_VAL_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_val = np.array(x_normal_val)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_val = get_labels(x_normal_val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal testing images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal testing images...\")\n",
    "x_normal_test = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TEST_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_test = np.array(x_normal_test)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_test = get_labels(x_normal_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Class 1 Data (i.e. 'metal' sampels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly class 1 ('metal') images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly class 1 ('metal') images...\")\n",
    "x_metal = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY1_AUG}/*.png\")]  # read as grayscale\n",
    "x_metal = np.array(x_metal)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_metal = get_labels(x_metal, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Class 2 Data (i.e. 'hollow' sampels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly class 2 ('hollow') images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly class 2 ('hollow') images...\")\n",
    "x_hollow = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY2_AUG}/*.png\")]  # read as grayscale\n",
    "x_hollow = np.array(x_hollow)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_hollow = get_labels(x_hollow, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine normal samples and anomaly samples and split them into training, validation and test sets\n",
    "\n",
    "\n",
    "`label 0` == Normal Samples (=> Perfect' Samples)\n",
    "\n",
    "`label 1` == Anomaly 1 Samples (=> 'Metal' Samples)\n",
    "\n",
    "`label 2` == Anomaly 2 Samples (=> 'Hollow' Samples)\n",
    "\n",
    "Train and Validation sets only consists of `Normal Data`, aka. `label 0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testvalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testvalY = get_labels(testvalX, 0)\n",
    "\n",
    "# randomly split in order to make new validation set:\n",
    "NoUsageX, valX, NoUsageY, valY = train_test_split(testvalX, testvalY, test_size=0.25, random_state=4345672)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Testing set \"\"\"\n",
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testNormalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testNormalY = get_labels(testNormalX, 0)\n",
    "\n",
    "# Anomaly Class 1 (i.e. 'metal' samples):\n",
    "testAnomaly1X, testAnomaly1Y = x_metal, y_metal\n",
    "\n",
    "# Anomaly Class 2 (i.e. 'hollow' samples):\n",
    "testAnomaly2X, testAnomaly2Y = x_hollow, y_hollow\n",
    "\n",
    "# Combine all testing data in one array (the test set is NOT used during any part but inference):\n",
    "testAllX = np.vstack((testNormalX, testAnomaly1X, testAnomaly2X))\n",
    "testAllY = np.hstack((testNormalY, testAnomaly1Y, testAnomaly2Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Dimensions and Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      " > images: (1639, 128, 128)\n",
      " > labels: (1639,)\n",
      "\n",
      "Validation set:\n",
      " > images: (100, 128, 128)\n",
      " > labels: (100,)\n",
      "\n",
      "Test set:\n",
      " > images: (1080, 128, 128)\n",
      " > labels: (1080,)\n",
      "\t'Normal' test set:\n",
      "\t  > images: (400, 128, 128)\n",
      "\t  > labels: (400,)\n",
      "\t'Anomaly 1' test set:\n",
      "\t  > images: (392, 128, 128)\n",
      "\t  > labels: (392,)\n",
      "\t'Anomaly 2' test set:\n",
      "\t  > images: (288, 128, 128)\n",
      "\t  > labels: (288,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "print(\" > images:\", trainX.shape)\n",
    "print(\" > labels:\", trainY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Validation set:\")\n",
    "print(\" > images:\", valX.shape)\n",
    "print(\" > labels:\", valY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Test set:\")\n",
    "print(\" > images:\", testAllX.shape)\n",
    "print(\" > labels:\", testAllY.shape)\n",
    "print(\"\\t'Normal' test set:\")\n",
    "print(\"\\t  > images:\", testNormalX.shape)\n",
    "print(\"\\t  > labels:\", testNormalY.shape)\n",
    "print(\"\\t'Anomaly 1' test set:\")\n",
    "print(\"\\t  > images:\", testAnomaly1X.shape)\n",
    "print(\"\\t  > labels:\", testAnomaly1Y.shape)\n",
    "print(\"\\t'Anomaly 2' test set:\")\n",
    "print(\"\\t  > images:\", testAnomaly2X.shape)\n",
    "print(\"\\t  > labels:\", testAnomaly2Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 76.62 %\n",
      "Procentage of validation samples: 4.68 %\n",
      "Procentage of (only normal) testing samples: 18.70 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets (only normal data):\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (only normal) testing samples: {(len(testNormalX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 58.14 %\n",
      "Procentage of validation samples: 3.55 %\n",
      "Procentage of (total) testing samples: 38.31 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets:\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (total) testing samples: {(len(testAllX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for (un)balanced data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['0: normal', '1: metal', '2: hollow']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,4))\n",
    "ax[0].hist(trainY, align=\"left\"); ax[0].set_title('train', fontsize=18); ax[0].set_xticks([0,1,2]); ax[0].set_xticklabels(labels); ax[0].tick_params(axis='both', which='major', labelsize=16); ax[0].set_xlim(-0.5, 2.5);\n",
    "ax[1].hist(valY, align=\"left\"); ax[1].set_title('validation', fontsize=18); ax[1].set_xticks([0,1,2]); ax[1].set_xticklabels(labels); ax[1].tick_params(axis='both', which='major', labelsize=16); ax[1].set_xlim(-0.5, 2.5);\n",
    "ax[2].hist(testAllY, align=\"left\"); ax[2].set_title('test', fontsize=18); ax[2].set_xticks([0,1,2]); ax[2].set_xticklabels(labels); ax[2].tick_params(axis='both', which='major', labelsize=16); ax[2].set_xlim(-0.5, 2.5);\n",
    "\n",
    "ax[0].set_ylabel(\"Number of images\", fontsize=18)\n",
    "ax[1].set_xlabel(\"Class Labels\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of normal test samples: 37.04 %\n",
      "Procentage of total anomaly test samples: 62.96 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of normal test images vs total anomaly test images:\n",
    "print(f\"Procentage of normal test samples: {(len(testNormalX) / (len(testNormalX) + (len(testAnomaly1X) + len(testAnomaly2X)))) * 100:.2f} %\")\n",
    "print(f\"Procentage of total anomaly test samples: {((len(testAnomaly1X) + len(testAnomaly2X)) / (len(testNormalX) + (len(testAnomaly1X) + len(testAnomaly2X)))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Only normalization has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "img_width, img_height = trainX.shape[1], trainX.shape[2]      # input image dimensions\n",
    "num_channels = 1                                              # gray-channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Neural Networks learns faster with normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to be in  the [0, 1] range:\n",
    "trainX = trainX.astype('float32') / 255.\n",
    "valX = valX.astype('float32') / 255.\n",
    "testAllX = testAllX.astype('float32') / 255.\n",
    "\n",
    "# reshape data to keras inputs --> (n_samples, img_rows, img_cols, n_channels):\n",
    "trainX = trainX.reshape(trainX.shape[0], img_height, img_width, num_channels)\n",
    "valX = valX.reshape(valX.shape[0], img_height, img_width, num_channels)\n",
    "testAllX = testAllX.reshape(testAllX.shape[0], img_height, img_width, num_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import architecture of VAE model and its hyperparameters:\n",
    "from J128_VAE_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Encoder Part*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 32)   320         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 64, 64, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 32)   9248        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 32, 32, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 32)   9248        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 16, 16, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 8, 32)     9248        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 8, 8, 32)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 4, 4, 32)     9248        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 4, 4, 32)     0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "latent (Dense)                  (None, 200)          102600      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 200)          0           latent[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 128)          25728       leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_log_mean (Dense)              (None, 128)          25728       leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 128)          0           z_mean[0][0]                     \n",
      "                                                                 z_log_mean[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 191,368\n",
      "Trainable params: 191,368\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Decoder Part*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 128, 128, 1)       289       \n",
      "=================================================================\n",
      "Total params: 112,577\n",
      "Trainable params: 112,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Total VAE Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         [(None, 128), (None, 128) 191368    \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 128, 128, 1)       112577    \n",
      "=================================================================\n",
      "Total params: 303,945\n",
      "Trainable params: 303,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "batch_size  = 256\n",
    "epochs      = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at 1'st epoch (inital model)\n",
    "\n",
    "https://stackoverflow.com/questions/54323960/save-keras-model-at-specific-epochs\n",
    "\"\"\"\n",
    "\n",
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 1:\n",
    "            self.model.save_weights(f\"{SAVE_FOLDER}/cp-0001.h5\")\n",
    "            \n",
    "# create and use callback:\n",
    "initial_checkpoint = CustomSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at every k'te epochs\n",
    "\"\"\"\n",
    "# Include the epoch in the file name (uses `str.format`):\n",
    "checkpoint_path = \"saved_models/latent128/cp-{epoch:04d}.h5\"\n",
    "\n",
    "# Create a callback that saves the model's weights every k'te epochs:\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path,\n",
    "                             monitor='loss',           # name of the metrics to monitor\n",
    "                             verbose=1, \n",
    "                             #save_best_only=False,     # if False, the best model will not be overridden.\n",
    "                             save_weights_only=True,   # if True, only the weights of the models will be saved.\n",
    "                                                        # if False, the whole models will be saved.\n",
    "                             #mode='auto', \n",
    "                             period=200                  # save after every k'te epochs\n",
    "                            )\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "vae.save_weights(checkpoint_path.format(epoch=0))          # save for zero epoch (inital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: Early stopping\n",
    "https://www.tensorflow.org/guide/keras/custom_callback\n",
    "\"\"\"\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=100):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create and Compile Model\"\"\"\n",
    "# import architecture of VAE model and its hyperparameters. learning rate choosen from graph above.\n",
    "vae = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "7/7 [==============================] - 1s 195ms/step - loss: 2317.1057 - val_loss: 2235.2393\n",
      "Epoch 2/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 2034.8596 - val_loss: 1617.1926\n",
      "Epoch 3/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 1330.2305 - val_loss: 1080.3552\n",
      "Epoch 4/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 888.9319 - val_loss: 722.5283\n",
      "Epoch 5/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 619.6476 - val_loss: 524.7014\n",
      "Epoch 6/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 475.8458 - val_loss: 435.2865\n",
      "Epoch 7/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 410.2049 - val_loss: 396.4774\n",
      "Epoch 8/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 377.9458 - val_loss: 370.7184\n",
      "Epoch 9/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 360.4300 - val_loss: 351.8015\n",
      "Epoch 10/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 347.1865 - val_loss: 344.0822\n",
      "Epoch 11/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 337.3983 - val_loss: 335.8729\n",
      "Epoch 12/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 331.3950 - val_loss: 336.2938\n",
      "Epoch 13/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 327.0291 - val_loss: 328.9787\n",
      "Epoch 14/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 319.9905 - val_loss: 323.8116\n",
      "Epoch 15/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 316.8978 - val_loss: 314.5827\n",
      "Epoch 16/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 314.4865 - val_loss: 314.9855\n",
      "Epoch 17/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 307.5473 - val_loss: 311.3629\n",
      "Epoch 18/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 301.5012 - val_loss: 302.2080\n",
      "Epoch 19/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 296.3642 - val_loss: 302.8921\n",
      "Epoch 20/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 293.1894 - val_loss: 299.1151\n",
      "Epoch 21/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 292.4635 - val_loss: 300.0552\n",
      "Epoch 22/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 284.2559 - val_loss: 288.0100\n",
      "Epoch 23/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 280.9296 - val_loss: 280.5145\n",
      "Epoch 24/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 277.1764 - val_loss: 279.1063\n",
      "Epoch 25/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 273.4769 - val_loss: 278.5082\n",
      "Epoch 26/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 269.7578 - val_loss: 268.3310\n",
      "Epoch 27/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 262.7106 - val_loss: 265.2438\n",
      "Epoch 28/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 262.6885 - val_loss: 258.2246\n",
      "Epoch 29/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 257.1071 - val_loss: 258.7118\n",
      "Epoch 30/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 254.8883 - val_loss: 259.7192\n",
      "Epoch 31/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 252.2369 - val_loss: 255.9346\n",
      "Epoch 32/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 253.1438 - val_loss: 257.9204\n",
      "Epoch 33/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 248.8743 - val_loss: 247.7222\n",
      "Epoch 34/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 243.8396 - val_loss: 244.5364\n",
      "Epoch 35/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 240.8338 - val_loss: 243.1274\n",
      "Epoch 36/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 238.2379 - val_loss: 242.0974\n",
      "Epoch 37/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 236.2520 - val_loss: 237.4713\n",
      "Epoch 38/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 233.2058 - val_loss: 233.4768\n",
      "Epoch 39/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 230.2433 - val_loss: 230.3059\n",
      "Epoch 40/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 224.9150 - val_loss: 224.5130\n",
      "Epoch 41/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 223.3960 - val_loss: 225.5114\n",
      "Epoch 42/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 222.6489 - val_loss: 222.0269\n",
      "Epoch 43/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 219.0826 - val_loss: 220.9405\n",
      "Epoch 44/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 216.4001 - val_loss: 218.1243\n",
      "Epoch 45/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 213.8522 - val_loss: 216.5161\n",
      "Epoch 46/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 210.5919 - val_loss: 212.0395\n",
      "Epoch 47/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 210.3734 - val_loss: 210.1399\n",
      "Epoch 48/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 207.2923 - val_loss: 209.3686\n",
      "Epoch 49/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 206.5936 - val_loss: 208.8240\n",
      "Epoch 50/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 204.6215 - val_loss: 203.6735\n",
      "Epoch 51/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 201.8151 - val_loss: 202.4884\n",
      "Epoch 52/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 200.1964 - val_loss: 200.1125\n",
      "Epoch 53/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 198.0768 - val_loss: 198.6802\n",
      "Epoch 54/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 196.0985 - val_loss: 195.6934\n",
      "Epoch 55/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 194.8988 - val_loss: 193.2291\n",
      "Epoch 56/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 191.4449 - val_loss: 191.0018\n",
      "Epoch 57/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 189.9250 - val_loss: 191.4056\n",
      "Epoch 58/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 189.3888 - val_loss: 188.6838\n",
      "Epoch 59/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 187.3295 - val_loss: 187.0094\n",
      "Epoch 60/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 184.6680 - val_loss: 183.6237\n",
      "Epoch 61/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 183.4012 - val_loss: 182.5416\n",
      "Epoch 62/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 181.7746 - val_loss: 183.2305\n",
      "Epoch 63/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 179.9961 - val_loss: 179.4107\n",
      "Epoch 64/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 177.2926 - val_loss: 180.3730\n",
      "Epoch 65/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 175.9051 - val_loss: 180.4289\n",
      "Epoch 66/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 176.1599 - val_loss: 177.6099\n",
      "Epoch 67/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 175.5315 - val_loss: 175.2618\n",
      "Epoch 68/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 173.2405 - val_loss: 171.3013\n",
      "Epoch 69/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 171.5243 - val_loss: 170.7285\n",
      "Epoch 70/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 170.4383 - val_loss: 169.8764\n",
      "Epoch 71/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 170.6034 - val_loss: 169.8368\n",
      "Epoch 72/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 167.5900 - val_loss: 168.2134\n",
      "Epoch 73/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 166.6529 - val_loss: 165.6712\n",
      "Epoch 74/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 168.8651 - val_loss: 165.7395\n",
      "Epoch 75/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 164.3077 - val_loss: 165.1927\n",
      "Epoch 76/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 162.3992 - val_loss: 160.7386\n",
      "Epoch 77/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 159.8927 - val_loss: 157.9563\n",
      "Epoch 78/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 157.8355 - val_loss: 160.7712\n",
      "Epoch 79/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 160.8706 - val_loss: 162.2001\n",
      "Epoch 80/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 160.4699 - val_loss: 159.4989\n",
      "Epoch 81/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 156.2854 - val_loss: 157.1544\n",
      "Epoch 82/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 153.8227 - val_loss: 152.9290\n",
      "Epoch 83/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 152.2381 - val_loss: 153.6245\n",
      "Epoch 84/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 151.0704 - val_loss: 154.4805\n",
      "Epoch 85/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 150.5405 - val_loss: 152.0642\n",
      "Epoch 86/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 148.3963 - val_loss: 149.8204\n",
      "Epoch 87/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 146.7303 - val_loss: 148.7593\n",
      "Epoch 88/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 145.0612 - val_loss: 146.3275\n",
      "Epoch 89/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 143.4668 - val_loss: 157.3126\n",
      "Epoch 90/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 142.9927 - val_loss: 144.3109\n",
      "Epoch 91/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 145.7178 - val_loss: 143.3030\n",
      "Epoch 92/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 144.7392 - val_loss: 143.7596\n",
      "Epoch 93/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 143.1872 - val_loss: 139.9570\n",
      "Epoch 94/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 139.8930 - val_loss: 143.2687\n",
      "Epoch 95/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 138.2681 - val_loss: 137.4697\n",
      "Epoch 96/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 140.8190 - val_loss: 139.1524\n",
      "Epoch 97/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 136.8302 - val_loss: 136.7308\n",
      "Epoch 98/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 135.5721 - val_loss: 135.5156\n",
      "Epoch 99/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 132.6550 - val_loss: 134.3280\n",
      "Epoch 100/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 133.2963 - val_loss: 134.6277\n",
      "Epoch 101/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 129.8766 - val_loss: 130.8867\n",
      "Epoch 102/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 129.4063 - val_loss: 131.3763\n",
      "Epoch 103/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 127.9673 - val_loss: 129.3051\n",
      "Epoch 104/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 126.7302 - val_loss: 127.3273\n",
      "Epoch 105/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 125.0731 - val_loss: 126.2796\n",
      "Epoch 106/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 123.6785 - val_loss: 124.6857\n",
      "Epoch 107/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 123.1130 - val_loss: 122.9050\n",
      "Epoch 108/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 121.6055 - val_loss: 120.9454\n",
      "Epoch 109/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 121.6166 - val_loss: 121.7246\n",
      "Epoch 110/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 120.1797 - val_loss: 123.6316\n",
      "Epoch 111/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 120.1440 - val_loss: 124.4720\n",
      "Epoch 112/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 117.6863 - val_loss: 121.2243\n",
      "Epoch 113/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 119.5418 - val_loss: 118.1365\n",
      "Epoch 114/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 117.9834 - val_loss: 122.2293\n",
      "Epoch 115/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 121.5143 - val_loss: 123.1003\n",
      "Epoch 116/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 118.4788 - val_loss: 117.4898\n",
      "Epoch 117/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 114.8628 - val_loss: 115.2804\n",
      "Epoch 118/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 113.1836 - val_loss: 111.9595\n",
      "Epoch 119/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 112.1394 - val_loss: 113.0423\n",
      "Epoch 120/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 110.4915 - val_loss: 112.2922\n",
      "Epoch 121/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 112.0163 - val_loss: 112.1362\n",
      "Epoch 122/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 109.4311 - val_loss: 109.9822\n",
      "Epoch 123/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 108.9162 - val_loss: 108.9049\n",
      "Epoch 124/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 108.5216 - val_loss: 109.7140\n",
      "Epoch 125/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 107.6966 - val_loss: 107.7169\n",
      "Epoch 126/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 106.2943 - val_loss: 107.3551\n",
      "Epoch 127/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 105.8504 - val_loss: 106.5157\n",
      "Epoch 128/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 104.3747 - val_loss: 105.7149\n",
      "Epoch 129/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 103.6995 - val_loss: 112.6172\n",
      "Epoch 130/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 103.1798 - val_loss: 103.6838\n",
      "Epoch 131/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 102.0434 - val_loss: 102.8387\n",
      "Epoch 132/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 101.4643 - val_loss: 102.9804\n",
      "Epoch 133/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 100.4029 - val_loss: 101.0978\n",
      "Epoch 134/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 100.4774 - val_loss: 100.9040\n",
      "Epoch 135/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 99.5089 - val_loss: 99.7954\n",
      "Epoch 136/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 100.6589 - val_loss: 102.6175\n",
      "Epoch 137/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 98.2019 - val_loss: 99.0851\n",
      "Epoch 138/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 97.5181 - val_loss: 98.0579\n",
      "Epoch 139/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 97.1572 - val_loss: 97.8008\n",
      "Epoch 140/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 96.5411 - val_loss: 97.3333\n",
      "Epoch 141/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 95.7525 - val_loss: 95.9257\n",
      "Epoch 142/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 95.0145 - val_loss: 95.9361\n",
      "Epoch 143/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 94.4303 - val_loss: 96.9462\n",
      "Epoch 144/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 93.7875 - val_loss: 94.3000\n",
      "Epoch 145/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 93.0991 - val_loss: 94.3639\n",
      "Epoch 146/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 92.1029 - val_loss: 92.7609\n",
      "Epoch 147/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 91.9313 - val_loss: 94.5148\n",
      "Epoch 148/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 91.0470 - val_loss: 91.2415\n",
      "Epoch 149/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 91.3042 - val_loss: 90.8472\n",
      "Epoch 150/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 90.2453 - val_loss: 90.9570\n",
      "Epoch 151/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 89.6786 - val_loss: 89.7967\n",
      "Epoch 152/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 90.0044 - val_loss: 89.9334\n",
      "Epoch 153/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 91.4855 - val_loss: 91.5114\n",
      "Epoch 154/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 89.8544 - val_loss: 90.2336\n",
      "Epoch 155/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 88.2529 - val_loss: 90.9731\n",
      "Epoch 156/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 88.7723 - val_loss: 89.2634\n",
      "Epoch 157/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 90.2489 - val_loss: 99.9674\n",
      "Epoch 158/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 100.0694 - val_loss: 101.2942\n",
      "Epoch 159/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 95.7518 - val_loss: 93.4395\n",
      "Epoch 160/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 90.2384 - val_loss: 90.2650\n",
      "Epoch 161/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 87.9797 - val_loss: 87.5268\n",
      "Epoch 162/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 85.0297 - val_loss: 86.0777\n",
      "Epoch 163/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 83.5169 - val_loss: 84.7710\n",
      "Epoch 164/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 83.9453 - val_loss: 84.6224\n",
      "Epoch 165/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 82.4240 - val_loss: 86.6748\n",
      "Epoch 166/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 82.1732 - val_loss: 83.4745\n",
      "Epoch 167/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 81.6062 - val_loss: 82.4974\n",
      "Epoch 168/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 81.4357 - val_loss: 81.3744\n",
      "Epoch 169/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 80.8413 - val_loss: 81.7792\n",
      "Epoch 170/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 81.2611 - val_loss: 80.6355\n",
      "Epoch 171/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 79.9540 - val_loss: 80.4565\n",
      "Epoch 172/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 78.9209 - val_loss: 80.2364\n",
      "Epoch 173/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 78.5473 - val_loss: 79.5651\n",
      "Epoch 174/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 77.9429 - val_loss: 78.6379\n",
      "Epoch 175/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 77.1784 - val_loss: 77.9327\n",
      "Epoch 176/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 76.7603 - val_loss: 78.5514\n",
      "Epoch 177/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 76.8128 - val_loss: 77.3482\n",
      "Epoch 178/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 75.9878 - val_loss: 77.6598\n",
      "Epoch 179/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 75.9103 - val_loss: 76.8732\n",
      "Epoch 180/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 75.5341 - val_loss: 75.9588\n",
      "Epoch 181/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 75.0521 - val_loss: 76.0918\n",
      "Epoch 182/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 77.9578 - val_loss: 78.6141\n",
      "Epoch 183/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 76.4413 - val_loss: 75.1570\n",
      "Epoch 184/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 75.9555 - val_loss: 75.7663\n",
      "Epoch 185/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 73.9407 - val_loss: 74.3008\n",
      "Epoch 186/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 72.9017 - val_loss: 73.0782\n",
      "Epoch 187/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 72.9468 - val_loss: 73.4607\n",
      "Epoch 188/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 72.6041 - val_loss: 73.0530\n",
      "Epoch 189/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 72.0003 - val_loss: 72.8262\n",
      "Epoch 190/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 71.6314 - val_loss: 72.4289\n",
      "Epoch 191/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 71.0630 - val_loss: 72.1898\n",
      "Epoch 192/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 70.4088 - val_loss: 71.0356\n",
      "Epoch 193/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 69.9949 - val_loss: 71.0250\n",
      "Epoch 194/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 69.8712 - val_loss: 70.0454\n",
      "Epoch 195/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 70.1524 - val_loss: 71.0687\n",
      "Epoch 196/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 69.5183 - val_loss: 70.2283\n",
      "Epoch 197/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 69.3689 - val_loss: 71.3458\n",
      "Epoch 198/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 70.2379 - val_loss: 71.6778\n",
      "Epoch 199/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 69.6032 - val_loss: 69.4619\n",
      "Epoch 200/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 68.8224\n",
      "Epoch 00200: saving model to saved_models/latent128/cp-0200.h5\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 68.8224 - val_loss: 68.9648\n",
      "Epoch 201/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 68.2104 - val_loss: 69.5901\n",
      "Epoch 202/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 67.3177 - val_loss: 67.0508\n",
      "Epoch 203/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 67.2976 - val_loss: 67.2786\n",
      "Epoch 204/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 66.6355 - val_loss: 66.9352\n",
      "Epoch 205/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 65.8863 - val_loss: 66.9698\n",
      "Epoch 206/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 66.4480 - val_loss: 72.5700\n",
      "Epoch 207/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 65.6456 - val_loss: 65.9484\n",
      "Epoch 208/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 65.8633 - val_loss: 67.0422\n",
      "Epoch 209/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 65.9707 - val_loss: 66.8694\n",
      "Epoch 210/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 65.6228 - val_loss: 65.7503\n",
      "Epoch 211/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 65.2488 - val_loss: 65.9677\n",
      "Epoch 212/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 64.7815 - val_loss: 64.0804\n",
      "Epoch 213/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 63.8625 - val_loss: 64.9893\n",
      "Epoch 214/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 64.0863 - val_loss: 65.1982\n",
      "Epoch 215/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 63.6077 - val_loss: 64.5531\n",
      "Epoch 216/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 63.0481 - val_loss: 64.1454\n",
      "Epoch 217/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 63.2855 - val_loss: 63.5069\n",
      "Epoch 218/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.2748 - val_loss: 63.1902\n",
      "Epoch 219/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.2741 - val_loss: 62.8485\n",
      "Epoch 220/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 63.5528 - val_loss: 65.1988\n",
      "Epoch 221/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 63.1494 - val_loss: 63.1906\n",
      "Epoch 222/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 61.9015 - val_loss: 61.2451\n",
      "Epoch 223/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.3764 - val_loss: 62.5254\n",
      "Epoch 224/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 61.3901 - val_loss: 63.1651\n",
      "Epoch 225/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 62.4190 - val_loss: 64.6408\n",
      "Epoch 226/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 64.7975 - val_loss: 65.4598\n",
      "Epoch 227/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 63.9531 - val_loss: 63.5491\n",
      "Epoch 228/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 62.1680 - val_loss: 62.0418\n",
      "Epoch 229/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 60.9918 - val_loss: 61.6537\n",
      "Epoch 230/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 61.5471 - val_loss: 61.7077\n",
      "Epoch 231/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 60.6130 - val_loss: 60.2450\n",
      "Epoch 232/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 59.5687 - val_loss: 59.4484\n",
      "Epoch 233/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 60.0117 - val_loss: 61.3433\n",
      "Epoch 234/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 59.6388 - val_loss: 60.4793\n",
      "Epoch 235/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 59.9901 - val_loss: 61.0112\n",
      "Epoch 236/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 59.2236 - val_loss: 59.1146\n",
      "Epoch 237/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 58.5825 - val_loss: 59.8638\n",
      "Epoch 238/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 58.9332 - val_loss: 59.5815\n",
      "Epoch 239/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 58.3052 - val_loss: 58.3316\n",
      "Epoch 240/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 58.0374 - val_loss: 57.9911\n",
      "Epoch 241/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 57.7609 - val_loss: 57.5139\n",
      "Epoch 242/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 57.5586 - val_loss: 57.2652\n",
      "Epoch 243/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 57.1061 - val_loss: 57.9118\n",
      "Epoch 244/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 56.9353 - val_loss: 56.9048\n",
      "Epoch 245/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 56.3633 - val_loss: 57.7264\n",
      "Epoch 246/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 56.4994 - val_loss: 56.7799\n",
      "Epoch 247/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 56.1360 - val_loss: 57.6283\n",
      "Epoch 248/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 56.0501 - val_loss: 58.2319\n",
      "Epoch 249/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 55.7004 - val_loss: 57.3504\n",
      "Epoch 250/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 55.3408 - val_loss: 56.0756\n",
      "Epoch 251/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.9576 - val_loss: 56.5499\n",
      "Epoch 252/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 55.4583 - val_loss: 56.0914\n",
      "Epoch 253/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 54.7195 - val_loss: 55.3467\n",
      "Epoch 254/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.7493 - val_loss: 55.4618\n",
      "Epoch 255/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.8406 - val_loss: 56.2199\n",
      "Epoch 256/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.2995 - val_loss: 54.5388\n",
      "Epoch 257/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 54.2348 - val_loss: 54.1607\n",
      "Epoch 258/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 53.9662 - val_loss: 55.1726\n",
      "Epoch 259/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.5172 - val_loss: 54.0098\n",
      "Epoch 260/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.1910 - val_loss: 54.2261\n",
      "Epoch 261/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.4338 - val_loss: 54.1382\n",
      "Epoch 262/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 52.9305 - val_loss: 54.6743\n",
      "Epoch 263/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.2290 - val_loss: 55.2133\n",
      "Epoch 264/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.9876 - val_loss: 53.2912\n",
      "Epoch 265/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 52.6196 - val_loss: 53.8621\n",
      "Epoch 266/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 52.3195 - val_loss: 52.6999\n",
      "Epoch 267/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.8321 - val_loss: 53.5275\n",
      "Epoch 268/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 51.8063 - val_loss: 53.6401\n",
      "Epoch 269/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.7752 - val_loss: 54.1827\n",
      "Epoch 270/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.2137 - val_loss: 52.7882\n",
      "Epoch 271/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 51.8274 - val_loss: 53.4237\n",
      "Epoch 272/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 51.9038 - val_loss: 52.0473\n",
      "Epoch 273/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 51.6249 - val_loss: 52.6500\n",
      "Epoch 274/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 51.0427 - val_loss: 51.8113\n",
      "Epoch 275/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.8255 - val_loss: 50.9009\n",
      "Epoch 276/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.9349 - val_loss: 51.8050\n",
      "Epoch 277/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 50.3852 - val_loss: 52.1567\n",
      "Epoch 278/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.1318 - val_loss: 51.3066\n",
      "Epoch 279/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.3923 - val_loss: 50.4809\n",
      "Epoch 280/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 49.9497 - val_loss: 51.1007\n",
      "Epoch 281/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 49.8681 - val_loss: 51.1319\n",
      "Epoch 282/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.8136 - val_loss: 51.0264\n",
      "Epoch 283/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.4438 - val_loss: 50.2019\n",
      "Epoch 284/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.5136 - val_loss: 50.7787\n",
      "Epoch 285/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.4760 - val_loss: 50.1664\n",
      "Epoch 286/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 49.4254 - val_loss: 50.0599\n",
      "Epoch 287/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 49.1711 - val_loss: 49.6870\n",
      "Epoch 288/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 49.1429 - val_loss: 49.9433\n",
      "Epoch 289/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.8078 - val_loss: 49.2891\n",
      "Epoch 290/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.5928 - val_loss: 49.3757\n",
      "Epoch 291/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.7859 - val_loss: 49.4130\n",
      "Epoch 292/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.4176 - val_loss: 49.4483\n",
      "Epoch 293/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.8776 - val_loss: 48.6679\n",
      "Epoch 294/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 48.2483 - val_loss: 49.4630\n",
      "Epoch 295/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 47.7298 - val_loss: 48.4286\n",
      "Epoch 296/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.9362 - val_loss: 48.7139\n",
      "Epoch 297/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 47.4427 - val_loss: 48.4669\n",
      "Epoch 298/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.5167 - val_loss: 48.6852\n",
      "Epoch 299/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 47.2944 - val_loss: 47.8663\n",
      "Epoch 300/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.4785 - val_loss: 47.7964\n",
      "Epoch 301/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.3322 - val_loss: 48.3389\n",
      "Epoch 302/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 46.9024 - val_loss: 49.5591\n",
      "Epoch 303/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.1500 - val_loss: 48.3165\n",
      "Epoch 304/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.5636 - val_loss: 48.5831\n",
      "Epoch 305/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.4821 - val_loss: 47.4843\n",
      "Epoch 306/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.8536 - val_loss: 47.9684\n",
      "Epoch 307/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 46.4872 - val_loss: 47.0965\n",
      "Epoch 308/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 46.2914 - val_loss: 47.0234\n",
      "Epoch 309/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.9649 - val_loss: 47.4322\n",
      "Epoch 310/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.1208 - val_loss: 45.9916\n",
      "Epoch 311/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 45.7609 - val_loss: 47.4884\n",
      "Epoch 312/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 45.5880 - val_loss: 46.9834\n",
      "Epoch 313/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.3881 - val_loss: 47.2122\n",
      "Epoch 314/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.3765 - val_loss: 47.1736\n",
      "Epoch 315/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 45.2577 - val_loss: 46.2564\n",
      "Epoch 316/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.2640 - val_loss: 45.9645\n",
      "Epoch 317/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.0112 - val_loss: 46.4486\n",
      "Epoch 318/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.9676 - val_loss: 46.0787\n",
      "Epoch 319/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.8775 - val_loss: 45.2091\n",
      "Epoch 320/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 44.8256 - val_loss: 45.5549\n",
      "Epoch 321/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.0980 - val_loss: 47.8137\n",
      "Epoch 322/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.0688 - val_loss: 47.6817\n",
      "Epoch 323/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.4497 - val_loss: 45.2421\n",
      "Epoch 324/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.7050 - val_loss: 46.3081\n",
      "Epoch 325/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 44.4181 - val_loss: 45.0206\n",
      "Epoch 326/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 44.2726 - val_loss: 44.7756\n",
      "Epoch 327/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.9604 - val_loss: 44.5950\n",
      "Epoch 328/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.0160 - val_loss: 44.2618\n",
      "Epoch 329/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 43.5074 - val_loss: 43.9859\n",
      "Epoch 330/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.8035 - val_loss: 45.5640\n",
      "Epoch 331/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.9264 - val_loss: 45.9102\n",
      "Epoch 332/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.3628 - val_loss: 44.8126\n",
      "Epoch 333/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.5483 - val_loss: 44.2062\n",
      "Epoch 334/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 43.2686 - val_loss: 44.2602\n",
      "Epoch 335/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 43.2449 - val_loss: 43.3831\n",
      "Epoch 336/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 42.7489 - val_loss: 43.4699\n",
      "Epoch 337/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.7803 - val_loss: 44.0986\n",
      "Epoch 338/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.5854 - val_loss: 43.4197\n",
      "Epoch 339/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.7420 - val_loss: 43.1938\n",
      "Epoch 340/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.6349 - val_loss: 42.9866\n",
      "Epoch 341/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.6357 - val_loss: 43.5064\n",
      "Epoch 342/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 42.2744 - val_loss: 43.6031\n",
      "Epoch 343/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.2894 - val_loss: 43.1817\n",
      "Epoch 344/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.6397 - val_loss: 43.0431\n",
      "Epoch 345/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.6199 - val_loss: 43.2381\n",
      "Epoch 346/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 42.0099 - val_loss: 42.8570\n",
      "Epoch 347/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.9575 - val_loss: 42.1556\n",
      "Epoch 348/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.1638 - val_loss: 43.5018\n",
      "Epoch 349/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.6241 - val_loss: 42.4004\n",
      "Epoch 350/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.6642 - val_loss: 43.3650\n",
      "Epoch 351/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.7803 - val_loss: 42.3356\n",
      "Epoch 352/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 41.3897 - val_loss: 42.1781\n",
      "Epoch 353/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 40.9997 - val_loss: 41.9263\n",
      "Epoch 354/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.8632 - val_loss: 42.9031\n",
      "Epoch 355/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.4998 - val_loss: 42.2360\n",
      "Epoch 356/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.3462 - val_loss: 41.8178\n",
      "Epoch 357/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.1636 - val_loss: 42.2527\n",
      "Epoch 358/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.8872 - val_loss: 42.3626\n",
      "Epoch 359/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.8645 - val_loss: 41.4416\n",
      "Epoch 360/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.0874 - val_loss: 41.5607\n",
      "Epoch 361/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.4386 - val_loss: 42.6562\n",
      "Epoch 362/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.8291 - val_loss: 41.1963\n",
      "Epoch 363/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.5715 - val_loss: 41.4306\n",
      "Epoch 364/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.4798 - val_loss: 41.1504\n",
      "Epoch 365/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 40.2497 - val_loss: 42.2971\n",
      "Epoch 366/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.4554 - val_loss: 40.8042\n",
      "Epoch 367/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.3623 - val_loss: 41.2124\n",
      "Epoch 368/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 40.1672 - val_loss: 40.7108\n",
      "Epoch 369/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.3089 - val_loss: 41.5511\n",
      "Epoch 370/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.1709 - val_loss: 40.8984\n",
      "Epoch 371/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 40.1298 - val_loss: 40.6462\n",
      "Epoch 372/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.8388 - val_loss: 41.0350\n",
      "Epoch 373/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.2189 - val_loss: 41.6838\n",
      "Epoch 374/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.9619 - val_loss: 40.0499\n",
      "Epoch 375/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 39.6277 - val_loss: 40.5650\n",
      "Epoch 376/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.4636 - val_loss: 39.7143\n",
      "Epoch 377/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 39.3493 - val_loss: 40.5446\n",
      "Epoch 378/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.7385 - val_loss: 40.6977\n",
      "Epoch 379/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.0791 - val_loss: 40.7491\n",
      "Epoch 380/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.3101 - val_loss: 40.6046\n",
      "Epoch 381/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.1419 - val_loss: 39.9740\n",
      "Epoch 382/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.0956 - val_loss: 40.1553\n",
      "Epoch 383/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.7884 - val_loss: 39.6938\n",
      "Epoch 384/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.8713 - val_loss: 39.8489\n",
      "Epoch 385/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 38.6614 - val_loss: 39.5633\n",
      "Epoch 386/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 38.5056 - val_loss: 38.8550\n",
      "Epoch 387/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.5079 - val_loss: 39.6053\n",
      "Epoch 388/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.6013 - val_loss: 39.4965\n",
      "Epoch 389/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.7433 - val_loss: 39.0293\n",
      "Epoch 390/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.6545 - val_loss: 41.6292\n",
      "Epoch 391/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.3914 - val_loss: 39.0742\n",
      "Epoch 392/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.6683 - val_loss: 40.7764\n",
      "Epoch 393/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.5573 - val_loss: 38.8427\n",
      "Epoch 394/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 38.0568 - val_loss: 38.8200\n",
      "Epoch 395/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.1959 - val_loss: 39.7576\n",
      "Epoch 396/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 38.0092 - val_loss: 37.8157\n",
      "Epoch 397/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.0108 - val_loss: 38.3311\n",
      "Epoch 398/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 37.8269 - val_loss: 38.7517\n",
      "Epoch 399/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.9939 - val_loss: 39.0541\n",
      "Epoch 400/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 37.7831\n",
      "Epoch 00400: saving model to saved_models/latent128/cp-0400.h5\n",
      "7/7 [==============================] - 1s 154ms/step - loss: 37.7831 - val_loss: 39.0826\n",
      "Epoch 401/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 37.6534 - val_loss: 38.8237\n",
      "Epoch 402/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 37.6352 - val_loss: 38.8326\n",
      "Epoch 403/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.5550 - val_loss: 38.8471\n",
      "Epoch 404/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 37.3980 - val_loss: 39.2165\n",
      "Epoch 405/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.8476 - val_loss: 39.2453\n",
      "Epoch 406/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.9056 - val_loss: 38.6289\n",
      "Epoch 407/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 37.3287 - val_loss: 38.9772\n",
      "Epoch 408/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 37.2557 - val_loss: 37.7177\n",
      "Epoch 409/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.3568 - val_loss: 44.0766\n",
      "Epoch 410/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.7911 - val_loss: 38.6441\n",
      "Epoch 411/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.0834 - val_loss: 37.9905\n",
      "Epoch 412/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.9725 - val_loss: 38.2483\n",
      "Epoch 413/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.1990 - val_loss: 38.3186\n",
      "Epoch 414/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.9041 - val_loss: 38.3731\n",
      "Epoch 415/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.0326 - val_loss: 37.5015\n",
      "Epoch 416/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.0056 - val_loss: 37.9523\n",
      "Epoch 417/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 36.6430 - val_loss: 39.1279\n",
      "Epoch 418/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.1058 - val_loss: 37.5761\n",
      "Epoch 419/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.2584 - val_loss: 38.2325\n",
      "Epoch 420/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.9724 - val_loss: 37.2337\n",
      "Epoch 421/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 36.5370 - val_loss: 37.3637\n",
      "Epoch 422/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.5910 - val_loss: 37.5638\n",
      "Epoch 423/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.4070 - val_loss: 38.2068\n",
      "Epoch 424/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.3514 - val_loss: 38.2180\n",
      "Epoch 425/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.7478 - val_loss: 36.9572\n",
      "Epoch 426/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.4142 - val_loss: 37.2516\n",
      "Epoch 427/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.1479 - val_loss: 37.4832\n",
      "Epoch 428/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 36.1440 - val_loss: 37.8956\n",
      "Epoch 429/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.2863 - val_loss: 37.4232\n",
      "Epoch 430/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 36.0552 - val_loss: 37.8970\n",
      "Epoch 431/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.4477 - val_loss: 36.8175\n",
      "Epoch 432/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.3188 - val_loss: 37.8719\n",
      "Epoch 433/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.2285 - val_loss: 36.7973\n",
      "Epoch 434/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 35.8854 - val_loss: 36.5732\n",
      "Epoch 435/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 35.5871 - val_loss: 36.5855\n",
      "Epoch 436/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.5072 - val_loss: 36.8040\n",
      "Epoch 437/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.7312 - val_loss: 36.1335\n",
      "Epoch 438/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.6016 - val_loss: 36.1229\n",
      "Epoch 439/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.6973 - val_loss: 38.0003\n",
      "Epoch 440/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.5487 - val_loss: 36.3779\n",
      "Epoch 441/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 35.4108 - val_loss: 36.0899\n",
      "Epoch 442/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.6100 - val_loss: 36.0603\n",
      "Epoch 443/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.3904 - val_loss: 37.2003\n",
      "Epoch 444/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 35.1778 - val_loss: 36.5728\n",
      "Epoch 445/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.1072 - val_loss: 36.5287\n",
      "Epoch 446/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.3301 - val_loss: 37.1169\n",
      "Epoch 447/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.3690 - val_loss: 36.2511\n",
      "Epoch 448/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.7630 - val_loss: 36.5507\n",
      "Epoch 449/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.2012 - val_loss: 35.8755\n",
      "Epoch 450/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 34.8588 - val_loss: 36.1366\n",
      "Epoch 451/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.5567 - val_loss: 35.7789\n",
      "Epoch 452/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.7750 - val_loss: 36.2753\n",
      "Epoch 453/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.9589 - val_loss: 36.2857\n",
      "Epoch 454/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.7917 - val_loss: 37.0748\n",
      "Epoch 455/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.7621 - val_loss: 35.4265\n",
      "Epoch 456/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.6336 - val_loss: 35.6508\n",
      "Epoch 457/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.7656 - val_loss: 36.6707\n",
      "Epoch 458/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.9207 - val_loss: 36.2271\n",
      "Epoch 459/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.9176 - val_loss: 35.4156\n",
      "Epoch 460/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.7377 - val_loss: 35.4182\n",
      "Epoch 461/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 34.4498 - val_loss: 35.3783\n",
      "Epoch 462/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.5518 - val_loss: 35.6508\n",
      "Epoch 463/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 34.4173 - val_loss: 36.0012\n",
      "Epoch 464/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.9335 - val_loss: 35.9565\n",
      "Epoch 465/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 34.2754 - val_loss: 35.4138\n",
      "Epoch 466/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.4679 - val_loss: 34.9349\n",
      "Epoch 467/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.4578 - val_loss: 35.8488\n",
      "Epoch 468/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.3064 - val_loss: 35.5708\n",
      "Epoch 469/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 34.2585 - val_loss: 36.4573\n",
      "Epoch 470/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.2897 - val_loss: 34.7289\n",
      "Epoch 471/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 34.0718 - val_loss: 35.1602\n",
      "Epoch 472/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1801 - val_loss: 35.1279\n",
      "Epoch 473/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.0572 - val_loss: 35.0970\n",
      "Epoch 474/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.7662 - val_loss: 34.9772\n",
      "Epoch 475/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.9461 - val_loss: 35.0353\n",
      "Epoch 476/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.8635 - val_loss: 34.7013\n",
      "Epoch 477/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.8766 - val_loss: 34.6282\n",
      "Epoch 478/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.6538 - val_loss: 35.1616\n",
      "Epoch 479/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 33.5093 - val_loss: 35.5125\n",
      "Epoch 480/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.7319 - val_loss: 34.1803\n",
      "Epoch 481/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.5129 - val_loss: 34.9596\n",
      "Epoch 482/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.5770 - val_loss: 35.0024\n",
      "Epoch 483/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.7396 - val_loss: 34.6771\n",
      "Epoch 484/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.8867 - val_loss: 35.1045\n",
      "Epoch 485/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.8435 - val_loss: 37.0344\n",
      "Epoch 486/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.4275 - val_loss: 35.0792\n",
      "Epoch 487/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.6867 - val_loss: 34.5078\n",
      "Epoch 488/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 33.3217 - val_loss: 34.0686\n",
      "Epoch 489/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.4219 - val_loss: 34.1975\n",
      "Epoch 490/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 33.2133 - val_loss: 34.8838\n",
      "Epoch 491/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.5085 - val_loss: 34.3980\n",
      "Epoch 492/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.3730 - val_loss: 34.9166\n",
      "Epoch 493/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.4424 - val_loss: 35.3064\n",
      "Epoch 494/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.6245 - val_loss: 34.9764\n",
      "Epoch 495/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.5362 - val_loss: 34.0542\n",
      "Epoch 496/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.3712 - val_loss: 33.8346\n",
      "Epoch 497/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.0753 - val_loss: 34.5615\n",
      "Epoch 498/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.2624 - val_loss: 34.2075\n",
      "Epoch 499/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.1996 - val_loss: 34.8291\n",
      "Epoch 500/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.2879 - val_loss: 35.2657\n",
      "Epoch 501/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.4379 - val_loss: 34.8546\n",
      "Epoch 502/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.2426 - val_loss: 34.1459\n",
      "Epoch 503/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.0820 - val_loss: 34.0342\n",
      "Epoch 504/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.2732 - val_loss: 33.8634\n",
      "Epoch 505/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.0713 - val_loss: 34.9613\n",
      "Epoch 506/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.0715 - val_loss: 34.4159\n",
      "Epoch 507/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.0492 - val_loss: 33.7038\n",
      "Epoch 508/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.8326 - val_loss: 34.4840\n",
      "Epoch 509/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.2844 - val_loss: 35.7887\n",
      "Epoch 510/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.2923 - val_loss: 33.6233\n",
      "Epoch 511/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 32.7156 - val_loss: 34.1812\n",
      "Epoch 512/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.0558 - val_loss: 32.9463\n",
      "Epoch 513/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 32.5590 - val_loss: 34.8909\n",
      "Epoch 514/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.8646 - val_loss: 33.1437\n",
      "Epoch 515/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.6585 - val_loss: 34.2773\n",
      "Epoch 516/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.1785 - val_loss: 34.3999\n",
      "Epoch 517/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5869 - val_loss: 33.7442\n",
      "Epoch 518/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 32.3847 - val_loss: 33.5689\n",
      "Epoch 519/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 32.3109 - val_loss: 34.2884\n",
      "Epoch 520/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.3782 - val_loss: 33.8431\n",
      "Epoch 521/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.4325 - val_loss: 34.1250\n",
      "Epoch 522/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.5455 - val_loss: 33.3224\n",
      "Epoch 523/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.3715 - val_loss: 33.7124\n",
      "Epoch 524/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 32.3026 - val_loss: 33.4226\n",
      "Epoch 525/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 32.2686 - val_loss: 33.3899\n",
      "Epoch 526/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.4376 - val_loss: 33.0010\n",
      "Epoch 527/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.3966 - val_loss: 33.9335\n",
      "Epoch 528/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.9842 - val_loss: 33.7226\n",
      "Epoch 529/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.3660 - val_loss: 33.6258\n",
      "Epoch 530/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.1345 - val_loss: 33.4331\n",
      "Epoch 531/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.0657 - val_loss: 33.9012\n",
      "Epoch 532/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.2569 - val_loss: 33.5505\n",
      "Epoch 533/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.0997 - val_loss: 33.7431\n",
      "Epoch 534/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.9457 - val_loss: 32.8857\n",
      "Epoch 535/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.1774 - val_loss: 33.2997\n",
      "Epoch 536/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.2134 - val_loss: 33.4120\n",
      "Epoch 537/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.9087 - val_loss: 33.4863\n",
      "Epoch 538/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.1814 - val_loss: 33.1028\n",
      "Epoch 539/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.8549 - val_loss: 33.6963\n",
      "Epoch 540/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.0947 - val_loss: 33.8992\n",
      "Epoch 541/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.2993 - val_loss: 34.5446\n",
      "Epoch 542/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.5560 - val_loss: 34.0718\n",
      "Epoch 543/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.1958 - val_loss: 34.5099\n",
      "Epoch 544/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.9949 - val_loss: 33.8611\n",
      "Epoch 545/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.8514 - val_loss: 36.2379\n",
      "Epoch 546/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.0436 - val_loss: 34.2695\n",
      "Epoch 547/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.9155 - val_loss: 32.9940\n",
      "Epoch 548/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.9817 - val_loss: 33.1130\n",
      "Epoch 549/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.0111 - val_loss: 33.1735\n",
      "Epoch 550/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.7256 - val_loss: 32.8608\n",
      "Epoch 551/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.6768 - val_loss: 32.9955\n",
      "Epoch 552/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.8109 - val_loss: 33.1261\n",
      "Epoch 553/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.4658 - val_loss: 32.8219\n",
      "Epoch 554/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.4039 - val_loss: 32.5257\n",
      "Epoch 555/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.4994 - val_loss: 34.2256\n",
      "Epoch 556/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.6508 - val_loss: 32.9416\n",
      "Epoch 557/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4878 - val_loss: 32.6250\n",
      "Epoch 558/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.5125 - val_loss: 33.0837\n",
      "Epoch 559/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.5419 - val_loss: 33.1806\n",
      "Epoch 560/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.3041 - val_loss: 33.5659\n",
      "Epoch 561/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3739 - val_loss: 32.6954\n",
      "Epoch 562/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4832 - val_loss: 33.7602\n",
      "Epoch 563/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.7315 - val_loss: 33.2720\n",
      "Epoch 564/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3395 - val_loss: 33.6018\n",
      "Epoch 565/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.6578 - val_loss: 33.2185\n",
      "Epoch 566/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4357 - val_loss: 33.4839\n",
      "Epoch 567/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4116 - val_loss: 32.6707\n",
      "Epoch 568/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.2832 - val_loss: 32.6856\n",
      "Epoch 569/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.2272 - val_loss: 32.6416\n",
      "Epoch 570/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.3177 - val_loss: 33.4085\n",
      "Epoch 571/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.1748 - val_loss: 33.3798\n",
      "Epoch 572/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.4216 - val_loss: 32.7590\n",
      "Epoch 573/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.1563 - val_loss: 33.1539\n",
      "Epoch 574/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.2605 - val_loss: 32.8196\n",
      "Epoch 575/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3962 - val_loss: 32.3188\n",
      "Epoch 576/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.2274 - val_loss: 32.7682\n",
      "Epoch 577/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.2270 - val_loss: 32.7142\n",
      "Epoch 578/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.2101 - val_loss: 32.7765\n",
      "Epoch 579/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.4287 - val_loss: 32.8404\n",
      "Epoch 580/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.2531 - val_loss: 33.1111\n",
      "Epoch 581/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.8923 - val_loss: 32.4779\n",
      "Epoch 582/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.9553 - val_loss: 32.9988\n",
      "Epoch 583/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.1421 - val_loss: 34.5015\n",
      "Epoch 584/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.1396 - val_loss: 32.1651\n",
      "Epoch 585/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.6932 - val_loss: 32.4552\n",
      "Epoch 586/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.8334 - val_loss: 33.0650\n",
      "Epoch 587/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.0062 - val_loss: 32.6391\n",
      "Epoch 588/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8815 - val_loss: 32.8258\n",
      "Epoch 589/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.8583 - val_loss: 33.1651\n",
      "Epoch 590/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3116 - val_loss: 32.0659\n",
      "Epoch 591/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.3468 - val_loss: 32.7841\n",
      "Epoch 592/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.9441 - val_loss: 31.7792\n",
      "Epoch 593/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8282 - val_loss: 32.2378\n",
      "Epoch 594/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.7382 - val_loss: 32.0086\n",
      "Epoch 595/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.6149 - val_loss: 32.3094\n",
      "Epoch 596/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.6396 - val_loss: 32.5934\n",
      "Epoch 597/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.4761 - val_loss: 31.9325\n",
      "Epoch 598/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.4951 - val_loss: 32.4173\n",
      "Epoch 599/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8517 - val_loss: 32.4890\n",
      "Epoch 600/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 30.5260\n",
      "Epoch 00600: saving model to saved_models/latent128/cp-0600.h5\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 30.5260 - val_loss: 32.8810\n",
      "Epoch 601/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.8286 - val_loss: 33.5524\n",
      "Epoch 602/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1455 - val_loss: 32.0651\n",
      "Epoch 603/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.7697 - val_loss: 32.4690\n",
      "Epoch 604/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7134 - val_loss: 32.2956\n",
      "Epoch 605/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.4749 - val_loss: 33.0641\n",
      "Epoch 606/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.3532 - val_loss: 31.9956\n",
      "Epoch 607/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.5953 - val_loss: 32.7109\n",
      "Epoch 608/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7320 - val_loss: 31.9390\n",
      "Epoch 609/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4207 - val_loss: 34.3737\n",
      "Epoch 610/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.5239 - val_loss: 31.9101\n",
      "Epoch 611/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.4195 - val_loss: 32.0584\n",
      "Epoch 612/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.3666 - val_loss: 32.6046\n",
      "Epoch 613/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.2244 - val_loss: 31.9403\n",
      "Epoch 614/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.3938 - val_loss: 32.5241\n",
      "Epoch 615/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.0626 - val_loss: 31.9961\n",
      "Epoch 616/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.5995 - val_loss: 32.6057\n",
      "Epoch 617/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.3533 - val_loss: 32.3467\n",
      "Epoch 618/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.4397 - val_loss: 32.3470\n",
      "Epoch 619/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.3936 - val_loss: 31.5184\n",
      "Epoch 620/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.3309 - val_loss: 32.1248\n",
      "Epoch 621/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.3440 - val_loss: 31.7758\n",
      "Epoch 622/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.3354 - val_loss: 33.2792\n",
      "Epoch 623/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.6792 - val_loss: 31.7885\n",
      "Epoch 624/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4981 - val_loss: 32.1438\n",
      "Epoch 625/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.3508 - val_loss: 32.3177\n",
      "Epoch 626/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.3280 - val_loss: 33.0360\n",
      "Epoch 627/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.6496 - val_loss: 31.9247\n",
      "Epoch 628/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.1474 - val_loss: 32.6649\n",
      "Epoch 629/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.3612 - val_loss: 32.5331\n",
      "Epoch 630/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.1417 - val_loss: 31.3368\n",
      "Epoch 631/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.8370 - val_loss: 32.1972\n",
      "Epoch 632/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.2205 - val_loss: 32.2023\n",
      "Epoch 633/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.2953 - val_loss: 31.1248\n",
      "Epoch 634/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.3519 - val_loss: 31.9308\n",
      "Epoch 635/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.8773 - val_loss: 31.5732\n",
      "Epoch 636/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.9350 - val_loss: 31.7806\n",
      "Epoch 637/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.0013 - val_loss: 31.0635\n",
      "Epoch 638/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.1580 - val_loss: 31.5085\n",
      "Epoch 639/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.3070 - val_loss: 33.4339\n",
      "Epoch 640/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8791 - val_loss: 32.6217\n",
      "Epoch 641/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.5651 - val_loss: 31.2439\n",
      "Epoch 642/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.0527 - val_loss: 31.4876\n",
      "Epoch 643/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.7558 - val_loss: 31.6428\n",
      "Epoch 644/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.6693 - val_loss: 30.9763\n",
      "Epoch 645/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.1141 - val_loss: 31.5643\n",
      "Epoch 646/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.6721 - val_loss: 32.2791\n",
      "Epoch 647/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.7412 - val_loss: 31.8079\n",
      "Epoch 648/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.6410 - val_loss: 31.2296\n",
      "Epoch 649/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.0496 - val_loss: 32.0186\n",
      "Epoch 650/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.0438 - val_loss: 30.8546\n",
      "Epoch 651/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.9246 - val_loss: 31.7411\n",
      "Epoch 652/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.6507 - val_loss: 30.8852\n",
      "Epoch 653/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.6168 - val_loss: 31.1210\n",
      "Epoch 654/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.5561 - val_loss: 32.0013\n",
      "Epoch 655/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.8872 - val_loss: 31.3174\n",
      "Epoch 656/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.6870 - val_loss: 32.3569\n",
      "Epoch 657/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.9230 - val_loss: 31.7282\n",
      "Epoch 658/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.7551 - val_loss: 33.3730\n",
      "Epoch 659/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.5860 - val_loss: 31.0799\n",
      "Epoch 660/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.4093 - val_loss: 31.3009\n",
      "Epoch 661/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.3758 - val_loss: 31.4693\n",
      "Epoch 662/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.4604 - val_loss: 31.0205\n",
      "Epoch 663/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.4134 - val_loss: 31.0233\n",
      "Epoch 664/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.4193 - val_loss: 31.7261\n",
      "Epoch 665/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 29.2508 - val_loss: 31.2279\n",
      "Epoch 666/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.5084 - val_loss: 31.1355\n",
      "Epoch 667/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.6350 - val_loss: 31.3348\n",
      "Epoch 668/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.1967 - val_loss: 31.4183\n",
      "Epoch 669/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.1579 - val_loss: 30.7319\n",
      "Epoch 670/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.1929 - val_loss: 31.2185\n",
      "Epoch 671/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.2106 - val_loss: 31.0063\n",
      "Epoch 672/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.2425 - val_loss: 30.9371\n",
      "Epoch 673/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.1113 - val_loss: 31.2830\n",
      "Epoch 674/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.2238 - val_loss: 31.2946\n",
      "Epoch 675/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.2497 - val_loss: 30.7252\n",
      "Epoch 676/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.1297 - val_loss: 30.6107\n",
      "Epoch 677/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.1958 - val_loss: 30.9738\n",
      "Epoch 678/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.3798 - val_loss: 31.1938\n",
      "Epoch 679/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.1413 - val_loss: 31.7020\n",
      "Epoch 680/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.1688 - val_loss: 31.7628\n",
      "Epoch 681/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.9312 - val_loss: 30.5194\n",
      "Epoch 682/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.0318 - val_loss: 30.8789\n",
      "Epoch 683/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.0857 - val_loss: 31.3804\n",
      "Epoch 684/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.3109 - val_loss: 31.1820\n",
      "Epoch 685/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.4753 - val_loss: 31.0795\n",
      "Epoch 686/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.4572 - val_loss: 31.8166\n",
      "Epoch 687/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.7504 - val_loss: 31.6459\n",
      "Epoch 688/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.1770 - val_loss: 30.8499\n",
      "Epoch 689/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.0462 - val_loss: 30.6182\n",
      "Epoch 690/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.1417 - val_loss: 35.0345\n",
      "Epoch 691/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.2004 - val_loss: 31.9623\n",
      "Epoch 692/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.1368 - val_loss: 30.2289\n",
      "Epoch 693/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.2170 - val_loss: 30.8608\n",
      "Epoch 694/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.9398 - val_loss: 30.6662\n",
      "Epoch 695/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.8409 - val_loss: 30.8913\n",
      "Epoch 696/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.9637 - val_loss: 31.2891\n",
      "Epoch 697/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.7871 - val_loss: 29.8171\n",
      "Epoch 698/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.8150 - val_loss: 30.1538\n",
      "Epoch 699/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.9290 - val_loss: 31.6040\n",
      "Epoch 700/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.1514 - val_loss: 31.0539\n",
      "Epoch 701/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.4414 - val_loss: 31.8078\n",
      "Epoch 702/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.6108 - val_loss: 31.4917\n",
      "Epoch 703/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.5161 - val_loss: 31.4775\n",
      "Epoch 704/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.9274 - val_loss: 30.8674\n",
      "Epoch 705/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.9835 - val_loss: 30.5658\n",
      "Epoch 706/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.1763 - val_loss: 31.7636\n",
      "Epoch 707/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.0213 - val_loss: 30.9706\n",
      "Epoch 708/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.1290 - val_loss: 31.1387\n",
      "Epoch 709/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.5457 - val_loss: 31.7001\n",
      "Epoch 710/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.0587 - val_loss: 30.6029\n",
      "Epoch 711/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.0863 - val_loss: 31.3164\n",
      "Epoch 712/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.1868 - val_loss: 31.2712\n",
      "Epoch 713/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.6648 - val_loss: 30.7471\n",
      "Epoch 714/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.8940 - val_loss: 30.7867\n",
      "Epoch 715/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.5714 - val_loss: 30.5454\n",
      "Epoch 716/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.5942 - val_loss: 30.5859\n",
      "Epoch 717/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6624 - val_loss: 30.9887\n",
      "Epoch 718/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6760 - val_loss: 30.8535\n",
      "Epoch 719/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6270 - val_loss: 30.7017\n",
      "Epoch 720/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.5179 - val_loss: 30.7071\n",
      "Epoch 721/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.5922 - val_loss: 30.1503\n",
      "Epoch 722/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.7692 - val_loss: 30.1304\n",
      "Epoch 723/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.4489 - val_loss: 30.3656\n",
      "Epoch 724/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.7970 - val_loss: 31.3176\n",
      "Epoch 725/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.4139 - val_loss: 32.2558\n",
      "Epoch 726/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.8161 - val_loss: 31.0040\n",
      "Epoch 727/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.8710 - val_loss: 32.0400\n",
      "Epoch 728/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.1428 - val_loss: 30.6400\n",
      "Epoch 729/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.1092 - val_loss: 32.0612\n",
      "Epoch 730/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.3303 - val_loss: 31.5063\n",
      "Epoch 731/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.9848 - val_loss: 30.7612\n",
      "Epoch 732/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.7635 - val_loss: 31.2469\n",
      "Epoch 733/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.8330 - val_loss: 30.7599\n",
      "Epoch 734/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6144 - val_loss: 30.8639\n",
      "Epoch 735/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.1015 - val_loss: 31.1576\n",
      "Epoch 736/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.9290 - val_loss: 30.3597\n",
      "Epoch 737/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.4155 - val_loss: 30.5052\n",
      "Epoch 738/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.4029 - val_loss: 30.3812\n",
      "Epoch 739/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.3444 - val_loss: 30.4746\n",
      "Epoch 740/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.4604 - val_loss: 31.4213\n",
      "Epoch 741/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.3859 - val_loss: 30.4157\n",
      "Epoch 742/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.3740 - val_loss: 29.7752\n",
      "Epoch 743/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.1901 - val_loss: 32.0632\n",
      "Epoch 744/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.5277 - val_loss: 30.8137\n",
      "Epoch 745/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.4893 - val_loss: 30.2083\n",
      "Epoch 746/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.3010 - val_loss: 31.5306\n",
      "Epoch 747/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.3578 - val_loss: 30.2841\n",
      "Epoch 748/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.4219 - val_loss: 30.6772\n",
      "Epoch 749/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.3094 - val_loss: 30.1996\n",
      "Epoch 750/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.2306 - val_loss: 31.0607\n",
      "Epoch 751/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.5441 - val_loss: 30.8676\n",
      "Epoch 752/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.2682 - val_loss: 31.8883\n",
      "Epoch 753/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.3873 - val_loss: 32.5732\n",
      "Epoch 754/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.3310 - val_loss: 29.9674\n",
      "Epoch 755/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.2183 - val_loss: 30.6200\n",
      "Epoch 756/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.8545 - val_loss: 31.4981\n",
      "Epoch 757/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6053 - val_loss: 30.8906\n",
      "Epoch 758/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2611 - val_loss: 30.0224\n",
      "Epoch 759/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.1274 - val_loss: 29.9147\n",
      "Epoch 760/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.3515 - val_loss: 29.8222\n",
      "Epoch 761/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.5326 - val_loss: 30.6614\n",
      "Epoch 762/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.3054 - val_loss: 30.4254\n",
      "Epoch 763/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.8036 - val_loss: 32.8821\n",
      "Epoch 764/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.1903 - val_loss: 31.0741\n",
      "Epoch 765/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.4948 - val_loss: 30.3298\n",
      "Epoch 766/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.0374 - val_loss: 29.8172\n",
      "Epoch 767/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2663 - val_loss: 30.5998\n",
      "Epoch 768/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.2969 - val_loss: 30.3048\n",
      "Epoch 769/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.3245 - val_loss: 30.8606\n",
      "Epoch 770/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.0148 - val_loss: 29.9892\n",
      "Epoch 771/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.0031 - val_loss: 30.7095\n",
      "Epoch 772/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2950 - val_loss: 31.7990\n",
      "Epoch 773/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.3440 - val_loss: 30.7285\n",
      "Epoch 774/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.1982 - val_loss: 31.4552\n",
      "Epoch 775/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2859 - val_loss: 31.0138\n",
      "Epoch 776/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.2149 - val_loss: 30.6583\n",
      "Epoch 777/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.0747 - val_loss: 30.8102\n",
      "Epoch 778/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.9096 - val_loss: 30.1991\n",
      "Epoch 779/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.0986 - val_loss: 30.1845\n",
      "Epoch 780/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.0515 - val_loss: 29.7759\n",
      "Epoch 781/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9555 - val_loss: 30.5902\n",
      "Epoch 782/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.8375 - val_loss: 31.3936\n",
      "Epoch 783/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.9850 - val_loss: 29.9317\n",
      "Epoch 784/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9317 - val_loss: 30.9663\n",
      "Epoch 785/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.0414 - val_loss: 30.4050\n",
      "Epoch 786/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9150 - val_loss: 29.9962\n",
      "Epoch 787/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.0163 - val_loss: 30.3405\n",
      "Epoch 788/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.1643 - val_loss: 30.3568\n",
      "Epoch 789/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.9725 - val_loss: 31.2465\n",
      "Epoch 790/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.7893 - val_loss: 30.7547\n",
      "Epoch 791/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8462 - val_loss: 29.4487\n",
      "Epoch 792/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9835 - val_loss: 30.5258\n",
      "Epoch 793/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9185 - val_loss: 30.3047\n",
      "Epoch 794/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.7947 - val_loss: 29.9204\n",
      "Epoch 795/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8606 - val_loss: 29.8114\n",
      "Epoch 796/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.5615 - val_loss: 31.0324\n",
      "Epoch 797/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8137 - val_loss: 30.2580\n",
      "Epoch 798/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8097 - val_loss: 30.1707\n",
      "Epoch 799/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6062 - val_loss: 29.5492\n",
      "Epoch 800/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 27.7467\n",
      "Epoch 00800: saving model to saved_models/latent128/cp-0800.h5\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 27.7467 - val_loss: 31.1440\n",
      "Epoch 801/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6661 - val_loss: 30.6997\n",
      "Epoch 802/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9064 - val_loss: 31.5949\n",
      "Epoch 803/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.9652 - val_loss: 30.1471\n",
      "Epoch 804/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.7261 - val_loss: 30.6416\n",
      "Epoch 805/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.0235 - val_loss: 32.0701\n",
      "Epoch 806/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.1240 - val_loss: 29.6504\n",
      "Epoch 807/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.1434 - val_loss: 30.4164\n",
      "Epoch 808/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8380 - val_loss: 30.5463\n",
      "Epoch 809/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8581 - val_loss: 30.8129\n",
      "Epoch 810/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.0135 - val_loss: 30.6656\n",
      "Epoch 811/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6124 - val_loss: 31.0829\n",
      "Epoch 812/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6894 - val_loss: 31.3761\n",
      "Epoch 813/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9191 - val_loss: 30.8977\n",
      "Epoch 814/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.1945 - val_loss: 30.0046\n",
      "Epoch 815/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9191 - val_loss: 30.1408\n",
      "Epoch 816/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9177 - val_loss: 30.1971\n",
      "Epoch 817/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.8035 - val_loss: 30.4527\n",
      "Epoch 818/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.0985 - val_loss: 30.9372\n",
      "Epoch 819/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.7892 - val_loss: 30.8456\n",
      "Epoch 820/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.0089 - val_loss: 30.1662\n",
      "Epoch 821/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8112 - val_loss: 29.8928\n",
      "Epoch 822/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.7094 - val_loss: 30.1102\n",
      "Epoch 823/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.5329 - val_loss: 29.6015\n",
      "Epoch 824/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.6267 - val_loss: 30.9317\n",
      "Epoch 825/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5893 - val_loss: 30.4779\n",
      "Epoch 826/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3578 - val_loss: 30.8165\n",
      "Epoch 827/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.5914 - val_loss: 29.9685\n",
      "Epoch 828/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5023 - val_loss: 31.0987\n",
      "Epoch 829/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.5614 - val_loss: 30.1152\n",
      "Epoch 830/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.8053 - val_loss: 30.0815\n",
      "Epoch 831/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6477 - val_loss: 30.6503\n",
      "Epoch 832/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.5349 - val_loss: 30.3562\n",
      "Epoch 833/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6635 - val_loss: 30.4302\n",
      "Epoch 834/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6428 - val_loss: 29.8062\n",
      "Epoch 835/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4995 - val_loss: 29.7290\n",
      "Epoch 836/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.5047 - val_loss: 30.3450\n",
      "Epoch 837/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.7143 - val_loss: 30.4482\n",
      "Epoch 838/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6414 - val_loss: 30.1440\n",
      "Epoch 839/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4499 - val_loss: 30.8207\n",
      "Epoch 840/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5294 - val_loss: 34.5772\n",
      "Epoch 841/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5621 - val_loss: 29.5805\n",
      "Epoch 842/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.6934 - val_loss: 30.7423\n",
      "Epoch 843/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5484 - val_loss: 30.7247\n",
      "Epoch 844/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.7983 - val_loss: 29.5176\n",
      "Epoch 845/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6849 - val_loss: 31.0504\n",
      "Epoch 846/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.8983 - val_loss: 29.9676\n",
      "Epoch 847/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4691 - val_loss: 30.4996\n",
      "Epoch 848/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4032 - val_loss: 29.3534\n",
      "Epoch 849/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.6168 - val_loss: 29.9512\n",
      "Epoch 850/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3764 - val_loss: 30.8273\n",
      "Epoch 851/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5069 - val_loss: 30.7022\n",
      "Epoch 852/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6946 - val_loss: 30.5765\n",
      "Epoch 853/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6106 - val_loss: 29.9898\n",
      "Epoch 854/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3037 - val_loss: 31.3564\n",
      "Epoch 855/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5120 - val_loss: 30.3720\n",
      "Epoch 856/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4832 - val_loss: 29.8814\n",
      "Epoch 857/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5381 - val_loss: 29.6646\n",
      "Epoch 858/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3683 - val_loss: 29.4922\n",
      "Epoch 859/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5169 - val_loss: 29.5420\n",
      "Epoch 860/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2252 - val_loss: 31.3919\n",
      "Epoch 861/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8886 - val_loss: 30.7776\n",
      "Epoch 862/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4643 - val_loss: 30.9327\n",
      "Epoch 863/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4768 - val_loss: 32.0474\n",
      "Epoch 864/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3708 - val_loss: 30.5682\n",
      "Epoch 865/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.1930 - val_loss: 29.6412\n",
      "Epoch 866/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4202 - val_loss: 30.4182\n",
      "Epoch 867/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4388 - val_loss: 31.7931\n",
      "Epoch 868/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8238 - val_loss: 29.9823\n",
      "Epoch 869/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5298 - val_loss: 31.1033\n",
      "Epoch 870/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.7551 - val_loss: 29.9924\n",
      "Epoch 871/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4621 - val_loss: 29.6847\n",
      "Epoch 872/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4224 - val_loss: 29.6512\n",
      "Epoch 873/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4746 - val_loss: 30.2190\n",
      "Epoch 874/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3147 - val_loss: 29.7778\n",
      "Epoch 875/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3543 - val_loss: 33.5917\n",
      "Epoch 876/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3010 - val_loss: 32.5749\n",
      "Epoch 877/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3480 - val_loss: 33.0134\n",
      "Epoch 878/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5924 - val_loss: 30.5084\n",
      "Epoch 879/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3177 - val_loss: 30.2704\n",
      "Epoch 880/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4320 - val_loss: 30.6500\n",
      "Epoch 881/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.9058 - val_loss: 30.8996\n",
      "Epoch 882/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8591 - val_loss: 30.5408\n",
      "Epoch 883/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3173 - val_loss: 29.9356\n",
      "Epoch 884/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3078 - val_loss: 30.9693\n",
      "Epoch 885/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4735 - val_loss: 29.9163\n",
      "Epoch 886/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.1570 - val_loss: 31.0202\n",
      "Epoch 887/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0529 - val_loss: 30.2156\n",
      "Epoch 888/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0799 - val_loss: 30.1878\n",
      "Epoch 889/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1347 - val_loss: 30.0126\n",
      "Epoch 890/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3424 - val_loss: 30.1795\n",
      "Epoch 891/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0694 - val_loss: 30.1699\n",
      "Epoch 892/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2241 - val_loss: 30.2937\n",
      "Epoch 893/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2695 - val_loss: 29.9928\n",
      "Epoch 894/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3503 - val_loss: 29.6049\n",
      "Epoch 895/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2409 - val_loss: 34.2238\n",
      "Epoch 896/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2074 - val_loss: 29.6367\n",
      "Epoch 897/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2055 - val_loss: 30.9266\n",
      "Epoch 898/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5306 - val_loss: 29.8797\n",
      "Epoch 899/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2902 - val_loss: 30.0686\n",
      "Epoch 900/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0866 - val_loss: 31.1926\n",
      "Epoch 901/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4195 - val_loss: 31.0471\n",
      "Epoch 902/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2678 - val_loss: 30.5874\n",
      "Epoch 903/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3212 - val_loss: 29.6086\n",
      "Epoch 904/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2903 - val_loss: 30.8093\n",
      "Epoch 905/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0740 - val_loss: 30.3158\n",
      "Epoch 906/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.2019 - val_loss: 30.3586\n",
      "Epoch 907/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1313 - val_loss: 30.8286\n",
      "Epoch 908/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1021 - val_loss: 30.1433\n",
      "Epoch 909/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2118 - val_loss: 33.5780\n",
      "Epoch 910/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.9704 - val_loss: 31.0412\n",
      "Epoch 911/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0057 - val_loss: 30.3057\n",
      "Epoch 912/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0036 - val_loss: 29.7374\n",
      "Epoch 913/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0828 - val_loss: 29.6741\n",
      "Epoch 914/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4223 - val_loss: 32.5071\n",
      "Epoch 915/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5738 - val_loss: 30.1971\n",
      "Epoch 916/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.2600 - val_loss: 31.2797\n",
      "Epoch 917/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0036 - val_loss: 33.0009\n",
      "Epoch 918/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2339 - val_loss: 30.1319\n",
      "Epoch 919/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1195 - val_loss: 29.9973\n",
      "Epoch 920/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0901 - val_loss: 33.9891\n",
      "Epoch 921/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9918 - val_loss: 30.8943\n",
      "Epoch 922/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0087 - val_loss: 31.1125\n",
      "Epoch 923/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.1137 - val_loss: 30.5126\n",
      "Epoch 924/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0385 - val_loss: 30.6027\n",
      "Epoch 925/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1041 - val_loss: 30.7508\n",
      "Epoch 926/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1604 - val_loss: 30.0233\n",
      "Epoch 927/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1138 - val_loss: 31.4147\n",
      "Epoch 928/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0201 - val_loss: 29.7863\n",
      "Epoch 929/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0513 - val_loss: 29.1968\n",
      "Epoch 930/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.9328 - val_loss: 30.1708\n",
      "Epoch 931/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1540 - val_loss: 31.2152\n",
      "Epoch 932/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1645 - val_loss: 31.7366\n",
      "Epoch 933/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4812 - val_loss: 31.8329\n",
      "Epoch 934/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3869 - val_loss: 30.1354\n",
      "Epoch 935/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2522 - val_loss: 30.1831\n",
      "Epoch 936/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.9147 - val_loss: 30.2777\n",
      "Epoch 937/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1303 - val_loss: 30.8292\n",
      "Epoch 938/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9556 - val_loss: 30.6502\n",
      "Epoch 939/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8661 - val_loss: 32.4676\n",
      "Epoch 940/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8233 - val_loss: 30.0850\n",
      "Epoch 941/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1071 - val_loss: 29.9963\n",
      "Epoch 942/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8720 - val_loss: 30.3874\n",
      "Epoch 943/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9499 - val_loss: 30.3444\n",
      "Epoch 944/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9986 - val_loss: 30.0583\n",
      "Epoch 945/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8930 - val_loss: 30.1101\n",
      "Epoch 946/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.6095 - val_loss: 31.0148\n",
      "Epoch 947/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8690 - val_loss: 29.5692\n",
      "Epoch 948/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8087 - val_loss: 29.5639\n",
      "Epoch 949/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9072 - val_loss: 29.0761\n",
      "Epoch 950/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.2069 - val_loss: 29.9308\n",
      "Epoch 951/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3432 - val_loss: 30.7060\n",
      "Epoch 952/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.6955 - val_loss: 32.3595\n",
      "Epoch 953/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1051 - val_loss: 30.2258\n",
      "Epoch 954/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9772 - val_loss: 30.0729\n",
      "Epoch 955/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2532 - val_loss: 30.6335\n",
      "Epoch 956/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0474 - val_loss: 30.3495\n",
      "Epoch 957/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0232 - val_loss: 30.3125\n",
      "Epoch 958/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9032 - val_loss: 29.6213\n",
      "Epoch 959/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8236 - val_loss: 30.5287\n",
      "Epoch 960/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8178 - val_loss: 30.1257\n",
      "Epoch 961/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9498 - val_loss: 30.8678\n",
      "Epoch 962/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1109 - val_loss: 30.0250\n",
      "Epoch 963/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0090 - val_loss: 31.6509\n",
      "Epoch 964/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1275 - val_loss: 30.4649\n",
      "Epoch 965/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.2435 - val_loss: 30.6863\n",
      "Epoch 966/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9050 - val_loss: 30.7848\n",
      "Epoch 967/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0518 - val_loss: 29.7697\n",
      "Epoch 968/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9845 - val_loss: 34.7189\n",
      "Epoch 969/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7668 - val_loss: 29.8755\n",
      "Epoch 970/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9748 - val_loss: 29.4623\n",
      "Epoch 971/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7330 - val_loss: 29.5236\n",
      "Epoch 972/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9179 - val_loss: 31.4159\n",
      "Epoch 973/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1155 - val_loss: 30.0510\n",
      "Epoch 974/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0464 - val_loss: 29.9607\n",
      "Epoch 975/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9724 - val_loss: 29.9040\n",
      "Epoch 976/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.2469 - val_loss: 29.5173\n",
      "Epoch 977/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9066 - val_loss: 32.7529\n",
      "Epoch 978/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9564 - val_loss: 30.3662\n",
      "Epoch 979/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7944 - val_loss: 29.0130\n",
      "Epoch 980/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8626 - val_loss: 31.0292\n",
      "Epoch 981/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7829 - val_loss: 29.9030\n",
      "Epoch 982/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9385 - val_loss: 29.5535\n",
      "Epoch 983/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8484 - val_loss: 30.2634\n",
      "Epoch 984/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7823 - val_loss: 31.5414\n",
      "Epoch 985/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7038 - val_loss: 29.9075\n",
      "Epoch 986/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7212 - val_loss: 30.3832\n",
      "Epoch 987/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8370 - val_loss: 30.8838\n",
      "Epoch 988/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7924 - val_loss: 30.2450\n",
      "Epoch 989/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8211 - val_loss: 31.8272\n",
      "Epoch 990/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8560 - val_loss: 31.5839\n",
      "Epoch 991/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9937 - val_loss: 30.7002\n",
      "Epoch 992/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8554 - val_loss: 30.3228\n",
      "Epoch 993/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8888 - val_loss: 31.1829\n",
      "Epoch 994/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9958 - val_loss: 29.8114\n",
      "Epoch 995/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6105 - val_loss: 29.5939\n",
      "Epoch 996/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6708 - val_loss: 31.1264\n",
      "Epoch 997/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6667 - val_loss: 29.8237\n",
      "Epoch 998/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6640 - val_loss: 29.9534\n",
      "Epoch 999/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.5858 - val_loss: 30.1334\n",
      "Epoch 1000/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 26.5837\n",
      "Epoch 01000: saving model to saved_models/latent128/cp-1000.h5\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 26.5837 - val_loss: 29.8297\n",
      "Epoch 1001/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6794 - val_loss: 30.6157\n",
      "Epoch 1002/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7053 - val_loss: 29.8701\n",
      "Epoch 1003/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9617 - val_loss: 30.3642\n",
      "Epoch 1004/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9443 - val_loss: 29.9458\n",
      "Epoch 1005/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8631 - val_loss: 29.6368\n",
      "Epoch 1006/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.5561 - val_loss: 31.0149\n",
      "Epoch 1007/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.4902 - val_loss: 30.6920\n",
      "Epoch 1008/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5062 - val_loss: 30.4693\n",
      "Epoch 1009/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6222 - val_loss: 29.8634\n",
      "Epoch 1010/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6397 - val_loss: 30.4497\n",
      "Epoch 1011/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7008 - val_loss: 31.0534\n",
      "Epoch 1012/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8764 - val_loss: 31.4561\n",
      "Epoch 1013/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9987 - val_loss: 31.3107\n",
      "Epoch 1014/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8804 - val_loss: 30.0049\n",
      "Epoch 1015/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7510 - val_loss: 29.3961\n",
      "Epoch 1016/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6963 - val_loss: 32.2067\n",
      "Epoch 1017/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5513 - val_loss: 30.3097\n",
      "Epoch 1018/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.4665 - val_loss: 28.8598\n",
      "Epoch 1019/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7138 - val_loss: 30.5020\n",
      "Epoch 1020/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7799 - val_loss: 31.0220\n",
      "Epoch 1021/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6116 - val_loss: 31.0066\n",
      "Epoch 1022/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4357 - val_loss: 29.5323\n",
      "Epoch 1023/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5377 - val_loss: 30.4060\n",
      "Epoch 1024/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5822 - val_loss: 30.8996\n",
      "Epoch 1025/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7246 - val_loss: 29.8258\n",
      "Epoch 1026/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9079 - val_loss: 29.7759\n",
      "Epoch 1027/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7403 - val_loss: 30.7250\n",
      "Epoch 1028/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7318 - val_loss: 33.6048\n",
      "Epoch 1029/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5943 - val_loss: 30.7247\n",
      "Epoch 1030/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8621 - val_loss: 31.0013\n",
      "Epoch 1031/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5269 - val_loss: 32.1184\n",
      "Epoch 1032/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5526 - val_loss: 29.8924\n",
      "Epoch 1033/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5338 - val_loss: 32.0004\n",
      "Epoch 1034/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7276 - val_loss: 31.0851\n",
      "Epoch 1035/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5968 - val_loss: 31.2974\n",
      "Epoch 1036/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6000 - val_loss: 29.8536\n",
      "Epoch 1037/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7931 - val_loss: 31.6616\n",
      "Epoch 1038/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5998 - val_loss: 33.6649\n",
      "Epoch 1039/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5165 - val_loss: 30.1362\n",
      "Epoch 1040/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.4140 - val_loss: 32.2120\n",
      "Epoch 1041/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5199 - val_loss: 30.6248\n",
      "Epoch 1042/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.4086 - val_loss: 31.8307\n",
      "Epoch 1043/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8961 - val_loss: 30.1557\n",
      "Epoch 1044/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8637 - val_loss: 31.0007\n",
      "Epoch 1045/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8037 - val_loss: 30.3955\n",
      "Epoch 1046/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4295 - val_loss: 29.9244\n",
      "Epoch 1047/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6123 - val_loss: 29.3949\n",
      "Epoch 1048/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7073 - val_loss: 30.6010\n",
      "Epoch 1049/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.2983 - val_loss: 30.5772\n",
      "Epoch 1050/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6302 - val_loss: 29.4927\n",
      "Epoch 1051/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5663 - val_loss: 30.5547\n",
      "Epoch 1052/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9233 - val_loss: 31.6290\n",
      "Epoch 1053/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4480 - val_loss: 30.0951\n",
      "Epoch 1054/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4359 - val_loss: 29.2392\n",
      "Epoch 1055/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.1952 - val_loss: 30.5927\n",
      "Epoch 1056/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1970 - val_loss: 29.8816\n",
      "Epoch 1057/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3984 - val_loss: 30.8359\n",
      "Epoch 1058/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4689 - val_loss: 29.7129\n",
      "Epoch 1059/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3248 - val_loss: 30.4519\n",
      "Epoch 1060/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4557 - val_loss: 29.9733\n",
      "Epoch 1061/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2966 - val_loss: 29.8322\n",
      "Epoch 1062/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4728 - val_loss: 29.6070\n",
      "Epoch 1063/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5063 - val_loss: 31.0927\n",
      "Epoch 1064/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6051 - val_loss: 36.2904\n",
      "Epoch 1065/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5429 - val_loss: 31.5334\n",
      "Epoch 1066/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3199 - val_loss: 31.7443\n",
      "Epoch 1067/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3580 - val_loss: 33.4155\n",
      "Epoch 1068/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4419 - val_loss: 30.2049\n",
      "Epoch 1069/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3360 - val_loss: 33.5521\n",
      "Epoch 1070/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4682 - val_loss: 29.7538\n",
      "Epoch 1071/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1771 - val_loss: 29.7929\n",
      "Epoch 1072/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3454 - val_loss: 29.3359\n",
      "Epoch 1073/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3584 - val_loss: 31.9691\n",
      "Epoch 1074/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3738 - val_loss: 33.4594\n",
      "Epoch 1075/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2998 - val_loss: 31.0586\n",
      "Epoch 1076/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3942 - val_loss: 31.1655\n",
      "Epoch 1077/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2784 - val_loss: 30.0140\n",
      "Epoch 1078/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6170 - val_loss: 30.9072\n",
      "Epoch 1079/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6572 - val_loss: 31.1774\n",
      "Epoch 1080/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7205 - val_loss: 30.9330\n",
      "Epoch 1081/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2654 - val_loss: 30.0870\n",
      "Epoch 1082/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5275 - val_loss: 29.9745\n",
      "Epoch 1083/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2574 - val_loss: 31.3569\n",
      "Epoch 1084/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3209 - val_loss: 30.5231\n",
      "Epoch 1085/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5945 - val_loss: 30.6134\n",
      "Epoch 1086/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3881 - val_loss: 30.0929\n",
      "Epoch 1087/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4312 - val_loss: 33.2987\n",
      "Epoch 1088/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4681 - val_loss: 34.5731\n",
      "Epoch 1089/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6794 - val_loss: 31.4524\n",
      "Epoch 1090/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3478 - val_loss: 32.4203\n",
      "Epoch 1091/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1416 - val_loss: 31.7479\n",
      "Epoch 1092/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0154 - val_loss: 30.9558\n",
      "Epoch 1093/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1209 - val_loss: 31.4531\n",
      "Epoch 1094/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6518 - val_loss: 31.1151\n",
      "Epoch 1095/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4814 - val_loss: 30.2373\n",
      "Epoch 1096/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3747 - val_loss: 29.2626\n",
      "Epoch 1097/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1669 - val_loss: 32.2450\n",
      "Epoch 1098/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3481 - val_loss: 32.7945\n",
      "Epoch 1099/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.1473 - val_loss: 33.6379\n",
      "Epoch 1100/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3434 - val_loss: 30.7484\n",
      "Epoch 1101/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6828 - val_loss: 31.0813\n",
      "Epoch 1102/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7321 - val_loss: 29.9632\n",
      "Epoch 1103/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3776 - val_loss: 30.7365\n",
      "Epoch 1104/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4622 - val_loss: 31.5603\n",
      "Epoch 1105/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4336 - val_loss: 30.4104\n",
      "Epoch 1106/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5867 - val_loss: 29.4294\n",
      "Epoch 1107/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6078 - val_loss: 31.4636\n",
      "Epoch 1108/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3258 - val_loss: 30.6149\n",
      "Epoch 1109/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2184 - val_loss: 29.3977\n",
      "Epoch 1110/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1492 - val_loss: 29.9207\n",
      "Epoch 1111/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1820 - val_loss: 29.0676\n",
      "Epoch 1112/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.0773 - val_loss: 31.2274\n",
      "Epoch 1113/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5684 - val_loss: 30.1487\n",
      "Epoch 1114/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3061 - val_loss: 30.1371\n",
      "Epoch 1115/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3286 - val_loss: 30.3824\n",
      "Epoch 1116/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6849 - val_loss: 33.8003\n",
      "Epoch 1117/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2475 - val_loss: 30.5041\n",
      "Epoch 1118/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4068 - val_loss: 30.6531\n",
      "Epoch 1119/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6092 - val_loss: 29.9049\n",
      "Epoch 1120/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4756 - val_loss: 29.7163\n",
      "Epoch 1121/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3971 - val_loss: 30.9102\n",
      "Epoch 1122/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2576 - val_loss: 29.7880\n",
      "Epoch 1123/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0935 - val_loss: 30.8550\n",
      "Epoch 1124/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.0495 - val_loss: 34.1400\n",
      "Epoch 1125/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4808 - val_loss: 32.1039\n",
      "Epoch 1126/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6764 - val_loss: 32.7552\n",
      "Epoch 1127/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4392 - val_loss: 32.1525\n",
      "Epoch 1128/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3022 - val_loss: 30.1529\n",
      "Epoch 1129/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2266 - val_loss: 29.6518\n",
      "Epoch 1130/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.9597 - val_loss: 32.2949\n",
      "Epoch 1131/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3194 - val_loss: 32.0234\n",
      "Epoch 1132/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0273 - val_loss: 31.4963\n",
      "Epoch 1133/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2337 - val_loss: 31.2565\n",
      "Epoch 1134/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2116 - val_loss: 29.7133\n",
      "Epoch 1135/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0566 - val_loss: 33.9471\n",
      "Epoch 1136/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4778 - val_loss: 31.8080\n",
      "Epoch 1137/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3131 - val_loss: 30.0303\n",
      "Epoch 1138/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1836 - val_loss: 31.3543\n",
      "Epoch 1139/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3412 - val_loss: 29.9633\n",
      "Epoch 1140/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3745 - val_loss: 30.6341\n",
      "Epoch 1141/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6092 - val_loss: 32.3238\n",
      "Epoch 1142/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6461 - val_loss: 29.5431\n",
      "Epoch 1143/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2888 - val_loss: 33.3386\n",
      "Epoch 1144/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2398 - val_loss: 29.9236\n",
      "Epoch 1145/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0688 - val_loss: 30.5321\n",
      "Epoch 1146/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2850 - val_loss: 29.4255\n",
      "Epoch 1147/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2503 - val_loss: 33.2861\n",
      "Epoch 1148/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0698 - val_loss: 30.2555\n",
      "Epoch 1149/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9687 - val_loss: 30.6175\n",
      "Epoch 1150/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2552 - val_loss: 31.4163\n",
      "Epoch 1151/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2872 - val_loss: 30.5335\n",
      "Epoch 1152/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1066 - val_loss: 31.8903\n",
      "Epoch 1153/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1796 - val_loss: 30.8509\n",
      "Epoch 1154/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9825 - val_loss: 31.7106\n",
      "Epoch 1155/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0672 - val_loss: 33.1084\n",
      "Epoch 1156/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4133 - val_loss: 31.8333\n",
      "Epoch 1157/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2248 - val_loss: 30.9441\n",
      "Epoch 1158/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1207 - val_loss: 30.4644\n",
      "Epoch 1159/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1818 - val_loss: 29.5860\n",
      "Epoch 1160/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0900 - val_loss: 28.8121\n",
      "Epoch 1161/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0475 - val_loss: 30.3947\n",
      "Epoch 1162/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.8006 - val_loss: 31.8378\n",
      "Epoch 1163/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3097 - val_loss: 30.6724\n",
      "Epoch 1164/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0213 - val_loss: 33.5970\n",
      "Epoch 1165/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0885 - val_loss: 32.2352\n",
      "Epoch 1166/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0485 - val_loss: 31.4107\n",
      "Epoch 1167/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0829 - val_loss: 31.8032\n",
      "Epoch 1168/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2404 - val_loss: 30.3203\n",
      "Epoch 1169/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9377 - val_loss: 29.3095\n",
      "Epoch 1170/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0401 - val_loss: 30.5058\n",
      "Epoch 1171/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1590 - val_loss: 31.0156\n",
      "Epoch 1172/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5072 - val_loss: 33.1262\n",
      "Epoch 1173/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5554 - val_loss: 30.7176\n",
      "Epoch 1174/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5535 - val_loss: 30.8377\n",
      "Epoch 1175/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2722 - val_loss: 34.8408\n",
      "Epoch 1176/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2451 - val_loss: 32.3097\n",
      "Epoch 1177/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9863 - val_loss: 29.2160\n",
      "Epoch 1178/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8817 - val_loss: 32.5757\n",
      "Epoch 1179/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0259 - val_loss: 33.1528\n",
      "Epoch 1180/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0166 - val_loss: 32.7719\n",
      "Epoch 1181/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3339 - val_loss: 30.6796\n",
      "Epoch 1182/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9805 - val_loss: 29.8722\n",
      "Epoch 1183/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0376 - val_loss: 30.2802\n",
      "Epoch 1184/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2391 - val_loss: 30.6436\n",
      "Epoch 1185/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1375 - val_loss: 32.7509\n",
      "Epoch 1186/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0814 - val_loss: 32.2378\n",
      "Epoch 1187/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0142 - val_loss: 29.8484\n",
      "Epoch 1188/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3338 - val_loss: 31.2038\n",
      "Epoch 1189/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7144 - val_loss: 31.7248\n",
      "Epoch 1190/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3293 - val_loss: 29.5473\n",
      "Epoch 1191/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1461 - val_loss: 29.3229\n",
      "Epoch 1192/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4019 - val_loss: 31.3366\n",
      "Epoch 1193/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2739 - val_loss: 32.8723\n",
      "Epoch 1194/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3580 - val_loss: 29.5507\n",
      "Epoch 1195/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0699 - val_loss: 32.6252\n",
      "Epoch 1196/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.1662 - val_loss: 30.4326\n",
      "Epoch 1197/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3590 - val_loss: 30.9330\n",
      "Epoch 1198/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1662 - val_loss: 31.2541\n",
      "Epoch 1199/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1938 - val_loss: 31.2403\n",
      "Epoch 1200/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 26.0585\n",
      "Epoch 01200: saving model to saved_models/latent128/cp-1200.h5\n",
      "7/7 [==============================] - 1s 156ms/step - loss: 26.0585 - val_loss: 31.0506\n",
      "Epoch 1201/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0729 - val_loss: 31.0712\n",
      "Epoch 1202/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9575 - val_loss: 29.8478\n",
      "Epoch 1203/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2779 - val_loss: 30.9093\n",
      "Epoch 1204/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1680 - val_loss: 31.9308\n",
      "Epoch 1205/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1609 - val_loss: 31.9219\n",
      "Epoch 1206/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0374 - val_loss: 31.2282\n",
      "Epoch 1207/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0779 - val_loss: 29.6619\n",
      "Epoch 1208/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8974 - val_loss: 30.0807\n",
      "Epoch 1209/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8107 - val_loss: 31.4509\n",
      "Epoch 1210/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0562 - val_loss: 30.2553\n",
      "Epoch 1211/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1517 - val_loss: 29.2358\n",
      "Epoch 1212/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1503 - val_loss: 32.2505\n",
      "Epoch 1213/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2809 - val_loss: 30.4154\n",
      "Epoch 1214/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0973 - val_loss: 29.6280\n",
      "Epoch 1215/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0960 - val_loss: 30.4698\n",
      "Epoch 1216/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9102 - val_loss: 33.1063\n",
      "Epoch 1217/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0422 - val_loss: 32.5545\n",
      "Epoch 1218/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9924 - val_loss: 31.1632\n",
      "Epoch 1219/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3481 - val_loss: 33.9099\n",
      "Epoch 1220/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.7996 - val_loss: 32.3875\n",
      "Epoch 1221/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9792 - val_loss: 31.0617\n",
      "Epoch 1222/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8619 - val_loss: 29.7838\n",
      "Epoch 1223/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.7506 - val_loss: 33.0921\n",
      "Epoch 1224/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0715 - val_loss: 30.8398\n",
      "Epoch 1225/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9866 - val_loss: 31.2814\n",
      "Epoch 1226/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1424 - val_loss: 30.9076\n",
      "Epoch 1227/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9823 - val_loss: 31.4912\n",
      "Epoch 1228/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8091 - val_loss: 30.0450\n",
      "Epoch 1229/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9300 - val_loss: 29.6258\n",
      "Epoch 1230/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8258 - val_loss: 30.9440\n",
      "Epoch 1231/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0536 - val_loss: 30.3693\n",
      "Epoch 1232/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1758 - val_loss: 31.4490\n",
      "Epoch 1233/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7737 - val_loss: 32.3392\n",
      "Epoch 1234/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7716 - val_loss: 29.8264\n",
      "Epoch 1235/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8851 - val_loss: 30.6392\n",
      "Epoch 1236/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0288 - val_loss: 31.1126\n",
      "Epoch 1237/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0078 - val_loss: 30.5677\n",
      "Epoch 1238/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8019 - val_loss: 30.2146\n",
      "Epoch 1239/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8978 - val_loss: 31.2980\n",
      "Epoch 1240/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8787 - val_loss: 31.4305\n",
      "Epoch 1241/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1258 - val_loss: 30.2548\n",
      "Epoch 1242/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0707 - val_loss: 31.2755\n",
      "Epoch 1243/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8566 - val_loss: 31.2676\n",
      "Epoch 1244/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3043 - val_loss: 31.0695\n",
      "Epoch 1245/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0499 - val_loss: 30.1986\n",
      "Epoch 1246/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.7169 - val_loss: 30.1227\n",
      "Epoch 1247/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8580 - val_loss: 28.9608\n",
      "Epoch 1248/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0510 - val_loss: 30.8540\n",
      "Epoch 1249/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8230 - val_loss: 30.5718\n",
      "Epoch 1250/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9177 - val_loss: 30.7754\n",
      "Epoch 1251/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9584 - val_loss: 31.8900\n",
      "Epoch 1252/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0337 - val_loss: 31.9645\n",
      "Epoch 1253/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2149 - val_loss: 31.2395\n",
      "Epoch 1254/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9563 - val_loss: 30.4198\n",
      "Epoch 1255/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0107 - val_loss: 33.5979\n",
      "Epoch 1256/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8675 - val_loss: 35.8003\n",
      "Epoch 1257/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7529 - val_loss: 31.4347\n",
      "Epoch 1258/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7748 - val_loss: 31.6943\n",
      "Epoch 1259/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.6388 - val_loss: 31.6597\n",
      "Epoch 1260/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7375 - val_loss: 32.3538\n",
      "Epoch 1261/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8978 - val_loss: 33.2607\n",
      "Epoch 1262/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8639 - val_loss: 32.0343\n",
      "Epoch 1263/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9472 - val_loss: 33.2357\n",
      "Epoch 1264/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1393 - val_loss: 33.1103\n",
      "Epoch 1265/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9621 - val_loss: 32.9438\n",
      "Epoch 1266/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8835 - val_loss: 31.2377\n",
      "Epoch 1267/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7758 - val_loss: 31.6483\n",
      "Epoch 1268/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7967 - val_loss: 31.6690\n",
      "Epoch 1269/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9285 - val_loss: 29.2971\n",
      "Epoch 1270/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8877 - val_loss: 30.5105\n",
      "Epoch 1271/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7671 - val_loss: 30.1227\n",
      "Epoch 1272/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8396 - val_loss: 32.0741\n",
      "Epoch 1273/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8007 - val_loss: 30.0563\n",
      "Epoch 1274/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.6293 - val_loss: 28.9009\n",
      "Epoch 1275/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9116 - val_loss: 31.2427\n",
      "Epoch 1276/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7650 - val_loss: 31.7487\n",
      "Epoch 1277/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6937 - val_loss: 32.4186\n",
      "Epoch 1278/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9423 - val_loss: 31.6509\n",
      "Epoch 1279/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0400 - val_loss: 32.0841\n",
      "Epoch 1280/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8974 - val_loss: 30.2044\n",
      "Epoch 1281/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9757 - val_loss: 29.8085\n",
      "Epoch 1282/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0344 - val_loss: 30.0213\n",
      "Epoch 1283/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8607 - val_loss: 35.0746\n",
      "Epoch 1284/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.5534 - val_loss: 33.4136\n",
      "Epoch 1285/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8561 - val_loss: 31.1221\n",
      "Epoch 1286/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9219 - val_loss: 30.6763\n",
      "Epoch 1287/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7022 - val_loss: 33.3005\n",
      "Epoch 1288/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7392 - val_loss: 29.5803\n",
      "Epoch 1289/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6573 - val_loss: 30.8436\n",
      "Epoch 1290/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7395 - val_loss: 30.9859\n",
      "Epoch 1291/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6886 - val_loss: 30.8725\n",
      "Epoch 1292/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9651 - val_loss: 30.6369\n",
      "Epoch 1293/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8575 - val_loss: 31.6882\n",
      "Epoch 1294/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5964 - val_loss: 30.0587\n",
      "Epoch 1295/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6201 - val_loss: 33.1169\n",
      "Epoch 1296/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7380 - val_loss: 32.7921\n",
      "Epoch 1297/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8912 - val_loss: 31.6813\n",
      "Epoch 1298/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6976 - val_loss: 32.8968\n",
      "Epoch 1299/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6249 - val_loss: 29.7734\n",
      "Epoch 1300/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7153 - val_loss: 33.5679\n",
      "Epoch 1301/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7718 - val_loss: 32.7668\n",
      "Epoch 1302/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8004 - val_loss: 33.7058\n",
      "Epoch 1303/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0418 - val_loss: 33.1191\n",
      "Epoch 1304/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3313 - val_loss: 33.0050\n",
      "Epoch 1305/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1544 - val_loss: 34.8797\n",
      "Epoch 1306/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6747 - val_loss: 30.6016\n",
      "Epoch 1307/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7761 - val_loss: 30.6251\n",
      "Epoch 1308/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0148 - val_loss: 30.1383\n",
      "Epoch 1309/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3644 - val_loss: 30.5024\n",
      "Epoch 1310/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3885 - val_loss: 32.7581\n",
      "Epoch 1311/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1329 - val_loss: 30.2596\n",
      "Epoch 1312/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0701 - val_loss: 31.6520\n",
      "Epoch 1313/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0627 - val_loss: 30.7042\n",
      "Epoch 1314/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6634 - val_loss: 30.7972\n",
      "Epoch 1315/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7347 - val_loss: 33.3219\n",
      "Epoch 1316/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6744 - val_loss: 32.4500\n",
      "Epoch 1317/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7001 - val_loss: 29.2477\n",
      "Epoch 1318/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.5526 - val_loss: 30.8402\n",
      "Epoch 1319/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5704 - val_loss: 29.1505\n",
      "Epoch 1320/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.4245 - val_loss: 34.0514\n",
      "Epoch 1321/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6572 - val_loss: 31.0388\n",
      "Epoch 1322/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.7524 - val_loss: 30.3817\n",
      "Epoch 1323/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9242 - val_loss: 28.8877\n",
      "Epoch 1324/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7054 - val_loss: 32.2605\n",
      "Epoch 1325/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6658 - val_loss: 30.9783\n",
      "Epoch 1326/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7706 - val_loss: 30.9324\n",
      "Epoch 1327/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8605 - val_loss: 34.9657\n",
      "Epoch 1328/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6414 - val_loss: 30.6568\n",
      "Epoch 1329/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8262 - val_loss: 31.3006\n",
      "Epoch 1330/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0267 - val_loss: 32.0569\n",
      "Epoch 1331/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7540 - val_loss: 32.7498\n",
      "Epoch 1332/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7831 - val_loss: 29.3393\n",
      "Epoch 1333/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6959 - val_loss: 34.1664\n",
      "Epoch 1334/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7541 - val_loss: 32.6409\n",
      "Epoch 1335/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6986 - val_loss: 29.3883\n",
      "Epoch 1336/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6232 - val_loss: 31.5602\n",
      "Epoch 1337/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5311 - val_loss: 31.0295\n",
      "Epoch 1338/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8816 - val_loss: 30.7584\n",
      "Epoch 1339/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8159 - val_loss: 34.1393\n",
      "Epoch 1340/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8836 - val_loss: 32.6407\n",
      "Epoch 1341/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7903 - val_loss: 36.4088\n",
      "Epoch 1342/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7687 - val_loss: 30.4619\n",
      "Epoch 1343/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6284 - val_loss: 32.3011\n",
      "Epoch 1344/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5493 - val_loss: 33.8933\n",
      "Epoch 1345/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4857 - val_loss: 28.3464\n",
      "Epoch 1346/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7865 - val_loss: 30.3962\n",
      "Epoch 1347/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5323 - val_loss: 30.5171\n",
      "Epoch 1348/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8090 - val_loss: 31.0411\n",
      "Epoch 1349/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7354 - val_loss: 29.9126\n",
      "Epoch 1350/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5316 - val_loss: 30.2625\n",
      "Epoch 1351/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6681 - val_loss: 33.5740\n",
      "Epoch 1352/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4503 - val_loss: 32.1834\n",
      "Epoch 1353/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7372 - val_loss: 30.0953\n",
      "Epoch 1354/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7088 - val_loss: 32.4797\n",
      "Epoch 1355/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1763 - val_loss: 31.2544\n",
      "Epoch 1356/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9249 - val_loss: 32.1091\n",
      "Epoch 1357/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7550 - val_loss: 32.5816\n",
      "Epoch 1358/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6049 - val_loss: 29.5784\n",
      "Epoch 1359/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4934 - val_loss: 30.1188\n",
      "Epoch 1360/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7223 - val_loss: 29.9154\n",
      "Epoch 1361/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4979 - val_loss: 29.8885\n",
      "Epoch 1362/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6048 - val_loss: 31.0692\n",
      "Epoch 1363/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5962 - val_loss: 30.7650\n",
      "Epoch 1364/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6704 - val_loss: 30.2223\n",
      "Epoch 1365/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6437 - val_loss: 29.6888\n",
      "Epoch 1366/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6568 - val_loss: 32.0587\n",
      "Epoch 1367/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5191 - val_loss: 32.1841\n",
      "Epoch 1368/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4747 - val_loss: 30.1363\n",
      "Epoch 1369/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4997 - val_loss: 31.2565\n",
      "Epoch 1370/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7146 - val_loss: 29.7653\n",
      "Epoch 1371/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7592 - val_loss: 32.0257\n",
      "Epoch 1372/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0298 - val_loss: 29.2760\n",
      "Epoch 1373/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7484 - val_loss: 29.9858\n",
      "Epoch 1374/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7317 - val_loss: 29.8979\n",
      "Epoch 1375/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7891 - val_loss: 33.1620\n",
      "Epoch 1376/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5781 - val_loss: 30.0536\n",
      "Epoch 1377/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5325 - val_loss: 30.3664\n",
      "Epoch 1378/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7561 - val_loss: 31.7859\n",
      "Epoch 1379/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6042 - val_loss: 32.4730\n",
      "Epoch 1380/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9249 - val_loss: 31.5677\n",
      "Epoch 1381/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5088 - val_loss: 32.4219\n",
      "Epoch 1382/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0596 - val_loss: 31.5439\n",
      "Epoch 1383/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7851 - val_loss: 31.7007\n",
      "Epoch 1384/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6452 - val_loss: 29.0308\n",
      "Epoch 1385/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7720 - val_loss: 30.9242\n",
      "Epoch 1386/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7990 - val_loss: 32.3730\n",
      "Epoch 1387/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4471 - val_loss: 31.1908\n",
      "Epoch 1388/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5799 - val_loss: 30.2141\n",
      "Epoch 1389/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6847 - val_loss: 32.8849\n",
      "Epoch 1390/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6438 - val_loss: 30.3881\n",
      "Epoch 1391/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8079 - val_loss: 31.9343\n",
      "Epoch 1392/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6859 - val_loss: 29.6669\n",
      "Epoch 1393/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5956 - val_loss: 32.4176\n",
      "Epoch 1394/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6789 - val_loss: 31.4680\n",
      "Epoch 1395/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5717 - val_loss: 32.5715\n",
      "Epoch 1396/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6304 - val_loss: 32.4529\n",
      "Epoch 1397/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8731 - val_loss: 32.1107\n",
      "Epoch 1398/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7441 - val_loss: 30.9620\n",
      "Epoch 1399/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6515 - val_loss: 32.7643\n",
      "Epoch 1400/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 25.6819\n",
      "Epoch 01400: saving model to saved_models/latent128/cp-1400.h5\n",
      "7/7 [==============================] - 1s 165ms/step - loss: 25.6819 - val_loss: 32.9591\n",
      "Epoch 1401/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5176 - val_loss: 30.3182\n",
      "Epoch 1402/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6083 - val_loss: 31.5399\n",
      "Epoch 1403/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.3919 - val_loss: 30.2697\n",
      "Epoch 1404/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4978 - val_loss: 30.9648\n",
      "Epoch 1405/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5228 - val_loss: 32.9524\n",
      "Epoch 1406/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4781 - val_loss: 29.7980\n",
      "Epoch 1407/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7523 - val_loss: 33.3975\n",
      "Epoch 1408/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5238 - val_loss: 34.9078\n",
      "Epoch 1409/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5639 - val_loss: 33.6539\n",
      "Epoch 1410/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6254 - val_loss: 33.8716\n",
      "Epoch 1411/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7125 - val_loss: 32.8574\n",
      "Epoch 1412/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8180 - val_loss: 31.0526\n",
      "Epoch 1413/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5611 - val_loss: 32.3273\n",
      "Epoch 1414/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0142 - val_loss: 33.2583\n",
      "Epoch 1415/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4761 - val_loss: 31.6781\n",
      "Epoch 1416/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9844 - val_loss: 31.8163\n",
      "Epoch 1417/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8802 - val_loss: 34.1349\n",
      "Epoch 1418/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7091 - val_loss: 31.0532\n",
      "Epoch 1419/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4227 - val_loss: 32.8361\n",
      "Epoch 1420/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4281 - val_loss: 33.1695\n",
      "Epoch 1421/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5654 - val_loss: 30.4954\n",
      "Epoch 1422/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5014 - val_loss: 32.4291\n",
      "Epoch 1423/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4428 - val_loss: 31.1554\n",
      "Epoch 1424/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4315 - val_loss: 32.7407\n",
      "Epoch 1425/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6962 - val_loss: 31.5513\n",
      "Epoch 1426/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6142 - val_loss: 30.7925\n",
      "Epoch 1427/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5620 - val_loss: 30.1770\n",
      "Epoch 1428/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8587 - val_loss: 30.0084\n",
      "Epoch 1429/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5437 - val_loss: 31.4932\n",
      "Epoch 1430/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7516 - val_loss: 30.4070\n",
      "Epoch 1431/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4366 - val_loss: 30.8988\n",
      "Epoch 1432/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5591 - val_loss: 29.7249\n",
      "Epoch 1433/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.5566 - val_loss: 28.9696\n",
      "Epoch 1434/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5049 - val_loss: 33.7006\n",
      "Epoch 1435/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5571 - val_loss: 31.4467\n",
      "Epoch 1436/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5446 - val_loss: 32.3574\n",
      "Epoch 1437/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5537 - val_loss: 30.8063\n",
      "Epoch 1438/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4530 - val_loss: 31.7418\n",
      "Epoch 1439/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4266 - val_loss: 32.1487\n",
      "Epoch 1440/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4196 - val_loss: 32.9821\n",
      "Epoch 1441/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7425 - val_loss: 30.6137\n",
      "Epoch 1442/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7671 - val_loss: 31.8368\n",
      "Epoch 1443/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 25.3966 - val_loss: 30.0118\n",
      "Epoch 1444/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6333 - val_loss: 32.9735\n",
      "Epoch 1445/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5148 - val_loss: 30.4475\n",
      "Epoch 1446/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.3596 - val_loss: 31.8694\n",
      "Epoch 1447/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5080 - val_loss: 29.8245\n",
      "Epoch 1448/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3917 - val_loss: 30.6121\n",
      "Epoch 1449/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4428 - val_loss: 30.3984\n",
      "Epoch 1450/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4679 - val_loss: 30.6400\n",
      "Epoch 1451/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8316 - val_loss: 32.5295\n",
      "Epoch 1452/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4872 - val_loss: 31.3961\n",
      "Epoch 1453/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3961 - val_loss: 30.6669\n",
      "Epoch 1454/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.4380 - val_loss: 28.9545\n",
      "Epoch 1455/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5350 - val_loss: 31.9113\n",
      "Epoch 1456/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5689 - val_loss: 36.6015\n",
      "Epoch 1457/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6070 - val_loss: 29.8369\n",
      "Epoch 1458/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4450 - val_loss: 30.9716\n",
      "Epoch 1459/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6613 - val_loss: 32.2859\n",
      "Epoch 1460/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7778 - val_loss: 31.8926\n",
      "Epoch 1461/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0532 - val_loss: 32.5506\n",
      "Epoch 1462/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4074 - val_loss: 30.8758\n",
      "Epoch 1463/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4165 - val_loss: 31.0127\n",
      "Epoch 1464/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5034 - val_loss: 30.4444\n",
      "Epoch 1465/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.3520 - val_loss: 31.0685\n",
      "Epoch 1466/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3912 - val_loss: 30.6702\n",
      "Epoch 1467/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8037 - val_loss: 31.5746\n",
      "Epoch 1468/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9643 - val_loss: 30.0012\n",
      "Epoch 1469/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8667 - val_loss: 31.0588\n",
      "Epoch 1470/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6899 - val_loss: 29.3409\n",
      "Epoch 1471/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5860 - val_loss: 30.9546\n",
      "Epoch 1472/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5459 - val_loss: 29.8241\n",
      "Epoch 1473/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0118 - val_loss: 30.9182\n",
      "Epoch 1474/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9891 - val_loss: 31.3724\n",
      "Epoch 1475/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6077 - val_loss: 29.2191\n",
      "Epoch 1476/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3935 - val_loss: 28.9047\n",
      "Epoch 1477/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.3212 - val_loss: 30.9572\n",
      "Epoch 1478/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.2872 - val_loss: 29.7347\n",
      "Epoch 1479/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4403 - val_loss: 31.2205\n",
      "Epoch 1480/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3334 - val_loss: 30.3657\n",
      "Epoch 1481/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2061 - val_loss: 29.0650\n",
      "Epoch 1482/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3512 - val_loss: 29.8072\n",
      "Epoch 1483/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9812 - val_loss: 29.6881\n",
      "Epoch 1484/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5501 - val_loss: 30.3035\n",
      "Epoch 1485/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9181 - val_loss: 32.3940\n",
      "Epoch 1486/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6817 - val_loss: 29.2005\n",
      "Epoch 1487/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4424 - val_loss: 29.9576\n",
      "Epoch 1488/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3973 - val_loss: 30.0955\n",
      "Epoch 1489/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3191 - val_loss: 33.6109\n",
      "Epoch 1490/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4197 - val_loss: 31.8197\n",
      "Epoch 1491/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5346 - val_loss: 30.3407\n",
      "Epoch 1492/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4158 - val_loss: 29.5441\n",
      "Epoch 1493/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5118 - val_loss: 30.8932\n",
      "Epoch 1494/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4355 - val_loss: 31.0921\n",
      "Epoch 1495/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3084 - val_loss: 29.3126\n",
      "Epoch 1496/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3876 - val_loss: 29.4739\n",
      "Epoch 1497/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4047 - val_loss: 32.3771\n",
      "Epoch 1498/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3752 - val_loss: 30.7155\n",
      "Epoch 1499/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5996 - val_loss: 30.1820\n",
      "Epoch 1500/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6861 - val_loss: 31.7259\n",
      "Epoch 1501/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4006 - val_loss: 32.0460\n",
      "Epoch 1502/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5338 - val_loss: 31.6554\n",
      "Epoch 1503/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5616 - val_loss: 29.8575\n",
      "Epoch 1504/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4131 - val_loss: 30.0358\n",
      "Epoch 1505/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3147 - val_loss: 30.1135\n",
      "Epoch 1506/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.1409 - val_loss: 31.2508\n",
      "Epoch 1507/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3753 - val_loss: 30.7712\n",
      "Epoch 1508/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5473 - val_loss: 31.6185\n",
      "Epoch 1509/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5477 - val_loss: 30.7499\n",
      "Epoch 1510/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7272 - val_loss: 32.9343\n",
      "Epoch 1511/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7390 - val_loss: 32.1353\n",
      "Epoch 1512/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5455 - val_loss: 31.6267\n",
      "Epoch 1513/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3834 - val_loss: 30.9193\n",
      "Epoch 1514/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4219 - val_loss: 30.2365\n",
      "Epoch 1515/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4438 - val_loss: 32.2962\n",
      "Epoch 1516/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3100 - val_loss: 29.4181\n",
      "Epoch 1517/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1831 - val_loss: 33.9868\n",
      "Epoch 1518/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3629 - val_loss: 30.4391\n",
      "Epoch 1519/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2791 - val_loss: 30.3854\n",
      "Epoch 1520/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3519 - val_loss: 32.4879\n",
      "Epoch 1521/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3713 - val_loss: 33.2959\n",
      "Epoch 1522/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3527 - val_loss: 30.0169\n",
      "Epoch 1523/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5007 - val_loss: 31.1405\n",
      "Epoch 1524/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4640 - val_loss: 30.7965\n",
      "Epoch 1525/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4735 - val_loss: 32.5282\n",
      "Epoch 1526/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6366 - val_loss: 32.0765\n",
      "Epoch 1527/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5919 - val_loss: 32.1350\n",
      "Epoch 1528/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9149 - val_loss: 31.8515\n",
      "Epoch 1529/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5302 - val_loss: 31.7907\n",
      "Epoch 1530/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6190 - val_loss: 33.0676\n",
      "Epoch 1531/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6823 - val_loss: 31.4139\n",
      "Epoch 1532/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6362 - val_loss: 31.0843\n",
      "Epoch 1533/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4190 - val_loss: 30.6112\n",
      "Epoch 1534/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2978 - val_loss: 32.4022\n",
      "Epoch 1535/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3674 - val_loss: 32.1021\n",
      "Epoch 1536/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6320 - val_loss: 31.4650\n",
      "Epoch 1537/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6908 - val_loss: 30.6832\n",
      "Epoch 1538/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3935 - val_loss: 31.8364\n",
      "Epoch 1539/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5668 - val_loss: 30.5804\n",
      "Epoch 1540/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4424 - val_loss: 31.5026\n",
      "Epoch 1541/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3897 - val_loss: 30.6930\n",
      "Epoch 1542/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3616 - val_loss: 30.2195\n",
      "Epoch 1543/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4218 - val_loss: 30.5960\n",
      "Epoch 1544/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5332 - val_loss: 30.8269\n",
      "Epoch 1545/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6142 - val_loss: 33.4177\n",
      "Epoch 1546/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3296 - val_loss: 30.8762\n",
      "Epoch 1547/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2137 - val_loss: 31.8907\n",
      "Epoch 1548/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3622 - val_loss: 30.3211\n",
      "Epoch 1549/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4097 - val_loss: 32.3801\n",
      "Epoch 1550/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4525 - val_loss: 32.1345\n",
      "Epoch 1551/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1177 - val_loss: 33.5438\n",
      "Epoch 1552/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9517 - val_loss: 31.6311\n",
      "Epoch 1553/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6177 - val_loss: 32.6269\n",
      "Epoch 1554/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5377 - val_loss: 30.0294\n",
      "Epoch 1555/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4532 - val_loss: 31.0139\n",
      "Epoch 1556/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4965 - val_loss: 31.4321\n",
      "Epoch 1557/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7791 - val_loss: 29.4039\n",
      "Epoch 1558/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6420 - val_loss: 31.6716\n",
      "Epoch 1559/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3297 - val_loss: 30.1116\n",
      "Epoch 1560/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3184 - val_loss: 30.6602\n",
      "Epoch 1561/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2566 - val_loss: 30.9139\n",
      "Epoch 1562/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4336 - val_loss: 29.2494\n",
      "Epoch 1563/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2050 - val_loss: 29.9430\n",
      "Epoch 1564/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3134 - val_loss: 32.4301\n",
      "Epoch 1565/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4407 - val_loss: 32.8801\n",
      "Epoch 1566/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2798 - val_loss: 30.5308\n",
      "Epoch 1567/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6132 - val_loss: 30.0882\n",
      "Epoch 1568/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5893 - val_loss: 29.3432\n",
      "Epoch 1569/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2191 - val_loss: 29.6020\n",
      "Epoch 1570/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4551 - val_loss: 34.6822\n",
      "Epoch 1571/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4682 - val_loss: 31.3045\n",
      "Epoch 1572/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5814 - val_loss: 31.8565\n",
      "Epoch 1573/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3887 - val_loss: 31.0332\n",
      "Epoch 1574/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3321 - val_loss: 29.9828\n",
      "Epoch 1575/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5302 - val_loss: 30.2282\n",
      "Epoch 1576/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5093 - val_loss: 31.0897\n",
      "Epoch 1577/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6353 - val_loss: 31.8507\n",
      "Epoch 1578/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7592 - val_loss: 30.0917\n",
      "Epoch 1579/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4525 - val_loss: 30.4713\n",
      "Epoch 1580/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2533 - val_loss: 32.4561\n",
      "Epoch 1581/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5125 - val_loss: 29.7621\n",
      "Epoch 1582/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3554 - val_loss: 30.8469\n",
      "Epoch 1583/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2418 - val_loss: 30.9524\n",
      "Epoch 1584/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3603 - val_loss: 32.9782\n",
      "Epoch 1585/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3606 - val_loss: 30.6020\n",
      "Epoch 1586/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3523 - val_loss: 31.1661\n",
      "Epoch 1587/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3726 - val_loss: 32.6711\n",
      "Epoch 1588/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4418 - val_loss: 30.3848\n",
      "Epoch 1589/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 25.4349 - val_loss: 31.2356\n",
      "Epoch 1590/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1908 - val_loss: 31.4722\n",
      "Epoch 1591/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4202 - val_loss: 30.4059\n",
      "Epoch 1592/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1924 - val_loss: 31.0874\n",
      "Epoch 1593/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2527 - val_loss: 31.2362\n",
      "Epoch 1594/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2934 - val_loss: 29.5662\n",
      "Epoch 1595/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.1141 - val_loss: 29.9910\n",
      "Epoch 1596/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1759 - val_loss: 32.9766\n",
      "Epoch 1597/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3483 - val_loss: 30.8091\n",
      "Epoch 1598/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1789 - val_loss: 32.3880\n",
      "Epoch 1599/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2848 - val_loss: 30.4541\n",
      "Epoch 1600/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 25.2400\n",
      "Epoch 01600: saving model to saved_models/latent128/cp-1600.h5\n",
      "7/7 [==============================] - 1s 175ms/step - loss: 25.2400 - val_loss: 30.8030\n",
      "Epoch 1601/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4122 - val_loss: 29.7983\n",
      "Epoch 1602/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1243 - val_loss: 31.7236\n",
      "Epoch 1603/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2628 - val_loss: 30.6097\n",
      "Epoch 1604/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2362 - val_loss: 32.3573\n",
      "Epoch 1605/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3914 - val_loss: 30.8993\n",
      "Epoch 1606/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1931 - val_loss: 31.0511\n",
      "Epoch 1607/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3638 - val_loss: 31.1515\n",
      "Epoch 1608/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3281 - val_loss: 33.1172\n",
      "Epoch 1609/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2323 - val_loss: 32.5975\n",
      "Epoch 1610/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2192 - val_loss: 32.5728\n",
      "Epoch 1611/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3466 - val_loss: 31.2928\n",
      "Epoch 1612/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2922 - val_loss: 32.5733\n",
      "Epoch 1613/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2871 - val_loss: 30.2458\n",
      "Epoch 1614/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2151 - val_loss: 30.6618\n",
      "Epoch 1615/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2834 - val_loss: 30.0006\n",
      "Epoch 1616/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.1023 - val_loss: 31.0849\n",
      "Epoch 1617/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.0347 - val_loss: 29.7235\n",
      "Epoch 1618/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1938 - val_loss: 31.4143\n",
      "Epoch 1619/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1108 - val_loss: 30.1634\n",
      "Epoch 1620/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7434 - val_loss: 33.6268\n",
      "Epoch 1621/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7067 - val_loss: 30.2725\n",
      "Epoch 1622/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7825 - val_loss: 36.1098\n",
      "Epoch 1623/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9824 - val_loss: 34.4109\n",
      "Epoch 1624/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7437 - val_loss: 32.4952\n",
      "Epoch 1625/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4024 - val_loss: 33.1021\n",
      "Epoch 1626/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3772 - val_loss: 31.4735\n",
      "Epoch 1627/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1249 - val_loss: 29.8759\n",
      "Epoch 1628/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1707 - val_loss: 31.1745\n",
      "Epoch 1629/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3853 - val_loss: 30.6608\n",
      "Epoch 1630/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2033 - val_loss: 30.3899\n",
      "Epoch 1631/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3689 - val_loss: 29.3756\n",
      "Epoch 1632/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1443 - val_loss: 32.9177\n",
      "Epoch 1633/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1773 - val_loss: 29.8131\n",
      "Epoch 1634/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1277 - val_loss: 30.0253\n",
      "Epoch 1635/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1691 - val_loss: 30.6138\n",
      "Epoch 1636/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3018 - val_loss: 31.4347\n",
      "Epoch 1637/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3280 - val_loss: 30.3511\n",
      "Epoch 1638/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4902 - val_loss: 31.0977\n",
      "Epoch 1639/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2160 - val_loss: 32.6348\n",
      "Epoch 1640/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2369 - val_loss: 30.6690\n",
      "Epoch 1641/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3472 - val_loss: 30.2863\n",
      "Epoch 1642/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3610 - val_loss: 31.8807\n",
      "Epoch 1643/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1677 - val_loss: 31.6541\n",
      "Epoch 1644/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.0103 - val_loss: 32.3959\n",
      "Epoch 1645/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2026 - val_loss: 30.3700\n",
      "Epoch 1646/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1411 - val_loss: 32.5066\n",
      "Epoch 1647/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3894 - val_loss: 31.8987\n",
      "Epoch 1648/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2284 - val_loss: 29.5000\n",
      "Epoch 1649/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3743 - val_loss: 29.9243\n",
      "Epoch 1650/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3892 - val_loss: 32.4154\n",
      "Epoch 1651/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4088 - val_loss: 29.8757\n",
      "Epoch 1652/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2843 - val_loss: 29.8039\n",
      "Epoch 1653/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2332 - val_loss: 30.3838\n",
      "Epoch 1654/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1745 - val_loss: 33.6412\n",
      "Epoch 1655/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3568 - val_loss: 31.6094\n",
      "Epoch 1656/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2137 - val_loss: 31.3228\n",
      "Epoch 1657/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1320 - val_loss: 31.0138\n",
      "Epoch 1658/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1887 - val_loss: 30.1548\n",
      "Epoch 1659/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9554 - val_loss: 31.0540\n",
      "Epoch 1660/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0294 - val_loss: 30.2486\n",
      "Epoch 1661/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2753 - val_loss: 32.0080\n",
      "Epoch 1662/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1818 - val_loss: 30.0990\n",
      "Epoch 1663/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1886 - val_loss: 30.4348\n",
      "Epoch 1664/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5924 - val_loss: 33.1023\n",
      "Epoch 1665/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6181 - val_loss: 30.0980\n",
      "Epoch 1666/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5795 - val_loss: 30.6271\n",
      "Epoch 1667/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3831 - val_loss: 30.0412\n",
      "Epoch 1668/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5028 - val_loss: 33.8389\n",
      "Epoch 1669/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6654 - val_loss: 29.9930\n",
      "Epoch 1670/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3878 - val_loss: 30.8062\n",
      "Epoch 1671/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2685 - val_loss: 30.3892\n",
      "Epoch 1672/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1509 - val_loss: 30.0183\n",
      "Epoch 1673/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2864 - val_loss: 30.8180\n",
      "Epoch 1674/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1422 - val_loss: 31.6482\n",
      "Epoch 1675/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0888 - val_loss: 31.6157\n",
      "Epoch 1676/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1808 - val_loss: 32.2155\n",
      "Epoch 1677/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2273 - val_loss: 30.4687\n",
      "Epoch 1678/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2047 - val_loss: 31.1083\n",
      "Epoch 1679/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3650 - val_loss: 32.0131\n",
      "Epoch 1680/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.9190 - val_loss: 30.8210\n",
      "Epoch 1681/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0079 - val_loss: 32.4040\n",
      "Epoch 1682/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2063 - val_loss: 31.9141\n",
      "Epoch 1683/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0603 - val_loss: 30.9768\n",
      "Epoch 1684/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1898 - val_loss: 32.8829\n",
      "Epoch 1685/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2895 - val_loss: 30.3318\n",
      "Epoch 1686/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0154 - val_loss: 30.5762\n",
      "Epoch 1687/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0184 - val_loss: 32.7168\n",
      "Epoch 1688/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1183 - val_loss: 30.7454\n",
      "Epoch 1689/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1505 - val_loss: 29.9883\n",
      "Epoch 1690/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2398 - val_loss: 29.1888\n",
      "Epoch 1691/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2718 - val_loss: 31.6079\n",
      "Epoch 1692/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4199 - val_loss: 32.3680\n",
      "Epoch 1693/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5684 - val_loss: 30.3935\n",
      "Epoch 1694/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2325 - val_loss: 33.1558\n",
      "Epoch 1695/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3925 - val_loss: 32.2003\n",
      "Epoch 1696/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2796 - val_loss: 31.0024\n",
      "Epoch 1697/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2997 - val_loss: 31.6962\n",
      "Epoch 1698/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5040 - val_loss: 31.1067\n",
      "Epoch 1699/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3183 - val_loss: 33.3889\n",
      "Epoch 1700/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4322 - val_loss: 29.6774\n",
      "Epoch 1701/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5078 - val_loss: 30.2990\n",
      "Epoch 1702/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1645 - val_loss: 31.0485\n",
      "Epoch 1703/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1895 - val_loss: 29.5694\n",
      "Epoch 1704/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2217 - val_loss: 31.3755\n",
      "Epoch 1705/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1075 - val_loss: 29.5654\n",
      "Epoch 1706/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1817 - val_loss: 32.4752\n",
      "Epoch 1707/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4141 - val_loss: 29.6136\n",
      "Epoch 1708/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4939 - val_loss: 31.4487\n",
      "Epoch 1709/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3693 - val_loss: 30.9333\n",
      "Epoch 1710/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2501 - val_loss: 33.3218\n",
      "Epoch 1711/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1753 - val_loss: 30.6286\n",
      "Epoch 1712/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0112 - val_loss: 32.3623\n",
      "Epoch 1713/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0343 - val_loss: 31.4558\n",
      "Epoch 1714/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1340 - val_loss: 30.8573\n",
      "Epoch 1715/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2422 - val_loss: 29.2390\n",
      "Epoch 1716/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2725 - val_loss: 31.6284\n",
      "Epoch 1717/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1514 - val_loss: 31.3156\n",
      "Epoch 1718/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3840 - val_loss: 31.2479\n",
      "Epoch 1719/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3773 - val_loss: 33.6575\n",
      "Epoch 1720/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2058 - val_loss: 30.8813\n",
      "Epoch 1721/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0754 - val_loss: 32.5423\n",
      "Epoch 1722/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0482 - val_loss: 29.6522\n",
      "Epoch 1723/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1060 - val_loss: 31.3287\n",
      "Epoch 1724/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1546 - val_loss: 30.9343\n",
      "Epoch 1725/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2151 - val_loss: 30.2104\n",
      "Epoch 1726/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1327 - val_loss: 31.7616\n",
      "Epoch 1727/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1000 - val_loss: 32.2477\n",
      "Epoch 1728/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0143 - val_loss: 30.3956\n",
      "Epoch 1729/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1069 - val_loss: 30.0039\n",
      "Epoch 1730/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9853 - val_loss: 28.9519\n",
      "Epoch 1731/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.8634 - val_loss: 29.7633\n",
      "Epoch 1732/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4863 - val_loss: 30.7461\n",
      "Epoch 1733/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3804 - val_loss: 30.3564\n",
      "Epoch 1734/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3880 - val_loss: 31.7080\n",
      "Epoch 1735/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0347 - val_loss: 31.2320\n",
      "Epoch 1736/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4173 - val_loss: 31.9728\n",
      "Epoch 1737/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2185 - val_loss: 31.8659\n",
      "Epoch 1738/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0615 - val_loss: 29.6554\n",
      "Epoch 1739/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2157 - val_loss: 32.1166\n",
      "Epoch 1740/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4746 - val_loss: 32.5380\n",
      "Epoch 1741/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2280 - val_loss: 31.6737\n",
      "Epoch 1742/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3116 - val_loss: 30.2410\n",
      "Epoch 1743/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0810 - val_loss: 31.9803\n",
      "Epoch 1744/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0848 - val_loss: 30.9614\n",
      "Epoch 1745/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1964 - val_loss: 33.3295\n",
      "Epoch 1746/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3494 - val_loss: 30.3922\n",
      "Epoch 1747/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1542 - val_loss: 32.5357\n",
      "Epoch 1748/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2101 - val_loss: 31.2784\n",
      "Epoch 1749/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2621 - val_loss: 31.1223\n",
      "Epoch 1750/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4210 - val_loss: 31.3426\n",
      "Epoch 1751/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.0346 - val_loss: 32.9855\n",
      "Epoch 1752/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4620 - val_loss: 34.3177\n",
      "Epoch 1753/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1951 - val_loss: 32.1495\n",
      "Epoch 1754/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2563 - val_loss: 32.3733\n",
      "Epoch 1755/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3040 - val_loss: 30.6318\n",
      "Epoch 1756/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2065 - val_loss: 31.6674\n",
      "Epoch 1757/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3109 - val_loss: 31.7900\n",
      "Epoch 1758/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0363 - val_loss: 32.1376\n",
      "Epoch 1759/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1075 - val_loss: 30.3308\n",
      "Epoch 1760/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0627 - val_loss: 29.7857\n",
      "Epoch 1761/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0766 - val_loss: 33.7651\n",
      "Epoch 1762/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9783 - val_loss: 32.5624\n",
      "Epoch 1763/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1739 - val_loss: 30.3518\n",
      "Epoch 1764/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3409 - val_loss: 30.2761\n",
      "Epoch 1765/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1363 - val_loss: 31.1433\n",
      "Epoch 1766/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1188 - val_loss: 32.1531\n",
      "Epoch 1767/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9738 - val_loss: 31.0089\n",
      "Epoch 1768/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.8500 - val_loss: 31.4394\n",
      "Epoch 1769/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4783 - val_loss: 30.7723\n",
      "Epoch 1770/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5042 - val_loss: 30.2445\n",
      "Epoch 1771/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4667 - val_loss: 30.8582\n",
      "Epoch 1772/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1665 - val_loss: 29.3747\n",
      "Epoch 1773/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0429 - val_loss: 31.5789\n",
      "Epoch 1774/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0939 - val_loss: 30.6113\n",
      "Epoch 1775/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1681 - val_loss: 31.0623\n",
      "Epoch 1776/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1934 - val_loss: 30.9233\n",
      "Epoch 1777/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1411 - val_loss: 29.4170\n",
      "Epoch 1778/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2133 - val_loss: 31.0336\n",
      "Epoch 1779/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5507 - val_loss: 31.5750\n",
      "Epoch 1780/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0455 - val_loss: 33.1690\n",
      "Epoch 1781/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3418 - val_loss: 29.5372\n",
      "Epoch 1782/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1923 - val_loss: 29.8132\n",
      "Epoch 1783/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1784 - val_loss: 30.7291\n",
      "Epoch 1784/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1168 - val_loss: 31.8632\n",
      "Epoch 1785/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1397 - val_loss: 32.3691\n",
      "Epoch 1786/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9728 - val_loss: 33.7427\n",
      "Epoch 1787/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1062 - val_loss: 33.7600\n",
      "Epoch 1788/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1853 - val_loss: 30.4873\n",
      "Epoch 1789/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9931 - val_loss: 32.4367\n",
      "Epoch 1790/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1532 - val_loss: 31.2988\n",
      "Epoch 1791/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1453 - val_loss: 33.0286\n",
      "Epoch 1792/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1629 - val_loss: 30.0352\n",
      "Epoch 1793/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8778 - val_loss: 32.2563\n",
      "Epoch 1794/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9841 - val_loss: 31.3558\n",
      "Epoch 1795/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0812 - val_loss: 31.1630\n",
      "Epoch 1796/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3408 - val_loss: 31.6310\n",
      "Epoch 1797/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2046 - val_loss: 31.0753\n",
      "Epoch 1798/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3020 - val_loss: 31.4531\n",
      "Epoch 1799/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0741 - val_loss: 32.4721\n",
      "Epoch 1800/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 25.0585\n",
      "Epoch 01800: saving model to saved_models/latent128/cp-1800.h5\n",
      "7/7 [==============================] - 1s 178ms/step - loss: 25.0585 - val_loss: 32.1732\n",
      "Epoch 1801/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1587 - val_loss: 32.8536\n",
      "Epoch 1802/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1508 - val_loss: 31.2148\n",
      "Epoch 1803/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1035 - val_loss: 31.4905\n",
      "Epoch 1804/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9786 - val_loss: 31.1663\n",
      "Epoch 1805/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9496 - val_loss: 30.1941\n",
      "Epoch 1806/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9502 - val_loss: 29.2589\n",
      "Epoch 1807/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9638 - val_loss: 32.8720\n",
      "Epoch 1808/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9807 - val_loss: 29.4721\n",
      "Epoch 1809/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1506 - val_loss: 32.5777\n",
      "Epoch 1810/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0315 - val_loss: 31.0054\n",
      "Epoch 1811/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1483 - val_loss: 30.1998\n",
      "Epoch 1812/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1714 - val_loss: 33.0010\n",
      "Epoch 1813/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0800 - val_loss: 31.2040\n",
      "Epoch 1814/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9659 - val_loss: 32.8533\n",
      "Epoch 1815/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1849 - val_loss: 31.6385\n",
      "Epoch 1816/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5831 - val_loss: 30.7204\n",
      "Epoch 1817/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4626 - val_loss: 30.7731\n",
      "Epoch 1818/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2752 - val_loss: 31.4075\n",
      "Epoch 1819/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2056 - val_loss: 30.7829\n",
      "Epoch 1820/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3441 - val_loss: 31.5876\n",
      "Epoch 1821/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2424 - val_loss: 30.2648\n",
      "Epoch 1822/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2063 - val_loss: 33.0342\n",
      "Epoch 1823/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4084 - val_loss: 30.9551\n",
      "Epoch 1824/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3558 - val_loss: 32.6147\n",
      "Epoch 1825/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3383 - val_loss: 30.7517\n",
      "Epoch 1826/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1547 - val_loss: 31.0427\n",
      "Epoch 1827/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1597 - val_loss: 32.2781\n",
      "Epoch 1828/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0404 - val_loss: 31.4808\n",
      "Epoch 1829/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1269 - val_loss: 32.5164\n",
      "Epoch 1830/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0910 - val_loss: 30.7916\n",
      "Epoch 1831/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9698 - val_loss: 31.8801\n",
      "Epoch 1832/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1998 - val_loss: 30.7700\n",
      "Epoch 1833/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3284 - val_loss: 31.6745\n",
      "Epoch 1834/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9884 - val_loss: 30.8450\n",
      "Epoch 1835/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0987 - val_loss: 32.7831\n",
      "Epoch 1836/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9524 - val_loss: 29.9890\n",
      "Epoch 1837/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8934 - val_loss: 32.5311\n",
      "Epoch 1838/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0139 - val_loss: 31.9989\n",
      "Epoch 1839/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3389 - val_loss: 31.2345\n",
      "Epoch 1840/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0117 - val_loss: 31.5348\n",
      "Epoch 1841/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1142 - val_loss: 33.1051\n",
      "Epoch 1842/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7723 - val_loss: 29.9757\n",
      "Epoch 1843/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8546 - val_loss: 31.7136\n",
      "Epoch 1844/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9816 - val_loss: 31.5043\n",
      "Epoch 1845/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1030 - val_loss: 32.8350\n",
      "Epoch 1846/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0335 - val_loss: 32.2803\n",
      "Epoch 1847/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0674 - val_loss: 32.1665\n",
      "Epoch 1848/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0788 - val_loss: 30.8328\n",
      "Epoch 1849/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.9684 - val_loss: 33.0676\n",
      "Epoch 1850/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8063 - val_loss: 30.6807\n",
      "Epoch 1851/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9930 - val_loss: 30.9126\n",
      "Epoch 1852/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0413 - val_loss: 31.2703\n",
      "Epoch 1853/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9598 - val_loss: 30.2082\n",
      "Epoch 1854/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1216 - val_loss: 31.1250\n",
      "Epoch 1855/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9499 - val_loss: 31.0833\n",
      "Epoch 1856/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9475 - val_loss: 31.5164\n",
      "Epoch 1857/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8446 - val_loss: 32.3438\n",
      "Epoch 1858/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0281 - val_loss: 31.3987\n",
      "Epoch 1859/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9990 - val_loss: 33.8508\n",
      "Epoch 1860/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0760 - val_loss: 31.7367\n",
      "Epoch 1861/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0851 - val_loss: 31.6886\n",
      "Epoch 1862/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8399 - val_loss: 32.8168\n",
      "Epoch 1863/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9106 - val_loss: 30.6548\n",
      "Epoch 1864/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9014 - val_loss: 29.7067\n",
      "Epoch 1865/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8269 - val_loss: 33.1287\n",
      "Epoch 1866/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9259 - val_loss: 30.3189\n",
      "Epoch 1867/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8098 - val_loss: 29.6019\n",
      "Epoch 1868/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9237 - val_loss: 31.0180\n",
      "Epoch 1869/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8447 - val_loss: 32.8104\n",
      "Epoch 1870/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8990 - val_loss: 31.1317\n",
      "Epoch 1871/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1129 - val_loss: 32.9245\n",
      "Epoch 1872/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9911 - val_loss: 31.7725\n",
      "Epoch 1873/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1012 - val_loss: 31.0917\n",
      "Epoch 1874/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7989 - val_loss: 31.2963\n",
      "Epoch 1875/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9484 - val_loss: 30.5766\n",
      "Epoch 1876/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8571 - val_loss: 30.9922\n",
      "Epoch 1877/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0827 - val_loss: 30.2972\n",
      "Epoch 1878/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8690 - val_loss: 29.3700\n",
      "Epoch 1879/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2842 - val_loss: 31.5118\n",
      "Epoch 1880/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2216 - val_loss: 31.3865\n",
      "Epoch 1881/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1334 - val_loss: 30.3167\n",
      "Epoch 1882/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7946 - val_loss: 31.6249\n",
      "Epoch 1883/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0417 - val_loss: 30.3463\n",
      "Epoch 1884/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3256 - val_loss: 29.9105\n",
      "Epoch 1885/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9700 - val_loss: 31.5419\n",
      "Epoch 1886/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5060 - val_loss: 32.3637\n",
      "Epoch 1887/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9998 - val_loss: 31.2723\n",
      "Epoch 1888/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1888 - val_loss: 32.3649\n",
      "Epoch 1889/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2072 - val_loss: 31.2338\n",
      "Epoch 1890/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1045 - val_loss: 31.0939\n",
      "Epoch 1891/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8154 - val_loss: 31.0947\n",
      "Epoch 1892/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9395 - val_loss: 32.7103\n",
      "Epoch 1893/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1525 - val_loss: 29.6956\n",
      "Epoch 1894/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9788 - val_loss: 29.9126\n",
      "Epoch 1895/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8535 - val_loss: 30.4632\n",
      "Epoch 1896/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8074 - val_loss: 30.5237\n",
      "Epoch 1897/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7804 - val_loss: 31.0590\n",
      "Epoch 1898/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8440 - val_loss: 32.5790\n",
      "Epoch 1899/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1487 - val_loss: 31.3787\n",
      "Epoch 1900/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9552 - val_loss: 31.2295\n",
      "Epoch 1901/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1455 - val_loss: 30.4628\n",
      "Epoch 1902/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2428 - val_loss: 31.6030\n",
      "Epoch 1903/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8361 - val_loss: 32.5280\n",
      "Epoch 1904/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0433 - val_loss: 32.5032\n",
      "Epoch 1905/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9570 - val_loss: 30.8048\n",
      "Epoch 1906/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0360 - val_loss: 30.6285\n",
      "Epoch 1907/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1642 - val_loss: 30.1897\n",
      "Epoch 1908/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1385 - val_loss: 32.1772\n",
      "Epoch 1909/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5549 - val_loss: 30.4527\n",
      "Epoch 1910/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0740 - val_loss: 31.8267\n",
      "Epoch 1911/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2481 - val_loss: 31.3388\n",
      "Epoch 1912/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2024 - val_loss: 33.9443\n",
      "Epoch 1913/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1788 - val_loss: 30.2236\n",
      "Epoch 1914/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8920 - val_loss: 31.6340\n",
      "Epoch 1915/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1825 - val_loss: 30.3039\n",
      "Epoch 1916/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0658 - val_loss: 31.4274\n",
      "Epoch 1917/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1230 - val_loss: 30.7012\n",
      "Epoch 1918/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9054 - val_loss: 30.0480\n",
      "Epoch 1919/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9375 - val_loss: 29.3479\n",
      "Epoch 1920/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9796 - val_loss: 31.5243\n",
      "Epoch 1921/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8693 - val_loss: 30.9021\n",
      "Epoch 1922/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8706 - val_loss: 31.1685\n",
      "Epoch 1923/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1077 - val_loss: 31.5995\n",
      "Epoch 1924/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2609 - val_loss: 31.3646\n",
      "Epoch 1925/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0660 - val_loss: 32.5926\n",
      "Epoch 1926/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1759 - val_loss: 30.5733\n",
      "Epoch 1927/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0004 - val_loss: 29.8378\n",
      "Epoch 1928/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1486 - val_loss: 32.3282\n",
      "Epoch 1929/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9859 - val_loss: 32.2162\n",
      "Epoch 1930/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0592 - val_loss: 29.3562\n",
      "Epoch 1931/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9574 - val_loss: 30.4852\n",
      "Epoch 1932/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8956 - val_loss: 29.5183\n",
      "Epoch 1933/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8948 - val_loss: 32.8203\n",
      "Epoch 1934/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7746 - val_loss: 31.6090\n",
      "Epoch 1935/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.7494 - val_loss: 31.2660\n",
      "Epoch 1936/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.7302 - val_loss: 31.4095\n",
      "Epoch 1937/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8072 - val_loss: 30.5523\n",
      "Epoch 1938/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7980 - val_loss: 30.4655\n",
      "Epoch 1939/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8062 - val_loss: 29.3982\n",
      "Epoch 1940/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9831 - val_loss: 30.6709\n",
      "Epoch 1941/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7646 - val_loss: 30.9453\n",
      "Epoch 1942/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9537 - val_loss: 31.0693\n",
      "Epoch 1943/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9570 - val_loss: 31.8539\n",
      "Epoch 1944/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8929 - val_loss: 31.0841\n",
      "Epoch 1945/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0100 - val_loss: 31.4181\n",
      "Epoch 1946/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0832 - val_loss: 29.6337\n",
      "Epoch 1947/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8505 - val_loss: 30.8669\n",
      "Epoch 1948/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9234 - val_loss: 30.4523\n",
      "Epoch 1949/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9367 - val_loss: 31.0608\n",
      "Epoch 1950/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8776 - val_loss: 29.7720\n",
      "Epoch 1951/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9780 - val_loss: 30.2638\n",
      "Epoch 1952/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2191 - val_loss: 31.3863\n",
      "Epoch 1953/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1855 - val_loss: 32.1234\n",
      "Epoch 1954/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8815 - val_loss: 31.7890\n",
      "Epoch 1955/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3501 - val_loss: 32.4737\n",
      "Epoch 1956/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9314 - val_loss: 30.6370\n",
      "Epoch 1957/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0416 - val_loss: 31.2481\n",
      "Epoch 1958/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2442 - val_loss: 31.9835\n",
      "Epoch 1959/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0059 - val_loss: 30.5548\n",
      "Epoch 1960/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9954 - val_loss: 28.9511\n",
      "Epoch 1961/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8495 - val_loss: 30.5630\n",
      "Epoch 1962/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0054 - val_loss: 30.4848\n",
      "Epoch 1963/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9921 - val_loss: 30.1421\n",
      "Epoch 1964/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3986 - val_loss: 30.5395\n",
      "Epoch 1965/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2937 - val_loss: 30.8782\n",
      "Epoch 1966/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0308 - val_loss: 31.1391\n",
      "Epoch 1967/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9953 - val_loss: 30.2468\n",
      "Epoch 1968/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9218 - val_loss: 31.0334\n",
      "Epoch 1969/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9732 - val_loss: 30.5127\n",
      "Epoch 1970/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8778 - val_loss: 33.0145\n",
      "Epoch 1971/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8518 - val_loss: 32.3575\n",
      "Epoch 1972/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9789 - val_loss: 29.7784\n",
      "Epoch 1973/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0451 - val_loss: 30.9088\n",
      "Epoch 1974/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0151 - val_loss: 30.0479\n",
      "Epoch 1975/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8946 - val_loss: 31.7537\n",
      "Epoch 1976/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9984 - val_loss: 30.6224\n",
      "Epoch 1977/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1879 - val_loss: 31.0107\n",
      "Epoch 1978/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9183 - val_loss: 29.4215\n",
      "Epoch 1979/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0317 - val_loss: 30.6834\n",
      "Epoch 1980/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9609 - val_loss: 31.1468\n",
      "Epoch 1981/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9747 - val_loss: 29.5772\n",
      "Epoch 1982/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8530 - val_loss: 31.0556\n",
      "Epoch 1983/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7855 - val_loss: 33.1248\n",
      "Epoch 1984/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8119 - val_loss: 29.7063\n",
      "Epoch 1985/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0496 - val_loss: 31.3087\n",
      "Epoch 1986/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.7186 - val_loss: 31.3278\n",
      "Epoch 1987/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0177 - val_loss: 32.1693\n",
      "Epoch 1988/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8681 - val_loss: 31.1181\n",
      "Epoch 1989/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0251 - val_loss: 30.7898\n",
      "Epoch 1990/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7318 - val_loss: 31.2968\n",
      "Epoch 1991/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8819 - val_loss: 34.5588\n",
      "Epoch 1992/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8620 - val_loss: 29.9725\n",
      "Epoch 1993/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0468 - val_loss: 30.6388\n",
      "Epoch 1994/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8051 - val_loss: 31.8101\n",
      "Epoch 1995/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8449 - val_loss: 31.2057\n",
      "Epoch 1996/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9635 - val_loss: 30.1071\n",
      "Epoch 1997/2000\n",
      "7/7 [==============================] - 1s 103ms/step - loss: 24.9932 - val_loss: 31.1322\n",
      "Epoch 1998/2000\n",
      "7/7 [==============================] - 1s 103ms/step - loss: 24.9681 - val_loss: 31.8562\n",
      "Epoch 1999/2000\n",
      "7/7 [==============================] - 1s 103ms/step - loss: 25.0407 - val_loss: 30.5587\n",
      "Epoch 2000/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 24.8968\n",
      "Epoch 02000: saving model to saved_models/latent128/cp-2000.h5\n",
      "7/7 [==============================] - 1s 185ms/step - loss: 24.8968 - val_loss: 32.7214\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train VAE model with callbacks \"\"\"\n",
    "history = vae.fit(trainX, trainX, \n",
    "                  batch_size = batch_size, \n",
    "                  epochs = epochs, \n",
    "                  callbacks=[initial_checkpoint, checkpoint, EarlyStoppingAtMinLoss()],\n",
    "                  #callbacks=[initial_checkpoint, checkpoint],\n",
    "                  shuffle=True,\n",
    "                  validation_data=(valX, valX)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 7.0\n"
     ]
    }
   ],
   "source": [
    "# In this GPU implementation, it shows the number of batches/iterations for one epoch (and not the training samples), which is:\n",
    "print (\"Step size:\", np.ceil(trainX.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save the entire model \"\"\"\n",
    "# Save notes about hyperparameters used:\n",
    "epochs = len(history.history['loss'])\n",
    "with open(f'{SAVE_FOLDER}/notes.txt', 'w') as f:\n",
    "    f.write(f'Epochs: {epochs} \\n')\n",
    "    f.write(f'Batch size: {batch_size} \\n')\n",
    "    f.write(f'Latent dimension: {latent_dim} \\n')\n",
    "    f.write(f'Filter sizes: {filters} \\n')\n",
    "    f.write(f'img_width: {img_width} \\n')\n",
    "    f.write(f'img_height: {img_height} \\n')\n",
    "    f.write(f'num_channels: {num_channels}')\n",
    "    f.close()\n",
    "\n",
    "# Save weights for encoder model:\n",
    "encoder.save_weights(f'{SAVE_FOLDER}/ENCODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for decoder model:\n",
    "decoder.save_weights(f'{SAVE_FOLDER}/DECODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for total VAE:\n",
    "vae.save_weights(f'{SAVE_FOLDER}/VAE_WEIGHTS.h5')\n",
    "\n",
    "# Save the history at each epochs (cost of train and validation sets):\n",
    "np.save(f'{SAVE_FOLDER}/HISTORY.npy', history.history)\n",
    "\n",
    "# Save exact training, validation and testing data set (as numpy arrays):\n",
    "np.save(f'{SAVE_FOLDER}/trainX.npy', trainX)\n",
    "np.save(f'{SAVE_FOLDER}/valX.npy', valX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllX.npy', testAllX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllY.npy', testAllY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate training process\n",
    "\n",
    "Now that the training process is complete, let’s evaluate the average loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyEAAAGQCAYAAAC5528KAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAABf4ElEQVR4nO3deXwTZeLH8W+StpQe9KKcVTmkVSmlCIjQsggiqICKiOABAiqyK6uLyqGuIohivX+CB8ulrPdyyamuKCLgAVpFDgFRlEPsDfRuk/n9MZvQ0HKnTRo+79eLF83MZOaZJ5NkvnmeZ8ZiGIYhAAAAAKghVm8XAAAAAMDZhRACAAAAoEYRQgAAAADUKEIIAAAAgBpFCAEAAABQowghAAAAAGoUIQQAgBNYuHChEhIS9PXXX3u7KDVmwoQJSkhIOO3n7927VwkJCZo2bZoHSwXAXwR4uwAAcLoOHjyorl27qqSkRGlpabruuuu8XSSf9/XXX2vo0KEaN26cbr/9dm8X56Ts3btXl19+ueuxxWJRaGio6tevr4suuki9evXSFVdcoYAA//1KmzZtmqZPn35Sy/bv319PPfVUNZcIAM6M/35iA/B7S5cuVWlpqeLi4rRgwQJCiJ9LSUnRtddeK0kqLCzUnj17tHr1aq1YsUKtW7fW9OnT1aRJk2rZ9rXXXqs+ffooMDCwWtZ/IldccYXOPfdct2lTp06VJD344INu049e7nQ9/vjjmjRp0mk/v2nTptq0aZNsNptHygPAvxBCANRa8+fPV6dOnXT55ZfrySef1J49e3TOOed4pSyGYaiwsFChoaFe2f7ZoFmzZq4Q4jRu3Di9/vrrmjp1qu666y4tWrTIoy0i+fn5CgsLk81m8+rJ9AUXXKALLrjAbdr//d//SVKlOjma3W5XaWmp6tate0rbPNPAZbFYVKdOnTNaBwD/xZgQALXSli1btG3bNvXv3199+/ZVQECA5s+f75pvt9uVmpqq/v37V/n8d999VwkJCfrkk09c00pLS/Xaa6+pT58+atOmjTp06KBRo0Zp69atbs/9+uuvlZCQoIULF+qtt97S1VdfrTZt2mjOnDmSpE2bNmnChAnq3bu32rZtq3bt2mnw4MH673//W2VZvvnmGw0aNEhJSUlKSUnRlClTtHPnzir70xuGobffflvXX3+9a91DhgzRV199dVr1eDwbNmzQ8OHD1b59eyUlJal///76z3/+U2m5nTt36p577lHXrl2VmJiolJQUDRkyRKtXr3YtU1JSomnTprnqpEOHDurXr5/S0tLOuJzDhg1Tv379tGPHDi1fvtw1fdq0aUpISNDevXsrPadHjx4aMmSI27SEhARNmDBBX375pW666Sa1a9dOf/3rXyVVPSbEOe3LL7/U7Nmz1bNnTyUmJqp3795atGhRpW3a7Xa9/PLL6t69u9q0aaN+/fppxYoVxy3nqXKWaf369Xr55ZfVs2dPJSUlaeXKlZKktWvX6h//+Icuv/xyJSUlqUOHDhoxYoS++eabSuuqakyIc9rhw4c1ceJEde7cWW3atNHgwYP1ww8/uC1b1ZiQitM+++wzDRgwQG3atFFqaqrS0tJUXl5eqRwfffSRrrnmGrVp00aXXXaZpk+frvXr17vegwBqJ1pCANRK8+fPV0hIiHr16qWQkBBddtllWrx4se69915ZrVbZbDZdc801mj17tnbu3KlWrVq5PX/x4sWKiopSt27dJEllZWW6/fbblZ6ermuvvVa33HKL8vPz9f777+umm27Sm2++qTZt2rit44033lBeXp4GDhyo2NhYNWrUSJL03//+V7/88ouuvPJKNW3aVHl5eVq0aJFGjx6tZ599Vv369XOtY+PGjRoxYoQiIiI0cuRIhYeHa+XKlfruu++q3O+xY8dq+fLl6t27t66//nqVlpZq6dKlGjFihKZNm+Y2duJMfPrppxo9erTq16+v4cOHKywsTMuXL9c///lP7d27V2PGjJEk5ebm6rbbbpMkDR48WE2aNFFubq42b96sH374QZdddpkkadKkSa4uc+3atZPdbtfu3bs9NtB74MCBWrp0qT7//PMTtgwcz+bNm/XRRx/pxhtvPGaAPdoLL7yg4uJiDRo0SEFBQXrnnXc0YcIEnXvuuWrfvr1rucmTJ+vdd99Vp06dNGLECOXk5GjSpElq2rTpaZf3WJwn9DfeeKNCQ0PVvHlzSdKiRYt08OBBXXfddWrUqJH+/PNP/ec//9GwYcM0b948dejQ4aTWf/vttys6Olp333238vLyNHfuXI0cOVKrVq1SWFjYCZ//+eef6+2339bgwYM1YMAArVq1SnPmzFFERIRGjRrlWm7FihW67777dO6552r06NGy2WxavHixPv3009OrGAC+wwCAWqa4uNjo0KGDMX78eNe0//73v0Z8fLyxevVq17QdO3YY8fHxRlpamtvzf/vtNyM+Pt54/PHHXdPmzp1rxMfHG2vWrHFb9vDhw0a3bt2MW2+91TXtq6++MuLj442OHTsaWVlZlcpXUFBQaVphYaHRq1cv46qrrnKbPmDAACMxMdH4/fffXdNKS0uNQYMGGfHx8cZLL73kmv7xxx8b8fHxxrvvvuu2jrKyMqN///5G9+7dDYfDUWnbFTnLPmvWrGMuU15eblx22WVG+/btjQMHDriml5SUGIMGDTIuuOAC49dffzUMwzA++eQTIz4+3li+fPlxt9uxY0fjjjvuOO4yx7Jnzx4jPj7emDRp0jGXyc3NNeLj443+/fu7pr300ktGfHy8sWfPnkrLd+/e3e01NQzDiI+PN+Lj441169ZVWn7BggVGfHy88dVXX1Wadu211xolJSWu6QcOHDBat25tjBkzxjXNeSyOGDHCsNvtruk//fSTccEFFxyznMfTvXt3o3v37lWWs1evXkZhYWGl51R1bGZmZhqXXHJJpddn/PjxRnx8fJXTJk6c6DZ9xYoVRnx8vPHOO++4pjlft4rHsHNa27Zt3fbX4XAYffr0MVJSUlzTysrKjNTUVKNz585GXl6ea3p+fr7Ro0cPIz4+3liwYEFVVQOgFqA7FoBa5+OPP9ahQ4fcBqJ369ZN0dHRWrBggWtaq1at1Lp1ay1dulQOh8M1ffHixZLk9vwlS5aoRYsWat26tXJyclz/SktL1aVLF3377bcqLi52K8e1116rmJiYSuULCQlx/V1UVKTc3FwVFRXp0ksv1a5du5Sfny9JysrK0o8//qjLL7/cbSxLYGCghg4dWmm9S5YsUWhoqHr27OlWxkOHDqlHjx7at2+fdu/efVJ1eDxbtmzR/v37NWDAADVs2NA1PSgoSHfccYccDodWrVolSQoPD5ckffHFF679qkpYWJh+/vln7dix44zLd6z1SzpuGU7GBRdcoC5dupzSc26++WYFBQW5Hjds2FDNmzd3ey0+++wzSdLQoUNltR756k1ISFBqauoZlbkqN910U5VjQCoemwUFBcrNzZXValXbtm21adOmk17/sGHD3B5feumlkqTffvvtpJ5/+eWXKy4uzvXYYrGoU6dOyszMVEFBgSTzOMzIyFD//v0VERHhWjY0NFSDBw8+6bIC8E10xwJQ68yfP1/R0dFq1KiR20lPSkqKPvzwQ+Xk5Cg6OlqSebnSKVOmaP369UpNTZVhGFqyZIlatWqlxMRE13N37dql4uJide7c+Zjbzc3NVePGjV2PmzVrVuVy2dnZevHFF7Vq1SplZ2dXmn/o0CGFhYW5xgA4u8pU1KJFi0rTdu3apYKCguOeJGdnZ1e5vlPhLNf5559faZ6zW9uePXskSZdccomuu+46LVy4UEuXLlViYqK6dOmiq6++2u35Dz30kMaNG6d+/frpnHPOUadOndS9e3f16NHD7aT8dDnDx8l0BTqeY72mx1PVxRAiIyO1b98+12NnnVb1ujZv3lxr1qw55e0ez7GOgd9//10vvPCC1q5dq0OHDrnNs1gsJ73+o/c5KipKkpSXl3daz5fMOnOuIzQ09LjvjzM9xgF4HyEEQK2yZ88eff311zIMQ717965ymSVLlrh+qe3Tp4/S0tK0ePFipaam6ttvv9WePXv0wAMPuD3HMAzFx8dXutxpRc5g41TVL82GYWjEiBHatWuXhg4dqsTERIWHh8tms2nBggVatmyZW6vMqTAMQ9HR0XruueeOuczRY19qQlpamm6//XatWbNGGzdu1Ny5c/Xaa6/poYce0q233ipJ6tmzpz799FN9/vnn2rBhg9avX6/58+erQ4cOmjt3rltLwunYvn27JPeT0+OdVFc1AFqq+jU9EU+EKE8LDg6uNK2goEC33HKLioqKdNtttyk+Pl6hoaGyWq2aMWPGKV3c4FhXCjMM44yefyrrAFC7EUIA1CoLFy6UYRiaMmWKqytQRS+++KIWLFjgCiHR0dH6y1/+ok8++UQFBQVavHixrFarrrnmGrfnnXfeecrNzdWll156RieV27dv108//aS7775b99xzj9u8o68s5RyQ/Ouvv1Zazy+//FJp2nnnnafdu3erbdu21XopYGc3mZ9//rnSPOe0o3/Jjo+PV3x8vO644w4dOnRIAwcO1HPPPadbbrnFFQYiIyN17bXX6tprr5VhGHr22Wc1a9YsrVq1SlddddUZldlZt84LDUhydeE5ePCgW9efkpISZWZm6rzzzjujbZ4K5/Z/+eWXSnVX1etfHb788ktlZGToySef1IABA9zmvfjiizVShlNxvPdHTdUZgOrjez/fAMAxOBwOLVq0SPHx8Ro4cKCuvPLKSv/69u2rHTt2uPVv79+/v4qKirRkyRJ9+OGH6tKli9tYB8kcH5KZmam5c+dWue2srKyTKqMzwBz9a+6OHTsqXaI3NjZWiYmJWrVqlat7k2ReqWvevHmV1n3dddfJ4XDo+eefP6Mynkjr1q3VpEkTLVy4UJmZmW7lmj17tiwWi+sqXHl5eZVadurVq6e4uDgVFRWppKREdru9yq4/F110kSQzJJyJN954Q0uXLlVCQoKuvvpq13Rn16r169e7Lf/666+fdmvU6erevbskad68eW7b3r59u9auXVsjZXC2Phx9bK5du7bS5XV9QWJiomJjY11X9HIqKCjQu+++68WSAfAEWkIA1Bpr167VH3/8oRtuuOGYy/Tq1UvTpk3T/PnzlZSUJMn8dTwyMlLPPvus8vPzq7z06tChQ7V+/Xo9/fTT+uqrr3TppZcqLCxM+/fv11dffaWgoCD9+9//PmEZW7ZsqVatWmnWrFkqLi5W8+bN9euvv+q9995TfHy8tmzZ4rb8+PHjNWLECA0ePFg33XST6xK9ZWVlkty7FF155ZW6/vrr9eabb2rLli3q3r27oqKidODAAX3//ff67bffXAPGT+TLL79USUlJpelRUVG66aab9Mgjj2j06NG64YYbXJd5Xblypb7//nuNGjXKdYK/ePFivfHGG+rZs6fOO+88BQQEaMOGDVq7dq2uuuoqBQcH69ChQ0pNTVWPHj100UUXKTo6Wnv37tU777yjiIgI1wn6iezevVsffPCBJKm4uFi///67Vq9erZ9//lmtW7fWK6+84najwi5duqh58+Z66aWXlJeXp7i4OH377bf64YcfXGMYakqrVq00aNAgvffeexo2bJiuuOIK5eTk6O2339aFF16oLVu2nNKYjNPRvn17xcbGKi0tTfv27VOjRo20bds2ffDBB4qPj6+2iwacroCAAI0fP14PPPCABg4cqBtuuEE2m02LFi1SZGSk9u7dW+11BqD6EEIA1BrOmxFeccUVx1wmPj5ezZo104oVK/TQQw8pODhYQUFB6tu3r958802FhYWpZ8+elZ4XGBioGTNm6O2339YHH3zgusFagwYN1KZNm5O+Z4TNZtOMGTOUlpamRYsWqaioSK1atVJaWpp++umnSiHkkksu0cyZM/XCCy9oxowZqlevnq666ir169dPN954Y6U7Tk+dOlWdOnXS+++/rxkzZqisrEyxsbG66KKLdP/9959UGSXzalZffPFFpenNmzfXTTfdpB49euj111/Xq6++qtmzZ6usrEwtW7bUlClTNHDgQNfynTp10rZt27R69WplZmbKarUqLi5O48ePd40HCQ4O1m233aYvv/xSX375pQoKCtSgQQP16NFDd911V6VWqWNZt26d1q1bJ4vFopCQENd+jx49WldccUWlO6XbbDa9+uqrmjJlit58800FBgYqJSVFb775pm666aaTritPmThxoho0aKD58+crLS1NzZs318SJE/Xjjz9qy5YtVY7j8KR69epp1qxZeuaZZ/Tmm2+qvLxciYmJmjlzpubPn+9zIUSS+vXrp4CAAL3yyit66aWXVL9+fd1www1KSEjQ6NGjuSM7UItZDEaAAYDP+eijj3TPPffo+eefV58+fbxdHFSjUaNG6auvvtK333573AHbOGLOnDlKS0vTe++9p+TkZG8XB8BpYEwIAHiRYRiVukWVlZVp7ty5CggI0CWXXOKlksHTjr7PjCT99NNPWrNmjS699FICSBVKS0tlt9vdphUUFOitt95SZGSka1wRgNqH7lgA4EWlpaXq3r27+vXrp+bNmysvL08rVqzQ9u3bdeeddyo2NtbbRYSHLFq0SB988IHrxpq//PKL3n//fQUGBla6khpMe/bs0Z133qk+ffooLi5OmZmZWrRokfbu3avHHnvsjC/tDMB7CCEA4EUBAQHq1q2bVq1apczMTBmGoebNm+vRRx/VLbfc4u3iwYNat26tTz75RP/+97918OBBhYaGqlOnTho9ejS/6B9DdHS0kpOTtXTpUmVnZysgIEDx8fG6//773a6EBqD2YUwIAAAAgBrFmBAAAAAANYruWEdxOByy273bOGSzWbxeBn9BXXoG9eg51KVnUI+eQ116BvXoOdSlZ/hCPQYGHvuCG4SQo9jthvLyCr1ahsjIEK+XwV9Ql55BPXoOdekZ1KPnUJeeQT16DnXpGb5Qj7Gx4cecR3csAAAAADWKEAIAAACgRhFCAAAAANQoQggAAACAGkUIAQAAAFCjCCEAAAAAahSX6AUAAECViooKlJ+fJ7u9vMa2+eefFhkG9wk5U9VZjzZbgMLCIlW3buhpr4MQAgAAgEqKigp0+HCuIiNjFRgYJIvFUiPbtdmsstsdNbItf1Zd9WgYhsrKSpWXlylJpx1E6I4FAACASvLz8xQZGaugoDo1FkDg+ywWi4KC6igyMlb5+XmnvR5CCAAAACqx28sVGBjk7WLARwUGBp1RNz1CCAAAAKpECwiO5UyPDUKIjzEM6aefvF0KAAAAoPoQQnzMmjU2tW1r1Z49/PIAAAAA/0QI8TGHD1tkGBYdOkQIAQAA8KQ1a1br3Xff9Ph6n3jiMd1wQz+Pr9efEUJ8jPV/r4iDK9MBAAB41BdfrNZ7773t8fUOG3aHnnzyGY+v159xnxAfY7WaN5UhhAAAAHhHaWmpgoJO/spgTZvGVWNp/BMhxMfYbOb/hBAAAADPeeKJx7Ry5TJJUmpqB0lSo0aN9dBDE3XPPaP0xBNP66uv1uuLL1arvLxcH364Wnv37tHcuf/Spk0/KDs7WzEx9dWp06UaOfJu1atXz23d6enfav78pZKkP/7Yr4EDr9EDDzyorKxMLV26SCUlJUpKaqcHHpigBg0a1vTu+xxCiI+hOxYAAPBV770XoHfeCazWbVgsFhmGccz5N91UpkGDTv3+FMOG3aG8vFxt27ZVTz31vCQpKChQ+fn5kqQXXnhGl17aRf/852SVlpZKkrKyMtWgQSPdc8/lCg+vp/3792nevLnaufNezZgx94TbfPPN15WYmKQJEx5VXl6upk9/QZMnP6Lp0/91yuX3N4QQH+O85LLd7t1yAAAA+JOmTeMUGRmlwMBAJSa2cU3/7ruNkqQLL2ytCRMecXtOcvLFSk6+2PU4MTFJTZueo7vvvkM7dvyk+PgLjrvNRo0a67HHnnA9zs3N1Suv/J+ysjJVv36sJ3ar1iKE+Jgj3bG4OhYAAPAtgwaVn1YrxKmw2ayy22u+S8hf/nJZpWllZWV6551/68MPl+vAgQMqLS1xzfv9999OGEI6d05xe9yy5fmSpAMHDhBCvF0AuHN2xzpOKyQAAAA8rH79+pWmvfbadC1Y8J6GDbtDbdq0VUhIiDIyMvTww2NdXbaOp169CLfHgYFmV7aKYeZsRQjxMc4QQncsAACAmlS5F8qqVR/ryiv7aNiwO1zTioqKarJQfov7hPgYZ/gor96WTgAAgLNOYGCgSkpOvhWiuLhYAQHuv9kvX77E08U6K9ES4mNyc80U/r8LNQAAAMBDmjVroUOHFmnRovm64IILFRRU57jLd+rUWStXLlOLFucrLu4cff75p9q8eVMNlda/EUJ8DDcrBAAAqB79+l2nLVt+1IwZLys//7DrPiHHMmbMOEmG/vWvVySZA80fe+wJ3XnnbTVUYv9lMY53IeazUFmZXXl5hV7b/ooVNg0bFqIZMwrVvz8DQ85UZGSIV19Pf0E9eg516RnUo+dQl57hj/V44MBvatTovBrfrreujuVvaqIeT3SMxMaGH3MeY0J8DDcrBAAAgL8jhPiYI1fH4j4hAAAA8E+EEB9DSwgAAAD8HSHExxBCAAAA4O8IIT6GEAIAAAB/RwjxMTab+T8hBAAAAP6KEOJjnPcJsXN1XgAAAPgpQoiPoTsWAAAA/B0hxMccuUSvd8sBAAAAVBdCiI9xjgnhPvYAAADwV4QQH8PNCgEAAHzbH3/sV2pqB61YsdQ17YknHtMNN/Q74XNXrFiq1NQO+uOP/ae0zcOHD2v27Bnavv2nSvNGjx6p0aNHntL6vC3A2wWAO8aEAAAA1D7Dht2hgQMHV9v68/MPa+7cmWrQoKESEi5wm3f//ROqbbvVhRDiY5whhO5YAAAAtUfTpnFe23bz5i28tu3TRXcsH+McE1Je7t1yAAAA+JNPP/1Eqakd9PPPOyvNe+CBe3TbbTdJkhYseE933TVcV13VQ1deeZlGjhym9evXnnD9VXXH2rdvr8aOvVeXX56ivn176sUXn1VpaWml537yyUe6555R6tu3p664oquGD79ZK1cuc83/44/9GjjwGklSWtoUpaZ2cOsOVlV3rN9+260HH3xAV155mXr0SNHIkcP01Vfr3ZaZPXuGUlM7aM+e3zV27L264oquGjCgr+bOnSlHNXfLoSXExzjvE0JLCAAA8DW7dln088/V+xu21WqVw3HssbHnn+9Qy5anfqKUktJVYWFh+vjjFTr//Htd03NysrVhw9caNervkqQ//vhD/fpdq0aNmshut2vdujUaN+4fevbZl3TppV1OentlZWUaM+ZulZSU6L77xisqKloffLBAa9Z8VmnZ/fv36bLLLtettw6TxWLRDz+k66mnHldJSbGuu+4GxcTU1xNPPKOHHx6rIUOGKyXlL5KO3fqSlZWpUaNGqG7dUI0ZM06hoWFauPA/GjfuH0pLe0GdO6e4Lf/QQw/o6quv0Y033qx1677Q7Nkz1KBBQ/Xpc81J7++pIoT4GMaEAAAAeF6dOnXUvXtP/fe/H2nUqL/L+r+Trk8++UiSdMUVV0qSRo/+h+s5DodD7dt31J49v2vx4vmnFEJWrlym/fv36bXX5ioxsY0k6dJLu2jo0MrjRoYOHeG2zXbt2is7O0uLFi3QddfdoKCgIMXHJ0iSmjRp6lrfsbz77ls6fPiwXnttruLizpEkde6coltvHaiZM1+pFEIGD77VFTg6duyk777boE8++YgQcjZxdsfiPiEAAMDXtGxpqGXL6j1JsdkM2e3V82vslVf20dKli/XttxvUsWMnSdKHH65Q+/YdVb9+fUnSTz9t05w5M7Rt21bl5eXK+F/3lHPPPe+UtrV58yY1aNDQLTBYrVb16NFTc+b8y23ZPXt+16xZr+mHH9KVk5Pt6goVFBR0Wvv5ww/fqXXrNq4AIkk2m009e/bW66/PUkFBvkJDw1zzunRJdXt+8+YttXPn9tPa9skihPgYWkIAAACqR1JSsho3bqKPPlqhjh07affuX7Vjx0969NHHJUl//nlA//jHX9WsWQv94x9j1bBhIwUE2DRz5mv67bdfT2lb2dnZio6OqTQ9Ojra7XFhYaHGjLlbwcHBGjVqtJo2jVNgYKAWLZqv5cuXnNZ+Hjp0SPHxF1SaHhMTI8MwdPjwYbcQEh5ez225oKCgKseueBIhxMccCSHcJwQAAMCTLBaLevW6Su+//44eeOBBffTRCtWtG6K//KW7JOnrr79Ufn6+Jk+eqgYNGrqeV1JSfMrbiomJ0a+/7qo0PScnx+3xli2bdODAH3r55Vlq2zbZNd1+Bt1i6tWrp5ycrErTs7OzZbFYFB4eftrr9hSujuVjaAkBAACoPr17X62iokJ9/vmn+vjjlerWrbuCg4MlScXFZtgICDjyO/3vv/+mH3/84ZS3k5iYpIyMP7V584+uaQ6HQ59++onbclVt89ChQ1q79nO35QIDza5ZJxOIkpPba/PmzW43RLTb7fr00/+qVasEt1YQb6ElxMc4jz9CCAAAgOede+55uuiiRL322nRlZmboyiv7uOZ16HCJbDabpkyZqMGDb1V2dtb/rhTVSIZxaidnV13VV2+++boefnis7rrrbkVFRWnx4gUqLCxwWy4xsa1CQ0P1/PNpuv32u1RUVKR582YrIiJS+fn5ruWio6MVERGhVas+VsuWrVS3bl01btxEERGRlbY9aNDNWrlyqcaMuVsjRtyl0NBQLVr0H+3Z87uefvrFU9qP6kJLiI+hJQQAAKB69e59tTIzMxQb20AXX9zBNb1Fi5Z69NEpOnDgD02YcJ/eemueRo0areTkdqe8jcDAQL3wwstq1Spezz33lJ544jE1btzU7UpYkhQVFaUnn3xWDodd//zneM2YMV19+16nXr2uclvOarVq/PhHdPjwYf3jH3/THXcM1bp1X1S57fr1Y/Xaa3PUvHkLPffcVD3yyHgdOnRITz/94ild4as6WQyDO1JUVFZmV15eode2n5FhUWJimEaOLNGUKdU7IOhsEBkZ4tXX019Qj55DXXoG9eg51KVn+GM9Hjjwmxo1OrUrQnmCzWattqtjnU1qoh5PdIzExh577AktIT7G8r/x6ERDAAAA+KsaDSEzZszQgAEDdPHFF+vSSy/VqFGjtGPHDrdlDMPQtGnTlJqaqqSkJA0ZMkQ7d+50W+bgwYMaO3as2rdvr/bt22vs2LE6dOiQ2zLbt2/XrbfeqqSkJHXt2lXTp09XbWr0qUVFBQAAAE5JjYaQb775RjfffLPeffddvfHGG7LZbBo+fLjy8vJcy8ycOVNz5szRI488ovnz5ys6OlrDhw93G5hz//33a+vWrZo1a5ZmzZqlrVu3aty4ca75+fn5GjFihGJiYjR//nw9/PDDmj17tubOnVuTu3tazJYQEggAAAD8V41eHWv27Nluj59++ml16NBB3333nXr06CHDMDRv3jyNHDlSvXv3liSlpaWpc+fOWrZsmQYPHqxdu3bpiy++0Ntvv6127cxBQpMmTdItt9yiX375RS1atNCSJUtUVFSktLQ0BQcHKz4+Xr/88ovmzp2r4cOHy2Lx7XtwWCy0hAAAAMB/efUSvQUFBXI4HKpXz7xL4969e5WZmamUlBTXMsHBwerYsaPS09M1ePBgpaenKyQkRBdffLFrmfbt2yskJETp6elq0aKFvv/+e3Xo0MF1zWdJSk1N1f/93/9p7969OuecI7ewP5rNZlFkZEg17O3JKS83/w8MDFRkJFdQPlM2m9Wrr6e/oB49h7r0DOrRc6hLz/DHeszIsMpqtXjlx1ubjWHLnlCd9WgYhqzW0z/uvXqW+8QTT+jCCy90tWhkZmZKkurXr++2XExMjDIyMiRJWVlZio6OdntDWCwWRUdHKysry7VMw4YN3dbhXGdWVtZxQ4jdbnj16hYHD1pksYSqpKRMeXlcHetM+ePVSryBevQc6tIzqEfPoS49wx/r0WKxqri4WEFBdWp0u1wdyzOqux5LS0tksViPe9wf7+pYXgshU6dO1bfffqt33nlHNpvNW8XwORaLQXcsAADgdWFhkcrLy1RkZKwCA4N8vjs7aoZhGCorK1VeXqbCw6NOez1eCSFPPvmkVqxYoTfeeMOtVSI2NlaS2VrRpEkT1/Ts7GxXS0b9+vWVk5MjwzBcbwbDMJSTk+O2THZ2tts2na0kR7ey+CpuVggAALypbt1QSdLBg1my28trbLsWi6VWXdHUV1VnPdpsAQoPj3IdI6ejxkPIlClTtHLlSs2bN08tW7Z0mxcXF6fY2FitX79eSUlJkqSSkhJt3LjRdfWrdu3aqbCwUOnp6a5xIenp6SosLHR160pOTtazzz6rkpIS1aljNiGuX79eDRo0UFxcXE3t6mmxWI7cKwQAAMCb6tYNPaMTzdPhj13bvMHX67FGR/1MmjRJCxcu1LPPPqt69eopMzNTmZmZKigokGQmtqFDh2rmzJn6+OOPtWPHDk2YMEEhISHq27evJKlly5bq2rWrJk6cqPT0dKWnp2vixInq3r27WrRoIUnq16+f6tatqwkTJmjHjh36+OOP9a9//atWXBnLyeGoHeUEAAAATlWNtoS8/fbbkqRhw4a5TR89erT+/ve/S5LuvPNOlZSUaPLkyTp48KDatm2rOXPmKCwszLX8c889p8cff1y33367JKlHjx569NFHXfPDw8M1Z84cTZ48WQMGDFBERIRGjBih4cOHV/MeegZjQgAAAODPLAad7tyUldm92nSVlyclJoZp4MAyvfBCidfK4S98vSmytqAePYe69Azq0XOoS8+gHj2HuvQMX6jH410di4sw+xhnbzEGpgMAAMBfEUJ8UC0ZtgIAAACcFkKIj6KTHAAAAPwVIcTH0AoCAAAAf0cI8UFcHQsAAAD+jBDiY5w3KySEAAAAwF8RQnwUIQQAAAD+ihDig2gJAQAAgD8jhPgY58B0QggAAAD8FSHEB9ESAgAAAH9GCPFBFgt3TAcAAID/IoQAAAAAqFGEEB/DJXoBAADg7wghPooQAgAAAH9FCPExzqtjAQAAAP6KEOKD6I4FAAAAf0YI8TGMCQEAAIC/I4T4KMOgXxYAAAD8EyHEx9ASAgAAAH9HCPFRhBAAAAD4K0KID6IlBAAAAP6MEOJj6I4FAAAAf0cI8VGEEAAAAPgrQoiPcbaEAAAAAP6KEOKjHA5vlwAAAACoHoQQH0NLCAAAAPwdIcQHMTAdAAAA/owQ4mOcrSCEEAAAAPgrQggAAACAGkUI8VG0hAAAAMBfEUJ8DDcrBAAAgL8jhPgYxoQAAADA3xFCfBAtIQAAAPBnhBAfRAgBAACAPyOE+BhuVAgAAAB/RwjxQbSEAAAAwJ8RQnwMA9MBAADg7wghPshsCaFfFgAAAPwTIcQH0R0LAAAA/owQAgAAAKBGEUIAAAAA1ChCiA+yWCSHw9ulAAAAAKoHIcQHca8QAAAA+DNCiI9iYDoAAAD8FSHEB3F1LAAAAPgzQogPIoQAAADAnxFCfBAhBAAAAP6MEOKjCCEAAADwV4QQH0RLCAAAAPwZIcQHEUIAAADgzwghPooQAgAAAH9FCPFB3KwQAAAA/owQ4oMIIQAAAPBnNR5CNmzYoFGjRqlr165KSEjQwoUL3eZPmDBBCQkJbv9uvPFGt2VKS0v1+OOPq1OnTkpOTtaoUaN04MABt2X279+vUaNGKTk5WZ06ddKUKVNUWlpa7fvnKXTHAgAAgL8KqOkNFhYWKj4+Xtddd53Gjx9f5TJdunTR008/7XocGBjoNv+JJ57QqlWr9PzzzysyMlJPPfWU7rrrLi1cuFA2m012u1133XWXIiMj9dZbbykvL0/jx4+XYRh65JFHqnX/PIGB6QAAAPBnNd4S0q1bN91333268sorZbVWvfmgoCDFxsa6/kVGRrrmHT58WAsWLNC4ceOUkpKi1q1b6+mnn9b27du1fv16SdLatWu1c+dOPf3002rdurVSUlI0duxYvf/++8rPz6+J3QQAAABwDD45JuTbb79V586d1bt3b/3zn/9Udna2a97mzZtVVlam1NRU17TGjRurZcuWSk9PlyR9//33atmypRo3buxapmvXriotLdXmzZtrbkdOEy0hAAAA8Gc13h3rRLp27aorrrhCcXFx2rdvn1588UXddtttWrhwoYKCgpSVlSWbzaaoqCi358XExCgrK0uSlJWVpZiYGLf5UVFRstlsrmWOxWazKDIyxLM7dYqsVslqtXm9HP7AZrNSjx5APXoOdekZ1KPnUJeeQT16DnXpGb5ejz4XQvr06eP6OyEhQa1bt1aPHj20evVq9erVq9q3b7cbyssrrPbtHF+Yysvtyssr8nI5ar/IyBAfeD1rP+rRc6hLz6AePYe69Azq0XOoS8/whXqMjQ0/5jyf7I5VUcOGDdWwYUPt3r1bklS/fn3Z7Xbl5ua6LZedna369eu7lqnYhUuScnNzZbfbXcv4OrpjAQAAwF/5fAjJyclRRkaGGjRoIElKTExUYGCg1q1b51rmwIED2rVrl9q1aydJSk5O1q5du9wu27tu3ToFBQUpMTGxZnfgNDAmBAAAAP6sxrtjFRQU6Pfff5ckORwO7d+/X9u2bVNERIQiIiI0ffp09erVS7Gxsdq3b5+ef/55RUdHq2fPnpKk8PBwDRgwQM8884xiYmIUGRmpqVOnKiEhQV26dJEkpaamqlWrVho3bpwmTJigvLw8Pf3007rxxhsVFhZW07t8yswQwh0LAQAA4J9qPIRs3rxZQ4cOdT2eNm2apk2bpv79++uxxx7Tjh07tHjxYh0+fFixsbHq1KmTXnzxRbfw8PDDDysgIEBjxoxRcXGxOnfurKefflo2m02SZLPZNGPGDE2aNEk33XSTgoOD1a9fP40bN66md/e00BICAAAAf2YxDE53Kyors3t9EE+PHmEqLXVo7VoGZZ0pXxiU5Q+oR8+hLj2DevQc6tIzqEfPoS49wxfqsVYPTAcAAADgXwghAAAAAGoUIcQHMSYEAAAA/owQ4oMIIQAAAPBnhBAfRAgBAACAPyOE+ChCCAAAAPwVIcQH0RICAAAAf0YIAQAAAFCjCCE+iJYQAAAA+DNCiA8ihAAAAMCfEUJ8ECEEAAAA/owQ4oMIIQAAAPBnhBAfZLF4uwQAAABA9SGEAAAAAKhRhBAAAAAANYoQ4oOsVsnh8HYpAAAAgOpBCPFBFovBwHQAAAD4LUKID+LqWAAAAPBnhBAfZIYQLpEFAAAA/0QI8UG0hAAAAMCfEUJ8ECEEAAAA/uykQ8iFF16oTZs2VTlv8+bNuvDCCz1WqLOd1UoIAQAAgP866RBiHOes2OFwyMJtvj2GlhAAAAD4s4ATLeBwOFwBxOFwyHHUDSyKi4u1Zs0aRUVFVU8Jz0KEEAAAAPiz44aQ6dOn6+WXX5YkWSwW3XTTTcdc9uabb/Zsyc5ihBAAAAD4s+OGkEsuuUSS2RXr5Zdf1g033KBGjRq5LRMUFKSWLVuqe/fu1VfKswwhBAAAAP7shCHEGUQsFosGDhyohg0b1kjBzmYMTAcAAIA/O+GYEKfRo0dXmvbzzz9r165dSk5OJpx4EC0hAAAA8GcnHUImT56s8vJyTZ48WZL08ccfa8yYMbLb7QoLC9OcOXOUlJRUbQU9m1gs0lHj/wEAAAC/cdKX6F2zZo0uvvhi1+Np06bpsssu0wcffKCkpCTXAHacOa52DAAAAH920iEkMzNTTZs2lSQdOHBAO3fu1F133aWEhAQNGTJEP/74Y7UV8mzDmBAAAAD4s5MOIcHBwSosLJQkffPNNwoLC1NiYqIkKSQkRAUFBdVTwrOQOSaE5hAAAAD4p5MeE9K6dWu99dZbaty4sd5++2116dJFVquZYfbu3avY2NhqK+TZhoHpAAAA8Gcn3RLyj3/8Qz/88IOuvfZa/frrr/rb3/7mmvfJJ58wKN2DCCEAAADwZyfdEpKUlKTPPvtMv/zyi5o1a6awsDDXvEGDBum8886rlgKejQghAAAA8GcnHUIkc+yHcxxIRZdddpmnygMxMB0AAAD+7ZRCyPbt2/Xyyy/rm2++0aFDh1SvXj116tRJd999t+Lj46urjGcdWkIAAADgz046hGzatElDhgxRcHCwevToofr16ysrK0uffvqpPv/8c7355ptVtpLg1HGfEAAAAPizkw4hzz//vFq1aqXXX3/dbTxIfn6+hg8frueff15z5syplkKebbhjOgAAAPzZSV8d64cfftBdd93lFkAkKSwsTHfeeafS09M9XrizlfWkXxUAAACg9vHY6a6FPkQeRUsIAAAA/NVJh5C2bdvqtddeU35+vtv0wsJCzZw5U8nJyZ4u21mLlhAAAAD4s5MeE3LfffdpyJAh6tGjhy677DLFxsYqKytLn3/+uYqKivTvf/+7Ost5VmFMCAAAAPzZKd2s8L333tMrr7yitWvX6uDBg4qIiFCnTp30t7/9TQkJCdVZzrOK2RJC9zYAAAD4p+OGEIfDodWrVysuLk7x8fG64IIL9NJLL7kts337du3bt48QUg0Mg8v1AgAAwP8cd/TBkiVLdP/996tu3brHXCY0NFT333+/li1b5vHCna2cwYMbFgIAAMAfnTCEXH/99TrnnHOOuUxcXJwGDBigRYsWebxwZyvnwHTGhQAAAMAfHTeEbNmyRSkpKSdcSZcuXbR582aPFepsR0sIAAAA/NlxQ0hBQYHq1at3wpXUq1dPBQUFHivU2c7ZEkIIAQAAgD86bgiJiorS/v37T7iSP/74Q1FRUR4r1NnO2RJCdywAAAD4o+OGkPbt22vx4sUnXMmiRYvUvn17T5XprEdLCAAAAPzZcUPIbbfdpi+//FJPPvmkSktLK80vKyvTE088oa+++krDhg2rrjKedWgJAQAAgD877n1C2rVrp/HjxystLU1Lly5VSkqKmjZtKknat2+f1q9fr7y8PI0fP17Jyck1Ud6zAi0hAAAA8GcnvGP6sGHD1Lp1a82cOVOffPKJiouLJUnBwcG65JJLNHLkSHXo0KHaC3o24epYAAAA8GcnDCGS1LFjR3Xs2FEOh0O5ubmSpMjISNlstlPe4IYNGzR79mxt2bJFGRkZmjp1qq6//nrXfMMwNH36dL333ns6dOiQ2rZtq0cffVStWrVyLXPw4EFNmTJFn376qSSpR48eeuSRR9yu5LV9+3Y9/vjj2rRpkyIiIjRo0CDdfffdstSCW5DTEgIAAAB/dtwxIZUWtloVExOjmJiY0wogklRYWKj4+Hg9/PDDCg4OrjR/5syZmjNnjh555BHNnz9f0dHRGj58uPLz813L3H///dq6datmzZqlWbNmaevWrRo3bpxrfn5+vkaMGKGYmBjNnz9fDz/8sGbPnq25c+eeVplrGmNCAAAA4M9OKYR4Qrdu3XTffffpyiuvlNXqvnnDMDRv3jyNHDlSvXv3Vnx8vNLS0lRQUKBly5ZJknbt2qUvvvhCkydPVrt27dSuXTtNmjRJn332mX755RdJ5p3ei4qKlJaWpvj4eF155ZW68847NXfuXBm1oHmBlhAAAAD4sxoPIcezd+9eZWZmut2lPTg4WB07dlR6erokKT09XSEhIbr44otdy7Rv314hISGuZb7//nt16NDBraUlNTVVGRkZ2rt3bw3tzek70hLi+13HAAAAgFN1UmNCakpmZqYkqX79+m7TY2JilJGRIUnKyspSdHS029gOi8Wi6OhoZWVluZZp2LCh2zqc68zKytI555xzzDLYbBZFRoac+c6cgYAAc9/q1auryEivFqXWs9msXn89/QH16DnUpWdQj55DXXoG9eg51KVn+Ho9+lQI8QV2u6G8vEKvlsEwQiVZlJtbpMBA+mSdicjIEK+/nv6AevQc6tIzqEfPoS49g3r0HOrSM3yhHmNjw485z6e6Y8XGxkqSq0XDKTs729WSUb9+feXk5LiN7TAMQzk5OW7LZGdnu63Duc6jW1l8EWNCAAAA4M98KoTExcUpNjZW69evd00rKSnRxo0b1a5dO0nmDRQLCwtd4z8kc5xIYWGha5nk5GRt3LhRJSUlrmXWr1+vBg0aKC4urob25vQRQgAAAODPajyEFBQUaNu2bdq2bZscDof279+vbdu2af/+/bJYLBo6dKhmzpypjz/+WDt27NCECRMUEhKivn37SpJatmyprl27auLEiUpPT1d6eromTpyo7t27q0WLFpKkfv36qW7dupowYYJ27Nihjz/+WP/61780fPjwWnGfEG5WCAAAAH9W42NCNm/erKFDh7oeT5s2TdOmTVP//v311FNP6c4771RJSYkmT56sgwcPqm3btpozZ47CwsJcz3nuuef0+OOP6/bbb5dk3qzw0Ucfdc0PDw/XnDlzNHnyZA0YMEAREREaMWKEhg8fXnM7egacLSHcJwQAAAD+yGLUhhtn1KCyMrvXB/E8/3yonnrKqu++y1dcHC/PmfCFQVn+gHr0HOrSM6hHz6EuPYN69Bzq0jN8oR5rzcB0mLhjOgAAAPwZIcQHMSYEAAAA/owQ4oNoCQEAAIA/I4T4IC7RCwAAAH9GCPFBhBAAAAD4M0KID2JMCAAAAPwZIcQHHblPiO/fWBEAAAA4VYQQH+QMIXa7d8sBAAAAVAdCiA+yWs1+WIQQAAAA+CNCiA+y2cz/uUQvAAAA/BEhxAfRHQsAAAD+jBDig2gJAQAAgD8jhPggZwgpL/duOQAAAIDqQAjxQc77hNAdCwAAAP6IEOKDjnTH4j4hAAAA8D+EEB/EmBAAAAD4M0KID+LqWAAAAPBnhBAf5GwJIYQAAADAHxFCfBDdsQAAAODPCCE+iO5YAAAA8GeEEB9ECAEAAIA/I4T4ILpjAQAAwJ8RQnzQkYHp3CcEAAAA/ocQ4oPojgUAAAB/RgjxQc4QQncsAAAA+CNCiA8ihAAAAMCfEUJ8UECA+T8hBAAAAP6IEOKDnC0h5eXeLQcAAABQHQghPohL9AIAAMCfEUJ80JFL9Hq3HAAAAEB1IIT4IFpCAAAA4M8IIT7IOTCdmxUCAADAHxFCfBA3KwQAAIA/I4T4oMBA839CCAAAAPwRIcQHcbNCAAAA+DNCiA86MibEu+UAAAAAqgMhxAdxdSwAAAD4M0KID3KGEO6YDgAAAH9ECPFBzu5YtIQAAADAHxFCfBD3CQEAAIA/I4T4IMaEAAAAwJ8RQnwQIQQAAAD+jBDig5zdsRiYDgAAAH9ECPFBNptksRjcJwQAAAB+iRDigywWM4iUljIwHQAAAP6HEOKDLBbJaqU7FgAAAPwTIcQHOVtC6I4FAAAAf0QI8UFWq2SzGSor83ZJAAAAAM8jhPggZ3csblYIAAAAf0QI8UHOEEJLCAAAAPwRIcQHmd2xGBMCAAAA/0QI8UFcHQsAAAD+jBDig5xXxyKEAAAAwB8RQnzQke5YDEwHAACA//G5EDJt2jQlJCS4/UtJSXHNNwxD06ZNU2pqqpKSkjRkyBDt3LnTbR0HDx7U2LFj1b59e7Vv315jx47VoUOHanpXThvdsQAAAODPfC6ESFLz5s21du1a17+lS5e65s2cOVNz5szRI488ovnz5ys6OlrDhw9Xfn6+a5n7779fW7du1axZszRr1ixt3bpV48aN88aunBZndyyujgUAAAB/5JMhJCAgQLGxsa5/0dHRksxWkHnz5mnkyJHq3bu34uPjlZaWpoKCAi1btkyStGvXLn3xxReaPHmy2rVrp3bt2mnSpEn67LPP9Msvv3hzt06JzWZwdSwAAAD4pQBvF6Aqe/bsUWpqqoKCgtS2bVvdd999Ouecc7R3715lZma6dc8KDg5Wx44dlZ6ersGDBys9PV0hISG6+OKLXcu0b99eISEhSk9PV4sWLY67bZvNosjIkGrbt5Nhs1kVFCQZhrxeltrOZrNShx5APXoOdekZ1KPnUJeeQT16DnXpGb5ejz4XQpKSkjR16lS1aNFCOTk5evXVVzV48GAtW7ZMmZmZkqT69eu7PScmJkYZGRmSpKysLEVHR8tiOTKo22KxKDo6WllZWSfcvt1uKC+v0IN7dOqcB0xpqbxeltouMjKEOvQA6tFzqEvPoB49h7r0DOrRc6hLz/CFeoyNDT/mPJ8LId26dXN73LZtW/Xs2VOLFy9W27ZtvVSqmhcQIJWUeLsUAAAAgOf55JiQikJDQ3X++edr9+7dio2NlaRKLRrZ2dmu1pH69esrJydHhmG45huGoZycnEotKL4sIMDg6lgAAADwSz4fQkpKSvTrr78qNjZWcXFxio2N1fr1693mb9y4Ue3atZMktWvXToWFhUpPT3ctk56ersLCQtcytUFQkFRSwn1CAAAA4H98rjtWWlqaunfvrsaNGysnJ0evvPKKCgsL1b9/f1ksFg0dOlQzZsxQixYt1KxZM7366qsKCQlR3759JUktW7ZU165dNXHiRE2ePFmSNHHiRHXv3v2Eg9J9SXCwodJSb5cCAAAA8DyfCyEHDhzQfffdp7y8PEVFRSk5OVnvv/++mjZtKkm68847VVJSosmTJ+vgwYNq27at5syZo7CwMNc6nnvuOT3++OO6/fbbJUk9evTQo48+6pX9OV116tASAgAAAP9kMSoOnoDKyuxev5JAZGSIBgxwaOXKAO3fn3/iJ+CYfOHKEP6AevQc6tIzqEfPoS49g3r0HOrSM3yhHo93dSyfHxNytgoONlRebmFwOgAAAPwOIcRH1a1r/l9c7N1yAAAAAJ5GCPFRoaFmL7nCQsaFAAAAwL8QQnxUeLgZQvIZEgIAAAA/QwjxURER5v9ZWbSEAAAAwL8QQnxUbKzZEpKZSQgBAACAfyGE+KgGDcwQkpFBCAEAAIB/IYT4qIYNHZKkzExeIgAAAPgXznB9VKNGhmw2Q/v30xICAAAA/0II8VEhIea4kF9+4SUCAACAf+EM10dZLFLTpg7t3s1LBAAAAP/CGa4Pa9bM0IEDFhUVebskAAAAgOcQQnxYUpJdkkVff83LBAAAAP/B2a0P69OnTBaLoVmzgrxdFAAAAMBjCCE+7NxzpQ4d7PriiwD9+ae3SwMAAAB4BiHEx40dW6qiIosee6yOt4sCAAAAeAQhxMd162ZX587lWrAgSJ9+avN2cQAAAIAzRgjxcRaL9NprRbLZDE2bFqSsLG5eCAAAgNqNEFILNG4sXXZZuTZssOnddwO8XRwAAADgjBBCaomnniqRwyG98kqQSku9XRoAAADg9BFCaonzzjP00EMlysqy6vnng2QY3i4RAAAAcHoIIbXIyJFlOv98u159NUhbtjA2BAAAALUTIaQWCQqSZs0qUmmpdOeddVVS4u0SAQAAAKeOEFLLXHSRoUsvtWvXLptuuaWu9uyhRQQAAAC1CyGkFnrjjSJdeKFda9YE6J//5CaGAAAAqF0IIbVQvXrShx8W6sIL7Vq5MkBz5wZ6u0gAAADASSOE1FJ160qLFxeqcWNDjz5aR6tWcTd1AAAA1A6EkFosKkr64INChYYa+utfg/XNN7ycAAAA8H2ctdZyzZoZmjKlRIWFFg0eXFfp6bykAAAA8G2csfqBG24o1/LlhSovt2jAgLqaPTvA20UCAAAAjokQ4ifatnXo3/8uks0mPfhgXf31r8EqK/N2qQAAAIDKCCF+pFs3u779tkC9e5dpwYJAXXFFiHJyvF0qAAAAwB0hxM/Uqyf9+9/Fuv/+Ym3dalPXrqFavJjuWQAAAPAdhBA/NX58mWbMKJLDIY0cWVfXXVdXn3/OZXwBAADgfYQQP9a/f7nWrStQz55lWr8+QDfeWFeDBgVr5sxAlZd7u3QAAAA4WxFC/Fx0tPT228VatSpfXbuW67PPAvXww8EaMKCu3nwzQIbh7RICAADgbEMIOUu0aWNo/vxiffRRvlq1suvLLwN03311NWpUHf38s0VLlgSosNDbpQQAAMDZgBBylmnXztDq1YV67LFinXeeXYsWBWnAgBCtWBGgefPopgUAAIDqRwg5CwUGSn/7W5m++qpQL7xQpPJyaeHCQD36aLCaNAnTe+9xNS0AAABUH0LIWcxmk265pVwbNhTomWeK1KlTuSSL/v73uurVq6527bJ4u4gAAADwQ/zkDYWESLfdVq7bbivX3r3SrbfW1fffB6hbt1D161emv/zFrgYNDDVoYKhNG4e3iwsAAIBajhACN3Fx0urVRfruO6sef7yOFiwI0oIF5jyr1dDChYXq1Mkhm00qLZWCgrxbXgAAANQ+hBBU6eKLHVq0qEg//mjR/PmBWrAgUBkZVl13XagaNnSoR49ytWrlUK9edsXH0zoCAACAk0cIwXG1aWOoTZtSTZpUqh9/tCgtrY6+/damd94JkmTo9dcNNWvmUIcOdt15Z6nq1TMHvjutXWvVxo02/eMfZV7bBwAAAPgWQghOWps2ht58s1iStH27RXPnBumLL2xasyZAa9YE6KWXgtSsmUMXXeRQx452WSzStGlB+vNPq1q0cOiaa+xe3gMAAAD4AkIITktCgqGnniqRZI4NWbbMpv/+N1AbNli1ZEmAliwJdFv+gQeC9cMPZerRw66kJLvCw71RagAAAPgCQgjOWFCQdP31dl1/vdnSkZUlffedTTt2WFVQYFF8vF333ltX06bV0bRpUlCQoYsusqtFC3Msid0u3XNPqRITDVm4KjAAAIDfI4TA4+rXl3r1sqtXryPdr3r3ztfXX9u0datVX35p0+bNVv3wQ6AMw0wdH3wQpLAwh5o3NxQT45BhSG3aONSokaG6dQ317Fmuxo29tUcAAADwJEIIakRIiNS9u13du9t1993mIPXSUun33y0qLJTefTdIv/5q1fbtFu3ebVNpqUWff+7eLBIW5lCDBoasViky0tDevVY1aWKoe/dyRUYaOu88h5KSHGrY0JBhSFZuxQkAAOCTCCHwmqAg6fzzDUlSUlJJpflbt1r05Zc2ZWVZdfCgRXv2WLRnj1U5ORZlZlpkGNKmTVZ9910dt+fZbIbCwgyFh0sxMRaFh9dVdLSh8HBD9esbKimRAgKk5GSHzj3XoYYNHapbVwoONstEeAEAAKhehBD4rIsuMnTRReXHXcZul9LTrTp8WPr9d6t+/dWqzEyL8vKsysmRDh2yac8eqw4ftqis7NgDTqxWQwEB5niVoCCpXj2pbl1zmiRFRzsUESHVq2cGnIAAqbxcatLE7C5mt0vNmxsKD3coIMCikBBDUVHmciEhks1mhhvnmJfTGftSVCQZhrk+AACA2owQglrNZpM6dHDeLLHyTRMjI0OUl1cow5AKCqQ//rCotNSi4GBDGzbY9McfFh08aFF+vkXFxRbl50uFhRZlZVlUUiIVFprr+e03mw4ftrjGsJyswEBDNpsUEGCGm6AgM4AEBZnTAgKkOnXMv+vUMVtjiorMZeLiHAoMNPfRZpNycy0KDJTOOceugACLHA6ztaduXXMbDocUHm4GleBgw/XcyEjDFa7q1DGDVlmZFBhohqWK93UpLrbI4TCfU1Z2JDwVFZmhq7zcLFtAgDnvRPLzzfvG1Klz4mWdDOP0Qlp1sNtPbj99nS/V6fF4s5wlJebxHRp67GVqunwOR821zBYVSXXrHn+Z8nK5fpg52vHKWtW8Q4fMH3t8iWFI2dkW1a9veLsobjxx3JWXm8f48Y5voKYRQnBWsFiksDCpVStDkvkF07Ll8VtZjuZwmEGmuNiiggKprEzKyzODi2TR77+bAaesTCoqMruMmcuZgaagwOJqzSgpsbi+FA4elOx2i0pKzOeWl5vb+vLLY709A48x/fRZLIYrcFitZlBxOMyymiHKIqs1VFarOS0gwCyjs7XIYjnyJVlxPE5pqWQYFkVEOFwnLxaLuYzDYYaxwEBzn63WIyf8wcGGysststkMVwuSw2GRZE6vU8csb3m5GXAM40hZLRazZcvhMOs7OtpQcLC5XpvNUFmZRXa7FBFhBrCKrVPGUecemZkWRUcbCguTJLPFKyDgyD46A2J5uXk8HDxorszs3mcoNNRc1m4391UyX3PDCFRIiJSba56IBQVJZWWGW0tXWZkZSu32I61oFov5uKTE/LtOHXP7gYFmfdpskt1uuEKiw2FRbq60f79VTZs6VK+eGXhLSsx1l5ebx7PVaq63Xj1D+/dbFBBg1pvz9a5bV8rPN4/xc891uI6VP/+0KCTELGtBgRlgmzY1ZLEYCgmRDh+2/O+1PFInVqv+F/Atiow0VFpqvl8OH7YoO9uic84xw3dOjkUNGhiy2QzZ7eZ7IyTEfN3r1JEiI6XcXKtKSsz3WEGBWabwcHMfy8st+uMPi4KCzON0/36rQkIMxcSYrZfm+8wsn80m/fyzVaWl5usXF+dQQYHlf2PQzGNp716LioosatHCoYICc9+io826Dg42lykuNj8TzjnHoT17rCorM+u0pMSi3FyL4uIc2rvXfHOEh5utpSUlZrkbNDC0e7dVVqsUFWUoL8+inByzPA0aOBQebh5fkZGGfv/dIotFOvdcw/U+q1PH/KGiTh1DGRnm8wIDpdhYc38LCiz/e2+Zr1dQkPl+KC62qLDQogMHAo96zY+s23zNzH2wWqWYGPO9UFxsvo7O+m7Y0KHiYvPHkqwsc3uRkcb/fsAxj9GjnXeeQ4cOmWMDIyIMRUWZ71/nD0OBgYbr88EwpPJyc135+frfDyZmGQMDzQDh/PyoW9esD8k8bho1Ml+rP/80P3vDwsxylZbKNW/vXovKy81tNm5suD7nnD8eORzm84qKpOxsq8rLzfotKjL3JSJCkqzKyzPXW1ho/sAVGmpe+TE01HC9Lg6H+V63Ws1jvbDwyOdXUJBZF873+qFDZpnPO89wfZdIzh+ezB+YMjLMeg8ONlzfMVarWQ/l5dK+feZxFx1tKCLCrN/wcPP9Z7GY26hTx9znwkKL6tUz13vokHmsOxxSs2bm8mVlZp2Xl1tc+7Vvn1UxMQ4VFZn7m5d35DVv2NB8DQ8csComxlBIiKF69cz9KyqyKCPDLEtenrk+s/eARcHBNmVnm58xYWHm53VZmXmsFxaan0khIeZ72/zuMhQXZ/6A5nCYn2kZGVbVqWP+yNekiUMlJRbt329RTIxZpsBA6Y8/rIqIMFRQYIY0q9X8ATE01HzPFRSYdRQcfOSzPzra/I50BvOCgiPfZeHhZvmc4bG83PxckKRzzjGPgaIi87NXMt9Hzi7j5eXmZ53zu9g8vzCPiZAQc35+vllvkZFmnRQWmq9/kyaGMjPN92jDhmZ9du4sRUVVft/5Cr8PIW+99ZZmz56tzMxMtWrVSg899JA6dOjg7WKhFnKe4ISHG4qNdU6tvl/MnB/gxcXmB6/dbnF9ARQWmifoBQWW/30ZmyfW5gmd+b/dbnF9+JWUmB/IDodUWmp+WRUXmx92zpNku938UnGuIzDQkMVi+V/rh035+Q7Xh3tpqfmhWFh4ZL1mELC4TvTND23zgz431+wOVzGgWK3mchVPMMrK5AoJztDi/DKRjpwIOLfn7BbnDA9HQoTzJMw4bjc87/GD5hWfUJ0/61a+ZPixulM6Hzsc7n8bhkUWi+E65p1B1/k+cIbLil01y8srPt+c7gznFUNyxb8r/ghQ+e8TX6jDYrHIMIJcz3WWseL6jw7oFeugqvln4li/+le1Hed+Oaef7D4cr8wV6+pU9stqtcjhqOsqgzN0OfenYlmrKqPzs67ijyMnUnE553FU1WtVXi7X52rFHzUqqmpfzeB3pAXcWc7AQPfPZef/VQXNii05Rx/rzjqoWG7zvWGRwxHs9tyKyxx9vDvfe86/rdbKZXHuhzPsHf3DmfP/iq9bVfvkXPboOq64rxV/DKuqJcvZe6HiPjs5v1Mr/hDofP6xWuednyMVy2W1Sg8/bOj66ysv7yv8OoSsWLFCTz75pCZOnKj27dvr7bff1p133qnly5erSZMm3i4ecFzOD8sjzecVP/WMKqZVH7NbW1GNbMvTiovNX8yOfGkd+WXdbJVwTnf/3zwhsLi6oTm/wCoGnoof+Dab+UuZ2RphBjlni5dzWYdDioioo4KCElcLh/OXZucXnfMLyLktZ+uHYZjhzFm+wEBDRUWWSl/gzjDnbK1wfoHZbGZodP765wygzl97K5bF+WVmtj4dOYFxzqt4IuHcL+c+OMvi/HXaeRLjbOFzrsf5Req+vSMnYYZh1qHzxMe5fufrUqdOoAoKyhQYaLZ8OH/9dZ40OL+8K75GgYFy/bJuvsYWt+0698l58QrncVDxJMe5P87yO7dVsbXL2SrlDMnO17VimZwhxfl6l5SY85y/ujtbAcvKLG6BpGIrnNVquNZR8bip+Lo4jx/ntIrznSeldeoEqKTkSMuwc56zhczZ+nf0SWDFE8ijOevT+Xo668X5Wkrux6mT80cJp4onl871HWkddd/2scpS8b3hPFarep65bUulecc6xp3HrfN1DQy0qbTU7vZ8Z3lPhs12pCXCWffObVY8wazqPe/823lsOrfpfM+Y3WIN12eS83g8OiAf/TloGObnmvPHJ+f71fn30ftYVTiuWN6K75c6dY68148WGGhTWZndrb6d+1NVwHPOM5975P0gHSlbxc+jYwX6Y02rGESqOvaPfm3c12Wp8Lf7MhXf285WlYCAIz0TAgLk+q6yWs06M3sYVFUPR1rInOtv0cIXf4Q7wmIYnvz9wrcMHDhQCQkJmjJlimtar1691Lt3b91///1VPqeszK68vMKaKmKVnOMYcOaoS8+gHj2HuvQM6tFzqEvPoB49h7r0DF+ox9jY8GPO89uLkZaWlmrLli1KSUlxm56SkqL09HQvlQoAAACA33bHys3Nld1uV/369d2mx8TEaP369cd8ns1mUWSkd6+BarNZvV4Gf0Fdegb16DnUpWdQj55DXXoG9eg51KVn+Ho9+m0IOV12u+H1pitfaD7zF9SlZ1CPnkNdegb16DnUpWdQj55DXXqGL9TjWdkdKyoqSjabTVlZWW7Ts7OzFXvk0kYAAAAAapjfhpCgoCC1bt26Uter9evXq127dl4qFQAAAAC/7o41fPhwjRs3TklJSbr44ov1zjvvKCMjQ4MHD/Z20QAAAICzll+HkKuvvlq5ubl69dVXlZGRofj4eP3rX/9S06ZNvV00AAAA4Kzl1yFEkm655Rbdcsst3i4GAAAAgP/x2zEhAAAAAHwTIQQAAABAjSKEAAAAAKhRhBAAAAAANYoQAgAAAKBGWQzDMLxdCAAAAABnD1pCAAAAANQoQggAAACAGkUIAQAAAFCjCCEAAAAAahQhBAAAAECNIoQAAAAAqFGEEAAAAAA1ihDiY9566y316NFDbdq00fXXX6+NGzd6u0g+ZcaMGRowYIAuvvhiXXrppRo1apR27NjhtsyECROUkJDg9u/GG290W6a0tFSPP/64OnXqpOTkZI0aNUoHDhyoyV3xqmnTplWqo5SUFNd8wzA0bdo0paamKikpSUOGDNHOnTvd1nHw4EGNHTtW7du3V/v27TV27FgdOnSopnfF63r06FGpLhMSEjRy5EhJJ65r6eTq299s2LBBo0aNUteuXZWQkKCFCxe6zffUMbh9+3bdeuutSkpKUteuXTV9+nT52+2xjleXZWVleuaZZ9SvXz8lJycrNTVV999/v/bv3++2jiFDhlQ6TseMGeO2jL+/5090THrqu2X//v0aNWqUkpOT1alTJ02ZMkWlpaXVvn816UR1WdVnZkJCgiZNmuRahu/ykzvnqdWflQZ8xvLly42LLrrIeO+994yff/7ZmDx5spGcnGzs27fP20XzGSNGjDDmz59vbN++3fjpp5+Mv/3tb0aXLl2M3Nxc1zLjx483hg0bZmRkZLj+VZxvGIbx6KOPGikpKcbatWuNzZs3G7feeqtxzTXXGOXl5TW7Q17y0ksvGb1793aro+zsbNf8GTNmGMnJycaHH35obN++3bjnnnuMlJQU4/Dhw65lbr/9duPqq682vvvuO+O7774zrr76auOuu+7yxu54VXZ2tls9btmyxUhISDAWLlxoGMaJ69owTq6+/c3q1auN5557zli5cqWRlJRkLFiwwG2+J47Bw4cPG126dDHuueceY/v27cbKlSuN5ORkY/bs2TW2nzXheHV56NAhY9iwYcby5cuNXbt2GT/88INx0003GVdddZVRVlbmWu7WW281JkyY4HacHjp0yG07/v6eP9Ex6YnvlvLycqNv377GrbfeamzevNlYu3atkZKSYkyePLmmdrNGnKguK9ZhRkaG8emnnxrx8fHG119/7VqG7/KTO+epzZ+VhBAfcsMNNxgPP/yw27QrrrjCePbZZ71UIt+Xn59vXHDBBcaqVatc08aPH2+MHDnymM85dOiQ0bp1a+ODDz5wTdu/f7+RkJBgrFmzplrL6yteeuklo0+fPlXOczgcRkpKivHKK6+4phUVFRnJycnGO++8YxiGYfz8889GfHy8sXHjRtcyGzZsMOLj441du3ZVb+F93CuvvGK0b9/eKCoqMgzj+HVtGCdX3/4uOTnZ7STFU8fgW2+9ZbRr1871WhiGYbz88stGamqq4XA4qnu3vOLouqzKzp07jfj4eOOnn35yTbv11luNSZMmHfM5Z9t7vqp69MR3y+rVq42EhARj//79rmUWL15sJCYm+u2PDidzTD788MNGr1693KbxXV7Z0ec8tf2zku5YPqK0tFRbtmyp1E0jJSVF6enpXiqV7ysoKJDD4VC9evXcpn/77bfq3LmzevfurX/+85/Kzs52zdu8ebPKysqUmprqmta4cWO1bNnyrKrrPXv2KDU1VT169NCYMWO0Z88eSdLevXuVmZnpdiwGBwerY8eOrvpJT09XSEiILr74Ytcy7du3V0hIyFlVh0czDEPz58/XNddco+DgYNf0Y9W1dHL1fbbx1DH4/fffq0OHDm6vRWpqqjIyMrR3794a2hvfk5+fL0mKiIhwm758+XJ16tRJffr0UVpamms5ife805l+t3z//fdq2bKlGjdu7Fqma9euKi0t1ebNm2tuR3xIQUGBli9fXqmrlcR3+dGOPuep7Z+VAdW2ZpyS3Nxc2e121a9f3216TEyM1q9f76VS+b4nnnhCF154odq1a+ea1rVrV11xxRWKi4vTvn379OKLL+q2227TwoULFRQUpKysLNlsNkVFRbmtKyYmRllZWTW9C16RlJSkqVOnqkWLFsrJydGrr76qwYMHa9myZcrMzJSkKo/FjIwMSVJWVpaio6NlsVhc8y0Wi6Kjo8+aOqzKunXrtHfvXrcv0+PVdVRU1EnV99nGU8dgVlaWGjZs6LYO5zqzsrJ0zjnnVNs++KrS0lI99dRT6t69uxo1auSa3rdvXzVp0kQNGjTQzz//rOeee07bt2/XnDlzJPGelzzz3ZKVlaWYmBi3+VFRUbLZbGdNPR5t2bJlKisrU//+/d2m811e2dHnPLX9s5IQglpr6tSp+vbbb/XOO+/IZrO5pvfp08f1d0JCglq3bq0ePXpo9erV6tWrlzeK6nO6devm9rht27bq2bOnFi9erLZt23qpVLXf+++/rzZt2uiCCy5wTTteXQ8fPrymi4izWHl5ucaOHavDhw/r1VdfdZs3aNAg198JCQk655xzNHDgQG3ZskWtW7eu6aL6JL5bqsf777+vyy+/XNHR0W7TqW93xzrnqc3ojuUjjvVLSHZ2tmJjY71UKt/15JNPavny5XrjjTdOmNAbNmyohg0bavfu3ZLMdG+325Wbm+u2XHZ2dqVfE84WoaGhOv/887V7927X8VbVseisn/r16ysnJ8ftyhmGYSgnJ+esrcPs7Gx9+umnVXYpqKhiXUs6qfo+23jqGKxfv75b942K6zzb6ra8vFz33Xeftm/frtdff73Sr8dHS0xMlM1m02+//SaJ93xVTue7papj8lg9Ic4G27Zt0+bNm0/4uSmd3d/lxzrnqe2flYQQHxEUFKTWrVtX6nq1fv16t65GkKZMmeJ6M7Zs2fKEy+fk5CgjI0MNGjSQZH65BgYGat26da5lDhw4oF27dp21dV1SUqJff/1VsbGxiouLU2xsrNuxWFJSoo0bN7rqp127diosLHTrd5uenq7CwsKztg4XLlyowMBAt1/vqlKxriWdVH2fbTx1DCYnJ2vjxo0qKSlxLbN+/Xo1aNBAcXFxNbQ33ldWVqYxY8Zo+/btmjdv3kn9sLVjxw7Z7XbXsrznKzud75bk5GTt2rXL7TKy69atU1BQkBITE2t2B3zAe++9p7i4OHXp0uWEy56t3+XHO+ep7Z+VdMfyIcOHD9e4ceOUlJSkiy++WO+8844yMjI0ePBgbxfNZ0yaNEkffPCBXn75ZdWrV8/VHzIkJEShoaEqKCjQ9OnT1atXL8XGxmrfvn16/vnnFR0drZ49e0qSwsPDNWDAAD3zzDOKiYlRZGSkpk6dqoSEhJP6IPQHaWlp6t69uxo3bqycnBy98sorKiwsVP/+/WWxWDR06FDNmDFDLVq0ULNmzfTqq68qJCREffv2lSS1bNlSXbt21cSJEzV58mRJ0sSJE9W9e3e1aNHCm7vmFc4B6X369FFoaKjbvOPVtaSTqm9/VFBQoN9//12S5HA4tH//fm3btk0RERFq0qSJR47Bfv366eWXX9aECRP017/+Vbt379a//vUvjR492q1/dG13vLps0KCB7r33Xv3444967bXXZLFYXJ+b4eHhCg4O1u+//64lS5aoW7duioqK0q5du/TUU0/poosucg1mPRve88erx4iICI98t6SmpqpVq1YaN26cJkyYoLy8PD399NO68cYbFRYW5rV997QTvb8lqaioSEuXLtUdd9xR6f3Id7npROc8nvq+9tZnpcUw/OyuTbXcW2+9pdmzZysjI0Px8fF68MEH1bFjR28Xy2ckJCRUOX306NH6+9//ruLiYt19993aunWrDh8+rNjYWHXq1En33nuv29VISktLlZaWpmXLlqm4uFidO3fWxIkT3ZbxZ2PGjNGGDRuUl5enqKgoJScn695779X5558vyTypnj59ut577z0dPHhQbdu21aOPPqr4+HjXOg4ePKjHH39cn376qSTzpn2PPvpopSuVnQ2++uor3XbbbfrPf/6jpKQkt3knqmvp5Orb33z99dcaOnRopen9+/fXU0895bFjcPv27Zo8ebI2bdqkiIgIDR48WHfffbdfhZDj1eXo0aN1+eWXV/m8qVOn6vrrr9cff/yhsWPHaufOnSooKFDjxo3VrVs3jR49WpGRka7l/f09f7x6fOyxxzz23bJ//35NmjRJX331lYKDg9WvXz+NGzdOQUFBNbKfNeFE729JWrBggR555BF99tlnlQZF811uOtE5j+S572tvfFYSQgAAAADUKMaEAAAAAKhRhBAAAAAANYoQAgAAAKBGEUIAAAAA1ChCCAAAAIAaRQgBAAAAUKO4WSEAoFosXLhQDz74YJXzwsPDtXHjxhoukWnChAlav3691qxZ45XtAwAIIQCAavZ///d/atSokds0m83mpdIAAHwBIQQAUK0uvPBCnXfeed4uBgDAhzAmBADgNQsXLlRCQoI2bNigv/3tb2rXrp06deqkSZMmqbi42G3ZjIwMjRs3Tp06dVJiYqL69eunDz74oNI69+zZo7FjxyolJUWJiYm6/PLLNWXKlErLbd26VTfffLPatm2rXr166Z133qm2/QQAuKMlBABQrex2u8rLy92mWa1WWa1HfgcbO3asrrrqKt18883atGmTXnnlFRUVFempp56SJBUWFmrIkCE6ePCg7rvvPjVq1EhLlizRuHHjVFxcrEGDBkkyA8jAgQNVt25d3XPPPTrvvPP0xx9/aO3atW7bz8/P1/3336/bbrtNd999txYuXKjHHntMzZs316WXXlrNNQIAIIQAAKrVVVddVWnaZZddphkzZrge/+Uvf9H48eMlSampqbJYLHrppZd01113qXnz5lq4cKF2796tefPmqVOnTpKkbt26KTs7Wy+++KJuuOEG2Ww2TZs2TSUlJfrggw/UsGFD1/r79+/vtv2CggJNnDjRFTg6duyotWvXavny5YQQAKgBhBAAQLV6+eWX3QKBJNWrV8/t8dFBpU+fPnrxxRe1adMmNW/eXBs2bFDDhg1dAcTpmmuu0YMPPqiff/5ZCQkJWrdunS677LJK2zta3bp13cJGUFCQmjVrpv3795/OLgIAThEhBABQrVq1anXCgen169d3exwTEyNJ+vPPPyVJBw8eVGxs7DGfd/DgQUlSXl5epStxVeXoECSZQaS0tPSEzwUAnDkGpgMAvC4rK8vtcXZ2tiS5WjQiIiIqLVPxeREREZKkqKgoV3ABAPguQggAwOtWrlzp9nj58uWyWq1q27atJOmSSy7RgQMH9O2337ott2zZMsXExOj888+XJKWkpOizzz5TRkZGzRQcAHBa6I4FAKhW27ZtU25ubqXpiYmJrr/XrFmjtLQ0paamatOmTXr55Zd13XXXqVmzZpLMgeXz5s3T3//+d40ZM0YNGzbU0qVLtW7dOk2ePNl188O///3v+vzzzzV48GCNGjVK5557rv7880998cUXevbZZ2tkfwEAJ0YIAQBUq3vvvbfK6V9++aXr72eeeUZz5szRu+++q8DAQA0cONB1tSxJCgkJ0b///W8988wzevbZZ1VQUKDmzZvr6aef1rXXXutaLi4uTu+//75efPFFPffccyosLFTDhg11+eWXV98OAgBOmcUwDMPbhQAAnJ0WLlyoBx98UB9//DF3VQeAswhjQgAAAADUKEIIAAAAgBpFdywAAAAANYqWEAAAAAA1ihACAAAAoEYRQgAAAADUKEIIAAAAgBpFCAEAAABQowghAAAAAGrU/wOFUJLp4iYIQQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(13, 6))\n",
    "\n",
    "# summarize history for loss:\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='blue', alpha=0.4, label='validation')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Cost', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.title('Average Loss During Training', fontsize=18)\n",
    "#plt.xticks(range(0,epochs))\n",
    "plt.legend(loc='upper right', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
