{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import axis, colorbar, imshow, show, figure, subplot\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=18)\n",
    "mpl.rc('xtick', labelsize=16)\n",
    "mpl.rc('ytick', labelsize=16)\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## ----- GPU ------------------------------------\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'    # use of GPU 0 (ERDA) (use before importing torch or tensorflow/keeas)\n",
    "## ----------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization, ReLU, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.activations import relu\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K                         #contains calls for tensor manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL_TRAIN_AUG:       /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/NormalTrainAugmentations\n",
      "NORMAL_VALIDATION_AUG:  /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/NormalValidationAugmentations\n",
      "NORMAL_TEST_AUG:        /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/NormalTestAugmentations\n",
      "ANOMALY1_AUG:           /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/Anomaly1Augmentations\n",
      "ANOMALY2_AUG:           /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/Anomaly2Augmentations\n"
     ]
    }
   ],
   "source": [
    "from utils_vae_potato_paths import *\n",
    "\n",
    "# Get work directions for augmentated data set:\n",
    "print (\"NORMAL_TRAIN_AUG:      \", NORMAL_TRAIN_AUG)\n",
    "print (\"NORMAL_VALIDATION_AUG: \", NORMAL_VAL_AUG)\n",
    "print (\"NORMAL_TEST_AUG:       \", NORMAL_TEST_AUG)\n",
    "print (\"ANOMALY1_AUG:          \", ANOMALY1_AUG)\n",
    "print (\"ANOMALY2_AUG:          \", ANOMALY2_AUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which Folder The Models Are Saved In: saved_models/latent2\n"
     ]
    }
   ],
   "source": [
    "# What latent dimension and filter size are we using?:\n",
    "latent_dim = 2\n",
    "filters    = 32\n",
    "\n",
    "# Get work direction for saving files:\n",
    "SAVE_FOLDER = f'saved_models/latent{latent_dim}'\n",
    "print (\"Which Folder The Models Are Saved In:\", SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] deleting existing files in folder...\n",
      "         done in 0.002 minutes\n"
     ]
    }
   ],
   "source": [
    "# Delete all existing files in folder where models are saved:\n",
    "print(\"[INFO] deleting existing files in folder...\")\n",
    "t0 = time()\n",
    "\n",
    "files = glob.glob(f'{SAVE_FOLDER}/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "print (\"         done in %0.3f minutes\" % ((time() - t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_VAE_AE import get_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Investigation of Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"normal\" training images is:     1639\n",
      "Number of \"normal\" validation images is:   80\n",
      "Number of \"normal\" test images is:         320\n",
      "Total number of \"anomaly\" images is:       680\n"
     ]
    }
   ],
   "source": [
    "# Glob the directories and get the lists of good and bad images\n",
    "good_train_fns = [f for f in os.listdir(NORMAL_TRAIN_AUG) if not f.startswith('.')]\n",
    "good_val_fns = [f for f in os.listdir(NORMAL_VAL_AUG) if not f.startswith('.')]\n",
    "good_test_fns = [f for f in os.listdir(NORMAL_TEST_AUG) if not f.startswith('.')]\n",
    "bad1_fns = [f for f in os.listdir(ANOMALY1_AUG) if not f.startswith('.')]\n",
    "bad2_fns = [f for f in os.listdir(ANOMALY2_AUG) if not f.startswith('.')]\n",
    "\n",
    "print('Number of \"normal\" training images is:     {}'.format(len(good_train_fns)))\n",
    "print('Number of \"normal\" validation images is:   {}'.format(len(good_val_fns)))\n",
    "print('Number of \"normal\" test images is:         {}'.format(len(good_test_fns)))\n",
    "print('Total number of \"anomaly\" images is:       {}'.format(len(bad1_fns) + len(bad2_fns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Procentage of normal samples (ground truth):   74.99 %\n",
      "> Procentage of anomaly samples (ground truth):  25.01 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of bad images vs good images:\n",
    "normal_fns = len(good_train_fns) + len(good_val_fns) + len(good_test_fns)\n",
    "anomaly_fns = len(bad1_fns + bad2_fns)\n",
    "print(f\"> Procentage of normal samples (ground truth):   {(normal_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")\n",
    "print(f\"> Procentage of anomaly samples (ground truth):  {(anomaly_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Normal Data\n",
    "\n",
    "Load normal samples in pre-splitted data sets: train, valdiation and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal training images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal training images...\")\n",
    "trainX = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TRAIN_AUG}/*.png\")]  # read as grayscale\n",
    "trainX = np.array(trainX)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "trainY = get_labels(trainX, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal validation images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal validation images...\")\n",
    "x_normal_val = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_VAL_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_val = np.array(x_normal_val)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_val = get_labels(x_normal_val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal testing images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal testing images...\")\n",
    "x_normal_test = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TEST_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_test = np.array(x_normal_test)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_test = get_labels(x_normal_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Class 1 Data (i.e. 'metal' sampels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly class 1 ('metal') images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly class 1 ('metal') images...\")\n",
    "x_metal = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY1_AUG}/*.png\")]  # read as grayscale\n",
    "x_metal = np.array(x_metal)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_metal = get_labels(x_metal, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Class 2 Data (i.e. 'hollow' sampels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly class 2 ('hollow') images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly class 2 ('hollow') images...\")\n",
    "x_hollow = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY2_AUG}/*.png\")]  # read as grayscale\n",
    "x_hollow = np.array(x_hollow)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_hollow = get_labels(x_hollow, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine normal samples and anomaly samples and split them into training, validation and test sets\n",
    "\n",
    "\n",
    "`label 0` == Normal Samples (=> Perfect' Samples)\n",
    "\n",
    "`label 1` == Anomaly 1 Samples (=> 'Metal' Samples)\n",
    "\n",
    "`label 2` == Anomaly 2 Samples (=> 'Hollow' Samples)\n",
    "\n",
    "Train and Validation sets only consists of `Normal Data`, aka. `label 0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testvalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testvalY = get_labels(testvalX, 0)\n",
    "\n",
    "# randomly split in order to make new validation set:\n",
    "NoUsageX, valX, NoUsageY, valY = train_test_split(testvalX, testvalY, test_size=0.25, random_state=4345672)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Testing set \"\"\"\n",
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testNormalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testNormalY = get_labels(testNormalX, 0)\n",
    "\n",
    "# Anomaly Class 1 (i.e. 'metal' samples):\n",
    "testAnomaly1X, testAnomaly1Y = x_metal, y_metal\n",
    "\n",
    "# Anomaly Class 2 (i.e. 'hollow' samples):\n",
    "testAnomaly2X, testAnomaly2Y = x_hollow, y_hollow\n",
    "\n",
    "# Combine all testing data in one array (the test set is NOT used during any part but inference):\n",
    "testAllX = np.vstack((testNormalX, testAnomaly1X, testAnomaly2X))\n",
    "testAllY = np.hstack((testNormalY, testAnomaly1Y, testAnomaly2Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Dimensions and Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      " > images: (1639, 128, 128)\n",
      " > labels: (1639,)\n",
      "\n",
      "Validation set:\n",
      " > images: (100, 128, 128)\n",
      " > labels: (100,)\n",
      "\n",
      "Test set:\n",
      " > images: (1080, 128, 128)\n",
      " > labels: (1080,)\n",
      "\t'Normal' test set:\n",
      "\t  > images: (400, 128, 128)\n",
      "\t  > labels: (400,)\n",
      "\t'Anomaly 1' test set:\n",
      "\t  > images: (392, 128, 128)\n",
      "\t  > labels: (392,)\n",
      "\t'Anomaly 2' test set:\n",
      "\t  > images: (288, 128, 128)\n",
      "\t  > labels: (288,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "print(\" > images:\", trainX.shape)\n",
    "print(\" > labels:\", trainY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Validation set:\")\n",
    "print(\" > images:\", valX.shape)\n",
    "print(\" > labels:\", valY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Test set:\")\n",
    "print(\" > images:\", testAllX.shape)\n",
    "print(\" > labels:\", testAllY.shape)\n",
    "print(\"\\t'Normal' test set:\")\n",
    "print(\"\\t  > images:\", testNormalX.shape)\n",
    "print(\"\\t  > labels:\", testNormalY.shape)\n",
    "print(\"\\t'Anomaly 1' test set:\")\n",
    "print(\"\\t  > images:\", testAnomaly1X.shape)\n",
    "print(\"\\t  > labels:\", testAnomaly1Y.shape)\n",
    "print(\"\\t'Anomaly 2' test set:\")\n",
    "print(\"\\t  > images:\", testAnomaly2X.shape)\n",
    "print(\"\\t  > labels:\", testAnomaly2Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 76.62 %\n",
      "Procentage of validation samples: 4.68 %\n",
      "Procentage of (only normal) testing samples: 18.70 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets (only normal data):\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (only normal) testing samples: {(len(testNormalX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 58.14 %\n",
      "Procentage of validation samples: 3.55 %\n",
      "Procentage of (total) testing samples: 38.31 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets:\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (total) testing samples: {(len(testAllX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for (un)balanced data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['0: normal', '1: metal', '2: hollow']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,4))\n",
    "ax[0].hist(trainY, align=\"left\"); ax[0].set_title('train', fontsize=18); ax[0].set_xticks([0,1,2]); ax[0].set_xticklabels(labels); ax[0].tick_params(axis='both', which='major', labelsize=16); ax[0].set_xlim(-0.5, 2.5);\n",
    "ax[1].hist(valY, align=\"left\"); ax[1].set_title('validation', fontsize=18); ax[1].set_xticks([0,1,2]); ax[1].set_xticklabels(labels); ax[1].tick_params(axis='both', which='major', labelsize=16); ax[1].set_xlim(-0.5, 2.5);\n",
    "ax[2].hist(testAllY, align=\"left\"); ax[2].set_title('test', fontsize=18); ax[2].set_xticks([0,1,2]); ax[2].set_xticklabels(labels); ax[2].tick_params(axis='both', which='major', labelsize=16); ax[2].set_xlim(-0.5, 2.5);\n",
    "\n",
    "ax[0].set_ylabel(\"Number of images\", fontsize=18)\n",
    "ax[1].set_xlabel(\"Class Labels\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of normal test samples: 37.04 %\n",
      "Procentage of total anomaly test samples: 62.96 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of normal test images vs total anomaly test images:\n",
    "print(f\"Procentage of normal test samples: {(len(testNormalX) / (len(testNormalX) + (len(testAnomaly1X) + len(testAnomaly2X)))) * 100:.2f} %\")\n",
    "print(f\"Procentage of total anomaly test samples: {((len(testAnomaly1X) + len(testAnomaly2X)) / (len(testNormalX) + (len(testAnomaly1X) + len(testAnomaly2X)))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Only normalization has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "img_width, img_height = trainX.shape[1], trainX.shape[2]      # input image dimensions\n",
    "num_channels = 1                                              # gray-channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Neural Networks learns faster with normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to be in  the [0, 1] range:\n",
    "trainX = trainX.astype('float32') / 255.\n",
    "valX = valX.astype('float32') / 255.\n",
    "testAllX = testAllX.astype('float32') / 255.\n",
    "\n",
    "# reshape data to keras inputs --> (n_samples, img_rows, img_cols, n_channels):\n",
    "trainX = trainX.reshape(trainX.shape[0], img_height, img_width, num_channels)\n",
    "valX = valX.reshape(valX.shape[0], img_height, img_width, num_channels)\n",
    "testAllX = testAllX.reshape(testAllX.shape[0], img_height, img_width, num_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import architecture of VAE model and its hyperparameters:\n",
    "from J2_VAE_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Encoder Part*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 32)   320         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 64, 64, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 32)   9248        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 32, 32, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 32)   9248        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 16, 16, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 8, 32)     9248        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 8, 8, 32)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 4, 4, 32)     9248        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 4, 4, 32)     0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "latent (Dense)                  (None, 200)          102600      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 200)          0           latent[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            402         leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_log_mean (Dense)              (None, 2)            402         leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 2)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_mean[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 140,716\n",
      "Trainable params: 140,716\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Decoder Part*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1536      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 128, 128, 1)       289       \n",
      "=================================================================\n",
      "Total params: 48,065\n",
      "Trainable params: 48,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Total VAE Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         [(None, 2), (None, 2), (N 140716    \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 128, 128, 1)       48065     \n",
      "=================================================================\n",
      "Total params: 188,781\n",
      "Trainable params: 188,781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "batch_size  = 256\n",
    "epochs      = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at 1'st epoch (inital model)\n",
    "\n",
    "https://stackoverflow.com/questions/54323960/save-keras-model-at-specific-epochs\n",
    "\"\"\"\n",
    "\n",
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 1:\n",
    "            self.model.save_weights(f\"{SAVE_FOLDER}/cp-0001.h5\")\n",
    "            \n",
    "# create and use callback:\n",
    "initial_checkpoint = CustomSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at every k'te epochs\n",
    "\"\"\"\n",
    "# Include the epoch in the file name (uses `str.format`):\n",
    "checkpoint_path = \"saved_models/latent2/cp-{epoch:04d}.h5\"\n",
    "\n",
    "# Create a callback that saves the model's weights every k'te epochs:\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path,\n",
    "                             monitor='loss',           # name of the metrics to monitor\n",
    "                             verbose=1, \n",
    "                             #save_best_only=False,     # if False, the best model will not be overridden.\n",
    "                             save_weights_only=True,   # if True, only the weights of the models will be saved.\n",
    "                                                        # if False, the whole models will be saved.\n",
    "                             #mode='auto', \n",
    "                             period=200                  # save after every k'te epochs\n",
    "                            )\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "vae.save_weights(checkpoint_path.format(epoch=0))          # save for zero epoch (inital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: Early stopping\n",
    "https://www.tensorflow.org/guide/keras/custom_callback\n",
    "\"\"\"\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=100):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create and Compile Model\"\"\"\n",
    "# import architecture of VAE model and its hyperparameters. learning rate choosen from graph above.\n",
    "vae = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "7/7 [==============================] - 1s 191ms/step - loss: 2326.7781 - val_loss: 2270.0557\n",
      "Epoch 2/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 2136.9253 - val_loss: 1850.0094\n",
      "Epoch 3/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 1527.4406 - val_loss: 1250.0247\n",
      "Epoch 4/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 1122.4808 - val_loss: 955.3797\n",
      "Epoch 5/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 820.2628 - val_loss: 680.0914\n",
      "Epoch 6/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 593.9794 - val_loss: 504.7500\n",
      "Epoch 7/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 450.0598 - val_loss: 405.5828\n",
      "Epoch 8/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 377.8736 - val_loss: 361.1185\n",
      "Epoch 9/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 343.5945 - val_loss: 332.3405\n",
      "Epoch 10/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 328.4549 - val_loss: 320.0463\n",
      "Epoch 11/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 306.4854 - val_loss: 298.5015\n",
      "Epoch 12/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 289.9750 - val_loss: 290.0377\n",
      "Epoch 13/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 283.9968 - val_loss: 287.5458\n",
      "Epoch 14/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 277.4207 - val_loss: 282.5267\n",
      "Epoch 15/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 277.3177 - val_loss: 274.6842\n",
      "Epoch 16/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 268.9002 - val_loss: 271.4196\n",
      "Epoch 17/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 264.4045 - val_loss: 266.1273\n",
      "Epoch 18/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 258.9483 - val_loss: 262.2665\n",
      "Epoch 19/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 255.5926 - val_loss: 258.6535\n",
      "Epoch 20/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 254.5636 - val_loss: 261.2075\n",
      "Epoch 21/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 251.4228 - val_loss: 251.9238\n",
      "Epoch 22/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 250.9595 - val_loss: 261.5956\n",
      "Epoch 23/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 248.9836 - val_loss: 251.6456\n",
      "Epoch 24/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 243.5668 - val_loss: 243.6354\n",
      "Epoch 25/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 240.3529 - val_loss: 242.2686\n",
      "Epoch 26/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 237.4063 - val_loss: 239.3731\n",
      "Epoch 27/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 234.5196 - val_loss: 237.0246\n",
      "Epoch 28/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 232.9813 - val_loss: 234.6081\n",
      "Epoch 29/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 231.9841 - val_loss: 232.6941\n",
      "Epoch 30/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 228.6407 - val_loss: 230.7864\n",
      "Epoch 31/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 225.9258 - val_loss: 229.8275\n",
      "Epoch 32/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 225.6273 - val_loss: 227.7163\n",
      "Epoch 33/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 223.0373 - val_loss: 226.6699\n",
      "Epoch 34/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 221.1398 - val_loss: 223.4930\n",
      "Epoch 35/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 219.3970 - val_loss: 222.2811\n",
      "Epoch 36/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 217.4287 - val_loss: 220.6096\n",
      "Epoch 37/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 216.6777 - val_loss: 219.6297\n",
      "Epoch 38/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 214.4442 - val_loss: 217.1324\n",
      "Epoch 39/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 213.4016 - val_loss: 215.0804\n",
      "Epoch 40/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 211.0582 - val_loss: 213.9216\n",
      "Epoch 41/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 210.3158 - val_loss: 212.5208\n",
      "Epoch 42/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 209.3990 - val_loss: 214.6555\n",
      "Epoch 43/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 209.2994 - val_loss: 212.2066\n",
      "Epoch 44/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 207.6854 - val_loss: 208.8123\n",
      "Epoch 45/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 204.2741 - val_loss: 207.5397\n",
      "Epoch 46/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 201.9434 - val_loss: 204.4405\n",
      "Epoch 47/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 201.3331 - val_loss: 202.9528\n",
      "Epoch 48/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 199.4276 - val_loss: 200.9348\n",
      "Epoch 49/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 197.8639 - val_loss: 199.9421\n",
      "Epoch 50/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 196.2587 - val_loss: 198.1016\n",
      "Epoch 51/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 194.7738 - val_loss: 196.8917\n",
      "Epoch 52/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 192.8994 - val_loss: 194.8777\n",
      "Epoch 53/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 191.9470 - val_loss: 193.3283\n",
      "Epoch 54/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 190.7141 - val_loss: 191.8656\n",
      "Epoch 55/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 189.1691 - val_loss: 190.8476\n",
      "Epoch 56/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 187.3081 - val_loss: 189.5215\n",
      "Epoch 57/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 186.5996 - val_loss: 188.0939\n",
      "Epoch 58/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 185.4628 - val_loss: 186.5901\n",
      "Epoch 59/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 183.3521 - val_loss: 185.5294\n",
      "Epoch 60/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 181.9529 - val_loss: 184.1398\n",
      "Epoch 61/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 181.3516 - val_loss: 183.5769\n",
      "Epoch 62/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 180.4453 - val_loss: 181.5558\n",
      "Epoch 63/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 178.5684 - val_loss: 180.9751\n",
      "Epoch 64/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 177.4588 - val_loss: 179.0032\n",
      "Epoch 65/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 176.7951 - val_loss: 179.4091\n",
      "Epoch 66/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 175.8933 - val_loss: 177.3468\n",
      "Epoch 67/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 174.0257 - val_loss: 175.2898\n",
      "Epoch 68/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 172.2929 - val_loss: 174.5827\n",
      "Epoch 69/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 171.0638 - val_loss: 173.2068\n",
      "Epoch 70/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 169.9975 - val_loss: 171.3658\n",
      "Epoch 71/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 169.3159 - val_loss: 169.8615\n",
      "Epoch 72/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 168.0735 - val_loss: 169.1321\n",
      "Epoch 73/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 166.7471 - val_loss: 168.0445\n",
      "Epoch 74/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 166.0635 - val_loss: 166.8067\n",
      "Epoch 75/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 164.1011 - val_loss: 165.7794\n",
      "Epoch 76/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 163.2132 - val_loss: 164.5833\n",
      "Epoch 77/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 162.1663 - val_loss: 163.6193\n",
      "Epoch 78/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 160.3395 - val_loss: 162.4757\n",
      "Epoch 79/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 160.0946 - val_loss: 161.5071\n",
      "Epoch 80/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 159.6308 - val_loss: 160.8175\n",
      "Epoch 81/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 157.6797 - val_loss: 159.0482\n",
      "Epoch 82/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 156.8683 - val_loss: 158.0672\n",
      "Epoch 83/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 154.9036 - val_loss: 157.1770\n",
      "Epoch 84/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 154.5914 - val_loss: 155.9654\n",
      "Epoch 85/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 153.6403 - val_loss: 155.1470\n",
      "Epoch 86/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 152.5552 - val_loss: 154.4443\n",
      "Epoch 87/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 151.3785 - val_loss: 153.1126\n",
      "Epoch 88/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 150.4607 - val_loss: 152.1418\n",
      "Epoch 89/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 149.8039 - val_loss: 151.2651\n",
      "Epoch 90/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 148.4006 - val_loss: 150.1231\n",
      "Epoch 91/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 148.2477 - val_loss: 149.3694\n",
      "Epoch 92/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 146.2809 - val_loss: 148.0743\n",
      "Epoch 93/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 146.2737 - val_loss: 147.8200\n",
      "Epoch 94/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 144.8862 - val_loss: 146.3252\n",
      "Epoch 95/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 143.9993 - val_loss: 145.8739\n",
      "Epoch 96/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 143.9611 - val_loss: 144.7019\n",
      "Epoch 97/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 142.9676 - val_loss: 143.9032\n",
      "Epoch 98/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 142.1101 - val_loss: 142.7589\n",
      "Epoch 99/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 140.6301 - val_loss: 142.7701\n",
      "Epoch 100/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 140.4939 - val_loss: 141.2061\n",
      "Epoch 101/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 139.0388 - val_loss: 140.6180\n",
      "Epoch 102/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 138.0327 - val_loss: 140.5726\n",
      "Epoch 103/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 137.9736 - val_loss: 138.8325\n",
      "Epoch 104/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 137.0542 - val_loss: 138.7897\n",
      "Epoch 105/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 135.4762 - val_loss: 136.6456\n",
      "Epoch 106/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 134.7715 - val_loss: 137.1196\n",
      "Epoch 107/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 134.7096 - val_loss: 135.7984\n",
      "Epoch 108/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 133.6607 - val_loss: 135.5202\n",
      "Epoch 109/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 133.4285 - val_loss: 135.6540\n",
      "Epoch 110/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 133.0853 - val_loss: 135.8993\n",
      "Epoch 111/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 131.3927 - val_loss: 132.4382\n",
      "Epoch 112/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 129.8701 - val_loss: 132.0077\n",
      "Epoch 113/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 130.0350 - val_loss: 131.3240\n",
      "Epoch 114/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 129.2191 - val_loss: 130.5639\n",
      "Epoch 115/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 129.1082 - val_loss: 130.2506\n",
      "Epoch 116/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 127.6704 - val_loss: 129.0178\n",
      "Epoch 117/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 127.0011 - val_loss: 129.8801\n",
      "Epoch 118/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 126.6999 - val_loss: 127.9181\n",
      "Epoch 119/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 125.8644 - val_loss: 127.9714\n",
      "Epoch 120/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 124.5981 - val_loss: 126.6157\n",
      "Epoch 121/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 124.9322 - val_loss: 126.6679\n",
      "Epoch 122/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 123.1306 - val_loss: 125.3247\n",
      "Epoch 123/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 123.3724 - val_loss: 127.1814\n",
      "Epoch 124/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 122.8683 - val_loss: 124.2203\n",
      "Epoch 125/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 121.6524 - val_loss: 123.9643\n",
      "Epoch 126/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 121.8194 - val_loss: 122.8010\n",
      "Epoch 127/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 120.9329 - val_loss: 122.4215\n",
      "Epoch 128/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 119.4951 - val_loss: 121.7673\n",
      "Epoch 129/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 119.7015 - val_loss: 121.0011\n",
      "Epoch 130/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 118.5901 - val_loss: 120.1031\n",
      "Epoch 131/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 118.2325 - val_loss: 119.4116\n",
      "Epoch 132/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 117.8621 - val_loss: 119.0635\n",
      "Epoch 133/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 117.2102 - val_loss: 118.6888\n",
      "Epoch 134/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 115.9964 - val_loss: 117.3470\n",
      "Epoch 135/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 115.9641 - val_loss: 117.2429\n",
      "Epoch 136/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 115.6475 - val_loss: 117.0106\n",
      "Epoch 137/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 114.4825 - val_loss: 117.1852\n",
      "Epoch 138/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 114.5545 - val_loss: 115.8832\n",
      "Epoch 139/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 113.9964 - val_loss: 115.3511\n",
      "Epoch 140/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 113.5098 - val_loss: 115.0336\n",
      "Epoch 141/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 112.4504 - val_loss: 114.3460\n",
      "Epoch 142/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 111.9164 - val_loss: 113.6394\n",
      "Epoch 143/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 111.9837 - val_loss: 112.9528\n",
      "Epoch 144/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 111.3890 - val_loss: 112.9514\n",
      "Epoch 145/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 111.0401 - val_loss: 112.5978\n",
      "Epoch 146/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 110.4987 - val_loss: 111.3579\n",
      "Epoch 147/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 109.8056 - val_loss: 112.6218\n",
      "Epoch 148/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 109.0190 - val_loss: 111.4583\n",
      "Epoch 149/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 108.7374 - val_loss: 109.6723\n",
      "Epoch 150/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 108.2780 - val_loss: 109.5359\n",
      "Epoch 151/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 107.3227 - val_loss: 109.5645\n",
      "Epoch 152/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 107.3287 - val_loss: 109.3179\n",
      "Epoch 153/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 107.0575 - val_loss: 108.6284\n",
      "Epoch 154/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 106.4575 - val_loss: 108.6077\n",
      "Epoch 155/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 105.7038 - val_loss: 106.9938\n",
      "Epoch 156/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 105.4219 - val_loss: 109.1400\n",
      "Epoch 157/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 106.1313 - val_loss: 108.2754\n",
      "Epoch 158/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 104.3727 - val_loss: 106.3834\n",
      "Epoch 159/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 104.5801 - val_loss: 106.5030\n",
      "Epoch 160/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 104.2313 - val_loss: 105.1549\n",
      "Epoch 161/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 103.1282 - val_loss: 105.0780\n",
      "Epoch 162/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 102.5906 - val_loss: 104.6892\n",
      "Epoch 163/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 102.4112 - val_loss: 104.0091\n",
      "Epoch 164/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 102.1553 - val_loss: 103.7678\n",
      "Epoch 165/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 101.6347 - val_loss: 104.4840\n",
      "Epoch 166/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 101.4161 - val_loss: 102.8854\n",
      "Epoch 167/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 100.7160 - val_loss: 102.8694\n",
      "Epoch 168/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 101.3104 - val_loss: 101.8300\n",
      "Epoch 169/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 99.7849 - val_loss: 101.7597\n",
      "Epoch 170/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 100.5667 - val_loss: 101.8390\n",
      "Epoch 171/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 99.8640 - val_loss: 100.8891\n",
      "Epoch 172/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 98.8288 - val_loss: 101.2477\n",
      "Epoch 173/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 98.6883 - val_loss: 100.1502\n",
      "Epoch 174/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 98.7086 - val_loss: 100.5380\n",
      "Epoch 175/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 98.4595 - val_loss: 99.5284\n",
      "Epoch 176/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 98.1199 - val_loss: 99.7825\n",
      "Epoch 177/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 97.7264 - val_loss: 99.1379\n",
      "Epoch 178/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 96.9370 - val_loss: 98.9027\n",
      "Epoch 179/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 96.6016 - val_loss: 97.8870\n",
      "Epoch 180/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 96.1374 - val_loss: 97.6545\n",
      "Epoch 181/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 96.0544 - val_loss: 98.0062\n",
      "Epoch 182/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 95.5076 - val_loss: 96.7528\n",
      "Epoch 183/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 95.1840 - val_loss: 96.5392\n",
      "Epoch 184/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 95.1022 - val_loss: 97.1352\n",
      "Epoch 185/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 94.3367 - val_loss: 95.7613\n",
      "Epoch 186/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 94.2255 - val_loss: 96.0236\n",
      "Epoch 187/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 94.3687 - val_loss: 95.2491\n",
      "Epoch 188/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 93.1896 - val_loss: 95.9181\n",
      "Epoch 189/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 93.9070 - val_loss: 94.9504\n",
      "Epoch 190/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 92.7725 - val_loss: 94.6780\n",
      "Epoch 191/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 92.3839 - val_loss: 94.6276\n",
      "Epoch 192/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 92.3506 - val_loss: 94.0046\n",
      "Epoch 193/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 92.0552 - val_loss: 93.8898\n",
      "Epoch 194/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 92.0812 - val_loss: 93.5869\n",
      "Epoch 195/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 91.7343 - val_loss: 93.6329\n",
      "Epoch 196/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 91.7227 - val_loss: 93.3751\n",
      "Epoch 197/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 90.8963 - val_loss: 92.7576\n",
      "Epoch 198/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 90.7703 - val_loss: 92.7122\n",
      "Epoch 199/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 91.1321 - val_loss: 92.9434\n",
      "Epoch 200/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 90.2503\n",
      "Epoch 00200: saving model to saved_models/latent2/cp-0200.h5\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 90.2503 - val_loss: 92.1106\n",
      "Epoch 201/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 89.9572 - val_loss: 91.4126\n",
      "Epoch 202/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 89.9854 - val_loss: 90.8957\n",
      "Epoch 203/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 89.6718 - val_loss: 90.6444\n",
      "Epoch 204/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 89.2220 - val_loss: 90.5539\n",
      "Epoch 205/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 89.0451 - val_loss: 90.8068\n",
      "Epoch 206/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 88.7133 - val_loss: 90.2254\n",
      "Epoch 207/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 88.7246 - val_loss: 89.9104\n",
      "Epoch 208/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 88.7398 - val_loss: 90.3390\n",
      "Epoch 209/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 88.1497 - val_loss: 89.9477\n",
      "Epoch 210/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 88.4793 - val_loss: 89.3617\n",
      "Epoch 211/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 87.4317 - val_loss: 89.1299\n",
      "Epoch 212/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 87.8436 - val_loss: 88.3962\n",
      "Epoch 213/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 86.9902 - val_loss: 88.3788\n",
      "Epoch 214/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 86.8281 - val_loss: 88.5029\n",
      "Epoch 215/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 87.2141 - val_loss: 89.2560\n",
      "Epoch 216/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 86.9357 - val_loss: 87.7747\n",
      "Epoch 217/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 86.4995 - val_loss: 88.7374\n",
      "Epoch 218/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 85.9750 - val_loss: 87.9560\n",
      "Epoch 219/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 86.1171 - val_loss: 87.9212\n",
      "Epoch 220/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 86.1404 - val_loss: 88.6605\n",
      "Epoch 221/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 86.0064 - val_loss: 87.2479\n",
      "Epoch 222/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 85.9985 - val_loss: 89.6227\n",
      "Epoch 223/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 85.6406 - val_loss: 87.3396\n",
      "Epoch 224/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 85.4044 - val_loss: 86.2922\n",
      "Epoch 225/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 84.2099 - val_loss: 86.0795\n",
      "Epoch 226/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 83.9484 - val_loss: 85.8864\n",
      "Epoch 227/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 84.1592 - val_loss: 85.7793\n",
      "Epoch 228/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 84.1120 - val_loss: 85.1320\n",
      "Epoch 229/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 83.7441 - val_loss: 84.9486\n",
      "Epoch 230/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 83.1682 - val_loss: 84.6631\n",
      "Epoch 231/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 82.8764 - val_loss: 84.6921\n",
      "Epoch 232/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 82.6830 - val_loss: 84.7964\n",
      "Epoch 233/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 82.9299 - val_loss: 84.6275\n",
      "Epoch 234/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 83.2337 - val_loss: 84.2520\n",
      "Epoch 235/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 83.0891 - val_loss: 84.7694\n",
      "Epoch 236/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 82.0461 - val_loss: 83.6507\n",
      "Epoch 237/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 82.1200 - val_loss: 83.6229\n",
      "Epoch 238/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 82.3127 - val_loss: 83.1099\n",
      "Epoch 239/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 81.6655 - val_loss: 83.1009\n",
      "Epoch 240/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 81.1796 - val_loss: 83.4349\n",
      "Epoch 241/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 81.4264 - val_loss: 82.3059\n",
      "Epoch 242/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 81.6129 - val_loss: 82.5904\n",
      "Epoch 243/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 81.7487 - val_loss: 82.8179\n",
      "Epoch 244/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 81.0704 - val_loss: 82.4857\n",
      "Epoch 245/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 80.3355 - val_loss: 82.3817\n",
      "Epoch 246/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 80.4354 - val_loss: 81.9464\n",
      "Epoch 247/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 80.4736 - val_loss: 82.4022\n",
      "Epoch 248/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 80.4463 - val_loss: 82.1669\n",
      "Epoch 249/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 79.7583 - val_loss: 81.8612\n",
      "Epoch 250/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 79.6917 - val_loss: 81.1299\n",
      "Epoch 251/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 79.1456 - val_loss: 81.5461\n",
      "Epoch 252/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 79.1674 - val_loss: 80.6117\n",
      "Epoch 253/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 79.0269 - val_loss: 80.8543\n",
      "Epoch 254/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 79.1505 - val_loss: 81.3261\n",
      "Epoch 255/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 79.2805 - val_loss: 80.4490\n",
      "Epoch 256/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 79.1437 - val_loss: 80.3188\n",
      "Epoch 257/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 78.6368 - val_loss: 81.1549\n",
      "Epoch 258/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 78.5667 - val_loss: 80.1383\n",
      "Epoch 259/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 78.7748 - val_loss: 80.4043\n",
      "Epoch 260/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 78.0647 - val_loss: 79.8428\n",
      "Epoch 261/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 78.0600 - val_loss: 79.8706\n",
      "Epoch 262/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 78.2071 - val_loss: 79.1876\n",
      "Epoch 263/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 77.6246 - val_loss: 79.5232\n",
      "Epoch 264/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 77.5299 - val_loss: 79.4524\n",
      "Epoch 265/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 77.4357 - val_loss: 79.3217\n",
      "Epoch 266/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 76.8013 - val_loss: 78.8846\n",
      "Epoch 267/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 77.6018 - val_loss: 79.7668\n",
      "Epoch 268/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 77.5376 - val_loss: 79.5779\n",
      "Epoch 269/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 77.2446 - val_loss: 79.3709\n",
      "Epoch 270/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 77.2009 - val_loss: 78.6054\n",
      "Epoch 271/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 76.7414 - val_loss: 79.0023\n",
      "Epoch 272/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 77.2702 - val_loss: 79.1065\n",
      "Epoch 273/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 76.7778 - val_loss: 78.4274\n",
      "Epoch 274/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 76.5128 - val_loss: 78.1944\n",
      "Epoch 275/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 76.3881 - val_loss: 77.7986\n",
      "Epoch 276/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 76.5044 - val_loss: 77.9874\n",
      "Epoch 277/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 75.8017 - val_loss: 77.7786\n",
      "Epoch 278/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 75.9145 - val_loss: 77.3790\n",
      "Epoch 279/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 76.1257 - val_loss: 77.8890\n",
      "Epoch 280/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 75.3861 - val_loss: 77.4238\n",
      "Epoch 281/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 75.1011 - val_loss: 77.3990\n",
      "Epoch 282/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 75.4129 - val_loss: 77.8348\n",
      "Epoch 283/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 75.0954 - val_loss: 77.6470\n",
      "Epoch 284/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 76.1806 - val_loss: 77.9633\n",
      "Epoch 285/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 74.9912 - val_loss: 76.6116\n",
      "Epoch 286/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 74.6907 - val_loss: 77.0239\n",
      "Epoch 287/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 74.7400 - val_loss: 76.8229\n",
      "Epoch 288/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 75.0458 - val_loss: 76.3960\n",
      "Epoch 289/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 74.1646 - val_loss: 76.1888\n",
      "Epoch 290/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 74.5154 - val_loss: 76.1801\n",
      "Epoch 291/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 74.4253 - val_loss: 76.1974\n",
      "Epoch 292/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 74.3138 - val_loss: 75.3741\n",
      "Epoch 293/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 73.9845 - val_loss: 75.9542\n",
      "Epoch 294/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 73.7781 - val_loss: 75.2642\n",
      "Epoch 295/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 73.9310 - val_loss: 76.1011\n",
      "Epoch 296/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 73.8599 - val_loss: 75.5158\n",
      "Epoch 297/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 73.2497 - val_loss: 75.4941\n",
      "Epoch 298/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 74.1780 - val_loss: 77.2102\n",
      "Epoch 299/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 74.1619 - val_loss: 76.0562\n",
      "Epoch 300/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 73.4375 - val_loss: 75.5751\n",
      "Epoch 301/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 73.1003 - val_loss: 74.5788\n",
      "Epoch 302/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 73.3463 - val_loss: 75.3471\n",
      "Epoch 303/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 72.8412 - val_loss: 74.7018\n",
      "Epoch 304/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 72.9400 - val_loss: 74.9552\n",
      "Epoch 305/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 72.3686 - val_loss: 74.5819\n",
      "Epoch 306/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 72.4141 - val_loss: 75.0186\n",
      "Epoch 307/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 73.1359 - val_loss: 76.6715\n",
      "Epoch 308/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 73.8083 - val_loss: 74.1367\n",
      "Epoch 309/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 73.6479 - val_loss: 76.1450\n",
      "Epoch 310/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 72.8258 - val_loss: 74.4501\n",
      "Epoch 311/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 72.2746 - val_loss: 73.9544\n",
      "Epoch 312/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 72.0875 - val_loss: 74.6461\n",
      "Epoch 313/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 72.6101 - val_loss: 74.0838\n",
      "Epoch 314/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 72.4901 - val_loss: 74.0536\n",
      "Epoch 315/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 72.1523 - val_loss: 73.7666\n",
      "Epoch 316/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 72.4500 - val_loss: 73.3310\n",
      "Epoch 317/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 71.6516 - val_loss: 74.5453\n",
      "Epoch 318/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 71.9284 - val_loss: 73.3100\n",
      "Epoch 319/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 71.8319 - val_loss: 74.4271\n",
      "Epoch 320/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 71.8550 - val_loss: 73.2494\n",
      "Epoch 321/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 71.5202 - val_loss: 73.2785\n",
      "Epoch 322/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 71.0169 - val_loss: 72.7293\n",
      "Epoch 323/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 71.2202 - val_loss: 73.0631\n",
      "Epoch 324/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 71.6102 - val_loss: 73.6138\n",
      "Epoch 325/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 71.0704 - val_loss: 72.7279\n",
      "Epoch 326/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 70.4653 - val_loss: 72.6295\n",
      "Epoch 327/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 70.9643 - val_loss: 73.1639\n",
      "Epoch 328/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 70.6621 - val_loss: 72.5387\n",
      "Epoch 329/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 70.8370 - val_loss: 72.3537\n",
      "Epoch 330/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 70.7328 - val_loss: 72.9877\n",
      "Epoch 331/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 70.4083 - val_loss: 72.2955\n",
      "Epoch 332/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 70.8236 - val_loss: 72.7410\n",
      "Epoch 333/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 70.8766 - val_loss: 72.5329\n",
      "Epoch 334/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 70.6205 - val_loss: 72.7927\n",
      "Epoch 335/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 70.4187 - val_loss: 72.4539\n",
      "Epoch 336/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 70.7338 - val_loss: 72.5889\n",
      "Epoch 337/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 70.6606 - val_loss: 72.5043\n",
      "Epoch 338/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 70.5604 - val_loss: 72.6701\n",
      "Epoch 339/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 69.9758 - val_loss: 72.4436\n",
      "Epoch 340/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 70.3341 - val_loss: 71.0066\n",
      "Epoch 341/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 69.4066 - val_loss: 72.4426\n",
      "Epoch 342/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 69.6575 - val_loss: 71.2933\n",
      "Epoch 343/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 69.6695 - val_loss: 71.7378\n",
      "Epoch 344/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 69.3976 - val_loss: 71.6097\n",
      "Epoch 345/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 69.5294 - val_loss: 71.4847\n",
      "Epoch 346/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 68.7823 - val_loss: 71.1618\n",
      "Epoch 347/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 69.5900 - val_loss: 71.7032\n",
      "Epoch 348/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 69.3560 - val_loss: 71.0491\n",
      "Epoch 349/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 68.9182 - val_loss: 71.5765\n",
      "Epoch 350/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 68.6918 - val_loss: 70.7857\n",
      "Epoch 351/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 68.9777 - val_loss: 71.6007\n",
      "Epoch 352/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 69.4079 - val_loss: 71.4172\n",
      "Epoch 353/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 68.5415 - val_loss: 71.6073\n",
      "Epoch 354/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 69.4357 - val_loss: 72.0274\n",
      "Epoch 355/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 68.9231 - val_loss: 70.8815\n",
      "Epoch 356/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 68.7282 - val_loss: 71.3448\n",
      "Epoch 357/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 68.4826 - val_loss: 70.8190\n",
      "Epoch 358/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 67.8238 - val_loss: 70.4932\n",
      "Epoch 359/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 68.4431 - val_loss: 70.6126\n",
      "Epoch 360/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 68.7314 - val_loss: 71.0763\n",
      "Epoch 361/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 68.3781 - val_loss: 70.6864\n",
      "Epoch 362/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 68.2108 - val_loss: 70.3394\n",
      "Epoch 363/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 67.9773 - val_loss: 70.8468\n",
      "Epoch 364/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 68.5930 - val_loss: 69.6934\n",
      "Epoch 365/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 68.8378 - val_loss: 70.5484\n",
      "Epoch 366/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 68.1089 - val_loss: 70.6750\n",
      "Epoch 367/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 68.3421 - val_loss: 70.2851\n",
      "Epoch 368/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 67.9588 - val_loss: 70.7431\n",
      "Epoch 369/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 67.5407 - val_loss: 70.3555\n",
      "Epoch 370/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 67.6536 - val_loss: 70.4295\n",
      "Epoch 371/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 67.9114 - val_loss: 70.3694\n",
      "Epoch 372/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 68.4465 - val_loss: 70.1605\n",
      "Epoch 373/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 67.7563 - val_loss: 70.6545\n",
      "Epoch 374/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 67.8506 - val_loss: 69.6056\n",
      "Epoch 375/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 67.6127 - val_loss: 70.6299\n",
      "Epoch 376/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 67.6507 - val_loss: 70.3638\n",
      "Epoch 377/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 67.4189 - val_loss: 69.6228\n",
      "Epoch 378/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 67.6006 - val_loss: 70.1210\n",
      "Epoch 379/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 66.8737 - val_loss: 69.4543\n",
      "Epoch 380/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 67.5080 - val_loss: 69.5462\n",
      "Epoch 381/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 67.1467 - val_loss: 69.8156\n",
      "Epoch 382/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 67.1815 - val_loss: 70.3501\n",
      "Epoch 383/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 67.6616 - val_loss: 69.9375\n",
      "Epoch 384/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 67.1988 - val_loss: 69.4312\n",
      "Epoch 385/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 67.4743 - val_loss: 70.0640\n",
      "Epoch 386/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 66.7670 - val_loss: 69.6624\n",
      "Epoch 387/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 66.7628 - val_loss: 68.9886\n",
      "Epoch 388/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 67.3633 - val_loss: 69.6704\n",
      "Epoch 389/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 67.3862 - val_loss: 70.4423\n",
      "Epoch 390/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 67.1202 - val_loss: 69.2526\n",
      "Epoch 391/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 66.2665 - val_loss: 68.8544\n",
      "Epoch 392/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 66.7643 - val_loss: 69.3491\n",
      "Epoch 393/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 66.1730 - val_loss: 69.5702\n",
      "Epoch 394/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 66.5729 - val_loss: 69.5197\n",
      "Epoch 395/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 66.2719 - val_loss: 69.3900\n",
      "Epoch 396/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 66.2908 - val_loss: 69.4066\n",
      "Epoch 397/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 66.5774 - val_loss: 69.1836\n",
      "Epoch 398/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 66.1246 - val_loss: 68.8728\n",
      "Epoch 399/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 65.6670 - val_loss: 69.1733\n",
      "Epoch 400/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 66.0390\n",
      "Epoch 00400: saving model to saved_models/latent2/cp-0400.h5\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 66.0390 - val_loss: 69.1146\n",
      "Epoch 401/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 66.3032 - val_loss: 68.6235\n",
      "Epoch 402/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 65.9115 - val_loss: 68.3481\n",
      "Epoch 403/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 66.5102 - val_loss: 69.7924\n",
      "Epoch 404/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 65.7631 - val_loss: 69.2485\n",
      "Epoch 405/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 66.3047 - val_loss: 68.7488\n",
      "Epoch 406/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 66.4813 - val_loss: 69.1562\n",
      "Epoch 407/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 65.9035 - val_loss: 69.4605\n",
      "Epoch 408/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 65.5010 - val_loss: 68.3172\n",
      "Epoch 409/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 66.2980 - val_loss: 69.1872\n",
      "Epoch 410/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 66.4765 - val_loss: 68.7749\n",
      "Epoch 411/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 66.0429 - val_loss: 69.7739\n",
      "Epoch 412/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 66.2977 - val_loss: 68.3020\n",
      "Epoch 413/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 65.3245 - val_loss: 68.8140\n",
      "Epoch 414/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 65.0272 - val_loss: 68.6276\n",
      "Epoch 415/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 65.1720 - val_loss: 68.2210\n",
      "Epoch 416/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 65.0391 - val_loss: 68.4451\n",
      "Epoch 417/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 64.5566 - val_loss: 68.2939\n",
      "Epoch 418/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 64.5689 - val_loss: 68.0060\n",
      "Epoch 419/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 65.3651 - val_loss: 67.9385\n",
      "Epoch 420/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 65.3376 - val_loss: 67.6625\n",
      "Epoch 421/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 64.9930 - val_loss: 68.9321\n",
      "Epoch 422/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 65.0636 - val_loss: 67.6048\n",
      "Epoch 423/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 65.0345 - val_loss: 68.5625\n",
      "Epoch 424/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 64.4209 - val_loss: 67.7645\n",
      "Epoch 425/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 64.7872 - val_loss: 67.6123\n",
      "Epoch 426/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 64.5535 - val_loss: 67.6572\n",
      "Epoch 427/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 65.1269 - val_loss: 69.2844\n",
      "Epoch 428/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 64.8656 - val_loss: 67.9208\n",
      "Epoch 429/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 64.6687 - val_loss: 67.2893\n",
      "Epoch 430/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 64.1200 - val_loss: 69.0098\n",
      "Epoch 431/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 64.4183 - val_loss: 68.6157\n",
      "Epoch 432/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 64.3158 - val_loss: 67.2135\n",
      "Epoch 433/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 64.6482 - val_loss: 67.8730\n",
      "Epoch 434/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 64.0499 - val_loss: 67.8075\n",
      "Epoch 435/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 64.0645 - val_loss: 67.5016\n",
      "Epoch 436/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 63.8245 - val_loss: 68.2297\n",
      "Epoch 437/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 64.4966 - val_loss: 68.0264\n",
      "Epoch 438/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 64.6488 - val_loss: 67.3961\n",
      "Epoch 439/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 64.2943 - val_loss: 68.4479\n",
      "Epoch 440/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 63.8400 - val_loss: 67.8286\n",
      "Epoch 441/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 64.0052 - val_loss: 67.2564\n",
      "Epoch 442/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 64.5173 - val_loss: 67.8086\n",
      "Epoch 443/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 64.6415 - val_loss: 67.7392\n",
      "Epoch 444/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 64.0343 - val_loss: 67.1431\n",
      "Epoch 445/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 63.5271 - val_loss: 66.6925\n",
      "Epoch 446/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 64.7474 - val_loss: 67.4932\n",
      "Epoch 447/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 63.6504 - val_loss: 67.0750\n",
      "Epoch 448/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 63.8826 - val_loss: 67.7805\n",
      "Epoch 449/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 63.8769 - val_loss: 67.3585\n",
      "Epoch 450/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 63.1256 - val_loss: 66.6031\n",
      "Epoch 451/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 63.3772 - val_loss: 67.1945\n",
      "Epoch 452/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 63.2032 - val_loss: 67.2729\n",
      "Epoch 453/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 63.6640 - val_loss: 68.3301\n",
      "Epoch 454/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 63.6802 - val_loss: 67.5310\n",
      "Epoch 455/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 64.1430 - val_loss: 67.3825\n",
      "Epoch 456/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 63.3750 - val_loss: 70.7716\n",
      "Epoch 457/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 65.3967 - val_loss: 68.0337\n",
      "Epoch 458/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 63.7195 - val_loss: 68.7704\n",
      "Epoch 459/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 63.7522 - val_loss: 67.4114\n",
      "Epoch 460/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 62.9434 - val_loss: 67.3438\n",
      "Epoch 461/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.5073 - val_loss: 66.8090\n",
      "Epoch 462/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 63.0069 - val_loss: 66.5386\n",
      "Epoch 463/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 62.8810 - val_loss: 66.6636\n",
      "Epoch 464/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.4776 - val_loss: 67.0887\n",
      "Epoch 465/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.7228 - val_loss: 65.9329\n",
      "Epoch 466/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 63.0930 - val_loss: 66.2370\n",
      "Epoch 467/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.4684 - val_loss: 66.2596\n",
      "Epoch 468/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.2359 - val_loss: 66.1754\n",
      "Epoch 469/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.1015 - val_loss: 66.4451\n",
      "Epoch 470/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 62.2663 - val_loss: 67.3102\n",
      "Epoch 471/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 62.1664 - val_loss: 66.6468\n",
      "Epoch 472/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 61.7710 - val_loss: 65.7749\n",
      "Epoch 473/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 61.6558 - val_loss: 65.9787\n",
      "Epoch 474/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 62.0451 - val_loss: 68.0430\n",
      "Epoch 475/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.8574 - val_loss: 66.6361\n",
      "Epoch 476/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 62.2462 - val_loss: 66.2604\n",
      "Epoch 477/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 62.8533 - val_loss: 66.3648\n",
      "Epoch 478/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 61.9849 - val_loss: 65.8514\n",
      "Epoch 479/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 62.2972 - val_loss: 67.3930\n",
      "Epoch 480/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.3038 - val_loss: 66.2417\n",
      "Epoch 481/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 61.7898 - val_loss: 65.9197\n",
      "Epoch 482/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 61.5314 - val_loss: 66.8819\n",
      "Epoch 483/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 61.8490 - val_loss: 66.2311\n",
      "Epoch 484/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 61.3577 - val_loss: 65.1729\n",
      "Epoch 485/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 60.9994 - val_loss: 65.6747\n",
      "Epoch 486/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 60.6915 - val_loss: 66.0009\n",
      "Epoch 487/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 60.9027 - val_loss: 65.2639\n",
      "Epoch 488/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 60.9212 - val_loss: 65.7905\n",
      "Epoch 489/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 60.8849 - val_loss: 65.8374\n",
      "Epoch 490/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 60.4674 - val_loss: 64.8629\n",
      "Epoch 491/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 61.1582 - val_loss: 66.1121\n",
      "Epoch 492/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 60.9767 - val_loss: 64.8504\n",
      "Epoch 493/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 60.5506 - val_loss: 65.1199\n",
      "Epoch 494/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 61.3320 - val_loss: 67.6353\n",
      "Epoch 495/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 61.2974 - val_loss: 65.7885\n",
      "Epoch 496/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 60.9313 - val_loss: 66.5496\n",
      "Epoch 497/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 60.4348 - val_loss: 64.7043\n",
      "Epoch 498/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 60.4026 - val_loss: 64.3722\n",
      "Epoch 499/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 59.6968 - val_loss: 64.9006\n",
      "Epoch 500/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 60.0506 - val_loss: 66.1009\n",
      "Epoch 501/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 59.9596 - val_loss: 66.0406\n",
      "Epoch 502/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 60.3566 - val_loss: 64.6511\n",
      "Epoch 503/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 60.1701 - val_loss: 64.5963\n",
      "Epoch 504/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 59.7462 - val_loss: 64.5037\n",
      "Epoch 505/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 60.7085 - val_loss: 65.7430\n",
      "Epoch 506/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 59.8651 - val_loss: 65.3991\n",
      "Epoch 507/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 60.2402 - val_loss: 64.3905\n",
      "Epoch 508/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 60.1568 - val_loss: 65.5703\n",
      "Epoch 509/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 59.5882 - val_loss: 64.8292\n",
      "Epoch 510/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 59.9259 - val_loss: 65.7304\n",
      "Epoch 511/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 60.5166 - val_loss: 65.3064\n",
      "Epoch 512/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 60.4087 - val_loss: 66.2562\n",
      "Epoch 513/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 59.1479 - val_loss: 64.3611\n",
      "Epoch 514/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 59.9489 - val_loss: 64.9365\n",
      "Epoch 515/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 58.7671 - val_loss: 64.8554\n",
      "Epoch 516/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 59.4719 - val_loss: 64.1580\n",
      "Epoch 517/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 58.9467 - val_loss: 64.9922\n",
      "Epoch 518/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 58.6957 - val_loss: 65.5040\n",
      "Epoch 519/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 59.1123 - val_loss: 63.5766\n",
      "Epoch 520/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 59.1663 - val_loss: 65.7799\n",
      "Epoch 521/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 59.5308 - val_loss: 65.5267\n",
      "Epoch 522/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 58.9160 - val_loss: 64.3123\n",
      "Epoch 523/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 58.4086 - val_loss: 63.6176\n",
      "Epoch 524/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 58.8170 - val_loss: 64.4708\n",
      "Epoch 525/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 58.7038 - val_loss: 64.7583\n",
      "Epoch 526/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 58.6993 - val_loss: 64.5282\n",
      "Epoch 527/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 58.6194 - val_loss: 64.2026\n",
      "Epoch 528/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 58.8679 - val_loss: 65.0427\n",
      "Epoch 529/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 58.7432 - val_loss: 65.0828\n",
      "Epoch 530/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 58.4995 - val_loss: 64.4866\n",
      "Epoch 531/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 58.4845 - val_loss: 63.5024\n",
      "Epoch 532/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 57.8709 - val_loss: 63.9441\n",
      "Epoch 533/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 58.0715 - val_loss: 64.9782\n",
      "Epoch 534/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 58.1721 - val_loss: 63.8886\n",
      "Epoch 535/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.9938 - val_loss: 64.8077\n",
      "Epoch 536/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.9651 - val_loss: 63.2079\n",
      "Epoch 537/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 57.8584 - val_loss: 63.7690\n",
      "Epoch 538/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.9299 - val_loss: 64.3728\n",
      "Epoch 539/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 58.1103 - val_loss: 63.8990\n",
      "Epoch 540/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 57.5581 - val_loss: 64.0798\n",
      "Epoch 541/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 57.2384 - val_loss: 65.1592\n",
      "Epoch 542/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.3066 - val_loss: 63.7152\n",
      "Epoch 543/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.5667 - val_loss: 65.7012\n",
      "Epoch 544/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.9396 - val_loss: 63.3766\n",
      "Epoch 545/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.6353 - val_loss: 63.8986\n",
      "Epoch 546/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 57.0718 - val_loss: 63.6057\n",
      "Epoch 547/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 56.9414 - val_loss: 62.5449\n",
      "Epoch 548/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.9542 - val_loss: 64.0692\n",
      "Epoch 549/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.8332 - val_loss: 63.6364\n",
      "Epoch 550/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.0826 - val_loss: 64.4254\n",
      "Epoch 551/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.3736 - val_loss: 63.0321\n",
      "Epoch 552/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.3702 - val_loss: 63.1858\n",
      "Epoch 553/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.4332 - val_loss: 65.6895\n",
      "Epoch 554/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.3256 - val_loss: 63.6533\n",
      "Epoch 555/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.4443 - val_loss: 65.2801\n",
      "Epoch 556/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 57.3462 - val_loss: 65.6196\n",
      "Epoch 557/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.2471 - val_loss: 63.4935\n",
      "Epoch 558/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 56.5513 - val_loss: 62.9632\n",
      "Epoch 559/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 56.9953 - val_loss: 64.2589\n",
      "Epoch 560/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.0219 - val_loss: 63.6131\n",
      "Epoch 561/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.1844 - val_loss: 64.1105\n",
      "Epoch 562/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 56.5814 - val_loss: 62.3264\n",
      "Epoch 563/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 56.7255 - val_loss: 63.8823\n",
      "Epoch 564/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 56.0686 - val_loss: 64.0707\n",
      "Epoch 565/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 56.2232 - val_loss: 63.1564\n",
      "Epoch 566/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 56.1501 - val_loss: 63.0078\n",
      "Epoch 567/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 55.8862 - val_loss: 63.4237\n",
      "Epoch 568/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 56.4866 - val_loss: 62.7908\n",
      "Epoch 569/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 56.1911 - val_loss: 64.3685\n",
      "Epoch 570/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 56.5483 - val_loss: 63.2255\n",
      "Epoch 571/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 55.7301 - val_loss: 63.2918\n",
      "Epoch 572/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 56.7119 - val_loss: 63.0434\n",
      "Epoch 573/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 56.9533 - val_loss: 64.6772\n",
      "Epoch 574/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 56.6902 - val_loss: 63.7566\n",
      "Epoch 575/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 56.9684 - val_loss: 63.2619\n",
      "Epoch 576/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 56.3631 - val_loss: 63.3515\n",
      "Epoch 577/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 55.6363 - val_loss: 63.7997\n",
      "Epoch 578/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.1799 - val_loss: 61.9251\n",
      "Epoch 579/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 56.4407 - val_loss: 62.5790\n",
      "Epoch 580/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 55.6060 - val_loss: 64.1633\n",
      "Epoch 581/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 55.2808 - val_loss: 62.7427\n",
      "Epoch 582/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 55.5152 - val_loss: 62.5917\n",
      "Epoch 583/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 55.5701 - val_loss: 62.7234\n",
      "Epoch 584/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 55.4935 - val_loss: 62.3902\n",
      "Epoch 585/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 55.0949 - val_loss: 62.2260\n",
      "Epoch 586/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 55.0767 - val_loss: 63.3807\n",
      "Epoch 587/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 55.3625 - val_loss: 62.7595\n",
      "Epoch 588/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 55.6833 - val_loss: 61.9698\n",
      "Epoch 589/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 55.3380 - val_loss: 63.4676\n",
      "Epoch 590/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 55.6760 - val_loss: 62.6142\n",
      "Epoch 591/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 55.4477 - val_loss: 62.8825\n",
      "Epoch 592/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 55.9595 - val_loss: 62.1281\n",
      "Epoch 593/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 55.5076 - val_loss: 61.8866\n",
      "Epoch 594/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 55.3749 - val_loss: 63.0155\n",
      "Epoch 595/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 55.2520 - val_loss: 62.0875\n",
      "Epoch 596/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 55.4727 - val_loss: 63.4531\n",
      "Epoch 597/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.8407 - val_loss: 62.1499\n",
      "Epoch 598/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 54.7675 - val_loss: 62.5070\n",
      "Epoch 599/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.7237 - val_loss: 62.5537\n",
      "Epoch 600/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 54.2857\n",
      "Epoch 00600: saving model to saved_models/latent2/cp-0600.h5\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 54.2857 - val_loss: 62.4227\n",
      "Epoch 601/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.7411 - val_loss: 63.9174\n",
      "Epoch 602/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 55.5661 - val_loss: 62.5221\n",
      "Epoch 603/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.6202 - val_loss: 61.6917\n",
      "Epoch 604/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.4240 - val_loss: 63.1231\n",
      "Epoch 605/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.6383 - val_loss: 62.5670\n",
      "Epoch 606/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.5577 - val_loss: 62.5381\n",
      "Epoch 607/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.9171 - val_loss: 61.8063\n",
      "Epoch 608/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.5121 - val_loss: 62.4927\n",
      "Epoch 609/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.8003 - val_loss: 63.5632\n",
      "Epoch 610/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.4237 - val_loss: 62.9899\n",
      "Epoch 611/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.3480 - val_loss: 63.8678\n",
      "Epoch 612/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.7678 - val_loss: 63.7278\n",
      "Epoch 613/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.0734 - val_loss: 62.2247\n",
      "Epoch 614/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.5182 - val_loss: 62.3459\n",
      "Epoch 615/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 55.1299 - val_loss: 63.3614\n",
      "Epoch 616/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 55.0342 - val_loss: 63.4602\n",
      "Epoch 617/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 55.0361 - val_loss: 62.4916\n",
      "Epoch 618/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.8124 - val_loss: 63.4103\n",
      "Epoch 619/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 55.2510 - val_loss: 62.5119\n",
      "Epoch 620/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.7365 - val_loss: 62.5043\n",
      "Epoch 621/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.8151 - val_loss: 62.9959\n",
      "Epoch 622/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.2484 - val_loss: 61.7123\n",
      "Epoch 623/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.1935 - val_loss: 61.4392\n",
      "Epoch 624/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.3762 - val_loss: 62.9308\n",
      "Epoch 625/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.3696 - val_loss: 64.5211\n",
      "Epoch 626/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.5203 - val_loss: 64.0326\n",
      "Epoch 627/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.9474 - val_loss: 61.9307\n",
      "Epoch 628/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.1172 - val_loss: 63.7873\n",
      "Epoch 629/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.9816 - val_loss: 63.2135\n",
      "Epoch 630/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.5780 - val_loss: 63.6430\n",
      "Epoch 631/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.3458 - val_loss: 62.5487\n",
      "Epoch 632/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.9966 - val_loss: 61.6119\n",
      "Epoch 633/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.8804 - val_loss: 62.9884\n",
      "Epoch 634/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.3433 - val_loss: 63.6185\n",
      "Epoch 635/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.3275 - val_loss: 63.7303\n",
      "Epoch 636/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.9130 - val_loss: 62.7752\n",
      "Epoch 637/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.8053 - val_loss: 63.0493\n",
      "Epoch 638/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 52.9764 - val_loss: 62.4919\n",
      "Epoch 639/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.8941 - val_loss: 64.0360\n",
      "Epoch 640/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.0454 - val_loss: 62.6818\n",
      "Epoch 641/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.1221 - val_loss: 64.2106\n",
      "Epoch 642/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.7218 - val_loss: 62.0689\n",
      "Epoch 643/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.2513 - val_loss: 62.6932\n",
      "Epoch 644/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.6732 - val_loss: 63.2516\n",
      "Epoch 645/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.4084 - val_loss: 62.3878\n",
      "Epoch 646/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 52.9973 - val_loss: 62.6287\n",
      "Epoch 647/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.3741 - val_loss: 62.5843\n",
      "Epoch 648/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.5986 - val_loss: 63.2183\n",
      "Epoch 649/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.1825 - val_loss: 62.9668\n",
      "Epoch 650/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.5509 - val_loss: 63.9013\n",
      "Epoch 651/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.5375 - val_loss: 62.0043\n",
      "Epoch 652/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 52.9994 - val_loss: 63.1572\n",
      "Epoch 653/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.8258 - val_loss: 65.0492\n",
      "Epoch 654/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.5413 - val_loss: 63.6058\n",
      "Epoch 655/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.4321 - val_loss: 63.8801\n",
      "Epoch 656/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.4635 - val_loss: 62.2159\n",
      "Epoch 657/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 52.8366 - val_loss: 62.6931\n",
      "Epoch 658/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.3871 - val_loss: 62.9456\n",
      "Epoch 659/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.3038 - val_loss: 62.1392\n",
      "Epoch 660/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.1548 - val_loss: 64.4533\n",
      "Epoch 661/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.8896 - val_loss: 63.8633\n",
      "Epoch 662/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.4860 - val_loss: 63.9879\n",
      "Epoch 663/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.8098 - val_loss: 65.2761\n",
      "Epoch 664/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.7040 - val_loss: 62.0087\n",
      "Epoch 665/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.4125 - val_loss: 62.8495\n",
      "Epoch 666/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.0584 - val_loss: 62.8569\n",
      "Epoch 667/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.1224 - val_loss: 63.1447\n",
      "Epoch 668/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.5289 - val_loss: 64.8397\n",
      "Epoch 669/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.7522 - val_loss: 62.6055\n",
      "Epoch 670/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.6495 - val_loss: 62.4275\n",
      "Epoch 671/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.0162 - val_loss: 63.1828\n",
      "Epoch 672/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.1947 - val_loss: 63.3334\n",
      "Epoch 673/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.0807 - val_loss: 61.8179\n",
      "Epoch 674/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 52.1959 - val_loss: 62.3176\n",
      "Epoch 675/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 52.5642 - val_loss: 61.5702\n",
      "Epoch 676/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 52.8398 - val_loss: 62.8433\n",
      "Epoch 677/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 52.7058 - val_loss: 61.4081\n",
      "Epoch 678/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 52.8889 - val_loss: 62.1842\n",
      "Epoch 679/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.9820 - val_loss: 62.4612\n",
      "Epoch 680/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 52.4799 - val_loss: 61.9701\n",
      "Epoch 681/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.2650 - val_loss: 62.4624\n",
      "Epoch 682/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.4959 - val_loss: 64.2909\n",
      "Epoch 683/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 52.3004 - val_loss: 62.7355\n",
      "Epoch 684/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 52.2972 - val_loss: 62.1708\n",
      "Epoch 685/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.7105 - val_loss: 62.2517\n",
      "Epoch 686/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 52.8517 - val_loss: 63.7013\n",
      "Epoch 687/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.8071 - val_loss: 63.5942\n",
      "Epoch 688/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 52.3095 - val_loss: 62.7875\n",
      "Epoch 689/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 52.4918 - val_loss: 63.3606\n",
      "Epoch 690/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.6173 - val_loss: 64.5730\n",
      "Epoch 691/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.8607 - val_loss: 62.1961\n",
      "Epoch 692/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.8246 - val_loss: 62.7234\n",
      "Epoch 693/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.1295 - val_loss: 62.8539\n",
      "Epoch 694/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.9281 - val_loss: 62.7927\n",
      "Epoch 695/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.4730 - val_loss: 61.6494\n",
      "Epoch 696/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.3374 - val_loss: 60.4905\n",
      "Epoch 697/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.1322 - val_loss: 63.6218\n",
      "Epoch 698/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 52.2346 - val_loss: 61.8939\n",
      "Epoch 699/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 51.7328 - val_loss: 60.9460\n",
      "Epoch 700/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 52.2319 - val_loss: 64.9425\n",
      "Epoch 701/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.1337 - val_loss: 61.5253\n",
      "Epoch 702/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.3282 - val_loss: 62.0546\n",
      "Epoch 703/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 52.1965 - val_loss: 62.6755\n",
      "Epoch 704/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 51.8246 - val_loss: 62.2898\n",
      "Epoch 705/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 51.4855 - val_loss: 61.4204\n",
      "Epoch 706/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 51.5301 - val_loss: 62.2860\n",
      "Epoch 707/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.8392 - val_loss: 61.2979\n",
      "Epoch 708/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 51.0976 - val_loss: 62.4926\n",
      "Epoch 709/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 51.1065 - val_loss: 63.6679\n",
      "Epoch 710/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.6125 - val_loss: 61.7734\n",
      "Epoch 711/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 51.3453 - val_loss: 62.7199\n",
      "Epoch 712/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 51.5995 - val_loss: 61.9380\n",
      "Epoch 713/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.6206 - val_loss: 61.5275\n",
      "Epoch 714/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 52.2767 - val_loss: 63.8979\n",
      "Epoch 715/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 51.1284 - val_loss: 62.1003\n",
      "Epoch 716/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 51.1720 - val_loss: 63.2562\n",
      "Epoch 717/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 51.9205 - val_loss: 62.3472\n",
      "Epoch 718/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.6302 - val_loss: 62.3818\n",
      "Epoch 719/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 51.3967 - val_loss: 62.3468\n",
      "Epoch 720/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 51.1452 - val_loss: 61.7857\n",
      "Epoch 721/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 51.2557 - val_loss: 62.0301\n",
      "Epoch 722/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 51.2309 - val_loss: 62.1717\n",
      "Epoch 723/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 51.2772 - val_loss: 62.6375\n",
      "Epoch 724/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 50.5151 - val_loss: 61.6705\n",
      "Epoch 725/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 51.0792 - val_loss: 63.2642\n",
      "Epoch 726/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.9318 - val_loss: 62.9804\n",
      "Epoch 727/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 51.5121 - val_loss: 63.6530\n",
      "Epoch 728/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 51.5391 - val_loss: 63.3385\n",
      "Epoch 729/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.8876 - val_loss: 62.2753\n",
      "Epoch 730/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.5941 - val_loss: 61.8741\n",
      "Epoch 731/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.7112 - val_loss: 62.3681\n",
      "Epoch 732/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.6373 - val_loss: 61.3063\n",
      "Epoch 733/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.5866 - val_loss: 62.3233\n",
      "Epoch 734/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.7527 - val_loss: 61.8524\n",
      "Epoch 735/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 51.1983 - val_loss: 61.8672\n",
      "Epoch 736/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.8153 - val_loss: 61.6243\n",
      "Epoch 737/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.8982 - val_loss: 61.9244\n",
      "Epoch 738/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.6658 - val_loss: 62.9190\n",
      "Epoch 739/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.4763 - val_loss: 61.9511\n",
      "Epoch 740/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.3462 - val_loss: 62.0420\n",
      "Epoch 741/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.3380 - val_loss: 61.9042\n",
      "Epoch 742/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.4469 - val_loss: 62.3130\n",
      "Epoch 743/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.5056 - val_loss: 62.0200\n",
      "Epoch 744/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.3767 - val_loss: 61.9462\n",
      "Epoch 745/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.6597 - val_loss: 61.2780\n",
      "Epoch 746/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 51.7012 - val_loss: 62.0735\n",
      "Epoch 747/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 51.6796 - val_loss: 63.0646\n",
      "Epoch 748/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.7255 - val_loss: 62.3871\n",
      "Epoch 749/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.2916 - val_loss: 61.9747\n",
      "Epoch 750/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.3130 - val_loss: 61.3470\n",
      "Epoch 751/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 51.0992 - val_loss: 62.7116\n",
      "Epoch 752/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.8204 - val_loss: 62.7065\n",
      "Epoch 753/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.7751 - val_loss: 62.0170\n",
      "Epoch 754/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.3542 - val_loss: 60.3380\n",
      "Epoch 755/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.4251 - val_loss: 61.7432\n",
      "Epoch 756/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.3033 - val_loss: 62.1438\n",
      "Epoch 757/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.5415 - val_loss: 62.9196\n",
      "Epoch 758/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.2237 - val_loss: 61.7322\n",
      "Epoch 759/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.5624 - val_loss: 61.8602\n",
      "Epoch 760/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.9888 - val_loss: 62.8740\n",
      "Epoch 761/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.8897 - val_loss: 61.5903\n",
      "Epoch 762/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.7219 - val_loss: 62.8429\n",
      "Epoch 763/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.4421 - val_loss: 61.6541\n",
      "Epoch 764/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.0237 - val_loss: 62.5219\n",
      "Epoch 765/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.9514 - val_loss: 61.9526\n",
      "Epoch 766/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.4405 - val_loss: 61.7257\n",
      "Epoch 767/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.3734 - val_loss: 62.7493\n",
      "Epoch 768/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.3058 - val_loss: 61.7928\n",
      "Epoch 769/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.1337 - val_loss: 62.1156\n",
      "Epoch 770/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.7792 - val_loss: 61.9949\n",
      "Epoch 771/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.7021 - val_loss: 62.8943\n",
      "Epoch 772/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.2711 - val_loss: 61.8489\n",
      "Epoch 773/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.1143 - val_loss: 62.0920\n",
      "Epoch 774/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.5967 - val_loss: 61.8954\n",
      "Epoch 775/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.2183 - val_loss: 63.3897\n",
      "Epoch 776/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.9952 - val_loss: 62.4137\n",
      "Epoch 777/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.2431 - val_loss: 62.0307\n",
      "Epoch 778/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.0249 - val_loss: 60.3451\n",
      "Epoch 779/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.3341 - val_loss: 62.4944\n",
      "Epoch 780/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.7453 - val_loss: 62.5608\n",
      "Epoch 781/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.9113 - val_loss: 61.1286\n",
      "Epoch 782/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.7766 - val_loss: 61.7170\n",
      "Epoch 783/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.7738 - val_loss: 62.2624\n",
      "Epoch 784/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.8128 - val_loss: 60.8479\n",
      "Epoch 785/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.0167 - val_loss: 62.0126\n",
      "Epoch 786/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.2886 - val_loss: 62.6304\n",
      "Epoch 787/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.9914 - val_loss: 64.1109\n",
      "Epoch 788/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.1156 - val_loss: 62.8462\n",
      "Epoch 789/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.7304 - val_loss: 62.0659\n",
      "Epoch 790/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.8519 - val_loss: 63.4852\n",
      "Epoch 791/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.1496 - val_loss: 61.8441\n",
      "Epoch 792/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.0380 - val_loss: 62.0336\n",
      "Epoch 793/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.4726 - val_loss: 61.4470\n",
      "Epoch 794/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.4997 - val_loss: 61.8482\n",
      "Epoch 795/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.1854 - val_loss: 63.4600\n",
      "Epoch 796/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.9749 - val_loss: 62.4115\n",
      "Epoch 797/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.6797 - val_loss: 60.7316\n",
      "Epoch 798/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.4217 - val_loss: 64.1503\n",
      "Epoch 799/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.7169 - val_loss: 61.5061\n",
      "Epoch 800/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 49.6011\n",
      "Epoch 00800: saving model to saved_models/latent2/cp-0800.h5\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 49.6011 - val_loss: 61.9647\n",
      "Epoch 801/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.8675 - val_loss: 63.6664\n",
      "Epoch 802/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.3476 - val_loss: 61.9066\n",
      "Epoch 803/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.2645 - val_loss: 62.8990\n",
      "Epoch 804/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.4562 - val_loss: 63.3492\n",
      "Epoch 805/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.8128 - val_loss: 63.2442\n",
      "Epoch 806/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 49.9172 - val_loss: 63.5008\n",
      "Epoch 807/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.4904 - val_loss: 62.0242\n",
      "Epoch 808/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.8354 - val_loss: 61.2193\n",
      "Epoch 809/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.5880 - val_loss: 61.9016\n",
      "Epoch 810/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.3503 - val_loss: 62.8156\n",
      "Epoch 811/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.0864 - val_loss: 64.0457\n",
      "Epoch 812/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.4724 - val_loss: 61.9782\n",
      "Epoch 813/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.2877 - val_loss: 60.8624\n",
      "Epoch 814/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.7435 - val_loss: 62.1490\n",
      "Epoch 815/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.6544 - val_loss: 61.9052\n",
      "Epoch 816/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.5698 - val_loss: 62.6133\n",
      "Epoch 817/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.7100 - val_loss: 61.9472\n",
      "Epoch 818/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.5241 - val_loss: 63.0567\n",
      "Epoch 819/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.2273 - val_loss: 62.4430\n",
      "Epoch 820/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.3064 - val_loss: 63.6048\n",
      "Epoch 821/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.5593 - val_loss: 63.4144\n",
      "Epoch 822/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.6389 - val_loss: 61.9314\n",
      "Epoch 823/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.8503 - val_loss: 61.5542\n",
      "Epoch 824/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.9403 - val_loss: 61.3152\n",
      "Epoch 825/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.5680 - val_loss: 63.3620\n",
      "Epoch 826/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.1156 - val_loss: 62.0870\n",
      "Epoch 827/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.5267 - val_loss: 63.7827\n",
      "Epoch 828/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.0608 - val_loss: 62.1924\n",
      "Epoch 829/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.5841 - val_loss: 63.0305\n",
      "Epoch 830/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.5087 - val_loss: 61.9496\n",
      "Epoch 831/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.8436 - val_loss: 62.6343\n",
      "Epoch 832/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.2338 - val_loss: 64.1243\n",
      "Epoch 833/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.2409 - val_loss: 62.2115\n",
      "Epoch 834/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.1824 - val_loss: 61.3739\n",
      "Epoch 835/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.7895 - val_loss: 62.9784\n",
      "Epoch 836/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.5909 - val_loss: 63.3503\n",
      "Epoch 837/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.8019 - val_loss: 63.3560\n",
      "Epoch 838/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.1842 - val_loss: 62.7353\n",
      "Epoch 839/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.1507 - val_loss: 63.5231\n",
      "Epoch 840/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.0660 - val_loss: 62.0931\n",
      "Epoch 841/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.6862 - val_loss: 62.5681\n",
      "Epoch 842/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.7164 - val_loss: 62.7813\n",
      "Epoch 843/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.0176 - val_loss: 63.6822\n",
      "Epoch 844/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.8151 - val_loss: 62.9170\n",
      "Epoch 845/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.9250 - val_loss: 62.3317\n",
      "Epoch 846/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.0425 - val_loss: 62.7367\n",
      "Epoch 847/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.5662 - val_loss: 63.7142\n",
      "Epoch 848/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.7145 - val_loss: 62.7326\n",
      "Epoch 849/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.5409 - val_loss: 61.4101\n",
      "Epoch 850/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.8997 - val_loss: 62.4539\n",
      "Epoch 851/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.2955 - val_loss: 62.6118\n",
      "Epoch 852/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.2179 - val_loss: 61.2081\n",
      "Epoch 853/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.4434 - val_loss: 63.0981\n",
      "Epoch 854/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.4090 - val_loss: 63.5168\n",
      "Epoch 855/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.2536 - val_loss: 63.9129\n",
      "Epoch 856/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.3179 - val_loss: 64.8345\n",
      "Epoch 857/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.8286 - val_loss: 63.5205\n",
      "Epoch 858/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.8618 - val_loss: 61.2274\n",
      "Epoch 859/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.4352 - val_loss: 62.7733\n",
      "Epoch 860/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.5017 - val_loss: 62.7089\n",
      "Epoch 861/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.9962 - val_loss: 63.7672\n",
      "Epoch 862/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.3982 - val_loss: 64.2265\n",
      "Epoch 863/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.7950 - val_loss: 62.2424\n",
      "Epoch 864/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.7197 - val_loss: 63.6975\n",
      "Epoch 865/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.6157 - val_loss: 62.8884\n",
      "Epoch 866/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.8546 - val_loss: 62.3031\n",
      "Epoch 867/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.4661 - val_loss: 62.6273\n",
      "Epoch 868/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.5094 - val_loss: 61.1144\n",
      "Epoch 869/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.3201 - val_loss: 61.8750\n",
      "Epoch 870/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.5571 - val_loss: 62.9885\n",
      "Epoch 871/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.0968 - val_loss: 63.6183\n",
      "Epoch 872/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.5442 - val_loss: 62.5411\n",
      "Epoch 873/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.8262 - val_loss: 61.6170\n",
      "Epoch 874/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.1477 - val_loss: 62.9644\n",
      "Epoch 875/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.6438 - val_loss: 64.8842\n",
      "Epoch 876/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.7436 - val_loss: 63.0613\n",
      "Epoch 877/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.7859 - val_loss: 64.3344\n",
      "Epoch 878/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.8922 - val_loss: 62.5325\n",
      "Epoch 879/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.1345 - val_loss: 65.7501\n",
      "Epoch 880/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.6592 - val_loss: 62.7649\n",
      "Epoch 881/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.0988 - val_loss: 63.1346\n",
      "Epoch 882/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.6522 - val_loss: 63.5505\n",
      "Epoch 883/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.5042 - val_loss: 62.8243\n",
      "Epoch 884/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.5777 - val_loss: 62.5618\n",
      "Epoch 885/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.4637 - val_loss: 61.8560\n",
      "Epoch 886/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.1280 - val_loss: 64.9231\n",
      "Epoch 887/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.5137 - val_loss: 62.6343\n",
      "Epoch 888/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.2663 - val_loss: 62.4021\n",
      "Epoch 889/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.5959 - val_loss: 63.2677\n",
      "Epoch 890/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.0181 - val_loss: 62.7667\n",
      "Epoch 891/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.9273 - val_loss: 62.9376\n",
      "Epoch 892/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.6670 - val_loss: 62.3965\n",
      "Epoch 893/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.2120 - val_loss: 62.9609\n",
      "Epoch 894/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.4474 - val_loss: 62.0945\n",
      "Epoch 895/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.2186 - val_loss: 62.8621\n",
      "Epoch 896/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.3927 - val_loss: 62.3801\n",
      "Epoch 897/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.7552 - val_loss: 63.3183\n",
      "Epoch 898/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.1282 - val_loss: 64.2143\n",
      "Epoch 899/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.7933 - val_loss: 63.6095\n",
      "Epoch 900/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.1860 - val_loss: 64.8435\n",
      "Epoch 901/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.2362 - val_loss: 63.2820\n",
      "Epoch 902/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.2457 - val_loss: 62.6996\n",
      "Epoch 903/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.1552 - val_loss: 62.6999\n",
      "Epoch 904/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.5612 - val_loss: 63.8812\n",
      "Epoch 905/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.6987 - val_loss: 61.8859\n",
      "Epoch 906/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.2894 - val_loss: 62.4827\n",
      "Epoch 907/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.1166 - val_loss: 62.4948\n",
      "Epoch 908/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.2923 - val_loss: 61.3978\n",
      "Epoch 909/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.8949 - val_loss: 62.2455\n",
      "Epoch 910/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.2486 - val_loss: 61.8184\n",
      "Epoch 911/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.7170 - val_loss: 63.7052\n",
      "Epoch 912/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.9962 - val_loss: 62.8268\n",
      "Epoch 913/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.2769 - val_loss: 64.9394\n",
      "Epoch 914/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.3088 - val_loss: 63.3464\n",
      "Epoch 915/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.6314 - val_loss: 62.9810\n",
      "Epoch 916/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.1543 - val_loss: 63.7379\n",
      "Epoch 917/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.8744 - val_loss: 62.7173\n",
      "Epoch 918/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.5360 - val_loss: 62.4588\n",
      "Epoch 919/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.0435 - val_loss: 62.9480\n",
      "Epoch 920/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.9230 - val_loss: 61.8398\n",
      "Epoch 921/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.5683 - val_loss: 62.9125\n",
      "Epoch 922/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.7659 - val_loss: 61.9307\n",
      "Epoch 923/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.8001 - val_loss: 65.0454\n",
      "Epoch 924/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.2350 - val_loss: 63.1769\n",
      "Epoch 925/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.3990 - val_loss: 62.0711\n",
      "Epoch 926/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.9642 - val_loss: 65.2047\n",
      "Epoch 927/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.3128 - val_loss: 63.7462\n",
      "Epoch 928/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.3139 - val_loss: 63.6100\n",
      "Epoch 929/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.3751 - val_loss: 63.4372\n",
      "Epoch 930/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.6951 - val_loss: 63.1608\n",
      "Epoch 931/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.9023 - val_loss: 64.7768\n",
      "Epoch 932/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.3528 - val_loss: 64.4527\n",
      "Epoch 933/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.3719 - val_loss: 63.8442\n",
      "Epoch 934/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.1566 - val_loss: 63.6414\n",
      "Epoch 935/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.0333 - val_loss: 62.4724\n",
      "Epoch 936/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.7223 - val_loss: 63.3940\n",
      "Epoch 937/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.7653 - val_loss: 62.1111\n",
      "Epoch 938/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.8645 - val_loss: 62.5191\n",
      "Epoch 939/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.4685 - val_loss: 63.4620\n",
      "Epoch 940/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.3300 - val_loss: 62.3938\n",
      "Epoch 941/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.4289 - val_loss: 64.2325\n",
      "Epoch 942/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.7593 - val_loss: 64.1240\n",
      "Epoch 943/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.2864 - val_loss: 62.5539\n",
      "Epoch 944/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.9876 - val_loss: 63.2426\n",
      "Epoch 945/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.8261 - val_loss: 64.6614\n",
      "Epoch 946/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.6553 - val_loss: 64.7867\n",
      "Epoch 947/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.9744 - val_loss: 63.8136\n",
      "Epoch 948/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.5994 - val_loss: 66.1962\n",
      "Epoch 949/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.6602 - val_loss: 62.7788\n",
      "Epoch 950/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.9654 - val_loss: 62.6694\n",
      "Epoch 951/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.5314 - val_loss: 62.2064\n",
      "Epoch 952/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.7779 - val_loss: 63.4520\n",
      "Epoch 953/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.4683 - val_loss: 62.9683\n",
      "Epoch 954/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.8282 - val_loss: 63.1070\n",
      "Epoch 955/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.3001 - val_loss: 63.9686\n",
      "Epoch 956/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.8155 - val_loss: 63.2691\n",
      "Epoch 957/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.2808 - val_loss: 63.5639\n",
      "Epoch 958/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.3478 - val_loss: 63.5853\n",
      "Epoch 959/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.7608 - val_loss: 63.3835\n",
      "Epoch 960/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.9119 - val_loss: 64.7721\n",
      "Epoch 961/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.6240 - val_loss: 63.8404\n",
      "Epoch 962/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.7539 - val_loss: 62.7538\n",
      "Epoch 963/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.5831 - val_loss: 63.2455\n",
      "Epoch 964/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.4326 - val_loss: 63.5149\n",
      "Epoch 965/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.5838 - val_loss: 64.2675\n",
      "Epoch 966/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.4318 - val_loss: 63.4818\n",
      "Epoch 967/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.0394 - val_loss: 63.0294\n",
      "Epoch 968/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.8324 - val_loss: 63.9412\n",
      "Epoch 969/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.6942 - val_loss: 62.1423\n",
      "Epoch 970/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.5721 - val_loss: 62.1654\n",
      "Epoch 971/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.7866 - val_loss: 64.5778\n",
      "Epoch 972/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.8880 - val_loss: 62.5851\n",
      "Epoch 973/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.8144 - val_loss: 64.4962\n",
      "Epoch 974/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.0110 - val_loss: 62.8644\n",
      "Epoch 975/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.2942 - val_loss: 63.8754\n",
      "Epoch 976/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.1888 - val_loss: 64.3149\n",
      "Epoch 977/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.2731 - val_loss: 62.5900\n",
      "Epoch 978/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.5693 - val_loss: 64.3255\n",
      "Epoch 979/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.9268 - val_loss: 63.9582\n",
      "Epoch 980/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.1494 - val_loss: 65.0457\n",
      "Epoch 981/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.7341 - val_loss: 62.9954\n",
      "Epoch 982/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.7071 - val_loss: 63.5997\n",
      "Epoch 983/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.3096 - val_loss: 61.5110\n",
      "Epoch 984/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.3762 - val_loss: 64.7097\n",
      "Epoch 985/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.3711 - val_loss: 63.6550\n",
      "Epoch 986/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.6437 - val_loss: 62.8056\n",
      "Epoch 987/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.1511 - val_loss: 63.0756\n",
      "Epoch 988/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.6420 - val_loss: 63.1746\n",
      "Epoch 989/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.5813 - val_loss: 63.0850\n",
      "Epoch 990/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.1537 - val_loss: 63.4573\n",
      "Epoch 991/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.9838 - val_loss: 63.0173\n",
      "Epoch 992/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.1268 - val_loss: 64.5163\n",
      "Epoch 993/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.5965 - val_loss: 64.5763\n",
      "Epoch 994/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.5796 - val_loss: 64.4919\n",
      "Epoch 995/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.1014 - val_loss: 64.2138\n",
      "Epoch 996/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.4235 - val_loss: 61.7404\n",
      "Epoch 997/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.0580 - val_loss: 61.9215\n",
      "Epoch 998/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.2494 - val_loss: 64.4564\n",
      "Epoch 999/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.6194 - val_loss: 63.9539\n",
      "Epoch 1000/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 47.2338\n",
      "Epoch 01000: saving model to saved_models/latent2/cp-1000.h5\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 47.2338 - val_loss: 64.7890\n",
      "Epoch 1001/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.6865 - val_loss: 63.1561\n",
      "Epoch 1002/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.2142 - val_loss: 65.8241\n",
      "Epoch 1003/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.9182 - val_loss: 64.4161\n",
      "Epoch 1004/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.1500 - val_loss: 63.1460\n",
      "Epoch 1005/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.7592 - val_loss: 63.9105\n",
      "Epoch 1006/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.2867 - val_loss: 62.6059\n",
      "Epoch 1007/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.0858 - val_loss: 64.2227\n",
      "Epoch 1008/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.7355 - val_loss: 63.1401\n",
      "Epoch 1009/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.4980 - val_loss: 65.3371\n",
      "Epoch 1010/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.6148 - val_loss: 64.7491\n",
      "Epoch 1011/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.6549 - val_loss: 64.5293\n",
      "Epoch 1012/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.1387 - val_loss: 63.1521\n",
      "Epoch 1013/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.7247 - val_loss: 64.1098\n",
      "Epoch 1014/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.7456 - val_loss: 64.4864\n",
      "Epoch 1015/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.1095 - val_loss: 63.7265\n",
      "Epoch 1016/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.2560 - val_loss: 63.9871\n",
      "Epoch 1017/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.9631 - val_loss: 63.6391\n",
      "Epoch 1018/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.5959 - val_loss: 63.2507\n",
      "Epoch 1019/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.3078 - val_loss: 63.5344\n",
      "Epoch 1020/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.5320 - val_loss: 64.5587\n",
      "Epoch 1021/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.4272 - val_loss: 64.3576\n",
      "Epoch 1022/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.3712 - val_loss: 63.1010\n",
      "Epoch 1023/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.5197 - val_loss: 63.5746\n",
      "Epoch 1024/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.1536 - val_loss: 63.1671\n",
      "Epoch 1025/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.4328 - val_loss: 64.4161\n",
      "Epoch 1026/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.2204 - val_loss: 65.2259\n",
      "Epoch 1027/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.3124 - val_loss: 63.9234\n",
      "Epoch 1028/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.7890 - val_loss: 64.1525\n",
      "Epoch 1029/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.6278 - val_loss: 63.8765\n",
      "Epoch 1030/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.2631 - val_loss: 62.5615\n",
      "Epoch 1031/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.0537 - val_loss: 65.0354\n",
      "Epoch 1032/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.1100 - val_loss: 63.6961\n",
      "Epoch 1033/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.0223 - val_loss: 64.8354\n",
      "Epoch 1034/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.4672 - val_loss: 64.2571\n",
      "Epoch 1035/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.1916 - val_loss: 62.9233\n",
      "Epoch 1036/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.6096 - val_loss: 64.5092\n",
      "Epoch 1037/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.4799 - val_loss: 62.5055\n",
      "Epoch 1038/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.0817 - val_loss: 65.0457\n",
      "Epoch 1039/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.5690 - val_loss: 64.1155\n",
      "Epoch 1040/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.3048 - val_loss: 63.4403\n",
      "Epoch 1041/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.1904 - val_loss: 64.3679\n",
      "Epoch 1042/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.8710 - val_loss: 62.8532\n",
      "Epoch 1043/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.0839 - val_loss: 65.2841\n",
      "Epoch 1044/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.8917 - val_loss: 62.5285\n",
      "Epoch 1045/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.0275 - val_loss: 63.1453\n",
      "Epoch 1046/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.9159 - val_loss: 64.2636\n",
      "Epoch 1047/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.9973 - val_loss: 63.8481\n",
      "Epoch 1048/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.2311 - val_loss: 64.3289\n",
      "Epoch 1049/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.2028 - val_loss: 62.6751\n",
      "Epoch 1050/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.4203 - val_loss: 66.0660\n",
      "Epoch 1051/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.0721 - val_loss: 65.4936\n",
      "Epoch 1052/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.1987 - val_loss: 64.2891\n",
      "Epoch 1053/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.8363 - val_loss: 64.1122\n",
      "Epoch 1054/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.1867 - val_loss: 64.8621\n",
      "Epoch 1055/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.2618 - val_loss: 63.4007\n",
      "Epoch 1056/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.3724 - val_loss: 63.3452\n",
      "Epoch 1057/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.8748 - val_loss: 62.9246\n",
      "Epoch 1058/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.1461 - val_loss: 63.9086\n",
      "Epoch 1059/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.8446 - val_loss: 64.7917\n",
      "Epoch 1060/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.9348 - val_loss: 62.4845\n",
      "Epoch 1061/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.7979 - val_loss: 63.7283\n",
      "Epoch 1062/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.8789 - val_loss: 65.3196\n",
      "Epoch 1063/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.2830 - val_loss: 64.1392\n",
      "Epoch 1064/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.8693 - val_loss: 64.4587\n",
      "Epoch 1065/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.8348 - val_loss: 64.2524\n",
      "Epoch 1066/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.5428 - val_loss: 65.3360\n",
      "Epoch 1067/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.4987 - val_loss: 64.1029\n",
      "Epoch 1068/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.5138 - val_loss: 63.5179\n",
      "Epoch 1069/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.2493 - val_loss: 61.8984\n",
      "Epoch 1070/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.8328 - val_loss: 65.1168\n",
      "Epoch 1071/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.9430 - val_loss: 62.4773\n",
      "Epoch 1072/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.6385 - val_loss: 64.8699\n",
      "Epoch 1073/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.5662 - val_loss: 63.5562\n",
      "Epoch 1074/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.1783 - val_loss: 66.9211\n",
      "Epoch 1075/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.8859 - val_loss: 64.9986\n",
      "Epoch 1076/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.4801 - val_loss: 64.6782\n",
      "Epoch 1077/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.9930 - val_loss: 64.2136\n",
      "Epoch 1078/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.7811 - val_loss: 62.9821\n",
      "Epoch 1079/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.1780 - val_loss: 64.1555\n",
      "Epoch 1080/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.9419 - val_loss: 64.4832\n",
      "Epoch 1081/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.5748 - val_loss: 64.7803\n",
      "Epoch 1082/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.5833 - val_loss: 63.6769\n",
      "Epoch 1083/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.8146 - val_loss: 64.2553\n",
      "Epoch 1084/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.5024 - val_loss: 63.1457\n",
      "Epoch 1085/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.8116 - val_loss: 63.9224\n",
      "Epoch 1086/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.8728 - val_loss: 63.7638\n",
      "Epoch 1087/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.4561 - val_loss: 62.8026\n",
      "Epoch 1088/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.7131 - val_loss: 62.9030\n",
      "Epoch 1089/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.2624 - val_loss: 62.5757\n",
      "Epoch 1090/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 46.1188 - val_loss: 64.4276\n",
      "Epoch 1091/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.6817 - val_loss: 64.5888\n",
      "Epoch 1092/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.4071 - val_loss: 63.8277\n",
      "Epoch 1093/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.0167 - val_loss: 66.9314\n",
      "Epoch 1094/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.4927 - val_loss: 64.6443\n",
      "Epoch 1095/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.3939 - val_loss: 64.2498\n",
      "Epoch 1096/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.2794 - val_loss: 64.1122\n",
      "Epoch 1097/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.9908 - val_loss: 63.6678\n",
      "Epoch 1098/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.8728 - val_loss: 64.3779\n",
      "Epoch 1099/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.0566 - val_loss: 64.7448\n",
      "Epoch 1100/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.9265 - val_loss: 65.9411\n",
      "Epoch 1101/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.3957 - val_loss: 64.5095\n",
      "Epoch 1102/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.6231 - val_loss: 62.7382\n",
      "Epoch 1103/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.0767 - val_loss: 61.3675\n",
      "Epoch 1104/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.5295 - val_loss: 69.8713\n",
      "Epoch 1105/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.0771 - val_loss: 67.3164\n",
      "Epoch 1106/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.8000 - val_loss: 65.8333\n",
      "Epoch 1107/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.9452 - val_loss: 62.7077\n",
      "Epoch 1108/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.4115 - val_loss: 64.9474\n",
      "Epoch 1109/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.6767 - val_loss: 63.6590\n",
      "Epoch 1110/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.3751 - val_loss: 63.8280\n",
      "Epoch 1111/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.7075 - val_loss: 65.0989\n",
      "Epoch 1112/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.2667 - val_loss: 62.5728\n",
      "Epoch 1113/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.4253 - val_loss: 63.4066\n",
      "Epoch 1114/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 45.9741 - val_loss: 64.0871\n",
      "Epoch 1115/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.3702 - val_loss: 64.0536\n",
      "Epoch 1116/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.1028 - val_loss: 64.0722\n",
      "Epoch 1117/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.7414 - val_loss: 65.4337\n",
      "Epoch 1118/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.5855 - val_loss: 64.9424\n",
      "Epoch 1119/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.4032 - val_loss: 63.4639\n",
      "Epoch 1120/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.4911 - val_loss: 64.3585\n",
      "Epoch 1121/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.1352 - val_loss: 64.6546\n",
      "Epoch 1122/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.3602 - val_loss: 63.6919\n",
      "Epoch 1123/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.1258 - val_loss: 63.3075\n",
      "Epoch 1124/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 45.6126 - val_loss: 62.7794\n",
      "Epoch 1125/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.4093 - val_loss: 64.3882\n",
      "Epoch 1126/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.1506 - val_loss: 65.7493\n",
      "Epoch 1127/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.9858 - val_loss: 63.4154\n",
      "Epoch 1128/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.1560 - val_loss: 65.2442\n",
      "Epoch 1129/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.2238 - val_loss: 64.4738\n",
      "Epoch 1130/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.8970 - val_loss: 64.3115\n",
      "Epoch 1131/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.0102 - val_loss: 64.0095\n",
      "Epoch 1132/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.0151 - val_loss: 64.0364\n",
      "Epoch 1133/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.3495 - val_loss: 64.0395\n",
      "Epoch 1134/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.4333 - val_loss: 63.8149\n",
      "Epoch 1135/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.9399 - val_loss: 63.9989\n",
      "Epoch 1136/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.1192 - val_loss: 65.3072\n",
      "Epoch 1137/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.4142 - val_loss: 63.7009\n",
      "Epoch 1138/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.7071 - val_loss: 64.1768\n",
      "Epoch 1139/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.2351 - val_loss: 64.7783\n",
      "Epoch 1140/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.1091 - val_loss: 64.9517\n",
      "Epoch 1141/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.2442 - val_loss: 63.8174\n",
      "Epoch 1142/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.5203 - val_loss: 63.7892\n",
      "Epoch 1143/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.3785 - val_loss: 64.6094\n",
      "Epoch 1144/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.3905 - val_loss: 63.8238\n",
      "Epoch 1145/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.9055 - val_loss: 65.1776\n",
      "Epoch 1146/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.6685 - val_loss: 64.2775\n",
      "Epoch 1147/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.0436 - val_loss: 64.2995\n",
      "Epoch 1148/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.0557 - val_loss: 64.5663\n",
      "Epoch 1149/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.7654 - val_loss: 63.8737\n",
      "Epoch 1150/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.5836 - val_loss: 65.0848\n",
      "Epoch 1151/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.4184 - val_loss: 64.0247\n",
      "Epoch 1152/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.4774 - val_loss: 64.7818\n",
      "Epoch 1153/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.0067 - val_loss: 65.2715\n",
      "Epoch 1154/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.6713 - val_loss: 63.3670\n",
      "Epoch 1155/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.3188 - val_loss: 63.5016\n",
      "Epoch 1156/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.0936 - val_loss: 65.0319\n",
      "Epoch 1157/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.3802 - val_loss: 63.9980\n",
      "Epoch 1158/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.1062 - val_loss: 66.2884\n",
      "Epoch 1159/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.3260 - val_loss: 63.2999\n",
      "Epoch 1160/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.6851 - val_loss: 64.4083\n",
      "Epoch 1161/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.3118 - val_loss: 64.4717\n",
      "Epoch 1162/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.9507 - val_loss: 64.4372\n",
      "Epoch 1163/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.9512 - val_loss: 66.8354\n",
      "Epoch 1164/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.3055 - val_loss: 63.8403\n",
      "Epoch 1165/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.6744 - val_loss: 63.5284\n",
      "Epoch 1166/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.6592 - val_loss: 63.4256\n",
      "Epoch 1167/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.6809 - val_loss: 64.0012\n",
      "Epoch 1168/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.0353 - val_loss: 63.9719\n",
      "Epoch 1169/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.9361 - val_loss: 64.3371\n",
      "Epoch 1170/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 45.5280 - val_loss: 64.7212\n",
      "Epoch 1171/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.8453 - val_loss: 65.0530\n",
      "Epoch 1172/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.0672 - val_loss: 63.7684\n",
      "Epoch 1173/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.7346 - val_loss: 64.3379\n",
      "Epoch 1174/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.2868 - val_loss: 64.5492\n",
      "Epoch 1175/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.9228 - val_loss: 64.9575\n",
      "Epoch 1176/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.3253 - val_loss: 66.0902\n",
      "Epoch 1177/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.9539 - val_loss: 63.1049\n",
      "Epoch 1178/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.9317 - val_loss: 64.2415\n",
      "Epoch 1179/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.9684 - val_loss: 64.3514\n",
      "Epoch 1180/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.8859 - val_loss: 64.1377\n",
      "Epoch 1181/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.9213 - val_loss: 63.4390\n",
      "Epoch 1182/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.6368 - val_loss: 65.7575\n",
      "Epoch 1183/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.6407 - val_loss: 64.2199\n",
      "Epoch 1184/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.9034 - val_loss: 65.4465\n",
      "Epoch 1185/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.5729 - val_loss: 65.1944\n",
      "Epoch 1186/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.2324 - val_loss: 65.3985\n",
      "Epoch 1187/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.1912 - val_loss: 64.4754\n",
      "Epoch 1188/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.1095 - val_loss: 64.3766\n",
      "Epoch 1189/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.1484 - val_loss: 63.6204\n",
      "Epoch 1190/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.4995 - val_loss: 64.5734\n",
      "Epoch 1191/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.0436 - val_loss: 65.5712\n",
      "Epoch 1192/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.9434 - val_loss: 64.0138\n",
      "Epoch 1193/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.7521 - val_loss: 64.6865\n",
      "Epoch 1194/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.7366 - val_loss: 64.2063\n",
      "Epoch 1195/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 45.3405 - val_loss: 65.3127\n",
      "Epoch 1196/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.5442 - val_loss: 65.1693\n",
      "Epoch 1197/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.0085 - val_loss: 64.4315\n",
      "Epoch 1198/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.7723 - val_loss: 65.8679\n",
      "Epoch 1199/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.8204 - val_loss: 65.7682\n",
      "Epoch 1200/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 46.3135\n",
      "Epoch 01200: saving model to saved_models/latent2/cp-1200.h5\n",
      "7/7 [==============================] - 1s 152ms/step - loss: 46.3135 - val_loss: 62.8749\n",
      "Epoch 1201/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.9762 - val_loss: 64.3750\n",
      "Epoch 1202/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.0009 - val_loss: 63.6154\n",
      "Epoch 1203/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.4879 - val_loss: 64.3878\n",
      "Epoch 1204/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.9693 - val_loss: 62.9057\n",
      "Epoch 1205/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.8919 - val_loss: 64.1956\n",
      "Epoch 1206/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.1232 - val_loss: 64.2790\n",
      "Epoch 1207/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.0728 - val_loss: 64.2609\n",
      "Epoch 1208/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.9794 - val_loss: 64.4174\n",
      "Epoch 1209/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.8503 - val_loss: 63.5882\n",
      "Epoch 1210/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.9399 - val_loss: 64.7759\n",
      "Epoch 1211/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.6181 - val_loss: 65.6748\n",
      "Epoch 1212/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.0624 - val_loss: 65.9012\n",
      "Epoch 1213/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.9517 - val_loss: 64.4257\n",
      "Epoch 1214/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.6896 - val_loss: 64.0705\n",
      "Epoch 1215/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.0171 - val_loss: 64.0618\n",
      "Epoch 1216/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.5839 - val_loss: 65.9002\n",
      "Epoch 1217/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.4191 - val_loss: 63.9718\n",
      "Epoch 1218/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.1748 - val_loss: 63.8378\n",
      "Epoch 1219/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.4045 - val_loss: 66.1990\n",
      "Epoch 1220/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.2219 - val_loss: 66.3036\n",
      "Epoch 1221/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.6114 - val_loss: 63.3568\n",
      "Epoch 1222/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.7097 - val_loss: 64.4710\n",
      "Epoch 1223/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.8310 - val_loss: 65.1161\n",
      "Epoch 1224/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.6429 - val_loss: 65.7126\n",
      "Epoch 1225/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.1733 - val_loss: 65.8810\n",
      "Epoch 1226/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.3683 - val_loss: 65.0480\n",
      "Epoch 1227/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.1723 - val_loss: 65.2953\n",
      "Epoch 1228/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.7692 - val_loss: 63.6108\n",
      "Epoch 1229/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.1792 - val_loss: 66.2403\n",
      "Epoch 1230/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.3642 - val_loss: 64.8472\n",
      "Epoch 1231/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.2340 - val_loss: 63.3632\n",
      "Epoch 1232/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.1695 - val_loss: 64.3605\n",
      "Epoch 1233/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.9787 - val_loss: 65.4471\n",
      "Epoch 1234/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.1084 - val_loss: 63.2379\n",
      "Epoch 1235/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.0094 - val_loss: 65.4461\n",
      "Epoch 1236/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.5425 - val_loss: 64.8262\n",
      "Epoch 1237/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.6516 - val_loss: 64.9745\n",
      "Epoch 1238/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.6248 - val_loss: 63.9837\n",
      "Epoch 1239/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.8169 - val_loss: 64.5772\n",
      "Epoch 1240/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.7081 - val_loss: 64.5984\n",
      "Epoch 1241/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.3243 - val_loss: 64.7980\n",
      "Epoch 1242/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.7832 - val_loss: 65.1427\n",
      "Epoch 1243/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.5973 - val_loss: 63.4746\n",
      "Epoch 1244/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.9851 - val_loss: 64.6616\n",
      "Epoch 1245/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.2130 - val_loss: 65.6129\n",
      "Epoch 1246/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.0337 - val_loss: 65.4025\n",
      "Epoch 1247/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.2201 - val_loss: 65.9088\n",
      "Epoch 1248/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.0572 - val_loss: 64.2379\n",
      "Epoch 1249/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.0797 - val_loss: 66.0387\n",
      "Epoch 1250/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.7766 - val_loss: 64.8597\n",
      "Epoch 1251/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.9765 - val_loss: 65.5374\n",
      "Epoch 1252/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.7265 - val_loss: 64.5946\n",
      "Epoch 1253/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.2038 - val_loss: 64.5591\n",
      "Epoch 1254/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.3796 - val_loss: 63.7210\n",
      "Epoch 1255/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.3949 - val_loss: 63.7314\n",
      "Epoch 1256/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.0694 - val_loss: 66.5046\n",
      "Epoch 1257/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 45.0089 - val_loss: 64.5534\n",
      "Epoch 1258/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.3223 - val_loss: 64.0236\n",
      "Epoch 1259/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.3997 - val_loss: 65.3350\n",
      "Epoch 1260/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.9814 - val_loss: 63.4419\n",
      "Epoch 1261/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.3047 - val_loss: 65.7118\n",
      "Epoch 1262/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.2675 - val_loss: 63.9559\n",
      "Epoch 1263/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.4753 - val_loss: 64.0027\n",
      "Epoch 1264/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.9376 - val_loss: 64.8605\n",
      "Epoch 1265/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.1350 - val_loss: 64.9651\n",
      "Epoch 1266/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.5654 - val_loss: 64.4417\n",
      "Epoch 1267/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.3805 - val_loss: 64.6922\n",
      "Epoch 1268/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.6758 - val_loss: 64.7125\n",
      "Epoch 1269/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.2031 - val_loss: 65.1575\n",
      "Epoch 1270/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.0904 - val_loss: 63.1351\n",
      "Epoch 1271/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.9518 - val_loss: 64.9553\n",
      "Epoch 1272/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.5255 - val_loss: 65.5953\n",
      "Epoch 1273/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.2052 - val_loss: 66.2622\n",
      "Epoch 1274/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.3388 - val_loss: 63.7111\n",
      "Epoch 1275/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.7467 - val_loss: 65.0407\n",
      "Epoch 1276/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.7174 - val_loss: 65.0340\n",
      "Epoch 1277/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.2497 - val_loss: 65.1028\n",
      "Epoch 1278/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.8918 - val_loss: 63.7068\n",
      "Epoch 1279/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.4360 - val_loss: 65.4338\n",
      "Epoch 1280/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.2822 - val_loss: 64.6380\n",
      "Epoch 1281/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.3762 - val_loss: 64.2091\n",
      "Epoch 1282/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.3625 - val_loss: 65.1642\n",
      "Epoch 1283/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.3112 - val_loss: 64.0016\n",
      "Epoch 1284/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.2515 - val_loss: 65.4468\n",
      "Epoch 1285/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.6382 - val_loss: 66.2399\n",
      "Epoch 1286/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.0079 - val_loss: 64.4641\n",
      "Epoch 1287/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.6428 - val_loss: 64.9625\n",
      "Epoch 1288/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.2442 - val_loss: 63.1205\n",
      "Epoch 1289/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.1403 - val_loss: 65.7800\n",
      "Epoch 1290/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.7397 - val_loss: 65.5738\n",
      "Epoch 1291/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.9371 - val_loss: 65.3968\n",
      "Epoch 1292/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.4739 - val_loss: 64.7160\n",
      "Epoch 1293/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.1801 - val_loss: 65.9667\n",
      "Epoch 1294/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.6646 - val_loss: 64.2968\n",
      "Epoch 1295/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.6889 - val_loss: 67.2928\n",
      "Epoch 1296/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.2637 - val_loss: 64.1623\n",
      "Epoch 1297/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.8114 - val_loss: 64.4701\n",
      "Epoch 1298/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.9879 - val_loss: 63.8297\n",
      "Epoch 1299/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.5876 - val_loss: 65.0490\n",
      "Epoch 1300/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.2638 - val_loss: 64.8878\n",
      "Epoch 1301/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.7451 - val_loss: 63.5941\n",
      "Epoch 1302/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.8587 - val_loss: 64.7752\n",
      "Epoch 1303/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.7124 - val_loss: 64.4149\n",
      "Epoch 1304/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.4123 - val_loss: 65.3812\n",
      "Epoch 1305/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.0944 - val_loss: 64.4709\n",
      "Epoch 1306/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.9449 - val_loss: 65.5488\n",
      "Epoch 1307/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.3815 - val_loss: 65.7456\n",
      "Epoch 1308/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.0690 - val_loss: 65.1802\n",
      "Epoch 1309/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.3682 - val_loss: 65.2996\n",
      "Epoch 1310/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.7510 - val_loss: 65.2066\n",
      "Epoch 1311/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.0712 - val_loss: 65.1075\n",
      "Epoch 1312/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.5431 - val_loss: 65.1448\n",
      "Epoch 1313/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.6597 - val_loss: 64.9007\n",
      "Epoch 1314/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.6713 - val_loss: 66.0756\n",
      "Epoch 1315/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.9585 - val_loss: 68.2557\n",
      "Epoch 1316/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.9530 - val_loss: 66.8032\n",
      "Epoch 1317/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.3860 - val_loss: 65.5589\n",
      "Epoch 1318/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.7161 - val_loss: 65.0100\n",
      "Epoch 1319/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.3106 - val_loss: 64.8856\n",
      "Epoch 1320/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.7131 - val_loss: 64.5536\n",
      "Epoch 1321/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.4936 - val_loss: 65.1960\n",
      "Epoch 1322/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.9859 - val_loss: 64.1851\n",
      "Epoch 1323/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 44.8545 - val_loss: 64.8233\n",
      "Epoch 1324/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.4782 - val_loss: 65.6488\n",
      "Epoch 1325/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.4996 - val_loss: 64.0886\n",
      "Epoch 1326/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.2405 - val_loss: 66.3199\n",
      "Epoch 1327/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.3799 - val_loss: 65.6233\n",
      "Epoch 1328/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.7674 - val_loss: 65.7645\n",
      "Epoch 1329/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.1828 - val_loss: 65.2479\n",
      "Epoch 1330/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.7199 - val_loss: 65.0294\n",
      "Epoch 1331/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.3610 - val_loss: 66.2409\n",
      "Epoch 1332/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.0090 - val_loss: 63.5790\n",
      "Epoch 1333/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.5524 - val_loss: 64.8641\n",
      "Epoch 1334/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.1456 - val_loss: 65.2157\n",
      "Epoch 1335/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.8630 - val_loss: 64.7462\n",
      "Epoch 1336/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.0740 - val_loss: 65.4269\n",
      "Epoch 1337/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.1912 - val_loss: 65.8404\n",
      "Epoch 1338/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.8963 - val_loss: 65.4722\n",
      "Epoch 1339/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.3404 - val_loss: 66.5835\n",
      "Epoch 1340/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.2514 - val_loss: 65.3873\n",
      "Epoch 1341/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.3237 - val_loss: 65.5800\n",
      "Epoch 1342/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.1195 - val_loss: 65.6447\n",
      "Epoch 1343/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.9767 - val_loss: 64.2510\n",
      "Epoch 1344/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.9889 - val_loss: 64.3618\n",
      "Epoch 1345/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.0660 - val_loss: 65.3396\n",
      "Epoch 1346/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.0270 - val_loss: 64.7206\n",
      "Epoch 1347/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.8551 - val_loss: 65.3903\n",
      "Epoch 1348/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.3016 - val_loss: 63.5986\n",
      "Epoch 1349/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.5134 - val_loss: 65.1502\n",
      "Epoch 1350/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.5290 - val_loss: 66.0582\n",
      "Epoch 1351/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.0846 - val_loss: 64.3044\n",
      "Epoch 1352/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.3912 - val_loss: 67.5319\n",
      "Epoch 1353/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.2730 - val_loss: 66.8617\n",
      "Epoch 1354/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.4982 - val_loss: 65.3620\n",
      "Epoch 1355/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.1354 - val_loss: 64.4121\n",
      "Epoch 1356/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.9483 - val_loss: 64.8659\n",
      "Epoch 1357/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.8583 - val_loss: 64.9580\n",
      "Epoch 1358/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.7756 - val_loss: 64.8046\n",
      "Epoch 1359/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.0583 - val_loss: 65.5327\n",
      "Epoch 1360/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.4970 - val_loss: 65.3429\n",
      "Epoch 1361/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.0990 - val_loss: 64.4259\n",
      "Epoch 1362/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.4691 - val_loss: 65.4156\n",
      "Epoch 1363/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.6016 - val_loss: 64.3764\n",
      "Epoch 1364/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.0126 - val_loss: 65.1860\n",
      "Epoch 1365/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 44.6638 - val_loss: 64.6442\n",
      "Epoch 1366/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.9655 - val_loss: 65.9823\n",
      "Epoch 1367/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7924 - val_loss: 64.4290\n",
      "Epoch 1368/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7396 - val_loss: 65.4993\n",
      "Epoch 1369/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.1507 - val_loss: 64.9613\n",
      "Epoch 1370/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.2686 - val_loss: 65.4046\n",
      "Epoch 1371/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.2020 - val_loss: 63.9140\n",
      "Epoch 1372/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.1525 - val_loss: 66.9874\n",
      "Epoch 1373/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.0912 - val_loss: 65.0476\n",
      "Epoch 1374/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.8419 - val_loss: 65.7946\n",
      "Epoch 1375/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.2545 - val_loss: 66.0074\n",
      "Epoch 1376/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.1695 - val_loss: 64.9258\n",
      "Epoch 1377/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.5408 - val_loss: 66.2487\n",
      "Epoch 1378/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.9741 - val_loss: 66.8829\n",
      "Epoch 1379/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.9385 - val_loss: 65.5181\n",
      "Epoch 1380/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.3699 - val_loss: 64.8186\n",
      "Epoch 1381/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.6421 - val_loss: 64.2630\n",
      "Epoch 1382/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.1551 - val_loss: 65.3521\n",
      "Epoch 1383/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.2590 - val_loss: 66.0286\n",
      "Epoch 1384/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.9573 - val_loss: 65.2933\n",
      "Epoch 1385/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.3442 - val_loss: 66.0484\n",
      "Epoch 1386/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.3146 - val_loss: 64.5035\n",
      "Epoch 1387/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.2770 - val_loss: 65.8542\n",
      "Epoch 1388/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.3751 - val_loss: 64.9147\n",
      "Epoch 1389/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.4668 - val_loss: 65.5317\n",
      "Epoch 1390/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.3255 - val_loss: 66.0260\n",
      "Epoch 1391/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.0572 - val_loss: 64.2062\n",
      "Epoch 1392/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.0989 - val_loss: 65.6016\n",
      "Epoch 1393/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.5923 - val_loss: 65.3014\n",
      "Epoch 1394/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.0629 - val_loss: 65.9101\n",
      "Epoch 1395/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.7425 - val_loss: 64.5859\n",
      "Epoch 1396/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.0516 - val_loss: 66.3720\n",
      "Epoch 1397/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.5184 - val_loss: 64.3185\n",
      "Epoch 1398/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.2313 - val_loss: 65.2208\n",
      "Epoch 1399/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7731 - val_loss: 65.1282\n",
      "Epoch 1400/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 44.9621\n",
      "Epoch 01400: saving model to saved_models/latent2/cp-1400.h5\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 44.9621 - val_loss: 66.2597\n",
      "Epoch 1401/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.7876 - val_loss: 65.1945\n",
      "Epoch 1402/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.0650 - val_loss: 66.3557\n",
      "Epoch 1403/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7302 - val_loss: 65.3687\n",
      "Epoch 1404/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.0884 - val_loss: 64.7674\n",
      "Epoch 1405/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.9834 - val_loss: 65.9698\n",
      "Epoch 1406/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.0751 - val_loss: 65.9355\n",
      "Epoch 1407/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.9905 - val_loss: 65.9439\n",
      "Epoch 1408/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.0468 - val_loss: 65.7828\n",
      "Epoch 1409/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.9396 - val_loss: 64.6710\n",
      "Epoch 1410/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.8318 - val_loss: 64.7615\n",
      "Epoch 1411/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.9387 - val_loss: 66.2627\n",
      "Epoch 1412/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.9876 - val_loss: 64.8510\n",
      "Epoch 1413/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.0686 - val_loss: 64.9752\n",
      "Epoch 1414/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.0920 - val_loss: 65.0681\n",
      "Epoch 1415/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.0250 - val_loss: 65.2104\n",
      "Epoch 1416/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.2096 - val_loss: 64.0366\n",
      "Epoch 1417/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7142 - val_loss: 65.6053\n",
      "Epoch 1418/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.1831 - val_loss: 65.6948\n",
      "Epoch 1419/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.1078 - val_loss: 66.0500\n",
      "Epoch 1420/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.9483 - val_loss: 65.6055\n",
      "Epoch 1421/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.9470 - val_loss: 65.4032\n",
      "Epoch 1422/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.5521 - val_loss: 64.6126\n",
      "Epoch 1423/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.5174 - val_loss: 65.3273\n",
      "Epoch 1424/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.5226 - val_loss: 65.7311\n",
      "Epoch 1425/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.7697 - val_loss: 65.7239\n",
      "Epoch 1426/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.3552 - val_loss: 65.4900\n",
      "Epoch 1427/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.9940 - val_loss: 64.5309\n",
      "Epoch 1428/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.2914 - val_loss: 64.3186\n",
      "Epoch 1429/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.8077 - val_loss: 66.0219\n",
      "Epoch 1430/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.9738 - val_loss: 66.2997\n",
      "Epoch 1431/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.5227 - val_loss: 65.5263\n",
      "Epoch 1432/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7835 - val_loss: 64.6048\n",
      "Epoch 1433/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.6977 - val_loss: 64.9390\n",
      "Epoch 1434/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.4034 - val_loss: 65.1958\n",
      "Epoch 1435/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.3680 - val_loss: 66.1425\n",
      "Epoch 1436/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.4873 - val_loss: 65.9362\n",
      "Epoch 1437/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.1422 - val_loss: 64.4141\n",
      "Epoch 1438/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.5301 - val_loss: 64.6832\n",
      "Epoch 1439/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.6746 - val_loss: 65.2304\n",
      "Epoch 1440/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.5387 - val_loss: 63.8227\n",
      "Epoch 1441/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7797 - val_loss: 64.2762\n",
      "Epoch 1442/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.0440 - val_loss: 64.1280\n",
      "Epoch 1443/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7018 - val_loss: 64.7284\n",
      "Epoch 1444/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.8788 - val_loss: 65.2618\n",
      "Epoch 1445/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.5197 - val_loss: 65.5800\n",
      "Epoch 1446/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.4125 - val_loss: 64.2103\n",
      "Epoch 1447/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.1876 - val_loss: 63.5189\n",
      "Epoch 1448/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.8643 - val_loss: 64.1696\n",
      "Epoch 1449/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.9407 - val_loss: 65.6906\n",
      "Epoch 1450/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7271 - val_loss: 67.6938\n",
      "Epoch 1451/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7805 - val_loss: 66.1016\n",
      "Epoch 1452/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.9505 - val_loss: 64.2981\n",
      "Epoch 1453/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.2631 - val_loss: 65.2994\n",
      "Epoch 1454/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.1125 - val_loss: 64.3034\n",
      "Epoch 1455/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.1269 - val_loss: 63.9631\n",
      "Epoch 1456/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.5256 - val_loss: 65.7016\n",
      "Epoch 1457/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.4993 - val_loss: 67.0132\n",
      "Epoch 1458/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.6324 - val_loss: 65.1786\n",
      "Epoch 1459/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.6137 - val_loss: 65.8904\n",
      "Epoch 1460/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.4516 - val_loss: 65.2281\n",
      "Epoch 1461/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.5623 - val_loss: 65.3708\n",
      "Epoch 1462/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.8648 - val_loss: 64.7278\n",
      "Epoch 1463/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.5853 - val_loss: 65.2816\n",
      "Epoch 1464/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.9144 - val_loss: 64.3872\n",
      "Epoch 1465/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.6352 - val_loss: 66.3259\n",
      "Epoch 1466/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.8659 - val_loss: 65.5215\n",
      "Epoch 1467/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2461 - val_loss: 64.5991\n",
      "Epoch 1468/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.6946 - val_loss: 65.8108\n",
      "Epoch 1469/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.1416 - val_loss: 65.3377\n",
      "Epoch 1470/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1728 - val_loss: 65.8658\n",
      "Epoch 1471/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.5501 - val_loss: 64.8831\n",
      "Epoch 1472/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.4193 - val_loss: 66.2550\n",
      "Epoch 1473/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.1444 - val_loss: 65.3143\n",
      "Epoch 1474/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.9595 - val_loss: 65.0482\n",
      "Epoch 1475/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.8758 - val_loss: 64.8480\n",
      "Epoch 1476/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.8145 - val_loss: 65.8490\n",
      "Epoch 1477/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.4591 - val_loss: 65.7008\n",
      "Epoch 1478/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.6171 - val_loss: 65.8326\n",
      "Epoch 1479/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.8537 - val_loss: 64.6487\n",
      "Epoch 1480/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.6197 - val_loss: 65.9979\n",
      "Epoch 1481/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.2107 - val_loss: 64.7498\n",
      "Epoch 1482/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.0381 - val_loss: 66.4313\n",
      "Epoch 1483/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.4857 - val_loss: 65.6079\n",
      "Epoch 1484/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.2155 - val_loss: 64.3492\n",
      "Epoch 1485/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.8627 - val_loss: 65.9508\n",
      "Epoch 1486/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.9352 - val_loss: 64.3506\n",
      "Epoch 1487/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.3654 - val_loss: 65.5997\n",
      "Epoch 1488/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.5812 - val_loss: 65.7097\n",
      "Epoch 1489/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.5102 - val_loss: 65.0270\n",
      "Epoch 1490/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.5305 - val_loss: 66.6384\n",
      "Epoch 1491/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.0676 - val_loss: 65.5181\n",
      "Epoch 1492/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.6006 - val_loss: 64.1058\n",
      "Epoch 1493/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.2023 - val_loss: 64.7594\n",
      "Epoch 1494/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.0097 - val_loss: 64.9403\n",
      "Epoch 1495/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.3843 - val_loss: 65.7280\n",
      "Epoch 1496/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.6076 - val_loss: 65.9990\n",
      "Epoch 1497/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2539 - val_loss: 65.9005\n",
      "Epoch 1498/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.2865 - val_loss: 64.9474\n",
      "Epoch 1499/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.4663 - val_loss: 66.1282\n",
      "Epoch 1500/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.0162 - val_loss: 65.3695\n",
      "Epoch 1501/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.4621 - val_loss: 65.3169\n",
      "Epoch 1502/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.0267 - val_loss: 65.6330\n",
      "Epoch 1503/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.1090 - val_loss: 65.2394\n",
      "Epoch 1504/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.5640 - val_loss: 66.3345\n",
      "Epoch 1505/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.4009 - val_loss: 66.8765\n",
      "Epoch 1506/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.1566 - val_loss: 64.8064\n",
      "Epoch 1507/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.4980 - val_loss: 64.9253\n",
      "Epoch 1508/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.0292 - val_loss: 66.2052\n",
      "Epoch 1509/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.6670 - val_loss: 65.5303\n",
      "Epoch 1510/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.5863 - val_loss: 66.3299\n",
      "Epoch 1511/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.2449 - val_loss: 66.3259\n",
      "Epoch 1512/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.3204 - val_loss: 65.4667\n",
      "Epoch 1513/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.6022 - val_loss: 65.4063\n",
      "Epoch 1514/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1845 - val_loss: 65.8334\n",
      "Epoch 1515/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2503 - val_loss: 66.0656\n",
      "Epoch 1516/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.9722 - val_loss: 66.2043\n",
      "Epoch 1517/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.4250 - val_loss: 65.0618\n",
      "Epoch 1518/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.8723 - val_loss: 65.3478\n",
      "Epoch 1519/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1719 - val_loss: 65.3632\n",
      "Epoch 1520/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.8956 - val_loss: 65.6441\n",
      "Epoch 1521/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7321 - val_loss: 64.9502\n",
      "Epoch 1522/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.6325 - val_loss: 65.1229\n",
      "Epoch 1523/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.0328 - val_loss: 66.3000\n",
      "Epoch 1524/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.5862 - val_loss: 67.8684\n",
      "Epoch 1525/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.6730 - val_loss: 66.6090\n",
      "Epoch 1526/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.5139 - val_loss: 65.8263\n",
      "Epoch 1527/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.2782 - val_loss: 65.0077\n",
      "Epoch 1528/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.3395 - val_loss: 66.1545\n",
      "Epoch 1529/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.7742 - val_loss: 66.0564\n",
      "Epoch 1530/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.9249 - val_loss: 64.3137\n",
      "Epoch 1531/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1927 - val_loss: 65.6621\n",
      "Epoch 1532/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 43.6539 - val_loss: 65.1607\n",
      "Epoch 1533/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.5637 - val_loss: 65.4837\n",
      "Epoch 1534/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.5921 - val_loss: 64.6254\n",
      "Epoch 1535/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.4663 - val_loss: 65.8267\n",
      "Epoch 1536/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.5958 - val_loss: 65.0886\n",
      "Epoch 1537/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1555 - val_loss: 66.2136\n",
      "Epoch 1538/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.9540 - val_loss: 67.6527\n",
      "Epoch 1539/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.2538 - val_loss: 68.1947\n",
      "Epoch 1540/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7072 - val_loss: 67.0142\n",
      "Epoch 1541/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.8444 - val_loss: 65.5488\n",
      "Epoch 1542/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.3782 - val_loss: 65.7570\n",
      "Epoch 1543/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7447 - val_loss: 66.5444\n",
      "Epoch 1544/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2572 - val_loss: 65.6179\n",
      "Epoch 1545/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7596 - val_loss: 65.0443\n",
      "Epoch 1546/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2066 - val_loss: 64.8396\n",
      "Epoch 1547/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.3244 - val_loss: 64.9738\n",
      "Epoch 1548/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2346 - val_loss: 65.0556\n",
      "Epoch 1549/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1559 - val_loss: 65.5430\n",
      "Epoch 1550/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 44.2711 - val_loss: 66.0502\n",
      "Epoch 1551/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.9614 - val_loss: 66.0189\n",
      "Epoch 1552/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.6862 - val_loss: 66.4937\n",
      "Epoch 1553/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.0431 - val_loss: 67.0153\n",
      "Epoch 1554/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.2352 - val_loss: 68.2696\n",
      "Epoch 1555/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.3806 - val_loss: 65.6359\n",
      "Epoch 1556/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.3060 - val_loss: 64.6162\n",
      "Epoch 1557/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.8764 - val_loss: 66.4461\n",
      "Epoch 1558/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.5835 - val_loss: 64.1557\n",
      "Epoch 1559/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2850 - val_loss: 65.9623\n",
      "Epoch 1560/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.1861 - val_loss: 64.6053\n",
      "Epoch 1561/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.6441 - val_loss: 64.4798\n",
      "Epoch 1562/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.9069 - val_loss: 65.2416\n",
      "Epoch 1563/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.9627 - val_loss: 63.1735\n",
      "Epoch 1564/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.8100 - val_loss: 65.7970\n",
      "Epoch 1565/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.9976 - val_loss: 65.5637\n",
      "Epoch 1566/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.9295 - val_loss: 65.3635\n",
      "Epoch 1567/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.7882 - val_loss: 64.9069\n",
      "Epoch 1568/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8883 - val_loss: 65.3958\n",
      "Epoch 1569/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.5343 - val_loss: 65.1783\n",
      "Epoch 1570/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2130 - val_loss: 65.7437\n",
      "Epoch 1571/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1822 - val_loss: 66.0450\n",
      "Epoch 1572/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.4755 - val_loss: 66.5140\n",
      "Epoch 1573/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2345 - val_loss: 65.0406\n",
      "Epoch 1574/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7252 - val_loss: 64.8728\n",
      "Epoch 1575/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.9595 - val_loss: 65.3393\n",
      "Epoch 1576/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.3087 - val_loss: 65.6713\n",
      "Epoch 1577/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8482 - val_loss: 66.0329\n",
      "Epoch 1578/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.0754 - val_loss: 64.9706\n",
      "Epoch 1579/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1037 - val_loss: 64.9428\n",
      "Epoch 1580/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.1175 - val_loss: 65.8018\n",
      "Epoch 1581/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.3254 - val_loss: 64.6791\n",
      "Epoch 1582/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.9114 - val_loss: 65.2844\n",
      "Epoch 1583/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2562 - val_loss: 65.6345\n",
      "Epoch 1584/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.3590 - val_loss: 64.7629\n",
      "Epoch 1585/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1797 - val_loss: 66.0084\n",
      "Epoch 1586/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.3725 - val_loss: 65.9858\n",
      "Epoch 1587/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.0592 - val_loss: 64.0831\n",
      "Epoch 1588/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2862 - val_loss: 66.7503\n",
      "Epoch 1589/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1989 - val_loss: 65.1038\n",
      "Epoch 1590/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.0145 - val_loss: 65.5872\n",
      "Epoch 1591/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.4473 - val_loss: 67.2597\n",
      "Epoch 1592/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.0427 - val_loss: 65.4592\n",
      "Epoch 1593/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.5329 - val_loss: 65.7866\n",
      "Epoch 1594/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7255 - val_loss: 67.2107\n",
      "Epoch 1595/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1951 - val_loss: 66.1136\n",
      "Epoch 1596/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2571 - val_loss: 66.0850\n",
      "Epoch 1597/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.6693 - val_loss: 65.3682\n",
      "Epoch 1598/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8463 - val_loss: 66.2908\n",
      "Epoch 1599/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.0470 - val_loss: 64.7048\n",
      "Epoch 1600/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 44.1865\n",
      "Epoch 01600: saving model to saved_models/latent2/cp-1600.h5\n",
      "7/7 [==============================] - 1s 157ms/step - loss: 44.1865 - val_loss: 64.9336\n",
      "Epoch 1601/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8595 - val_loss: 65.5900\n",
      "Epoch 1602/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2485 - val_loss: 65.3930\n",
      "Epoch 1603/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.4099 - val_loss: 66.5015\n",
      "Epoch 1604/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.6389 - val_loss: 65.5077\n",
      "Epoch 1605/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.5029 - val_loss: 65.2448\n",
      "Epoch 1606/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.9568 - val_loss: 65.4854\n",
      "Epoch 1607/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.4926 - val_loss: 65.6399\n",
      "Epoch 1608/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.4292 - val_loss: 65.5570\n",
      "Epoch 1609/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1658 - val_loss: 66.5512\n",
      "Epoch 1610/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2771 - val_loss: 66.6702\n",
      "Epoch 1611/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.0604 - val_loss: 65.4676\n",
      "Epoch 1612/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.3832 - val_loss: 66.9246\n",
      "Epoch 1613/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7399 - val_loss: 65.4446\n",
      "Epoch 1614/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7237 - val_loss: 65.1947\n",
      "Epoch 1615/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5959 - val_loss: 65.3436\n",
      "Epoch 1616/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1752 - val_loss: 65.6366\n",
      "Epoch 1617/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.6058 - val_loss: 63.9327\n",
      "Epoch 1618/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.4257 - val_loss: 66.2275\n",
      "Epoch 1619/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.3311 - val_loss: 65.5959\n",
      "Epoch 1620/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7094 - val_loss: 64.8312\n",
      "Epoch 1621/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.9777 - val_loss: 65.6171\n",
      "Epoch 1622/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7605 - val_loss: 66.2165\n",
      "Epoch 1623/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.0502 - val_loss: 66.1115\n",
      "Epoch 1624/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.0093 - val_loss: 65.7808\n",
      "Epoch 1625/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7687 - val_loss: 65.1880\n",
      "Epoch 1626/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8856 - val_loss: 65.7507\n",
      "Epoch 1627/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.6724 - val_loss: 65.6303\n",
      "Epoch 1628/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.9990 - val_loss: 65.0119\n",
      "Epoch 1629/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.3635 - val_loss: 65.8076\n",
      "Epoch 1630/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.6947 - val_loss: 67.1364\n",
      "Epoch 1631/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.3032 - val_loss: 65.3609\n",
      "Epoch 1632/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2662 - val_loss: 65.2858\n",
      "Epoch 1633/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.9126 - val_loss: 65.1065\n",
      "Epoch 1634/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.0575 - val_loss: 64.2757\n",
      "Epoch 1635/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5329 - val_loss: 64.7848\n",
      "Epoch 1636/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.6193 - val_loss: 66.6628\n",
      "Epoch 1637/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.0992 - val_loss: 64.5864\n",
      "Epoch 1638/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 44.1156 - val_loss: 67.3163\n",
      "Epoch 1639/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.3675 - val_loss: 64.5844\n",
      "Epoch 1640/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5293 - val_loss: 65.6145\n",
      "Epoch 1641/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7132 - val_loss: 65.8232\n",
      "Epoch 1642/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.9016 - val_loss: 66.5753\n",
      "Epoch 1643/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8330 - val_loss: 65.3916\n",
      "Epoch 1644/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5770 - val_loss: 66.1493\n",
      "Epoch 1645/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2278 - val_loss: 64.9774\n",
      "Epoch 1646/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7042 - val_loss: 65.6172\n",
      "Epoch 1647/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2566 - val_loss: 65.8615\n",
      "Epoch 1648/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1781 - val_loss: 67.8798\n",
      "Epoch 1649/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2685 - val_loss: 66.0321\n",
      "Epoch 1650/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.0915 - val_loss: 64.6068\n",
      "Epoch 1651/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7890 - val_loss: 65.7597\n",
      "Epoch 1652/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.6139 - val_loss: 65.2651\n",
      "Epoch 1653/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1220 - val_loss: 64.7635\n",
      "Epoch 1654/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.0059 - val_loss: 65.1980\n",
      "Epoch 1655/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8842 - val_loss: 66.0509\n",
      "Epoch 1656/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1604 - val_loss: 67.2845\n",
      "Epoch 1657/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2348 - val_loss: 64.2342\n",
      "Epoch 1658/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.9416 - val_loss: 64.9746\n",
      "Epoch 1659/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5414 - val_loss: 66.3194\n",
      "Epoch 1660/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.6507 - val_loss: 67.3499\n",
      "Epoch 1661/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2505 - val_loss: 66.4844\n",
      "Epoch 1662/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2214 - val_loss: 66.6148\n",
      "Epoch 1663/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7953 - val_loss: 65.2891\n",
      "Epoch 1664/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.4006 - val_loss: 66.4149\n",
      "Epoch 1665/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.0469 - val_loss: 65.8183\n",
      "Epoch 1666/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.0113 - val_loss: 65.2703\n",
      "Epoch 1667/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.0202 - val_loss: 66.7242\n",
      "Epoch 1668/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8347 - val_loss: 67.8342\n",
      "Epoch 1669/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.9151 - val_loss: 67.8165\n",
      "Epoch 1670/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.9192 - val_loss: 65.3467\n",
      "Epoch 1671/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.8088 - val_loss: 64.3160\n",
      "Epoch 1672/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.0161 - val_loss: 66.0595\n",
      "Epoch 1673/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.6745 - val_loss: 64.5364\n",
      "Epoch 1674/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3837 - val_loss: 66.4894\n",
      "Epoch 1675/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2718 - val_loss: 66.2052\n",
      "Epoch 1676/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.9407 - val_loss: 65.0473\n",
      "Epoch 1677/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.3125 - val_loss: 64.2609\n",
      "Epoch 1678/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2396 - val_loss: 65.6659\n",
      "Epoch 1679/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.4355 - val_loss: 67.5716\n",
      "Epoch 1680/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.4139 - val_loss: 63.7852\n",
      "Epoch 1681/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.0806 - val_loss: 66.5041\n",
      "Epoch 1682/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7828 - val_loss: 64.4619\n",
      "Epoch 1683/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8105 - val_loss: 65.7217\n",
      "Epoch 1684/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2944 - val_loss: 64.9791\n",
      "Epoch 1685/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.5739 - val_loss: 65.4858\n",
      "Epoch 1686/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.9816 - val_loss: 67.0343\n",
      "Epoch 1687/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.1781 - val_loss: 66.2636\n",
      "Epoch 1688/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8849 - val_loss: 65.4008\n",
      "Epoch 1689/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.9452 - val_loss: 66.1309\n",
      "Epoch 1690/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.0999 - val_loss: 64.9527\n",
      "Epoch 1691/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.4046 - val_loss: 66.3771\n",
      "Epoch 1692/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1741 - val_loss: 66.2597\n",
      "Epoch 1693/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.4148 - val_loss: 65.7635\n",
      "Epoch 1694/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8156 - val_loss: 65.6909\n",
      "Epoch 1695/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.7950 - val_loss: 65.9963\n",
      "Epoch 1696/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.0936 - val_loss: 67.2360\n",
      "Epoch 1697/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.9656 - val_loss: 65.0681\n",
      "Epoch 1698/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.3220 - val_loss: 65.0693\n",
      "Epoch 1699/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.2213 - val_loss: 65.5049\n",
      "Epoch 1700/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7683 - val_loss: 65.3544\n",
      "Epoch 1701/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.2773 - val_loss: 66.2664\n",
      "Epoch 1702/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3515 - val_loss: 67.5329\n",
      "Epoch 1703/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7420 - val_loss: 64.2364\n",
      "Epoch 1704/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.1128 - val_loss: 65.2873\n",
      "Epoch 1705/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.4161 - val_loss: 65.1184\n",
      "Epoch 1706/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8653 - val_loss: 65.2899\n",
      "Epoch 1707/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3025 - val_loss: 66.6837\n",
      "Epoch 1708/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7407 - val_loss: 65.3102\n",
      "Epoch 1709/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.4508 - val_loss: 65.2512\n",
      "Epoch 1710/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3327 - val_loss: 65.2272\n",
      "Epoch 1711/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.4321 - val_loss: 63.9860\n",
      "Epoch 1712/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.1057 - val_loss: 66.2612\n",
      "Epoch 1713/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.1501 - val_loss: 66.0892\n",
      "Epoch 1714/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.2628 - val_loss: 65.9240\n",
      "Epoch 1715/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8503 - val_loss: 66.0412\n",
      "Epoch 1716/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5912 - val_loss: 66.5395\n",
      "Epoch 1717/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3477 - val_loss: 65.0131\n",
      "Epoch 1718/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.9445 - val_loss: 67.3038\n",
      "Epoch 1719/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.9822 - val_loss: 66.6885\n",
      "Epoch 1720/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2394 - val_loss: 65.2856\n",
      "Epoch 1721/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.0391 - val_loss: 66.4023\n",
      "Epoch 1722/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.7973 - val_loss: 64.0016\n",
      "Epoch 1723/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2736 - val_loss: 66.6452\n",
      "Epoch 1724/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.7895 - val_loss: 66.0335\n",
      "Epoch 1725/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.6976 - val_loss: 65.1300\n",
      "Epoch 1726/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8592 - val_loss: 65.4426\n",
      "Epoch 1727/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.3641 - val_loss: 65.6477\n",
      "Epoch 1728/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3187 - val_loss: 67.3272\n",
      "Epoch 1729/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8054 - val_loss: 65.1391\n",
      "Epoch 1730/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2746 - val_loss: 65.6893\n",
      "Epoch 1731/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2270 - val_loss: 66.1563\n",
      "Epoch 1732/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.4243 - val_loss: 65.4946\n",
      "Epoch 1733/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.4884 - val_loss: 65.9832\n",
      "Epoch 1734/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.4967 - val_loss: 66.2980\n",
      "Epoch 1735/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.3360 - val_loss: 67.5974\n",
      "Epoch 1736/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3604 - val_loss: 65.0901\n",
      "Epoch 1737/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5815 - val_loss: 64.2034\n",
      "Epoch 1738/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1215 - val_loss: 65.1980\n",
      "Epoch 1739/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2654 - val_loss: 67.0678\n",
      "Epoch 1740/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.6997 - val_loss: 65.8385\n",
      "Epoch 1741/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.7757 - val_loss: 66.3229\n",
      "Epoch 1742/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8724 - val_loss: 65.6405\n",
      "Epoch 1743/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.9119 - val_loss: 66.7922\n",
      "Epoch 1744/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8448 - val_loss: 65.6480\n",
      "Epoch 1745/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.4604 - val_loss: 66.5494\n",
      "Epoch 1746/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2490 - val_loss: 66.0155\n",
      "Epoch 1747/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2061 - val_loss: 66.4365\n",
      "Epoch 1748/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3085 - val_loss: 64.3866\n",
      "Epoch 1749/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.8501 - val_loss: 64.8445\n",
      "Epoch 1750/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1351 - val_loss: 64.4385\n",
      "Epoch 1751/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.2419 - val_loss: 67.1937\n",
      "Epoch 1752/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.4514 - val_loss: 66.7404\n",
      "Epoch 1753/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7072 - val_loss: 65.9824\n",
      "Epoch 1754/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.9212 - val_loss: 64.9625\n",
      "Epoch 1755/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1165 - val_loss: 66.8377\n",
      "Epoch 1756/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2293 - val_loss: 68.1594\n",
      "Epoch 1757/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1303 - val_loss: 67.1367\n",
      "Epoch 1758/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.9341 - val_loss: 66.2988\n",
      "Epoch 1759/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2258 - val_loss: 66.0094\n",
      "Epoch 1760/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5113 - val_loss: 65.2624\n",
      "Epoch 1761/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7327 - val_loss: 66.7855\n",
      "Epoch 1762/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.9140 - val_loss: 65.5530\n",
      "Epoch 1763/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7790 - val_loss: 65.8028\n",
      "Epoch 1764/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.7381 - val_loss: 66.5375\n",
      "Epoch 1765/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1095 - val_loss: 67.1587\n",
      "Epoch 1766/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5865 - val_loss: 66.0053\n",
      "Epoch 1767/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7858 - val_loss: 66.2480\n",
      "Epoch 1768/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.3296 - val_loss: 66.0787\n",
      "Epoch 1769/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.5832 - val_loss: 65.3989\n",
      "Epoch 1770/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5963 - val_loss: 65.4059\n",
      "Epoch 1771/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7739 - val_loss: 65.4588\n",
      "Epoch 1772/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8863 - val_loss: 65.7407\n",
      "Epoch 1773/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.4188 - val_loss: 65.8886\n",
      "Epoch 1774/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1990 - val_loss: 66.2725\n",
      "Epoch 1775/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.4891 - val_loss: 65.6363\n",
      "Epoch 1776/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5579 - val_loss: 65.3985\n",
      "Epoch 1777/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5751 - val_loss: 65.6713\n",
      "Epoch 1778/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.1045 - val_loss: 65.8165\n",
      "Epoch 1779/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5922 - val_loss: 66.0557\n",
      "Epoch 1780/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7617 - val_loss: 66.5403\n",
      "Epoch 1781/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2551 - val_loss: 65.4040\n",
      "Epoch 1782/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.4378 - val_loss: 65.3294\n",
      "Epoch 1783/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5199 - val_loss: 65.9574\n",
      "Epoch 1784/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5174 - val_loss: 65.8435\n",
      "Epoch 1785/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.9986 - val_loss: 66.2807\n",
      "Epoch 1786/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.9557 - val_loss: 66.0246\n",
      "Epoch 1787/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.1464 - val_loss: 66.1293\n",
      "Epoch 1788/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.4956 - val_loss: 67.3645\n",
      "Epoch 1789/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3717 - val_loss: 66.7185\n",
      "Epoch 1790/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3901 - val_loss: 66.3278\n",
      "Epoch 1791/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.8181 - val_loss: 67.4702\n",
      "Epoch 1792/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7803 - val_loss: 66.3947\n",
      "Epoch 1793/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.9126 - val_loss: 66.4326\n",
      "Epoch 1794/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.4941 - val_loss: 66.0867\n",
      "Epoch 1795/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2126 - val_loss: 65.7462\n",
      "Epoch 1796/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.0521 - val_loss: 65.5735\n",
      "Epoch 1797/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.9778 - val_loss: 65.8838\n",
      "Epoch 1798/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.6421 - val_loss: 64.8436\n",
      "Epoch 1799/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3720 - val_loss: 66.7848\n",
      "Epoch 1800/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 43.3537\n",
      "Epoch 01800: saving model to saved_models/latent2/cp-1800.h5\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 43.3537 - val_loss: 67.6881\n",
      "Epoch 1801/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.0314 - val_loss: 67.6596\n",
      "Epoch 1802/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.3540 - val_loss: 66.3849\n",
      "Epoch 1803/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.0850 - val_loss: 65.3678\n",
      "Epoch 1804/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.4954 - val_loss: 66.7627\n",
      "Epoch 1805/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8999 - val_loss: 67.7928\n",
      "Epoch 1806/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.9336 - val_loss: 67.1879\n",
      "Epoch 1807/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.8876 - val_loss: 65.8354\n",
      "Epoch 1808/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2794 - val_loss: 66.5209\n",
      "Epoch 1809/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.6727 - val_loss: 65.9429\n",
      "Epoch 1810/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.4637 - val_loss: 65.5608\n",
      "Epoch 1811/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.1950 - val_loss: 66.2071\n",
      "Epoch 1812/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.1707 - val_loss: 65.9861\n",
      "Epoch 1813/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2344 - val_loss: 66.3549\n",
      "Epoch 1814/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2145 - val_loss: 66.4480\n",
      "Epoch 1815/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3578 - val_loss: 65.2954\n",
      "Epoch 1816/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.9802 - val_loss: 66.4120\n",
      "Epoch 1817/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.0436 - val_loss: 64.9055\n",
      "Epoch 1818/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.1009 - val_loss: 67.9163\n",
      "Epoch 1819/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2309 - val_loss: 65.0463\n",
      "Epoch 1820/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2220 - val_loss: 66.5381\n",
      "Epoch 1821/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1905 - val_loss: 65.5166\n",
      "Epoch 1822/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7958 - val_loss: 66.7852\n",
      "Epoch 1823/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7102 - val_loss: 67.0566\n",
      "Epoch 1824/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.6811 - val_loss: 65.6588\n",
      "Epoch 1825/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.6252 - val_loss: 65.8297\n",
      "Epoch 1826/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2928 - val_loss: 66.6876\n",
      "Epoch 1827/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.0322 - val_loss: 67.2434\n",
      "Epoch 1828/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7631 - val_loss: 66.9922\n",
      "Epoch 1829/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.1169 - val_loss: 67.2343\n",
      "Epoch 1830/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.8045 - val_loss: 65.6975\n",
      "Epoch 1831/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.8706 - val_loss: 65.7187\n",
      "Epoch 1832/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.9497 - val_loss: 67.0543\n",
      "Epoch 1833/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.9383 - val_loss: 66.0015\n",
      "Epoch 1834/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.8295 - val_loss: 66.2995\n",
      "Epoch 1835/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.1160 - val_loss: 66.0720\n",
      "Epoch 1836/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.1886 - val_loss: 67.4647\n",
      "Epoch 1837/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2074 - val_loss: 65.4971\n",
      "Epoch 1838/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2178 - val_loss: 65.3132\n",
      "Epoch 1839/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.5253 - val_loss: 66.7943\n",
      "Epoch 1840/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3823 - val_loss: 66.3055\n",
      "Epoch 1841/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8141 - val_loss: 66.2845\n",
      "Epoch 1842/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8459 - val_loss: 66.7083\n",
      "Epoch 1843/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3551 - val_loss: 66.7015\n",
      "Epoch 1844/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3303 - val_loss: 66.6754\n",
      "Epoch 1845/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.4813 - val_loss: 66.7073\n",
      "Epoch 1846/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5481 - val_loss: 66.8178\n",
      "Epoch 1847/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.3840 - val_loss: 66.7294\n",
      "Epoch 1848/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.9125 - val_loss: 64.6147\n",
      "Epoch 1849/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5040 - val_loss: 65.4678\n",
      "Epoch 1850/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3273 - val_loss: 67.4315\n",
      "Epoch 1851/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.6439 - val_loss: 66.4247\n",
      "Epoch 1852/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2749 - val_loss: 64.8693\n",
      "Epoch 1853/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.7998 - val_loss: 65.5170\n",
      "Epoch 1854/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2693 - val_loss: 65.3116\n",
      "Epoch 1855/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.1556 - val_loss: 66.4638\n",
      "Epoch 1856/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.0642 - val_loss: 66.6462\n",
      "Epoch 1857/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.8613 - val_loss: 64.1699\n",
      "Epoch 1858/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.9010 - val_loss: 66.4051\n",
      "Epoch 1859/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.5909 - val_loss: 65.8366\n",
      "Epoch 1860/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.5525 - val_loss: 65.6256\n",
      "Epoch 1861/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.0227 - val_loss: 66.2765\n",
      "Epoch 1862/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.2209 - val_loss: 66.1420\n",
      "Epoch 1863/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.1722 - val_loss: 66.6518\n",
      "Epoch 1864/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5210 - val_loss: 66.4251\n",
      "Epoch 1865/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.5295 - val_loss: 65.3566\n",
      "Epoch 1866/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.1921 - val_loss: 64.1400\n",
      "Epoch 1867/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.9291 - val_loss: 66.6452\n",
      "Epoch 1868/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.0076 - val_loss: 66.4621\n",
      "Epoch 1869/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.6566 - val_loss: 66.5285\n",
      "Epoch 1870/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.7959 - val_loss: 66.0133\n",
      "Epoch 1871/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2696 - val_loss: 66.6646\n",
      "Epoch 1872/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2153 - val_loss: 64.1618\n",
      "Epoch 1873/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3105 - val_loss: 69.8831\n",
      "Epoch 1874/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.5784 - val_loss: 66.0920\n",
      "Epoch 1875/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.4321 - val_loss: 66.2199\n",
      "Epoch 1876/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8621 - val_loss: 65.9597\n",
      "Epoch 1877/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.9452 - val_loss: 66.6592\n",
      "Epoch 1878/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.4579 - val_loss: 65.5355\n",
      "Epoch 1879/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.9021 - val_loss: 64.3467\n",
      "Epoch 1880/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.0846 - val_loss: 65.3781\n",
      "Epoch 1881/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.1816 - val_loss: 65.8705\n",
      "Epoch 1882/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2649 - val_loss: 66.2620\n",
      "Epoch 1883/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2082 - val_loss: 67.6584\n",
      "Epoch 1884/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.7386 - val_loss: 66.0738\n",
      "Epoch 1885/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.7902 - val_loss: 67.2803\n",
      "Epoch 1886/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.3504 - val_loss: 67.3499\n",
      "Epoch 1887/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2473 - val_loss: 65.5390\n",
      "Epoch 1888/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.4533 - val_loss: 64.4686\n",
      "Epoch 1889/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2457 - val_loss: 65.8981\n",
      "Epoch 1890/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5599 - val_loss: 65.3895\n",
      "Epoch 1891/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3787 - val_loss: 67.7763\n",
      "Epoch 1892/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8213 - val_loss: 68.7273\n",
      "Epoch 1893/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.9910 - val_loss: 66.3965\n",
      "Epoch 1894/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8082 - val_loss: 66.2153\n",
      "Epoch 1895/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7812 - val_loss: 66.6957\n",
      "Epoch 1896/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.4226 - val_loss: 65.6371\n",
      "Epoch 1897/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.9604 - val_loss: 68.6671\n",
      "Epoch 1898/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5389 - val_loss: 68.6607\n",
      "Epoch 1899/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.2451 - val_loss: 66.0284\n",
      "Epoch 1900/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.1308 - val_loss: 66.0039\n",
      "Epoch 1901/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3719 - val_loss: 67.3249\n",
      "Epoch 1902/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5855 - val_loss: 64.8846\n",
      "Epoch 1903/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.0800 - val_loss: 65.7119\n",
      "Epoch 1904/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2210 - val_loss: 66.7640\n",
      "Epoch 1905/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.7642 - val_loss: 66.0003\n",
      "Epoch 1906/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.9402 - val_loss: 67.4972\n",
      "Epoch 1907/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5514 - val_loss: 66.4791\n",
      "Epoch 1908/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.6514 - val_loss: 66.5782\n",
      "Epoch 1909/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.1686 - val_loss: 67.5587\n",
      "Epoch 1910/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.0239 - val_loss: 67.2033\n",
      "Epoch 1911/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.4079 - val_loss: 67.4917\n",
      "Epoch 1912/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3620 - val_loss: 66.8282\n",
      "Epoch 1913/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.4385 - val_loss: 66.3555\n",
      "Epoch 1914/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.0931 - val_loss: 66.7015\n",
      "Epoch 1915/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.0073 - val_loss: 66.7844\n",
      "Epoch 1916/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.0473 - val_loss: 65.3722\n",
      "Epoch 1917/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.9579 - val_loss: 66.9981\n",
      "Epoch 1918/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.9082 - val_loss: 67.6852\n",
      "Epoch 1919/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.0257 - val_loss: 67.2910\n",
      "Epoch 1920/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.9534 - val_loss: 68.8919\n",
      "Epoch 1921/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2444 - val_loss: 66.7489\n",
      "Epoch 1922/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3828 - val_loss: 66.4713\n",
      "Epoch 1923/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.8816 - val_loss: 66.3467\n",
      "Epoch 1924/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.5413 - val_loss: 66.2738\n",
      "Epoch 1925/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.5558 - val_loss: 66.5821\n",
      "Epoch 1926/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.8106 - val_loss: 65.6132\n",
      "Epoch 1927/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.1734 - val_loss: 66.3111\n",
      "Epoch 1928/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.9336 - val_loss: 66.5874\n",
      "Epoch 1929/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 42.4617 - val_loss: 66.2062\n",
      "Epoch 1930/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.6496 - val_loss: 67.0013\n",
      "Epoch 1931/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.7559 - val_loss: 67.8466\n",
      "Epoch 1932/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.8881 - val_loss: 66.4243\n",
      "Epoch 1933/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.5989 - val_loss: 65.8398\n",
      "Epoch 1934/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.8991 - val_loss: 68.7933\n",
      "Epoch 1935/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.4641 - val_loss: 66.7947\n",
      "Epoch 1936/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3526 - val_loss: 66.6110\n",
      "Epoch 1937/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.0358 - val_loss: 66.3557\n",
      "Epoch 1938/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.1805 - val_loss: 67.7396\n",
      "Epoch 1939/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.0853 - val_loss: 66.4822\n",
      "Epoch 1940/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5188 - val_loss: 67.9696\n",
      "Epoch 1941/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 43.3683 - val_loss: 68.2635\n",
      "Epoch 1942/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 43.2864 - val_loss: 66.5595\n",
      "Epoch 1943/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.6907 - val_loss: 65.7297\n",
      "Epoch 1944/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.7053 - val_loss: 68.2295\n",
      "Epoch 1945/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.9539 - val_loss: 66.5593\n",
      "Epoch 1946/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.1168 - val_loss: 66.5973\n",
      "Epoch 1947/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.0443 - val_loss: 66.2812\n",
      "Epoch 1948/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.9260 - val_loss: 66.6396\n",
      "Epoch 1949/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.7702 - val_loss: 66.9857\n",
      "Epoch 1950/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.7254 - val_loss: 66.3132\n",
      "Epoch 1951/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2042 - val_loss: 67.1943\n",
      "Epoch 1952/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.5529 - val_loss: 67.0725\n",
      "Epoch 1953/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.5460 - val_loss: 68.4372\n",
      "Epoch 1954/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.1834 - val_loss: 68.2162\n",
      "Epoch 1955/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3662 - val_loss: 68.8535\n",
      "Epoch 1956/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.9409 - val_loss: 68.7520\n",
      "Epoch 1957/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2033 - val_loss: 66.2880\n",
      "Epoch 1958/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.9668 - val_loss: 64.9487\n",
      "Epoch 1959/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5210 - val_loss: 65.6841\n",
      "Epoch 1960/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2595 - val_loss: 65.4091\n",
      "Epoch 1961/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.7596 - val_loss: 65.0891\n",
      "Epoch 1962/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 42.8541 - val_loss: 65.3117\n",
      "Epoch 1963/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.9676 - val_loss: 65.9066\n",
      "Epoch 1964/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2467 - val_loss: 66.9671\n",
      "Epoch 1965/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.7638 - val_loss: 68.1376\n",
      "Epoch 1966/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3028 - val_loss: 66.9675\n",
      "Epoch 1967/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7885 - val_loss: 65.9361\n",
      "Epoch 1968/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2960 - val_loss: 64.2093\n",
      "Epoch 1969/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3712 - val_loss: 66.3876\n",
      "Epoch 1970/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.8532 - val_loss: 67.3441\n",
      "Epoch 1971/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.7798 - val_loss: 66.9934\n",
      "Epoch 1972/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.8104 - val_loss: 66.9176\n",
      "Epoch 1973/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.7764 - val_loss: 66.7375\n",
      "Epoch 1974/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.0043 - val_loss: 67.6978\n",
      "Epoch 1975/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.9517 - val_loss: 67.7688\n",
      "Epoch 1976/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.5808 - val_loss: 66.7284\n",
      "Epoch 1977/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.5834 - val_loss: 66.4408\n",
      "Epoch 1978/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.7901 - val_loss: 68.0353\n",
      "Epoch 1979/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.4089 - val_loss: 66.8533\n",
      "Epoch 1980/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.0813 - val_loss: 66.7250\n",
      "Epoch 1981/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.0265 - val_loss: 65.6437\n",
      "Epoch 1982/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.7967 - val_loss: 66.3038\n",
      "Epoch 1983/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.8154 - val_loss: 67.1461\n",
      "Epoch 1984/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.9379 - val_loss: 66.3677\n",
      "Epoch 1985/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.2740 - val_loss: 66.5764\n",
      "Epoch 1986/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.6546 - val_loss: 66.6596\n",
      "Epoch 1987/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.7473 - val_loss: 66.3393\n",
      "Epoch 1988/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.4033 - val_loss: 67.2609\n",
      "Epoch 1989/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.0304 - val_loss: 66.9132\n",
      "Epoch 1990/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.0687 - val_loss: 69.2724\n",
      "Epoch 1991/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.9471 - val_loss: 68.0081\n",
      "Epoch 1992/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.9252 - val_loss: 67.9956\n",
      "Epoch 1993/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.9268 - val_loss: 67.1383\n",
      "Epoch 1994/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.6675 - val_loss: 64.7225\n",
      "Epoch 1995/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2637 - val_loss: 67.3975\n",
      "Epoch 1996/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.9318 - val_loss: 67.9858\n",
      "Epoch 1997/2000\n",
      "7/7 [==============================] - 1s 103ms/step - loss: 42.7314 - val_loss: 69.9516\n",
      "Epoch 1998/2000\n",
      "7/7 [==============================] - 1s 103ms/step - loss: 43.3878 - val_loss: 66.3594\n",
      "Epoch 1999/2000\n",
      "7/7 [==============================] - 1s 102ms/step - loss: 42.8394 - val_loss: 65.4215\n",
      "Epoch 2000/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 42.7879\n",
      "Epoch 02000: saving model to saved_models/latent2/cp-2000.h5\n",
      "7/7 [==============================] - 1s 140ms/step - loss: 42.7879 - val_loss: 65.8653\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train VAE model with callbacks \"\"\"\n",
    "history = vae.fit(trainX, trainX, \n",
    "                  batch_size = batch_size, \n",
    "                  epochs = epochs, \n",
    "                  callbacks=[initial_checkpoint, checkpoint, EarlyStoppingAtMinLoss()],\n",
    "                  #callbacks=[initial_checkpoint, checkpoint],\n",
    "                  shuffle=True,\n",
    "                  validation_data=(valX, valX)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 7.0\n"
     ]
    }
   ],
   "source": [
    "# In this GPU implementation, it shows the number of batches/iterations for one epoch (and not the training samples), which is:\n",
    "print (\"Step size:\", np.ceil(trainX.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save the entire model \"\"\"\n",
    "# Save notes about hyperparameters used:\n",
    "epochs = len(history.history['loss'])\n",
    "with open(f'{SAVE_FOLDER}/notes.txt', 'w') as f:\n",
    "    f.write(f'Epochs: {epochs} \\n')\n",
    "    f.write(f'Batch size: {batch_size} \\n')\n",
    "    f.write(f'Latent dimension: {latent_dim} \\n')\n",
    "    f.write(f'Filter sizes: {filters} \\n')\n",
    "    f.write(f'img_width: {img_width} \\n')\n",
    "    f.write(f'img_height: {img_height} \\n')\n",
    "    f.write(f'num_channels: {num_channels}')\n",
    "    f.close()\n",
    "\n",
    "# Save weights for encoder model:\n",
    "encoder.save_weights(f'{SAVE_FOLDER}/ENCODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for decoder model:\n",
    "decoder.save_weights(f'{SAVE_FOLDER}/DECODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for total VAE:\n",
    "vae.save_weights(f'{SAVE_FOLDER}/VAE_WEIGHTS.h5')\n",
    "\n",
    "# Save the history at each epochs (cost of train and validation sets):\n",
    "np.save(f'{SAVE_FOLDER}/HISTORY.npy', history.history)\n",
    "\n",
    "# Save exact training, validation and testing data set (as numpy arrays):\n",
    "np.save(f'{SAVE_FOLDER}/trainX.npy', trainX)\n",
    "np.save(f'{SAVE_FOLDER}/valX.npy', valX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllX.npy', testAllX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllY.npy', testAllY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate training process\n",
    "\n",
    "Now that the training process is complete, let’s evaluate the average loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(13, 6))\n",
    "\n",
    "# summarize history for loss:\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='blue', alpha=0.4, label='validation')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Cost', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.title('Average Loss During Training', fontsize=18)\n",
    "#plt.xticks(range(0,epochs))\n",
    "plt.legend(loc='upper right', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
