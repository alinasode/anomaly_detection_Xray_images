{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import axis, colorbar, imshow, show, figure, subplot\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=18)\n",
    "mpl.rc('xtick', labelsize=16)\n",
    "mpl.rc('ytick', labelsize=16)\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## ----- GPU ------------------------------------\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'    # use of GPU 0 (ERDA) (use before importing torch or tensorflow/keeas)\n",
    "## ----------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization, ReLU, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.activations import relu\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K                         #contains calls for tensor manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL_TRAIN_AUG:       /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/NormalTrainAugmentations\n",
      "NORMAL_VALIDATION_AUG:  /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/NormalValidationAugmentations\n",
      "NORMAL_TEST_AUG:        /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/NormalTestAugmentations\n",
      "ANOMALY1_AUG:           /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/Anomaly1Augmentations\n",
      "ANOMALY2_AUG:           /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/Anomaly2Augmentations\n"
     ]
    }
   ],
   "source": [
    "from utils_vae_potato_paths import *\n",
    "\n",
    "# Get work directions for augmentated data set:\n",
    "print (\"NORMAL_TRAIN_AUG:      \", NORMAL_TRAIN_AUG)\n",
    "print (\"NORMAL_VALIDATION_AUG: \", NORMAL_VAL_AUG)\n",
    "print (\"NORMAL_TEST_AUG:       \", NORMAL_TEST_AUG)\n",
    "print (\"ANOMALY1_AUG:          \", ANOMALY1_AUG)\n",
    "print (\"ANOMALY2_AUG:          \", ANOMALY2_AUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which Folder The Models Are Saved In: saved_models/latent32\n"
     ]
    }
   ],
   "source": [
    "# What latent dimension and filter size are we using?:\n",
    "latent_dim = 32\n",
    "filters    = 32\n",
    "\n",
    "# Get work direction for saving files:\n",
    "SAVE_FOLDER = f'saved_models/latent{latent_dim}'\n",
    "print (\"Which Folder The Models Are Saved In:\", SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] deleting existing files in folder...\n",
      "         done in 0.003 minutes\n"
     ]
    }
   ],
   "source": [
    "# Delete all existing files in folder where models are saved:\n",
    "print(\"[INFO] deleting existing files in folder...\")\n",
    "t0 = time()\n",
    "\n",
    "files = glob.glob(f'{SAVE_FOLDER}/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "print (\"         done in %0.3f minutes\" % ((time() - t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_VAE_AE import get_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Investigation of Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"normal\" training images is:     1639\n",
      "Number of \"normal\" validation images is:   80\n",
      "Number of \"normal\" test images is:         320\n",
      "Total number of \"anomaly\" images is:       680\n"
     ]
    }
   ],
   "source": [
    "# Glob the directories and get the lists of good and bad images\n",
    "good_train_fns = [f for f in os.listdir(NORMAL_TRAIN_AUG) if not f.startswith('.')]\n",
    "good_val_fns = [f for f in os.listdir(NORMAL_VAL_AUG) if not f.startswith('.')]\n",
    "good_test_fns = [f for f in os.listdir(NORMAL_TEST_AUG) if not f.startswith('.')]\n",
    "bad1_fns = [f for f in os.listdir(ANOMALY1_AUG) if not f.startswith('.')]\n",
    "bad2_fns = [f for f in os.listdir(ANOMALY2_AUG) if not f.startswith('.')]\n",
    "\n",
    "print('Number of \"normal\" training images is:     {}'.format(len(good_train_fns)))\n",
    "print('Number of \"normal\" validation images is:   {}'.format(len(good_val_fns)))\n",
    "print('Number of \"normal\" test images is:         {}'.format(len(good_test_fns)))\n",
    "print('Total number of \"anomaly\" images is:       {}'.format(len(bad1_fns) + len(bad2_fns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Procentage of normal samples (ground truth):   74.99 %\n",
      "> Procentage of anomaly samples (ground truth):  25.01 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of bad images vs good images:\n",
    "normal_fns = len(good_train_fns) + len(good_val_fns) + len(good_test_fns)\n",
    "anomaly_fns = len(bad1_fns + bad2_fns)\n",
    "print(f\"> Procentage of normal samples (ground truth):   {(normal_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")\n",
    "print(f\"> Procentage of anomaly samples (ground truth):  {(anomaly_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Normal Data\n",
    "\n",
    "Load normal samples in pre-splitted data sets: train, valdiation and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal training images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal training images...\")\n",
    "trainX = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TRAIN_AUG}/*.png\")]  # read as grayscale\n",
    "trainX = np.array(trainX)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "trainY = get_labels(trainX, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal validation images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal validation images...\")\n",
    "x_normal_val = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_VAL_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_val = np.array(x_normal_val)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_val = get_labels(x_normal_val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal testing images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal testing images...\")\n",
    "x_normal_test = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TEST_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_test = np.array(x_normal_test)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_test = get_labels(x_normal_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Class 1 Data (i.e. 'metal' sampels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly class 1 ('metal') images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly class 1 ('metal') images...\")\n",
    "x_metal = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY1_AUG}/*.png\")]  # read as grayscale\n",
    "x_metal = np.array(x_metal)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_metal = get_labels(x_metal, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Class 2 Data (i.e. 'hollow' sampels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly class 2 ('hollow') images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly class 2 ('hollow') images...\")\n",
    "x_hollow = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY2_AUG}/*.png\")]  # read as grayscale\n",
    "x_hollow = np.array(x_hollow)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_hollow = get_labels(x_hollow, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine normal samples and anomaly samples and split them into training, validation and test sets\n",
    "\n",
    "\n",
    "`label 0` == Normal Samples (=> Perfect' Samples)\n",
    "\n",
    "`label 1` == Anomaly 1 Samples (=> 'Metal' Samples)\n",
    "\n",
    "`label 2` == Anomaly 2 Samples (=> 'Hollow' Samples)\n",
    "\n",
    "Train and Validation sets only consists of `Normal Data`, aka. `label 0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testvalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testvalY = get_labels(testvalX, 0)\n",
    "\n",
    "# randomly split in order to make new validation set:\n",
    "NoUsageX, valX, NoUsageY, valY = train_test_split(testvalX, testvalY, test_size=0.25, random_state=4345672)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Testing set \"\"\"\n",
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testNormalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testNormalY = get_labels(testNormalX, 0)\n",
    "\n",
    "# Anomaly Class 1 (i.e. 'metal' samples):\n",
    "testAnomaly1X, testAnomaly1Y = x_metal, y_metal\n",
    "\n",
    "# Anomaly Class 2 (i.e. 'hollow' samples):\n",
    "testAnomaly2X, testAnomaly2Y = x_hollow, y_hollow\n",
    "\n",
    "# Combine all testing data in one array (the test set is NOT used during any part but inference):\n",
    "testAllX = np.vstack((testNormalX, testAnomaly1X, testAnomaly2X))\n",
    "testAllY = np.hstack((testNormalY, testAnomaly1Y, testAnomaly2Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Dimensions and Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      " > images: (1639, 128, 128)\n",
      " > labels: (1639,)\n",
      "\n",
      "Validation set:\n",
      " > images: (100, 128, 128)\n",
      " > labels: (100,)\n",
      "\n",
      "Test set:\n",
      " > images: (1080, 128, 128)\n",
      " > labels: (1080,)\n",
      "\t'Normal' test set:\n",
      "\t  > images: (400, 128, 128)\n",
      "\t  > labels: (400,)\n",
      "\t'Anomaly 1' test set:\n",
      "\t  > images: (392, 128, 128)\n",
      "\t  > labels: (392,)\n",
      "\t'Anomaly 2' test set:\n",
      "\t  > images: (288, 128, 128)\n",
      "\t  > labels: (288,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "print(\" > images:\", trainX.shape)\n",
    "print(\" > labels:\", trainY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Validation set:\")\n",
    "print(\" > images:\", valX.shape)\n",
    "print(\" > labels:\", valY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Test set:\")\n",
    "print(\" > images:\", testAllX.shape)\n",
    "print(\" > labels:\", testAllY.shape)\n",
    "print(\"\\t'Normal' test set:\")\n",
    "print(\"\\t  > images:\", testNormalX.shape)\n",
    "print(\"\\t  > labels:\", testNormalY.shape)\n",
    "print(\"\\t'Anomaly 1' test set:\")\n",
    "print(\"\\t  > images:\", testAnomaly1X.shape)\n",
    "print(\"\\t  > labels:\", testAnomaly1Y.shape)\n",
    "print(\"\\t'Anomaly 2' test set:\")\n",
    "print(\"\\t  > images:\", testAnomaly2X.shape)\n",
    "print(\"\\t  > labels:\", testAnomaly2Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 76.62 %\n",
      "Procentage of validation samples: 4.68 %\n",
      "Procentage of (only normal) testing samples: 18.70 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets (only normal data):\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (only normal) testing samples: {(len(testNormalX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 58.14 %\n",
      "Procentage of validation samples: 3.55 %\n",
      "Procentage of (total) testing samples: 38.31 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets:\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (total) testing samples: {(len(testAllX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for (un)balanced data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['0: normal', '1: metal', '2: hollow']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,4))\n",
    "ax[0].hist(trainY, align=\"left\"); ax[0].set_title('train', fontsize=18); ax[0].set_xticks([0,1,2]); ax[0].set_xticklabels(labels); ax[0].tick_params(axis='both', which='major', labelsize=16); ax[0].set_xlim(-0.5, 2.5);\n",
    "ax[1].hist(valY, align=\"left\"); ax[1].set_title('validation', fontsize=18); ax[1].set_xticks([0,1,2]); ax[1].set_xticklabels(labels); ax[1].tick_params(axis='both', which='major', labelsize=16); ax[1].set_xlim(-0.5, 2.5);\n",
    "ax[2].hist(testAllY, align=\"left\"); ax[2].set_title('test', fontsize=18); ax[2].set_xticks([0,1,2]); ax[2].set_xticklabels(labels); ax[2].tick_params(axis='both', which='major', labelsize=16); ax[2].set_xlim(-0.5, 2.5);\n",
    "\n",
    "ax[0].set_ylabel(\"Number of images\", fontsize=18)\n",
    "ax[1].set_xlabel(\"Class Labels\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of normal test samples: 37.04 %\n",
      "Procentage of total anomaly test samples: 62.96 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of normal test images vs total anomaly test images:\n",
    "print(f\"Procentage of normal test samples: {(len(testNormalX) / (len(testNormalX) + (len(testAnomaly1X) + len(testAnomaly2X)))) * 100:.2f} %\")\n",
    "print(f\"Procentage of total anomaly test samples: {((len(testAnomaly1X) + len(testAnomaly2X)) / (len(testNormalX) + (len(testAnomaly1X) + len(testAnomaly2X)))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Only normalization has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "img_width, img_height = trainX.shape[1], trainX.shape[2]      # input image dimensions\n",
    "num_channels = 1                                              # gray-channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Neural Networks learns faster with normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to be in  the [0, 1] range:\n",
    "trainX = trainX.astype('float32') / 255.\n",
    "valX = valX.astype('float32') / 255.\n",
    "testAllX = testAllX.astype('float32') / 255.\n",
    "\n",
    "# reshape data to keras inputs --> (n_samples, img_rows, img_cols, n_channels):\n",
    "trainX = trainX.reshape(trainX.shape[0], img_height, img_width, num_channels)\n",
    "valX = valX.reshape(valX.shape[0], img_height, img_width, num_channels)\n",
    "testAllX = testAllX.reshape(testAllX.shape[0], img_height, img_width, num_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import architecture of VAE model and its hyperparameters:\n",
    "from J32_VAE_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Encoder Part*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 32)   320         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 64, 64, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 32)   9248        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 32, 32, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 32)   9248        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 16, 16, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 8, 32)     9248        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 8, 8, 32)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 4, 4, 32)     9248        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 4, 4, 32)     0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "latent (Dense)                  (None, 200)          102600      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 200)          0           latent[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 32)           6432        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_log_mean (Dense)              (None, 32)           6432        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 32)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_mean[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 152,776\n",
      "Trainable params: 152,776\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Decoder Part*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               16896     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 128, 128, 1)       289       \n",
      "=================================================================\n",
      "Total params: 63,425\n",
      "Trainable params: 63,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Total VAE Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         [(None, 32), (None, 32),  152776    \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 128, 128, 1)       63425     \n",
      "=================================================================\n",
      "Total params: 216,201\n",
      "Trainable params: 216,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "batch_size  = 256\n",
    "epochs      = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at 1'st epoch (inital model)\n",
    "\n",
    "https://stackoverflow.com/questions/54323960/save-keras-model-at-specific-epochs\n",
    "\"\"\"\n",
    "\n",
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 1:\n",
    "            self.model.save_weights(f\"{SAVE_FOLDER}/cp-0001.h5\")\n",
    "            \n",
    "# create and use callback:\n",
    "initial_checkpoint = CustomSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at every k'te epochs\n",
    "\"\"\"\n",
    "# Include the epoch in the file name (uses `str.format`):\n",
    "checkpoint_path = \"saved_models/latent32/cp-{epoch:04d}.h5\"\n",
    "\n",
    "# Create a callback that saves the model's weights every k'te epochs:\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path,\n",
    "                             monitor='loss',           # name of the metrics to monitor\n",
    "                             verbose=1, \n",
    "                             #save_best_only=False,     # if False, the best model will not be overridden.\n",
    "                             save_weights_only=True,   # if True, only the weights of the models will be saved.\n",
    "                                                        # if False, the whole models will be saved.\n",
    "                             #mode='auto', \n",
    "                             period=200                  # save after every k'te epochs\n",
    "                            )\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "vae.save_weights(checkpoint_path.format(epoch=0))          # save for zero epoch (inital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: Early stopping\n",
    "https://www.tensorflow.org/guide/keras/custom_callback\n",
    "\"\"\"\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=100):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create and Compile Model\"\"\"\n",
    "# import architecture of VAE model and its hyperparameters. learning rate choosen from graph above.\n",
    "vae = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "7/7 [==============================] - 1s 192ms/step - loss: 2321.4905 - val_loss: 2250.3943\n",
      "Epoch 2/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 2080.9673 - val_loss: 1738.6267\n",
      "Epoch 3/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 1448.2098 - val_loss: 1200.1754\n",
      "Epoch 4/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 990.3132 - val_loss: 785.1385\n",
      "Epoch 5/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 673.7469 - val_loss: 557.1516\n",
      "Epoch 6/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 495.5792 - val_loss: 440.6770\n",
      "Epoch 7/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 414.9588 - val_loss: 394.2587\n",
      "Epoch 8/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 377.2359 - val_loss: 369.9222\n",
      "Epoch 9/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 354.0742 - val_loss: 349.7461\n",
      "Epoch 10/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 337.0358 - val_loss: 346.6869\n",
      "Epoch 11/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 335.6043 - val_loss: 336.3764\n",
      "Epoch 12/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 323.1658 - val_loss: 320.4962\n",
      "Epoch 13/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 312.7601 - val_loss: 315.1949\n",
      "Epoch 14/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 307.1380 - val_loss: 312.0323\n",
      "Epoch 15/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 301.0795 - val_loss: 299.8173\n",
      "Epoch 16/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 293.1208 - val_loss: 302.9286\n",
      "Epoch 17/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 291.3828 - val_loss: 293.9725\n",
      "Epoch 18/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 282.3100 - val_loss: 290.6540\n",
      "Epoch 19/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 278.8645 - val_loss: 283.3772\n",
      "Epoch 20/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 276.8132 - val_loss: 280.8050\n",
      "Epoch 21/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 272.4767 - val_loss: 272.5880\n",
      "Epoch 22/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 267.3729 - val_loss: 271.3721\n",
      "Epoch 23/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 262.8870 - val_loss: 265.0371\n",
      "Epoch 24/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 260.0855 - val_loss: 262.7614\n",
      "Epoch 25/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 258.0957 - val_loss: 257.5446\n",
      "Epoch 26/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 253.9073 - val_loss: 254.6899\n",
      "Epoch 27/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 250.7400 - val_loss: 254.1345\n",
      "Epoch 28/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 247.4597 - val_loss: 248.1898\n",
      "Epoch 29/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 244.9695 - val_loss: 256.6779\n",
      "Epoch 30/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 247.8918 - val_loss: 245.5518\n",
      "Epoch 31/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 243.3170 - val_loss: 243.4452\n",
      "Epoch 32/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 239.2526 - val_loss: 242.9346\n",
      "Epoch 33/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 234.6288 - val_loss: 230.8699\n",
      "Epoch 34/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 228.2704 - val_loss: 228.7686\n",
      "Epoch 35/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 224.4529 - val_loss: 226.3916\n",
      "Epoch 36/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 221.2622 - val_loss: 222.9394\n",
      "Epoch 37/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 218.1847 - val_loss: 218.5209\n",
      "Epoch 38/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 216.3633 - val_loss: 217.1155\n",
      "Epoch 39/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 214.2315 - val_loss: 214.0384\n",
      "Epoch 40/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 210.6134 - val_loss: 212.0722\n",
      "Epoch 41/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 209.1898 - val_loss: 208.6009\n",
      "Epoch 42/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 207.3134 - val_loss: 207.0731\n",
      "Epoch 43/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 205.3186 - val_loss: 206.6284\n",
      "Epoch 44/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 203.8097 - val_loss: 201.2009\n",
      "Epoch 45/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 200.7664 - val_loss: 202.5916\n",
      "Epoch 46/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 198.5647 - val_loss: 198.3830\n",
      "Epoch 47/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 197.8725 - val_loss: 197.7286\n",
      "Epoch 48/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 194.6236 - val_loss: 194.7587\n",
      "Epoch 49/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 193.6308 - val_loss: 193.6214\n",
      "Epoch 50/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 191.0643 - val_loss: 193.1639\n",
      "Epoch 51/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 189.9956 - val_loss: 191.4059\n",
      "Epoch 52/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 187.6776 - val_loss: 187.2171\n",
      "Epoch 53/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 186.7225 - val_loss: 187.1074\n",
      "Epoch 54/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 184.3342 - val_loss: 184.1448\n",
      "Epoch 55/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 182.5249 - val_loss: 182.5210\n",
      "Epoch 56/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 180.7426 - val_loss: 183.2963\n",
      "Epoch 57/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 180.5157 - val_loss: 181.0740\n",
      "Epoch 58/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 177.9962 - val_loss: 178.2000\n",
      "Epoch 59/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 176.1310 - val_loss: 175.8441\n",
      "Epoch 60/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 175.2709 - val_loss: 176.3969\n",
      "Epoch 61/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 174.8299 - val_loss: 173.5420\n",
      "Epoch 62/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 172.4463 - val_loss: 172.0504\n",
      "Epoch 63/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 170.5647 - val_loss: 171.9526\n",
      "Epoch 64/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 168.5386 - val_loss: 170.2597\n",
      "Epoch 65/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 167.3003 - val_loss: 175.8396\n",
      "Epoch 66/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 165.6932 - val_loss: 165.3664\n",
      "Epoch 67/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 163.5686 - val_loss: 163.7616\n",
      "Epoch 68/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 162.5869 - val_loss: 162.6698\n",
      "Epoch 69/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 160.9808 - val_loss: 162.5068\n",
      "Epoch 70/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 159.5637 - val_loss: 160.8655\n",
      "Epoch 71/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 158.9178 - val_loss: 158.8374\n",
      "Epoch 72/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 157.7637 - val_loss: 158.0553\n",
      "Epoch 73/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 155.1754 - val_loss: 155.3231\n",
      "Epoch 74/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 155.0162 - val_loss: 154.6850\n",
      "Epoch 75/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 153.2491 - val_loss: 153.5670\n",
      "Epoch 76/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 151.8535 - val_loss: 153.9797\n",
      "Epoch 77/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 151.0779 - val_loss: 151.7216\n",
      "Epoch 78/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 147.7087 - val_loss: 146.5236\n",
      "Epoch 79/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 146.5403 - val_loss: 148.8231\n",
      "Epoch 80/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 145.8127 - val_loss: 144.8016\n",
      "Epoch 81/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 143.5878 - val_loss: 145.1657\n",
      "Epoch 82/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 142.7911 - val_loss: 144.9626\n",
      "Epoch 83/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 141.7276 - val_loss: 143.5477\n",
      "Epoch 84/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 140.5813 - val_loss: 141.9923\n",
      "Epoch 85/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 139.9298 - val_loss: 141.2831\n",
      "Epoch 86/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 137.7344 - val_loss: 140.5475\n",
      "Epoch 87/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 136.6391 - val_loss: 137.9712\n",
      "Epoch 88/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 134.8561 - val_loss: 136.3311\n",
      "Epoch 89/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 134.9351 - val_loss: 134.4630\n",
      "Epoch 90/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 132.6684 - val_loss: 134.8869\n",
      "Epoch 91/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 131.6485 - val_loss: 132.8182\n",
      "Epoch 92/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 130.3317 - val_loss: 132.1331\n",
      "Epoch 93/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 129.5664 - val_loss: 129.2389\n",
      "Epoch 94/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 128.5508 - val_loss: 128.1996\n",
      "Epoch 95/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 127.4217 - val_loss: 130.0812\n",
      "Epoch 96/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 126.0971 - val_loss: 128.1141\n",
      "Epoch 97/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 124.4846 - val_loss: 125.6361\n",
      "Epoch 98/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 124.0243 - val_loss: 125.8016\n",
      "Epoch 99/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 122.4580 - val_loss: 122.6533\n",
      "Epoch 100/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 121.3544 - val_loss: 122.3845\n",
      "Epoch 101/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 120.3979 - val_loss: 121.9065\n",
      "Epoch 102/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 119.2459 - val_loss: 120.3979\n",
      "Epoch 103/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 119.3127 - val_loss: 120.5634\n",
      "Epoch 104/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 118.1107 - val_loss: 118.7774\n",
      "Epoch 105/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 116.4910 - val_loss: 117.3911\n",
      "Epoch 106/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 115.9469 - val_loss: 115.9718\n",
      "Epoch 107/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 115.5726 - val_loss: 115.9550\n",
      "Epoch 108/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 113.7529 - val_loss: 113.9180\n",
      "Epoch 109/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 113.1128 - val_loss: 113.0142\n",
      "Epoch 110/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 111.7659 - val_loss: 113.0967\n",
      "Epoch 111/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 111.3829 - val_loss: 113.0998\n",
      "Epoch 112/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 110.4485 - val_loss: 112.2720\n",
      "Epoch 113/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 109.6270 - val_loss: 110.7876\n",
      "Epoch 114/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 110.0888 - val_loss: 108.8710\n",
      "Epoch 115/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 108.6972 - val_loss: 109.5543\n",
      "Epoch 116/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 107.1876 - val_loss: 109.9387\n",
      "Epoch 117/2000\n",
      "7/7 [==============================] - 1s 108ms/step - loss: 106.5075 - val_loss: 106.8916\n",
      "Epoch 118/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 105.6635 - val_loss: 106.5739\n",
      "Epoch 119/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 104.6205 - val_loss: 106.7864\n",
      "Epoch 120/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 104.8832 - val_loss: 107.1541\n",
      "Epoch 121/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 104.5446 - val_loss: 104.9278\n",
      "Epoch 122/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 102.9474 - val_loss: 103.6183\n",
      "Epoch 123/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 101.6070 - val_loss: 101.6641\n",
      "Epoch 124/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 100.7481 - val_loss: 102.2756\n",
      "Epoch 125/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 100.0429 - val_loss: 100.8304\n",
      "Epoch 126/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 100.1049 - val_loss: 101.7749\n",
      "Epoch 127/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 99.8202 - val_loss: 99.9786\n",
      "Epoch 128/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 98.5638 - val_loss: 99.2255\n",
      "Epoch 129/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 97.7550 - val_loss: 98.9254\n",
      "Epoch 130/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 96.7232 - val_loss: 97.9694\n",
      "Epoch 131/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 95.7858 - val_loss: 96.6716\n",
      "Epoch 132/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 95.5935 - val_loss: 96.0280\n",
      "Epoch 133/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 94.4704 - val_loss: 97.1798\n",
      "Epoch 134/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 94.5445 - val_loss: 95.4168\n",
      "Epoch 135/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 93.9668 - val_loss: 95.0706\n",
      "Epoch 136/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 93.1238 - val_loss: 94.5134\n",
      "Epoch 137/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 91.7625 - val_loss: 94.1272\n",
      "Epoch 138/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 91.1738 - val_loss: 92.6838\n",
      "Epoch 139/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 91.0748 - val_loss: 93.4903\n",
      "Epoch 140/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 90.5404 - val_loss: 91.5790\n",
      "Epoch 141/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 89.3639 - val_loss: 90.3769\n",
      "Epoch 142/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 88.7850 - val_loss: 89.4045\n",
      "Epoch 143/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 88.5131 - val_loss: 88.6304\n",
      "Epoch 144/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 87.5926 - val_loss: 88.6688\n",
      "Epoch 145/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 87.2490 - val_loss: 88.3578\n",
      "Epoch 146/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 86.7773 - val_loss: 87.6830\n",
      "Epoch 147/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 86.3006 - val_loss: 87.3883\n",
      "Epoch 148/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 85.2050 - val_loss: 87.0938\n",
      "Epoch 149/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 85.5313 - val_loss: 85.2254\n",
      "Epoch 150/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 84.4477 - val_loss: 84.8989\n",
      "Epoch 151/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 83.5082 - val_loss: 84.5434\n",
      "Epoch 152/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 83.1794 - val_loss: 85.2857\n",
      "Epoch 153/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 82.6203 - val_loss: 83.5501\n",
      "Epoch 154/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 82.2378 - val_loss: 82.8676\n",
      "Epoch 155/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 81.1340 - val_loss: 82.4908\n",
      "Epoch 156/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 81.1154 - val_loss: 82.2905\n",
      "Epoch 157/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 80.2898 - val_loss: 81.7115\n",
      "Epoch 158/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 79.8507 - val_loss: 81.0457\n",
      "Epoch 159/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 79.8470 - val_loss: 80.4659\n",
      "Epoch 160/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 78.9685 - val_loss: 80.1658\n",
      "Epoch 161/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 79.0009 - val_loss: 78.6830\n",
      "Epoch 162/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 78.3537 - val_loss: 79.2758\n",
      "Epoch 163/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 78.2472 - val_loss: 78.5746\n",
      "Epoch 164/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 77.7559 - val_loss: 79.2631\n",
      "Epoch 165/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 77.2878 - val_loss: 78.6345\n",
      "Epoch 166/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 76.6337 - val_loss: 77.7650\n",
      "Epoch 167/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 76.1337 - val_loss: 77.1753\n",
      "Epoch 168/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 75.4491 - val_loss: 75.8899\n",
      "Epoch 169/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 74.7143 - val_loss: 75.5821\n",
      "Epoch 170/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 75.0791 - val_loss: 76.7507\n",
      "Epoch 171/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 74.2432 - val_loss: 75.1826\n",
      "Epoch 172/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 73.2638 - val_loss: 75.9507\n",
      "Epoch 173/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 73.2058 - val_loss: 73.6759\n",
      "Epoch 174/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 73.1204 - val_loss: 73.3944\n",
      "Epoch 175/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 72.1236 - val_loss: 73.5468\n",
      "Epoch 176/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 72.1976 - val_loss: 73.2525\n",
      "Epoch 177/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 71.6916 - val_loss: 72.4381\n",
      "Epoch 178/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 71.1767 - val_loss: 72.1460\n",
      "Epoch 179/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 71.2515 - val_loss: 72.5716\n",
      "Epoch 180/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 70.6379 - val_loss: 71.3788\n",
      "Epoch 181/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 70.1124 - val_loss: 70.5874\n",
      "Epoch 182/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 69.5483 - val_loss: 70.3581\n",
      "Epoch 183/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 69.4975 - val_loss: 70.7543\n",
      "Epoch 184/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 69.6874 - val_loss: 70.4203\n",
      "Epoch 185/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 68.9572 - val_loss: 71.3589\n",
      "Epoch 186/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 69.3239 - val_loss: 69.5928\n",
      "Epoch 187/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 68.9540 - val_loss: 68.3367\n",
      "Epoch 188/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 67.9970 - val_loss: 68.9831\n",
      "Epoch 189/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 67.5991 - val_loss: 68.3098\n",
      "Epoch 190/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 66.5554 - val_loss: 67.0436\n",
      "Epoch 191/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 66.3720 - val_loss: 67.2473\n",
      "Epoch 192/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 65.9070 - val_loss: 67.1560\n",
      "Epoch 193/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 65.8265 - val_loss: 67.2888\n",
      "Epoch 194/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 65.8750 - val_loss: 65.7517\n",
      "Epoch 195/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 64.9106 - val_loss: 65.2962\n",
      "Epoch 196/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 64.6055 - val_loss: 65.9542\n",
      "Epoch 197/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 64.2587 - val_loss: 65.5496\n",
      "Epoch 198/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 64.0577 - val_loss: 64.1262\n",
      "Epoch 199/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 63.7383 - val_loss: 64.9005\n",
      "Epoch 200/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 63.4195\n",
      "Epoch 00200: saving model to saved_models/latent32/cp-0200.h5\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 63.4195 - val_loss: 64.3914\n",
      "Epoch 201/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 63.1305 - val_loss: 64.2683\n",
      "Epoch 202/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 63.0376 - val_loss: 64.5152\n",
      "Epoch 203/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 62.9367 - val_loss: 62.9997\n",
      "Epoch 204/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 62.0083 - val_loss: 63.4122\n",
      "Epoch 205/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.3796 - val_loss: 64.1365\n",
      "Epoch 206/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 61.9345 - val_loss: 62.9200\n",
      "Epoch 207/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 61.3912 - val_loss: 62.2573\n",
      "Epoch 208/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 60.9443 - val_loss: 62.3112\n",
      "Epoch 209/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 60.6831 - val_loss: 61.6276\n",
      "Epoch 210/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 60.8199 - val_loss: 60.8288\n",
      "Epoch 211/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 60.4053 - val_loss: 61.1956\n",
      "Epoch 212/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 60.0552 - val_loss: 60.4575\n",
      "Epoch 213/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 59.8429 - val_loss: 59.5487\n",
      "Epoch 214/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 59.2781 - val_loss: 60.5647\n",
      "Epoch 215/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 59.1253 - val_loss: 60.7773\n",
      "Epoch 216/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 58.8688 - val_loss: 58.8188\n",
      "Epoch 217/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 58.7099 - val_loss: 58.9499\n",
      "Epoch 218/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 58.2182 - val_loss: 59.1466\n",
      "Epoch 219/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 58.1744 - val_loss: 59.2558\n",
      "Epoch 220/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 57.6946 - val_loss: 58.2732\n",
      "Epoch 221/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 57.4036 - val_loss: 58.4678\n",
      "Epoch 222/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.6176 - val_loss: 58.1040\n",
      "Epoch 223/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 57.0420 - val_loss: 57.3268\n",
      "Epoch 224/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 56.2580 - val_loss: 57.3405\n",
      "Epoch 225/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 56.1813 - val_loss: 56.9954\n",
      "Epoch 226/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 55.9862 - val_loss: 56.9713\n",
      "Epoch 227/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 55.9516 - val_loss: 56.8790\n",
      "Epoch 228/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 55.7776 - val_loss: 56.5700\n",
      "Epoch 229/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 55.5561 - val_loss: 55.7951\n",
      "Epoch 230/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.8606 - val_loss: 56.3976\n",
      "Epoch 231/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.8519 - val_loss: 55.8304\n",
      "Epoch 232/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.7639 - val_loss: 55.0862\n",
      "Epoch 233/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.3759 - val_loss: 55.5644\n",
      "Epoch 234/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.1993 - val_loss: 55.0261\n",
      "Epoch 235/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.9063 - val_loss: 54.6739\n",
      "Epoch 236/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.4735 - val_loss: 55.0659\n",
      "Epoch 237/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.5476 - val_loss: 53.9426\n",
      "Epoch 238/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.7030 - val_loss: 53.6329\n",
      "Epoch 239/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 53.0405 - val_loss: 53.7880\n",
      "Epoch 240/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 52.8888 - val_loss: 54.0066\n",
      "Epoch 241/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.7073 - val_loss: 54.3421\n",
      "Epoch 242/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 52.7197 - val_loss: 53.3049\n",
      "Epoch 243/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.3736 - val_loss: 52.5161\n",
      "Epoch 244/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 51.9384 - val_loss: 53.4248\n",
      "Epoch 245/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 51.9458 - val_loss: 52.7200\n",
      "Epoch 246/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 51.7501 - val_loss: 53.2623\n",
      "Epoch 247/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 52.1150 - val_loss: 52.9149\n",
      "Epoch 248/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 51.8540 - val_loss: 52.1756\n",
      "Epoch 249/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 51.0739 - val_loss: 51.9915\n",
      "Epoch 250/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 51.3439 - val_loss: 52.1523\n",
      "Epoch 251/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.5395 - val_loss: 51.4646\n",
      "Epoch 252/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.5888 - val_loss: 51.2378\n",
      "Epoch 253/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.6186 - val_loss: 51.7733\n",
      "Epoch 254/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.9978 - val_loss: 50.6701\n",
      "Epoch 255/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.1909 - val_loss: 50.5798\n",
      "Epoch 256/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.8058 - val_loss: 51.6288\n",
      "Epoch 257/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 49.7048 - val_loss: 50.9618\n",
      "Epoch 258/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.5548 - val_loss: 50.0120\n",
      "Epoch 259/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.5092 - val_loss: 51.9919\n",
      "Epoch 260/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.1132 - val_loss: 51.7705\n",
      "Epoch 261/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.6324 - val_loss: 50.1067\n",
      "Epoch 262/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.3471 - val_loss: 49.2263\n",
      "Epoch 263/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.0890 - val_loss: 50.4206\n",
      "Epoch 264/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.9661 - val_loss: 48.8736\n",
      "Epoch 265/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.5690 - val_loss: 49.9855\n",
      "Epoch 266/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.9697 - val_loss: 48.6234\n",
      "Epoch 267/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.4187 - val_loss: 48.9351\n",
      "Epoch 268/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.8248 - val_loss: 49.1427\n",
      "Epoch 269/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 47.7488 - val_loss: 49.3344\n",
      "Epoch 270/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 47.5155 - val_loss: 48.6869\n",
      "Epoch 271/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.5570 - val_loss: 48.9398\n",
      "Epoch 272/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.7110 - val_loss: 48.2268\n",
      "Epoch 273/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.4326 - val_loss: 47.6363\n",
      "Epoch 274/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.8728 - val_loss: 47.5679\n",
      "Epoch 275/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.9223 - val_loss: 48.1974\n",
      "Epoch 276/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.7669 - val_loss: 47.8349\n",
      "Epoch 277/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.7494 - val_loss: 47.0502\n",
      "Epoch 278/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.4145 - val_loss: 47.2375\n",
      "Epoch 279/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.3468 - val_loss: 48.1721\n",
      "Epoch 280/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 45.9631 - val_loss: 46.3795\n",
      "Epoch 281/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.5833 - val_loss: 46.8008\n",
      "Epoch 282/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.7859 - val_loss: 46.4283\n",
      "Epoch 283/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.4899 - val_loss: 46.0128\n",
      "Epoch 284/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.6080 - val_loss: 46.0273\n",
      "Epoch 285/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.2247 - val_loss: 46.3197\n",
      "Epoch 286/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.4700 - val_loss: 45.8349\n",
      "Epoch 287/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.2234 - val_loss: 46.9486\n",
      "Epoch 288/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.5405 - val_loss: 45.7674\n",
      "Epoch 289/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.2907 - val_loss: 45.7528\n",
      "Epoch 290/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 44.6888 - val_loss: 46.1170\n",
      "Epoch 291/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.8895 - val_loss: 45.3496\n",
      "Epoch 292/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7499 - val_loss: 45.1163\n",
      "Epoch 293/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.2494 - val_loss: 45.6908\n",
      "Epoch 294/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.0818 - val_loss: 45.1394\n",
      "Epoch 295/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.8970 - val_loss: 44.6667\n",
      "Epoch 296/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.8072 - val_loss: 44.0183\n",
      "Epoch 297/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 43.7759 - val_loss: 44.1550\n",
      "Epoch 298/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.6518 - val_loss: 45.2937\n",
      "Epoch 299/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 43.4849 - val_loss: 43.6768\n",
      "Epoch 300/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 43.1556 - val_loss: 44.1829\n",
      "Epoch 301/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3382 - val_loss: 44.2662\n",
      "Epoch 302/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2259 - val_loss: 43.3635\n",
      "Epoch 303/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.9742 - val_loss: 44.0934\n",
      "Epoch 304/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 42.8535 - val_loss: 43.5542\n",
      "Epoch 305/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.8192 - val_loss: 43.4596\n",
      "Epoch 306/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.8293 - val_loss: 43.2356\n",
      "Epoch 307/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.7729 - val_loss: 43.8545\n",
      "Epoch 308/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.5601 - val_loss: 44.2756\n",
      "Epoch 309/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 42.3314 - val_loss: 44.0651\n",
      "Epoch 310/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.3211 - val_loss: 42.6547\n",
      "Epoch 311/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.1465 - val_loss: 43.3728\n",
      "Epoch 312/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.7298 - val_loss: 42.9104\n",
      "Epoch 313/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.2159 - val_loss: 42.3571\n",
      "Epoch 314/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.0241 - val_loss: 42.8347\n",
      "Epoch 315/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.6933 - val_loss: 43.0372\n",
      "Epoch 316/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.7560 - val_loss: 42.4910\n",
      "Epoch 317/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.4064 - val_loss: 43.2819\n",
      "Epoch 318/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.3944 - val_loss: 42.4117\n",
      "Epoch 319/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.5833 - val_loss: 43.0933\n",
      "Epoch 320/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.3766 - val_loss: 42.3853\n",
      "Epoch 321/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.2655 - val_loss: 41.7135\n",
      "Epoch 322/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 40.9291 - val_loss: 42.2167\n",
      "Epoch 323/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.0200 - val_loss: 41.4988\n",
      "Epoch 324/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.9995 - val_loss: 42.0756\n",
      "Epoch 325/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.9679 - val_loss: 42.2899\n",
      "Epoch 326/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.7862 - val_loss: 41.3632\n",
      "Epoch 327/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.8354 - val_loss: 41.4964\n",
      "Epoch 328/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.5685 - val_loss: 41.7945\n",
      "Epoch 329/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.5457 - val_loss: 41.7357\n",
      "Epoch 330/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.6374 - val_loss: 42.7668\n",
      "Epoch 331/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.8984 - val_loss: 44.9644\n",
      "Epoch 332/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.3490 - val_loss: 41.4601\n",
      "Epoch 333/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 40.3723 - val_loss: 43.9372\n",
      "Epoch 334/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.1231 - val_loss: 41.2938\n",
      "Epoch 335/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.0055 - val_loss: 41.5190\n",
      "Epoch 336/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.6186 - val_loss: 40.9424\n",
      "Epoch 337/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.6117 - val_loss: 40.5850\n",
      "Epoch 338/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.7364 - val_loss: 40.7200\n",
      "Epoch 339/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.8783 - val_loss: 39.9929\n",
      "Epoch 340/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.3773 - val_loss: 40.6013\n",
      "Epoch 341/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.0331 - val_loss: 40.6900\n",
      "Epoch 342/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.5599 - val_loss: 41.5447\n",
      "Epoch 343/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.3564 - val_loss: 40.1165\n",
      "Epoch 344/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.0280 - val_loss: 40.5547\n",
      "Epoch 345/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.1849 - val_loss: 39.8307\n",
      "Epoch 346/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.8374 - val_loss: 39.9210\n",
      "Epoch 347/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.7538 - val_loss: 39.6007\n",
      "Epoch 348/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.4038 - val_loss: 39.3713\n",
      "Epoch 349/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.3512 - val_loss: 39.7323\n",
      "Epoch 350/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.2756 - val_loss: 39.4199\n",
      "Epoch 351/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.5786 - val_loss: 40.3497\n",
      "Epoch 352/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.9971 - val_loss: 40.1700\n",
      "Epoch 353/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.6289 - val_loss: 39.4854\n",
      "Epoch 354/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.3826 - val_loss: 39.9093\n",
      "Epoch 355/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.9539 - val_loss: 39.6159\n",
      "Epoch 356/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.1736 - val_loss: 39.4451\n",
      "Epoch 357/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.1023 - val_loss: 40.0814\n",
      "Epoch 358/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.8455 - val_loss: 39.1297\n",
      "Epoch 359/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.8185 - val_loss: 38.8923\n",
      "Epoch 360/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.7927 - val_loss: 39.2081\n",
      "Epoch 361/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.9164 - val_loss: 39.7342\n",
      "Epoch 362/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.5606 - val_loss: 38.4119\n",
      "Epoch 363/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.8028 - val_loss: 38.8328\n",
      "Epoch 364/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.5551 - val_loss: 38.8982\n",
      "Epoch 365/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 37.3715 - val_loss: 38.8291\n",
      "Epoch 366/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.2915 - val_loss: 38.1727\n",
      "Epoch 367/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.4528 - val_loss: 38.8847\n",
      "Epoch 368/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.1670 - val_loss: 37.9421\n",
      "Epoch 369/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.0935 - val_loss: 37.9912\n",
      "Epoch 370/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.9874 - val_loss: 37.8003\n",
      "Epoch 371/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.1846 - val_loss: 37.9320\n",
      "Epoch 372/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.0585 - val_loss: 37.4705\n",
      "Epoch 373/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.7198 - val_loss: 37.2501\n",
      "Epoch 374/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.6432 - val_loss: 37.8170\n",
      "Epoch 375/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.7857 - val_loss: 37.3970\n",
      "Epoch 376/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.6798 - val_loss: 38.4126\n",
      "Epoch 377/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.9244 - val_loss: 37.8796\n",
      "Epoch 378/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.4321 - val_loss: 38.8551\n",
      "Epoch 379/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.0994 - val_loss: 38.1959\n",
      "Epoch 380/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.8128 - val_loss: 39.2333\n",
      "Epoch 381/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.6052 - val_loss: 38.3943\n",
      "Epoch 382/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.3743 - val_loss: 38.2680\n",
      "Epoch 383/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.6641 - val_loss: 38.7053\n",
      "Epoch 384/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.4179 - val_loss: 37.7183\n",
      "Epoch 385/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.0639 - val_loss: 36.8226\n",
      "Epoch 386/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 35.9318 - val_loss: 37.9319\n",
      "Epoch 387/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.8789 - val_loss: 38.9983\n",
      "Epoch 388/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.1309 - val_loss: 36.4273\n",
      "Epoch 389/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.8538 - val_loss: 37.0201\n",
      "Epoch 390/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.7792 - val_loss: 36.4205\n",
      "Epoch 391/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.6119 - val_loss: 36.9299\n",
      "Epoch 392/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.5844 - val_loss: 37.9766\n",
      "Epoch 393/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.4492 - val_loss: 36.5974\n",
      "Epoch 394/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.6110 - val_loss: 36.7893\n",
      "Epoch 395/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.6126 - val_loss: 37.0162\n",
      "Epoch 396/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.0293 - val_loss: 38.1353\n",
      "Epoch 397/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.6565 - val_loss: 36.8386\n",
      "Epoch 398/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.2883 - val_loss: 37.2671\n",
      "Epoch 399/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 35.1442 - val_loss: 37.0006\n",
      "Epoch 400/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 34.8685\n",
      "Epoch 00400: saving model to saved_models/latent32/cp-0400.h5\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 34.8685 - val_loss: 36.7777\n",
      "Epoch 401/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.8848 - val_loss: 36.1313\n",
      "Epoch 402/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.8626 - val_loss: 36.2401\n",
      "Epoch 403/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.9977 - val_loss: 36.4272\n",
      "Epoch 404/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.7646 - val_loss: 35.7951\n",
      "Epoch 405/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.6755 - val_loss: 37.7416\n",
      "Epoch 406/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.6996 - val_loss: 35.9493\n",
      "Epoch 407/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.6648 - val_loss: 36.0591\n",
      "Epoch 408/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.5196 - val_loss: 35.8590\n",
      "Epoch 409/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.5326 - val_loss: 36.1570\n",
      "Epoch 410/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.4619 - val_loss: 35.2304\n",
      "Epoch 411/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.4086 - val_loss: 35.8902\n",
      "Epoch 412/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.4283 - val_loss: 35.5448\n",
      "Epoch 413/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.4811 - val_loss: 35.8768\n",
      "Epoch 414/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 34.1741 - val_loss: 35.7085\n",
      "Epoch 415/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.4021 - val_loss: 36.1424\n",
      "Epoch 416/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.5046 - val_loss: 36.9323\n",
      "Epoch 417/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.1428 - val_loss: 35.7409\n",
      "Epoch 418/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.2103 - val_loss: 35.2603\n",
      "Epoch 419/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1636 - val_loss: 35.9958\n",
      "Epoch 420/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1942 - val_loss: 36.2880\n",
      "Epoch 421/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.7924 - val_loss: 35.3724\n",
      "Epoch 422/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.0322 - val_loss: 35.3419\n",
      "Epoch 423/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1272 - val_loss: 34.3828\n",
      "Epoch 424/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.8311 - val_loss: 34.7949\n",
      "Epoch 425/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 33.7555 - val_loss: 36.2353\n",
      "Epoch 426/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.0358 - val_loss: 36.1932\n",
      "Epoch 427/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.1384 - val_loss: 37.4896\n",
      "Epoch 428/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.0949 - val_loss: 35.3743\n",
      "Epoch 429/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 33.4048 - val_loss: 41.9217\n",
      "Epoch 430/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.3087 - val_loss: 35.6518\n",
      "Epoch 431/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.3500 - val_loss: 35.5681\n",
      "Epoch 432/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.5655 - val_loss: 36.4504\n",
      "Epoch 433/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.1802 - val_loss: 35.5342\n",
      "Epoch 434/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.6181 - val_loss: 37.4754\n",
      "Epoch 435/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.7769 - val_loss: 35.5296\n",
      "Epoch 436/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 33.3066 - val_loss: 36.0168\n",
      "Epoch 437/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.2492 - val_loss: 34.4772\n",
      "Epoch 438/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.1404 - val_loss: 33.8351\n",
      "Epoch 439/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 33.0988 - val_loss: 34.2322\n",
      "Epoch 440/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.0296 - val_loss: 33.7393\n",
      "Epoch 441/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 32.7767 - val_loss: 34.0156\n",
      "Epoch 442/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.8353 - val_loss: 34.7710\n",
      "Epoch 443/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.9363 - val_loss: 34.3448\n",
      "Epoch 444/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.3107 - val_loss: 34.7971\n",
      "Epoch 445/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.1499 - val_loss: 33.7852\n",
      "Epoch 446/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.8079 - val_loss: 34.3435\n",
      "Epoch 447/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.8181 - val_loss: 36.2688\n",
      "Epoch 448/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.8212 - val_loss: 33.8301\n",
      "Epoch 449/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 32.7619 - val_loss: 34.0506\n",
      "Epoch 450/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.4816 - val_loss: 34.0846\n",
      "Epoch 451/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5171 - val_loss: 35.8000\n",
      "Epoch 452/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5281 - val_loss: 33.9020\n",
      "Epoch 453/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.3227 - val_loss: 33.5948\n",
      "Epoch 454/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.1394 - val_loss: 33.6136\n",
      "Epoch 455/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.3106 - val_loss: 32.6536\n",
      "Epoch 456/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.2442 - val_loss: 33.5852\n",
      "Epoch 457/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5170 - val_loss: 33.6066\n",
      "Epoch 458/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.4120 - val_loss: 33.0913\n",
      "Epoch 459/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.2612 - val_loss: 33.6198\n",
      "Epoch 460/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 32.1147 - val_loss: 33.9623\n",
      "Epoch 461/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.9012 - val_loss: 33.2132\n",
      "Epoch 462/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.4841 - val_loss: 34.1286\n",
      "Epoch 463/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.6041 - val_loss: 35.2648\n",
      "Epoch 464/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5828 - val_loss: 34.3325\n",
      "Epoch 465/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.1435 - val_loss: 33.1466\n",
      "Epoch 466/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.4674 - val_loss: 34.4284\n",
      "Epoch 467/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.2823 - val_loss: 33.1464\n",
      "Epoch 468/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.4413 - val_loss: 34.2869\n",
      "Epoch 469/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.1975 - val_loss: 33.4193\n",
      "Epoch 470/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.7585 - val_loss: 33.1633\n",
      "Epoch 471/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.7340 - val_loss: 33.5072\n",
      "Epoch 472/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.7225 - val_loss: 33.5027\n",
      "Epoch 473/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.5342 - val_loss: 33.0548\n",
      "Epoch 474/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.4865 - val_loss: 32.5872\n",
      "Epoch 475/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.5682 - val_loss: 33.2105\n",
      "Epoch 476/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.6952 - val_loss: 32.8089\n",
      "Epoch 477/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.5740 - val_loss: 33.1779\n",
      "Epoch 478/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.7052 - val_loss: 32.7899\n",
      "Epoch 479/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.4142 - val_loss: 33.8072\n",
      "Epoch 480/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.2652 - val_loss: 32.6597\n",
      "Epoch 481/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.1678 - val_loss: 32.4924\n",
      "Epoch 482/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.0800 - val_loss: 32.7058\n",
      "Epoch 483/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.2280 - val_loss: 32.9150\n",
      "Epoch 484/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1012 - val_loss: 32.4661\n",
      "Epoch 485/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.9724 - val_loss: 34.0848\n",
      "Epoch 486/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3041 - val_loss: 32.5309\n",
      "Epoch 487/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1795 - val_loss: 32.8610\n",
      "Epoch 488/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.2487 - val_loss: 34.0217\n",
      "Epoch 489/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.7394 - val_loss: 33.2433\n",
      "Epoch 490/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.5656 - val_loss: 33.1022\n",
      "Epoch 491/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.5407 - val_loss: 32.2194\n",
      "Epoch 492/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.8563 - val_loss: 33.2780\n",
      "Epoch 493/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0745 - val_loss: 33.9283\n",
      "Epoch 494/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0712 - val_loss: 32.8654\n",
      "Epoch 495/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1667 - val_loss: 33.1177\n",
      "Epoch 496/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0618 - val_loss: 32.0856\n",
      "Epoch 497/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.8324 - val_loss: 34.3762\n",
      "Epoch 498/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1245 - val_loss: 33.6706\n",
      "Epoch 499/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.9320 - val_loss: 33.5994\n",
      "Epoch 500/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0936 - val_loss: 33.2904\n",
      "Epoch 501/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0643 - val_loss: 32.0331\n",
      "Epoch 502/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.7137 - val_loss: 32.3405\n",
      "Epoch 503/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.7204 - val_loss: 32.0258\n",
      "Epoch 504/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0507 - val_loss: 31.9913\n",
      "Epoch 505/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.9041 - val_loss: 32.8247\n",
      "Epoch 506/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.6574 - val_loss: 32.6438\n",
      "Epoch 507/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8500 - val_loss: 32.6980\n",
      "Epoch 508/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.9526 - val_loss: 33.0456\n",
      "Epoch 509/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8998 - val_loss: 33.3345\n",
      "Epoch 510/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.9108 - val_loss: 32.8460\n",
      "Epoch 511/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4442 - val_loss: 33.0189\n",
      "Epoch 512/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0040 - val_loss: 32.7427\n",
      "Epoch 513/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.5813 - val_loss: 33.5072\n",
      "Epoch 514/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3243 - val_loss: 31.8899\n",
      "Epoch 515/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.0105 - val_loss: 32.3334\n",
      "Epoch 516/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.6853 - val_loss: 32.5793\n",
      "Epoch 517/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.4798 - val_loss: 32.0855\n",
      "Epoch 518/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.3753 - val_loss: 32.4030\n",
      "Epoch 519/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.2415 - val_loss: 32.9091\n",
      "Epoch 520/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.1713 - val_loss: 32.7603\n",
      "Epoch 521/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.0132 - val_loss: 32.3094\n",
      "Epoch 522/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.2213 - val_loss: 32.2762\n",
      "Epoch 523/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.9418 - val_loss: 32.1954\n",
      "Epoch 524/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.0097 - val_loss: 32.1983\n",
      "Epoch 525/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.9108 - val_loss: 31.6077\n",
      "Epoch 526/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.9652 - val_loss: 32.0298\n",
      "Epoch 527/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.9301 - val_loss: 31.3379\n",
      "Epoch 528/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.9172 - val_loss: 31.0511\n",
      "Epoch 529/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.9378 - val_loss: 31.6113\n",
      "Epoch 530/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.7302 - val_loss: 31.1141\n",
      "Epoch 531/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.8354 - val_loss: 31.2277\n",
      "Epoch 532/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.8767 - val_loss: 31.2529\n",
      "Epoch 533/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.8199 - val_loss: 30.7921\n",
      "Epoch 534/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.5883 - val_loss: 31.7986\n",
      "Epoch 535/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.6304 - val_loss: 31.3001\n",
      "Epoch 536/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.8574 - val_loss: 30.7512\n",
      "Epoch 537/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.5881 - val_loss: 31.1979\n",
      "Epoch 538/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.8279 - val_loss: 31.1871\n",
      "Epoch 539/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.8805 - val_loss: 31.7268\n",
      "Epoch 540/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.7145 - val_loss: 31.4264\n",
      "Epoch 541/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.6523 - val_loss: 32.1991\n",
      "Epoch 542/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.7141 - val_loss: 31.3480\n",
      "Epoch 543/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.5493 - val_loss: 30.8907\n",
      "Epoch 544/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.6380 - val_loss: 30.9612\n",
      "Epoch 545/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.5304 - val_loss: 31.3096\n",
      "Epoch 546/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.4045 - val_loss: 32.0179\n",
      "Epoch 547/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.5786 - val_loss: 31.7303\n",
      "Epoch 548/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.9176 - val_loss: 30.9010\n",
      "Epoch 549/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.8400 - val_loss: 31.4839\n",
      "Epoch 550/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.6340 - val_loss: 31.0776\n",
      "Epoch 551/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.4516 - val_loss: 31.0714\n",
      "Epoch 552/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.6042 - val_loss: 31.7576\n",
      "Epoch 553/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.6287 - val_loss: 30.5879\n",
      "Epoch 554/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.5003 - val_loss: 32.2990\n",
      "Epoch 555/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.5363 - val_loss: 31.4834\n",
      "Epoch 556/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.4809 - val_loss: 30.4777\n",
      "Epoch 557/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.4986 - val_loss: 32.2773\n",
      "Epoch 558/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.2076 - val_loss: 32.0765\n",
      "Epoch 559/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.8853 - val_loss: 32.0846\n",
      "Epoch 560/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.4533 - val_loss: 31.5878\n",
      "Epoch 561/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.2749 - val_loss: 31.4232\n",
      "Epoch 562/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.2821 - val_loss: 30.5081\n",
      "Epoch 563/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.9963 - val_loss: 30.5198\n",
      "Epoch 564/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.0930 - val_loss: 30.4494\n",
      "Epoch 565/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.2838 - val_loss: 31.1529\n",
      "Epoch 566/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.1127 - val_loss: 31.2028\n",
      "Epoch 567/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.2548 - val_loss: 30.7491\n",
      "Epoch 568/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.4282 - val_loss: 30.6686\n",
      "Epoch 569/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.0935 - val_loss: 30.1616\n",
      "Epoch 570/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.0879 - val_loss: 30.7810\n",
      "Epoch 571/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.0856 - val_loss: 30.7162\n",
      "Epoch 572/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.9280 - val_loss: 30.9806\n",
      "Epoch 573/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.9524 - val_loss: 30.5540\n",
      "Epoch 574/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.8524 - val_loss: 30.6091\n",
      "Epoch 575/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.0043 - val_loss: 30.7320\n",
      "Epoch 576/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.1532 - val_loss: 31.0168\n",
      "Epoch 577/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.9216 - val_loss: 31.3853\n",
      "Epoch 578/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.9846 - val_loss: 30.0552\n",
      "Epoch 579/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.1353 - val_loss: 31.2559\n",
      "Epoch 580/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.9425 - val_loss: 31.3621\n",
      "Epoch 581/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.9108 - val_loss: 30.6153\n",
      "Epoch 582/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.6415 - val_loss: 29.9956\n",
      "Epoch 583/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6997 - val_loss: 30.8396\n",
      "Epoch 584/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.5761 - val_loss: 30.7517\n",
      "Epoch 585/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.7144 - val_loss: 31.0830\n",
      "Epoch 586/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.5919 - val_loss: 30.8998\n",
      "Epoch 587/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.7062 - val_loss: 30.5522\n",
      "Epoch 588/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.8440 - val_loss: 30.1545\n",
      "Epoch 589/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.8981 - val_loss: 30.0621\n",
      "Epoch 590/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.5805 - val_loss: 30.4343\n",
      "Epoch 591/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.7968 - val_loss: 30.3476\n",
      "Epoch 592/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6230 - val_loss: 29.7099\n",
      "Epoch 593/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.8227 - val_loss: 30.8941\n",
      "Epoch 594/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.7299 - val_loss: 31.4494\n",
      "Epoch 595/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.8548 - val_loss: 30.0366\n",
      "Epoch 596/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.7349 - val_loss: 30.8274\n",
      "Epoch 597/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.5380 - val_loss: 30.3370\n",
      "Epoch 598/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6264 - val_loss: 30.2703\n",
      "Epoch 599/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.1021 - val_loss: 31.9792\n",
      "Epoch 600/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 29.3828\n",
      "Epoch 00600: saving model to saved_models/latent32/cp-0600.h5\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 29.3828 - val_loss: 30.3197\n",
      "Epoch 601/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6503 - val_loss: 31.1716\n",
      "Epoch 602/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.9593 - val_loss: 30.2193\n",
      "Epoch 603/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.8932 - val_loss: 30.6173\n",
      "Epoch 604/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.5746 - val_loss: 30.4199\n",
      "Epoch 605/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.4373 - val_loss: 30.4856\n",
      "Epoch 606/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.4988 - val_loss: 30.4336\n",
      "Epoch 607/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.8465 - val_loss: 31.1506\n",
      "Epoch 608/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.9205 - val_loss: 30.3790\n",
      "Epoch 609/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.1371 - val_loss: 30.7335\n",
      "Epoch 610/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.4428 - val_loss: 32.0914\n",
      "Epoch 611/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.2304 - val_loss: 30.8918\n",
      "Epoch 612/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.8833 - val_loss: 31.1742\n",
      "Epoch 613/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.0112 - val_loss: 30.6835\n",
      "Epoch 614/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.8228 - val_loss: 30.1427\n",
      "Epoch 615/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.5587 - val_loss: 30.2214\n",
      "Epoch 616/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.2041 - val_loss: 30.3366\n",
      "Epoch 617/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.1744 - val_loss: 30.3313\n",
      "Epoch 618/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.0698 - val_loss: 30.2636\n",
      "Epoch 619/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6553 - val_loss: 30.9177\n",
      "Epoch 620/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6074 - val_loss: 30.6616\n",
      "Epoch 621/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.5438 - val_loss: 31.1251\n",
      "Epoch 622/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6820 - val_loss: 30.0943\n",
      "Epoch 623/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2906 - val_loss: 29.6942\n",
      "Epoch 624/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.1731 - val_loss: 30.1943\n",
      "Epoch 625/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.0904 - val_loss: 29.9557\n",
      "Epoch 626/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.1797 - val_loss: 30.0336\n",
      "Epoch 627/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.3329 - val_loss: 29.5283\n",
      "Epoch 628/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.9383 - val_loss: 29.9582\n",
      "Epoch 629/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.1859 - val_loss: 29.9325\n",
      "Epoch 630/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.0921 - val_loss: 29.5597\n",
      "Epoch 631/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.3657 - val_loss: 30.9292\n",
      "Epoch 632/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6160 - val_loss: 30.6543\n",
      "Epoch 633/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.4057 - val_loss: 30.2844\n",
      "Epoch 634/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9720 - val_loss: 29.9826\n",
      "Epoch 635/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.3205 - val_loss: 30.3163\n",
      "Epoch 636/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.0379 - val_loss: 29.9321\n",
      "Epoch 637/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.1642 - val_loss: 30.2266\n",
      "Epoch 638/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2150 - val_loss: 30.5204\n",
      "Epoch 639/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.3487 - val_loss: 30.6417\n",
      "Epoch 640/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.0911 - val_loss: 29.8964\n",
      "Epoch 641/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.5385 - val_loss: 31.2938\n",
      "Epoch 642/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.9397 - val_loss: 30.8477\n",
      "Epoch 643/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.4416 - val_loss: 30.2026\n",
      "Epoch 644/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.0991 - val_loss: 30.3345\n",
      "Epoch 645/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.0078 - val_loss: 29.8207\n",
      "Epoch 646/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.0297 - val_loss: 30.3094\n",
      "Epoch 647/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.1278 - val_loss: 30.2507\n",
      "Epoch 648/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.0567 - val_loss: 29.7173\n",
      "Epoch 649/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.7338 - val_loss: 29.3134\n",
      "Epoch 650/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8900 - val_loss: 30.3266\n",
      "Epoch 651/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.0047 - val_loss: 29.4776\n",
      "Epoch 652/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9567 - val_loss: 30.3416\n",
      "Epoch 653/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.9126 - val_loss: 30.1066\n",
      "Epoch 654/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9532 - val_loss: 29.7606\n",
      "Epoch 655/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.0763 - val_loss: 30.3624\n",
      "Epoch 656/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2318 - val_loss: 30.4448\n",
      "Epoch 657/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2779 - val_loss: 30.4336\n",
      "Epoch 658/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.3088 - val_loss: 30.7175\n",
      "Epoch 659/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.4396 - val_loss: 29.7556\n",
      "Epoch 660/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9282 - val_loss: 29.9365\n",
      "Epoch 661/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8806 - val_loss: 30.0757\n",
      "Epoch 662/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.8930 - val_loss: 29.9339\n",
      "Epoch 663/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8482 - val_loss: 30.1297\n",
      "Epoch 664/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2274 - val_loss: 29.8238\n",
      "Epoch 665/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.0824 - val_loss: 30.8632\n",
      "Epoch 666/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.1923 - val_loss: 29.9994\n",
      "Epoch 667/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.1460 - val_loss: 29.5333\n",
      "Epoch 668/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.7306 - val_loss: 30.3573\n",
      "Epoch 669/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8125 - val_loss: 30.0234\n",
      "Epoch 670/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.7671 - val_loss: 29.5511\n",
      "Epoch 671/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.5391 - val_loss: 30.7374\n",
      "Epoch 672/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8106 - val_loss: 29.3483\n",
      "Epoch 673/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5959 - val_loss: 30.4821\n",
      "Epoch 674/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6323 - val_loss: 29.7149\n",
      "Epoch 675/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.7136 - val_loss: 29.6876\n",
      "Epoch 676/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.6893 - val_loss: 29.4753\n",
      "Epoch 677/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5588 - val_loss: 29.5879\n",
      "Epoch 678/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6419 - val_loss: 29.9463\n",
      "Epoch 679/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4840 - val_loss: 30.0605\n",
      "Epoch 680/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8036 - val_loss: 29.5003\n",
      "Epoch 681/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6229 - val_loss: 29.6465\n",
      "Epoch 682/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5294 - val_loss: 29.5275\n",
      "Epoch 683/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5178 - val_loss: 29.2141\n",
      "Epoch 684/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4838 - val_loss: 29.6930\n",
      "Epoch 685/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.6450 - val_loss: 28.9032\n",
      "Epoch 686/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6039 - val_loss: 29.7404\n",
      "Epoch 687/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9084 - val_loss: 29.4406\n",
      "Epoch 688/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.7189 - val_loss: 30.2888\n",
      "Epoch 689/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.7442 - val_loss: 30.3096\n",
      "Epoch 690/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6149 - val_loss: 29.5026\n",
      "Epoch 691/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6793 - val_loss: 29.7612\n",
      "Epoch 692/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6938 - val_loss: 28.7145\n",
      "Epoch 693/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6532 - val_loss: 29.9527\n",
      "Epoch 694/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.7113 - val_loss: 28.8888\n",
      "Epoch 695/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.7020 - val_loss: 29.5569\n",
      "Epoch 696/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.5141 - val_loss: 28.9332\n",
      "Epoch 697/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.3297 - val_loss: 28.9024\n",
      "Epoch 698/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3949 - val_loss: 29.5157\n",
      "Epoch 699/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4548 - val_loss: 29.0188\n",
      "Epoch 700/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3333 - val_loss: 29.3194\n",
      "Epoch 701/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.2312 - val_loss: 29.2540\n",
      "Epoch 702/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5904 - val_loss: 29.9800\n",
      "Epoch 703/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5172 - val_loss: 29.7686\n",
      "Epoch 704/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.4745 - val_loss: 29.3068\n",
      "Epoch 705/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2719 - val_loss: 29.1458\n",
      "Epoch 706/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4039 - val_loss: 29.1970\n",
      "Epoch 707/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4382 - val_loss: 29.1273\n",
      "Epoch 708/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4097 - val_loss: 29.2073\n",
      "Epoch 709/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4595 - val_loss: 29.3555\n",
      "Epoch 710/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.2060 - val_loss: 29.6463\n",
      "Epoch 711/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4935 - val_loss: 29.6299\n",
      "Epoch 712/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1962 - val_loss: 28.9924\n",
      "Epoch 713/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1975 - val_loss: 29.5761\n",
      "Epoch 714/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3865 - val_loss: 29.1569\n",
      "Epoch 715/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4286 - val_loss: 29.9396\n",
      "Epoch 716/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4093 - val_loss: 29.7284\n",
      "Epoch 717/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3544 - val_loss: 28.7077\n",
      "Epoch 718/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2438 - val_loss: 29.9618\n",
      "Epoch 719/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.7649 - val_loss: 29.7790\n",
      "Epoch 720/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5166 - val_loss: 29.3568\n",
      "Epoch 721/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3447 - val_loss: 29.2582\n",
      "Epoch 722/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2551 - val_loss: 30.0315\n",
      "Epoch 723/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2354 - val_loss: 28.8303\n",
      "Epoch 724/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9247 - val_loss: 28.8529\n",
      "Epoch 725/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1759 - val_loss: 29.1344\n",
      "Epoch 726/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0101 - val_loss: 29.8229\n",
      "Epoch 727/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0808 - val_loss: 30.1087\n",
      "Epoch 728/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1785 - val_loss: 29.6503\n",
      "Epoch 729/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4878 - val_loss: 30.5689\n",
      "Epoch 730/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4789 - val_loss: 29.8789\n",
      "Epoch 731/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3569 - val_loss: 29.7750\n",
      "Epoch 732/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0927 - val_loss: 29.2843\n",
      "Epoch 733/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2151 - val_loss: 29.2314\n",
      "Epoch 734/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1858 - val_loss: 29.1904\n",
      "Epoch 735/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0280 - val_loss: 29.0679\n",
      "Epoch 736/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1975 - val_loss: 29.2372\n",
      "Epoch 737/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2896 - val_loss: 29.0315\n",
      "Epoch 738/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.1032 - val_loss: 29.7128\n",
      "Epoch 739/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3713 - val_loss: 28.9053\n",
      "Epoch 740/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2335 - val_loss: 29.2000\n",
      "Epoch 741/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8969 - val_loss: 29.3243\n",
      "Epoch 742/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1093 - val_loss: 28.9470\n",
      "Epoch 743/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9987 - val_loss: 29.4788\n",
      "Epoch 744/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3454 - val_loss: 28.9909\n",
      "Epoch 745/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2058 - val_loss: 29.0386\n",
      "Epoch 746/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9882 - val_loss: 28.9877\n",
      "Epoch 747/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9498 - val_loss: 28.6615\n",
      "Epoch 748/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.8929 - val_loss: 29.6287\n",
      "Epoch 749/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4167 - val_loss: 29.2307\n",
      "Epoch 750/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0144 - val_loss: 29.4663\n",
      "Epoch 751/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5693 - val_loss: 29.7031\n",
      "Epoch 752/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4115 - val_loss: 29.3948\n",
      "Epoch 753/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1840 - val_loss: 28.7598\n",
      "Epoch 754/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1158 - val_loss: 29.9338\n",
      "Epoch 755/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0939 - val_loss: 29.0277\n",
      "Epoch 756/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1718 - val_loss: 29.8733\n",
      "Epoch 757/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2745 - val_loss: 28.4618\n",
      "Epoch 758/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8495 - val_loss: 29.3626\n",
      "Epoch 759/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0037 - val_loss: 29.2097\n",
      "Epoch 760/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1052 - val_loss: 29.9052\n",
      "Epoch 761/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9428 - val_loss: 29.6522\n",
      "Epoch 762/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2513 - val_loss: 30.8782\n",
      "Epoch 763/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9697 - val_loss: 30.2748\n",
      "Epoch 764/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4247 - val_loss: 29.9597\n",
      "Epoch 765/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4533 - val_loss: 28.8892\n",
      "Epoch 766/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9664 - val_loss: 28.8654\n",
      "Epoch 767/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9193 - val_loss: 29.7390\n",
      "Epoch 768/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3611 - val_loss: 29.9023\n",
      "Epoch 769/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5159 - val_loss: 28.9596\n",
      "Epoch 770/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8466 - val_loss: 30.6305\n",
      "Epoch 771/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1468 - val_loss: 29.5617\n",
      "Epoch 772/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9195 - val_loss: 28.9423\n",
      "Epoch 773/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1204 - val_loss: 28.9627\n",
      "Epoch 774/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0536 - val_loss: 29.0052\n",
      "Epoch 775/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8469 - val_loss: 29.6037\n",
      "Epoch 776/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7413 - val_loss: 29.2813\n",
      "Epoch 777/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1855 - val_loss: 29.2958\n",
      "Epoch 778/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1502 - val_loss: 28.4719\n",
      "Epoch 779/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1403 - val_loss: 28.9360\n",
      "Epoch 780/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1377 - val_loss: 29.5835\n",
      "Epoch 781/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7363 - val_loss: 28.7238\n",
      "Epoch 782/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9341 - val_loss: 29.3224\n",
      "Epoch 783/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3010 - val_loss: 29.6797\n",
      "Epoch 784/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1531 - val_loss: 29.3854\n",
      "Epoch 785/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2648 - val_loss: 30.2222\n",
      "Epoch 786/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1813 - val_loss: 29.2944\n",
      "Epoch 787/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9835 - val_loss: 28.9391\n",
      "Epoch 788/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8975 - val_loss: 28.9470\n",
      "Epoch 789/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.7038 - val_loss: 28.7310\n",
      "Epoch 790/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5694 - val_loss: 29.7766\n",
      "Epoch 791/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0535 - val_loss: 29.0073\n",
      "Epoch 792/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9872 - val_loss: 29.2970\n",
      "Epoch 793/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8071 - val_loss: 28.9038\n",
      "Epoch 794/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9454 - val_loss: 29.2877\n",
      "Epoch 795/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9740 - val_loss: 29.5400\n",
      "Epoch 796/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6981 - val_loss: 29.0913\n",
      "Epoch 797/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7357 - val_loss: 29.4349\n",
      "Epoch 798/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9939 - val_loss: 28.5654\n",
      "Epoch 799/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0717 - val_loss: 28.9619\n",
      "Epoch 800/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 26.9420\n",
      "Epoch 00800: saving model to saved_models/latent32/cp-0800.h5\n",
      "7/7 [==============================] - 1s 142ms/step - loss: 26.9420 - val_loss: 29.0433\n",
      "Epoch 801/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8827 - val_loss: 29.0709\n",
      "Epoch 802/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7833 - val_loss: 29.9226\n",
      "Epoch 803/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6565 - val_loss: 29.5082\n",
      "Epoch 804/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8962 - val_loss: 29.2287\n",
      "Epoch 805/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9205 - val_loss: 29.2040\n",
      "Epoch 806/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8611 - val_loss: 29.0010\n",
      "Epoch 807/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.5290 - val_loss: 28.9333\n",
      "Epoch 808/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6092 - val_loss: 28.8638\n",
      "Epoch 809/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5893 - val_loss: 28.9816\n",
      "Epoch 810/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6963 - val_loss: 29.1564\n",
      "Epoch 811/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6602 - val_loss: 28.9067\n",
      "Epoch 812/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9103 - val_loss: 28.9238\n",
      "Epoch 813/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8888 - val_loss: 29.1674\n",
      "Epoch 814/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7994 - val_loss: 29.0016\n",
      "Epoch 815/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7299 - val_loss: 29.0515\n",
      "Epoch 816/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6976 - val_loss: 28.5737\n",
      "Epoch 817/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5239 - val_loss: 29.4052\n",
      "Epoch 818/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5257 - val_loss: 29.4736\n",
      "Epoch 819/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5878 - val_loss: 29.9300\n",
      "Epoch 820/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8680 - val_loss: 29.2644\n",
      "Epoch 821/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6515 - val_loss: 28.8441\n",
      "Epoch 822/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8706 - val_loss: 29.4224\n",
      "Epoch 823/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8224 - val_loss: 29.7617\n",
      "Epoch 824/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6418 - val_loss: 29.3759\n",
      "Epoch 825/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6286 - val_loss: 29.1058\n",
      "Epoch 826/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8698 - val_loss: 29.1844\n",
      "Epoch 827/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5909 - val_loss: 28.5459\n",
      "Epoch 828/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5804 - val_loss: 28.6511\n",
      "Epoch 829/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5268 - val_loss: 29.0426\n",
      "Epoch 830/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6490 - val_loss: 29.1681\n",
      "Epoch 831/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6195 - val_loss: 29.2645\n",
      "Epoch 832/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9479 - val_loss: 29.9191\n",
      "Epoch 833/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9203 - val_loss: 28.8994\n",
      "Epoch 834/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8278 - val_loss: 29.0298\n",
      "Epoch 835/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.4201 - val_loss: 31.0005\n",
      "Epoch 836/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1412 - val_loss: 29.6764\n",
      "Epoch 837/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7166 - val_loss: 29.4570\n",
      "Epoch 838/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7861 - val_loss: 28.0932\n",
      "Epoch 839/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5737 - val_loss: 29.6372\n",
      "Epoch 840/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5300 - val_loss: 29.3348\n",
      "Epoch 841/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2134 - val_loss: 28.8940\n",
      "Epoch 842/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4252 - val_loss: 29.2600\n",
      "Epoch 843/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4075 - val_loss: 29.0638\n",
      "Epoch 844/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4571 - val_loss: 28.4057\n",
      "Epoch 845/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3140 - val_loss: 28.3936\n",
      "Epoch 846/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4068 - val_loss: 29.9402\n",
      "Epoch 847/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7793 - val_loss: 29.1189\n",
      "Epoch 848/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7055 - val_loss: 28.9497\n",
      "Epoch 849/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4786 - val_loss: 29.1087\n",
      "Epoch 850/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4502 - val_loss: 28.7231\n",
      "Epoch 851/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4936 - val_loss: 28.8880\n",
      "Epoch 852/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2253 - val_loss: 28.6259\n",
      "Epoch 853/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3710 - val_loss: 29.1901\n",
      "Epoch 854/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4083 - val_loss: 28.7356\n",
      "Epoch 855/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3186 - val_loss: 29.0130\n",
      "Epoch 856/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4559 - val_loss: 28.9830\n",
      "Epoch 857/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3782 - val_loss: 28.3828\n",
      "Epoch 858/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3420 - val_loss: 28.7522\n",
      "Epoch 859/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7928 - val_loss: 29.2035\n",
      "Epoch 860/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6961 - val_loss: 28.6710\n",
      "Epoch 861/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4959 - val_loss: 29.7390\n",
      "Epoch 862/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8124 - val_loss: 28.6897\n",
      "Epoch 863/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4717 - val_loss: 29.1418\n",
      "Epoch 864/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4373 - val_loss: 28.5689\n",
      "Epoch 865/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4226 - val_loss: 29.0801\n",
      "Epoch 866/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4955 - val_loss: 28.5003\n",
      "Epoch 867/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3530 - val_loss: 29.6626\n",
      "Epoch 868/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4051 - val_loss: 29.8411\n",
      "Epoch 869/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5740 - val_loss: 28.9214\n",
      "Epoch 870/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6916 - val_loss: 28.6297\n",
      "Epoch 871/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2764 - val_loss: 28.7376\n",
      "Epoch 872/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4361 - val_loss: 28.5930\n",
      "Epoch 873/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3217 - val_loss: 28.8777\n",
      "Epoch 874/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3815 - val_loss: 28.6264\n",
      "Epoch 875/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2336 - val_loss: 28.9878\n",
      "Epoch 876/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6550 - val_loss: 29.0091\n",
      "Epoch 877/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8674 - val_loss: 28.6946\n",
      "Epoch 878/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9297 - val_loss: 30.1533\n",
      "Epoch 879/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5669 - val_loss: 28.9892\n",
      "Epoch 880/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5234 - val_loss: 28.6284\n",
      "Epoch 881/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2911 - val_loss: 28.5546\n",
      "Epoch 882/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3138 - val_loss: 28.4569\n",
      "Epoch 883/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2785 - val_loss: 29.6481\n",
      "Epoch 884/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6445 - val_loss: 29.8034\n",
      "Epoch 885/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4074 - val_loss: 28.8721\n",
      "Epoch 886/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8304 - val_loss: 29.4403\n",
      "Epoch 887/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3058 - val_loss: 31.3967\n",
      "Epoch 888/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.1356 - val_loss: 28.6363\n",
      "Epoch 889/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.1757 - val_loss: 28.8517\n",
      "Epoch 890/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1933 - val_loss: 28.7800\n",
      "Epoch 891/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1972 - val_loss: 29.8932\n",
      "Epoch 892/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7642 - val_loss: 28.8536\n",
      "Epoch 893/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6673 - val_loss: 28.7640\n",
      "Epoch 894/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9261 - val_loss: 29.9117\n",
      "Epoch 895/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7901 - val_loss: 29.3783\n",
      "Epoch 896/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7286 - val_loss: 29.6422\n",
      "Epoch 897/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5454 - val_loss: 29.0306\n",
      "Epoch 898/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2981 - val_loss: 29.6847\n",
      "Epoch 899/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3262 - val_loss: 28.7700\n",
      "Epoch 900/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2971 - val_loss: 29.2910\n",
      "Epoch 901/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4207 - val_loss: 29.2242\n",
      "Epoch 902/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5453 - val_loss: 30.4032\n",
      "Epoch 903/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5440 - val_loss: 28.7006\n",
      "Epoch 904/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1985 - val_loss: 28.4768\n",
      "Epoch 905/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.9882 - val_loss: 28.7766\n",
      "Epoch 906/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0497 - val_loss: 29.3234\n",
      "Epoch 907/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.9273 - val_loss: 29.1691\n",
      "Epoch 908/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3062 - val_loss: 28.6847\n",
      "Epoch 909/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1845 - val_loss: 28.9004\n",
      "Epoch 910/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0905 - val_loss: 28.3008\n",
      "Epoch 911/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0372 - val_loss: 28.8792\n",
      "Epoch 912/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3985 - val_loss: 28.1531\n",
      "Epoch 913/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2536 - val_loss: 28.7601\n",
      "Epoch 914/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2210 - val_loss: 28.8710\n",
      "Epoch 915/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3662 - val_loss: 28.6713\n",
      "Epoch 916/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2409 - val_loss: 28.7422\n",
      "Epoch 917/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1279 - val_loss: 28.4354\n",
      "Epoch 918/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2315 - val_loss: 28.0242\n",
      "Epoch 919/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2022 - val_loss: 29.3441\n",
      "Epoch 920/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2261 - val_loss: 28.7321\n",
      "Epoch 921/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3385 - val_loss: 28.5882\n",
      "Epoch 922/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4637 - val_loss: 29.2458\n",
      "Epoch 923/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3025 - val_loss: 28.7586\n",
      "Epoch 924/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2359 - val_loss: 28.9641\n",
      "Epoch 925/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1812 - val_loss: 28.5037\n",
      "Epoch 926/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1131 - val_loss: 28.7754\n",
      "Epoch 927/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9569 - val_loss: 29.4091\n",
      "Epoch 928/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9610 - val_loss: 29.0418\n",
      "Epoch 929/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1247 - val_loss: 29.0588\n",
      "Epoch 930/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2131 - val_loss: 28.5149\n",
      "Epoch 931/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9940 - val_loss: 28.2735\n",
      "Epoch 932/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1331 - val_loss: 28.6164\n",
      "Epoch 933/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0399 - val_loss: 27.9505\n",
      "Epoch 934/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9160 - val_loss: 28.0677\n",
      "Epoch 935/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9193 - val_loss: 28.7836\n",
      "Epoch 936/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0442 - val_loss: 28.5515\n",
      "Epoch 937/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1449 - val_loss: 29.2710\n",
      "Epoch 938/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3769 - val_loss: 28.7925\n",
      "Epoch 939/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8900 - val_loss: 29.1102\n",
      "Epoch 940/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0098 - val_loss: 29.3374\n",
      "Epoch 941/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0343 - val_loss: 28.9037\n",
      "Epoch 942/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8955 - val_loss: 28.8667\n",
      "Epoch 943/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0414 - val_loss: 28.3532\n",
      "Epoch 944/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0057 - val_loss: 28.4448\n",
      "Epoch 945/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8990 - val_loss: 29.0839\n",
      "Epoch 946/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9760 - val_loss: 29.0119\n",
      "Epoch 947/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0511 - val_loss: 28.4489\n",
      "Epoch 948/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0602 - val_loss: 29.1176\n",
      "Epoch 949/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0325 - val_loss: 28.9885\n",
      "Epoch 950/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4031 - val_loss: 28.5306\n",
      "Epoch 951/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.8345 - val_loss: 28.7882\n",
      "Epoch 952/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2297 - val_loss: 28.4822\n",
      "Epoch 953/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9322 - val_loss: 28.8155\n",
      "Epoch 954/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0547 - val_loss: 28.1054\n",
      "Epoch 955/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9981 - val_loss: 28.8348\n",
      "Epoch 956/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0728 - val_loss: 27.9958\n",
      "Epoch 957/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9886 - val_loss: 28.5198\n",
      "Epoch 958/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8050 - val_loss: 29.2267\n",
      "Epoch 959/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0760 - val_loss: 28.9821\n",
      "Epoch 960/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9205 - val_loss: 28.7589\n",
      "Epoch 961/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1704 - val_loss: 28.9652\n",
      "Epoch 962/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1461 - val_loss: 29.3338\n",
      "Epoch 963/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9611 - val_loss: 28.7180\n",
      "Epoch 964/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0376 - val_loss: 29.0125\n",
      "Epoch 965/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2357 - val_loss: 29.4124\n",
      "Epoch 966/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4291 - val_loss: 29.7527\n",
      "Epoch 967/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8184 - val_loss: 29.5539\n",
      "Epoch 968/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3500 - val_loss: 28.8169\n",
      "Epoch 969/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0493 - val_loss: 29.3409\n",
      "Epoch 970/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9402 - val_loss: 28.6480\n",
      "Epoch 971/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9605 - val_loss: 28.2031\n",
      "Epoch 972/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.7969 - val_loss: 28.4063\n",
      "Epoch 973/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8696 - val_loss: 28.8009\n",
      "Epoch 974/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9441 - val_loss: 28.2417\n",
      "Epoch 975/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9602 - val_loss: 28.7531\n",
      "Epoch 976/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7489 - val_loss: 29.5679\n",
      "Epoch 977/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2785 - val_loss: 28.5679\n",
      "Epoch 978/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0505 - val_loss: 28.6431\n",
      "Epoch 979/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4074 - val_loss: 28.5565\n",
      "Epoch 980/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2902 - val_loss: 28.8309\n",
      "Epoch 981/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1249 - val_loss: 29.9373\n",
      "Epoch 982/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1143 - val_loss: 28.6022\n",
      "Epoch 983/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1234 - val_loss: 29.4415\n",
      "Epoch 984/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8602 - val_loss: 29.0917\n",
      "Epoch 985/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7660 - val_loss: 28.3745\n",
      "Epoch 986/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8979 - val_loss: 28.8336\n",
      "Epoch 987/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2179 - val_loss: 28.9331\n",
      "Epoch 988/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2783 - val_loss: 29.5553\n",
      "Epoch 989/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3273 - val_loss: 29.1178\n",
      "Epoch 990/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4203 - val_loss: 29.2540\n",
      "Epoch 991/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1287 - val_loss: 28.4712\n",
      "Epoch 992/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8536 - val_loss: 28.7232\n",
      "Epoch 993/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1983 - val_loss: 28.8729\n",
      "Epoch 994/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0251 - val_loss: 28.8924\n",
      "Epoch 995/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8086 - val_loss: 28.5411\n",
      "Epoch 996/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0574 - val_loss: 28.6576\n",
      "Epoch 997/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7760 - val_loss: 28.7176\n",
      "Epoch 998/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1466 - val_loss: 28.6768\n",
      "Epoch 999/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8632 - val_loss: 28.7254\n",
      "Epoch 1000/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 25.8597\n",
      "Epoch 01000: saving model to saved_models/latent32/cp-1000.h5\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 25.8597 - val_loss: 28.6098\n",
      "Epoch 1001/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8151 - val_loss: 28.2761\n",
      "Epoch 1002/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7952 - val_loss: 28.4595\n",
      "Epoch 1003/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7920 - val_loss: 28.2480\n",
      "Epoch 1004/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9636 - val_loss: 28.8162\n",
      "Epoch 1005/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0667 - val_loss: 28.7562\n",
      "Epoch 1006/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9362 - val_loss: 28.8658\n",
      "Epoch 1007/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5293 - val_loss: 28.9288\n",
      "Epoch 1008/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4005 - val_loss: 29.5925\n",
      "Epoch 1009/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3554 - val_loss: 28.7664\n",
      "Epoch 1010/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9955 - val_loss: 28.8613\n",
      "Epoch 1011/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6761 - val_loss: 28.1747\n",
      "Epoch 1012/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0219 - val_loss: 28.6315\n",
      "Epoch 1013/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8817 - val_loss: 28.4444\n",
      "Epoch 1014/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8224 - val_loss: 28.7475\n",
      "Epoch 1015/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8092 - val_loss: 28.9663\n",
      "Epoch 1016/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9136 - val_loss: 28.9555\n",
      "Epoch 1017/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0863 - val_loss: 28.5589\n",
      "Epoch 1018/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6824 - val_loss: 28.7311\n",
      "Epoch 1019/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8433 - val_loss: 29.1292\n",
      "Epoch 1020/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0666 - val_loss: 28.6118\n",
      "Epoch 1021/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1485 - val_loss: 29.8831\n",
      "Epoch 1022/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1990 - val_loss: 28.5350\n",
      "Epoch 1023/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9125 - val_loss: 28.9812\n",
      "Epoch 1024/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9818 - val_loss: 28.2438\n",
      "Epoch 1025/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8955 - val_loss: 29.1646\n",
      "Epoch 1026/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9388 - val_loss: 28.2748\n",
      "Epoch 1027/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5955 - val_loss: 28.6325\n",
      "Epoch 1028/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5512 - val_loss: 28.6326\n",
      "Epoch 1029/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6271 - val_loss: 28.0369\n",
      "Epoch 1030/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7609 - val_loss: 28.9052\n",
      "Epoch 1031/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9412 - val_loss: 28.6332\n",
      "Epoch 1032/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0681 - val_loss: 29.0204\n",
      "Epoch 1033/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1114 - val_loss: 28.5582\n",
      "Epoch 1034/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9206 - val_loss: 28.8277\n",
      "Epoch 1035/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7701 - val_loss: 28.5143\n",
      "Epoch 1036/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7975 - val_loss: 29.2052\n",
      "Epoch 1037/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7749 - val_loss: 29.2850\n",
      "Epoch 1038/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6602 - val_loss: 28.9350\n",
      "Epoch 1039/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8948 - val_loss: 29.2975\n",
      "Epoch 1040/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7932 - val_loss: 28.9739\n",
      "Epoch 1041/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8827 - val_loss: 28.6530\n",
      "Epoch 1042/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7670 - val_loss: 28.5294\n",
      "Epoch 1043/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.6775 - val_loss: 28.6512\n",
      "Epoch 1044/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6051 - val_loss: 28.1991\n",
      "Epoch 1045/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6784 - val_loss: 28.4852\n",
      "Epoch 1046/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6736 - val_loss: 28.5562\n",
      "Epoch 1047/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8335 - val_loss: 28.7308\n",
      "Epoch 1048/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9607 - val_loss: 29.3667\n",
      "Epoch 1049/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1987 - val_loss: 28.5166\n",
      "Epoch 1050/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7827 - val_loss: 28.9024\n",
      "Epoch 1051/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7549 - val_loss: 28.6887\n",
      "Epoch 1052/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7512 - val_loss: 28.4872\n",
      "Epoch 1053/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6685 - val_loss: 28.3537\n",
      "Epoch 1054/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7771 - val_loss: 28.9037\n",
      "Epoch 1055/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7669 - val_loss: 28.4804\n",
      "Epoch 1056/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8039 - val_loss: 28.2731\n",
      "Epoch 1057/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8914 - val_loss: 28.8872\n",
      "Epoch 1058/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8398 - val_loss: 28.5478\n",
      "Epoch 1059/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6351 - val_loss: 28.3511\n",
      "Epoch 1060/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0363 - val_loss: 29.3419\n",
      "Epoch 1061/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8055 - val_loss: 28.5083\n",
      "Epoch 1062/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6705 - val_loss: 28.3437\n",
      "Epoch 1063/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8368 - val_loss: 28.3255\n",
      "Epoch 1064/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7139 - val_loss: 28.0959\n",
      "Epoch 1065/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8522 - val_loss: 28.5339\n",
      "Epoch 1066/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8056 - val_loss: 28.4614\n",
      "Epoch 1067/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5949 - val_loss: 28.2421\n",
      "Epoch 1068/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6653 - val_loss: 28.1588\n",
      "Epoch 1069/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6659 - val_loss: 28.1842\n",
      "Epoch 1070/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0015 - val_loss: 28.5361\n",
      "Epoch 1071/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4855 - val_loss: 28.1417\n",
      "Epoch 1072/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7421 - val_loss: 29.0183\n",
      "Epoch 1073/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7422 - val_loss: 28.7515\n",
      "Epoch 1074/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5632 - val_loss: 28.1541\n",
      "Epoch 1075/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6009 - val_loss: 28.5558\n",
      "Epoch 1076/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5245 - val_loss: 28.5295\n",
      "Epoch 1077/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7509 - val_loss: 29.1085\n",
      "Epoch 1078/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7742 - val_loss: 28.0836\n",
      "Epoch 1079/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5169 - val_loss: 29.2838\n",
      "Epoch 1080/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7505 - val_loss: 28.4985\n",
      "Epoch 1081/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7465 - val_loss: 28.6535\n",
      "Epoch 1082/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6024 - val_loss: 29.0858\n",
      "Epoch 1083/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7560 - val_loss: 28.1822\n",
      "Epoch 1084/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5526 - val_loss: 28.5860\n",
      "Epoch 1085/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6266 - val_loss: 28.6278\n",
      "Epoch 1086/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.4427 - val_loss: 29.3139\n",
      "Epoch 1087/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1399 - val_loss: 28.7032\n",
      "Epoch 1088/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0229 - val_loss: 28.9122\n",
      "Epoch 1089/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0723 - val_loss: 28.9850\n",
      "Epoch 1090/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9762 - val_loss: 28.6233\n",
      "Epoch 1091/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0589 - val_loss: 28.5509\n",
      "Epoch 1092/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8861 - val_loss: 29.0736\n",
      "Epoch 1093/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8324 - val_loss: 28.7051\n",
      "Epoch 1094/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8125 - val_loss: 28.5538\n",
      "Epoch 1095/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6337 - val_loss: 28.2179\n",
      "Epoch 1096/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6344 - val_loss: 28.7332\n",
      "Epoch 1097/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6474 - val_loss: 28.7292\n",
      "Epoch 1098/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8371 - val_loss: 29.0230\n",
      "Epoch 1099/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6953 - val_loss: 28.3778\n",
      "Epoch 1100/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8030 - val_loss: 29.0481\n",
      "Epoch 1101/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7491 - val_loss: 28.5034\n",
      "Epoch 1102/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6635 - val_loss: 27.5611\n",
      "Epoch 1103/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7871 - val_loss: 28.8601\n",
      "Epoch 1104/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7004 - val_loss: 28.6452\n",
      "Epoch 1105/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5395 - val_loss: 28.1924\n",
      "Epoch 1106/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7609 - val_loss: 28.3111\n",
      "Epoch 1107/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6060 - val_loss: 28.9228\n",
      "Epoch 1108/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7850 - val_loss: 28.6945\n",
      "Epoch 1109/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6777 - val_loss: 28.6503\n",
      "Epoch 1110/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5500 - val_loss: 29.1272\n",
      "Epoch 1111/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4703 - val_loss: 28.6015\n",
      "Epoch 1112/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6261 - val_loss: 29.0923\n",
      "Epoch 1113/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4875 - val_loss: 28.2499\n",
      "Epoch 1114/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6602 - val_loss: 28.6567\n",
      "Epoch 1115/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6396 - val_loss: 28.6086\n",
      "Epoch 1116/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7246 - val_loss: 28.8214\n",
      "Epoch 1117/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8742 - val_loss: 29.5002\n",
      "Epoch 1118/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8008 - val_loss: 29.1432\n",
      "Epoch 1119/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6519 - val_loss: 28.8745\n",
      "Epoch 1120/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6103 - val_loss: 28.4652\n",
      "Epoch 1121/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4640 - val_loss: 28.4348\n",
      "Epoch 1122/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4858 - val_loss: 28.5808\n",
      "Epoch 1123/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5013 - val_loss: 27.9926\n",
      "Epoch 1124/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5365 - val_loss: 28.4837\n",
      "Epoch 1125/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6507 - val_loss: 28.1181\n",
      "Epoch 1126/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5014 - val_loss: 28.7588\n",
      "Epoch 1127/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6960 - val_loss: 28.6668\n",
      "Epoch 1128/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0425 - val_loss: 30.2441\n",
      "Epoch 1129/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2827 - val_loss: 29.6143\n",
      "Epoch 1130/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8794 - val_loss: 28.6493\n",
      "Epoch 1131/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1272 - val_loss: 28.4865\n",
      "Epoch 1132/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6540 - val_loss: 29.6376\n",
      "Epoch 1133/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9290 - val_loss: 28.3246\n",
      "Epoch 1134/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4905 - val_loss: 28.6110\n",
      "Epoch 1135/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7498 - val_loss: 28.6985\n",
      "Epoch 1136/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7887 - val_loss: 28.7632\n",
      "Epoch 1137/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5961 - val_loss: 29.1101\n",
      "Epoch 1138/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5513 - val_loss: 29.2077\n",
      "Epoch 1139/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5198 - val_loss: 28.2747\n",
      "Epoch 1140/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2891 - val_loss: 28.5813\n",
      "Epoch 1141/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3120 - val_loss: 28.3731\n",
      "Epoch 1142/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5064 - val_loss: 28.7693\n",
      "Epoch 1143/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5350 - val_loss: 28.9576\n",
      "Epoch 1144/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5204 - val_loss: 28.4224\n",
      "Epoch 1145/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3669 - val_loss: 28.3677\n",
      "Epoch 1146/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5962 - val_loss: 28.6758\n",
      "Epoch 1147/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5195 - val_loss: 28.9604\n",
      "Epoch 1148/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4509 - val_loss: 28.7676\n",
      "Epoch 1149/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7511 - val_loss: 28.5016\n",
      "Epoch 1150/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6156 - val_loss: 28.8736\n",
      "Epoch 1151/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5262 - val_loss: 29.2065\n",
      "Epoch 1152/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8133 - val_loss: 28.6198\n",
      "Epoch 1153/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5994 - val_loss: 28.5402\n",
      "Epoch 1154/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9084 - val_loss: 28.8289\n",
      "Epoch 1155/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9257 - val_loss: 28.7725\n",
      "Epoch 1156/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5012 - val_loss: 28.0555\n",
      "Epoch 1157/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5154 - val_loss: 28.3838\n",
      "Epoch 1158/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5882 - val_loss: 28.4942\n",
      "Epoch 1159/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4833 - val_loss: 28.8132\n",
      "Epoch 1160/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4761 - val_loss: 28.9732\n",
      "Epoch 1161/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7308 - val_loss: 28.5169\n",
      "Epoch 1162/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4125 - val_loss: 28.7019\n",
      "Epoch 1163/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6856 - val_loss: 28.5047\n",
      "Epoch 1164/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4824 - val_loss: 28.3916\n",
      "Epoch 1165/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5138 - val_loss: 28.4050\n",
      "Epoch 1166/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6629 - val_loss: 28.3117\n",
      "Epoch 1167/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4538 - val_loss: 28.7289\n",
      "Epoch 1168/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2979 - val_loss: 28.2159\n",
      "Epoch 1169/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4890 - val_loss: 28.3677\n",
      "Epoch 1170/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5004 - val_loss: 29.2458\n",
      "Epoch 1171/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5404 - val_loss: 28.7386\n",
      "Epoch 1172/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4574 - val_loss: 28.3890\n",
      "Epoch 1173/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5011 - val_loss: 28.2965\n",
      "Epoch 1174/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5932 - val_loss: 30.3257\n",
      "Epoch 1175/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1720 - val_loss: 28.5571\n",
      "Epoch 1176/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5449 - val_loss: 28.7860\n",
      "Epoch 1177/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3057 - val_loss: 28.3551\n",
      "Epoch 1178/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3113 - val_loss: 28.0446\n",
      "Epoch 1179/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5677 - val_loss: 28.5921\n",
      "Epoch 1180/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4836 - val_loss: 29.0623\n",
      "Epoch 1181/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3822 - val_loss: 28.6975\n",
      "Epoch 1182/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6432 - val_loss: 28.0094\n",
      "Epoch 1183/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3971 - val_loss: 28.6541\n",
      "Epoch 1184/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5408 - val_loss: 28.4334\n",
      "Epoch 1185/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4974 - val_loss: 28.3548\n",
      "Epoch 1186/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5567 - val_loss: 28.5177\n",
      "Epoch 1187/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5629 - val_loss: 28.6709\n",
      "Epoch 1188/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3894 - val_loss: 28.2541\n",
      "Epoch 1189/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3798 - val_loss: 28.5825\n",
      "Epoch 1190/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2937 - val_loss: 27.9390\n",
      "Epoch 1191/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3667 - val_loss: 28.4615\n",
      "Epoch 1192/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2586 - val_loss: 28.0636\n",
      "Epoch 1193/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2807 - val_loss: 28.1542\n",
      "Epoch 1194/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5576 - val_loss: 28.5035\n",
      "Epoch 1195/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5048 - val_loss: 27.8649\n",
      "Epoch 1196/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4176 - val_loss: 28.4153\n",
      "Epoch 1197/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2845 - val_loss: 28.9095\n",
      "Epoch 1198/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6009 - val_loss: 29.0385\n",
      "Epoch 1199/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5783 - val_loss: 29.2151\n",
      "Epoch 1200/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 25.4670\n",
      "Epoch 01200: saving model to saved_models/latent32/cp-1200.h5\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 25.4670 - val_loss: 28.8390\n",
      "Epoch 1201/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7712 - val_loss: 28.6332\n",
      "Epoch 1202/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6151 - val_loss: 28.3395\n",
      "Epoch 1203/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4740 - val_loss: 28.8723\n",
      "Epoch 1204/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7793 - val_loss: 28.1775\n",
      "Epoch 1205/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5815 - val_loss: 28.8502\n",
      "Epoch 1206/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4543 - val_loss: 28.6197\n",
      "Epoch 1207/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2972 - val_loss: 28.9165\n",
      "Epoch 1208/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7143 - val_loss: 28.2672\n",
      "Epoch 1209/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9143 - val_loss: 30.0971\n",
      "Epoch 1210/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7092 - val_loss: 29.4009\n",
      "Epoch 1211/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7909 - val_loss: 29.2287\n",
      "Epoch 1212/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4292 - val_loss: 28.8913\n",
      "Epoch 1213/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5078 - val_loss: 28.5531\n",
      "Epoch 1214/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2895 - val_loss: 28.3410\n",
      "Epoch 1215/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3232 - val_loss: 28.2886\n",
      "Epoch 1216/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3037 - val_loss: 29.0707\n",
      "Epoch 1217/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4084 - val_loss: 28.3581\n",
      "Epoch 1218/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4055 - val_loss: 28.3994\n",
      "Epoch 1219/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4562 - val_loss: 29.1302\n",
      "Epoch 1220/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5772 - val_loss: 28.3777\n",
      "Epoch 1221/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3387 - val_loss: 28.2349\n",
      "Epoch 1222/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2763 - val_loss: 28.6092\n",
      "Epoch 1223/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4789 - val_loss: 28.6593\n",
      "Epoch 1224/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6331 - val_loss: 28.5234\n",
      "Epoch 1225/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4442 - val_loss: 28.5290\n",
      "Epoch 1226/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3829 - val_loss: 28.3756\n",
      "Epoch 1227/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3596 - val_loss: 28.0832\n",
      "Epoch 1228/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1406 - val_loss: 28.3973\n",
      "Epoch 1229/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4692 - val_loss: 28.9769\n",
      "Epoch 1230/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4134 - val_loss: 28.4561\n",
      "Epoch 1231/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3689 - val_loss: 28.5517\n",
      "Epoch 1232/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2137 - val_loss: 28.5249\n",
      "Epoch 1233/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2637 - val_loss: 28.7400\n",
      "Epoch 1234/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3066 - val_loss: 28.8128\n",
      "Epoch 1235/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3521 - val_loss: 28.5829\n",
      "Epoch 1236/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4437 - val_loss: 28.3985\n",
      "Epoch 1237/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4564 - val_loss: 28.5246\n",
      "Epoch 1238/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4494 - val_loss: 29.2495\n",
      "Epoch 1239/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2217 - val_loss: 28.0897\n",
      "Epoch 1240/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3368 - val_loss: 28.4674\n",
      "Epoch 1241/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1484 - val_loss: 28.9562\n",
      "Epoch 1242/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4584 - val_loss: 28.2726\n",
      "Epoch 1243/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4593 - val_loss: 29.5727\n",
      "Epoch 1244/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5145 - val_loss: 29.5218\n",
      "Epoch 1245/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7435 - val_loss: 29.4829\n",
      "Epoch 1246/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4009 - val_loss: 29.0327\n",
      "Epoch 1247/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5103 - val_loss: 29.0128\n",
      "Epoch 1248/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4378 - val_loss: 29.1866\n",
      "Epoch 1249/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5781 - val_loss: 28.7269\n",
      "Epoch 1250/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5218 - val_loss: 28.3344\n",
      "Epoch 1251/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5753 - val_loss: 29.1874\n",
      "Epoch 1252/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4519 - val_loss: 28.7197\n",
      "Epoch 1253/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5845 - val_loss: 28.3843\n",
      "Epoch 1254/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3601 - val_loss: 28.0689\n",
      "Epoch 1255/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4340 - val_loss: 29.3774\n",
      "Epoch 1256/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0333 - val_loss: 28.5078\n",
      "Epoch 1257/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1660 - val_loss: 29.6740\n",
      "Epoch 1258/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5819 - val_loss: 29.1548\n",
      "Epoch 1259/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3280 - val_loss: 28.5558\n",
      "Epoch 1260/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2960 - val_loss: 28.9610\n",
      "Epoch 1261/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2800 - val_loss: 28.8961\n",
      "Epoch 1262/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2009 - val_loss: 28.3714\n",
      "Epoch 1263/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1238 - val_loss: 28.5904\n",
      "Epoch 1264/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1923 - val_loss: 28.7487\n",
      "Epoch 1265/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1091 - val_loss: 29.0796\n",
      "Epoch 1266/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3784 - val_loss: 30.4279\n",
      "Epoch 1267/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8554 - val_loss: 28.4430\n",
      "Epoch 1268/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4042 - val_loss: 28.8761\n",
      "Epoch 1269/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1926 - val_loss: 28.5044\n",
      "Epoch 1270/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1456 - val_loss: 29.0816\n",
      "Epoch 1271/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4685 - val_loss: 29.0120\n",
      "Epoch 1272/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6609 - val_loss: 28.5914\n",
      "Epoch 1273/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3569 - val_loss: 28.4024\n",
      "Epoch 1274/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2502 - val_loss: 27.8989\n",
      "Epoch 1275/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0671 - val_loss: 28.9411\n",
      "Epoch 1276/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4118 - val_loss: 28.7179\n",
      "Epoch 1277/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4838 - val_loss: 28.5749\n",
      "Epoch 1278/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4028 - val_loss: 28.8647\n",
      "Epoch 1279/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4032 - val_loss: 28.4635\n",
      "Epoch 1280/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3335 - val_loss: 28.6575\n",
      "Epoch 1281/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3406 - val_loss: 29.0828\n",
      "Epoch 1282/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1419 - val_loss: 28.5713\n",
      "Epoch 1283/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5141 - val_loss: 28.8701\n",
      "Epoch 1284/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4699 - val_loss: 29.2986\n",
      "Epoch 1285/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5268 - val_loss: 28.1387\n",
      "Epoch 1286/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3883 - val_loss: 28.5805\n",
      "Epoch 1287/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4497 - val_loss: 28.6832\n",
      "Epoch 1288/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2760 - val_loss: 29.0013\n",
      "Epoch 1289/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8087 - val_loss: 29.2688\n",
      "Epoch 1290/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5262 - val_loss: 28.7306\n",
      "Epoch 1291/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3153 - val_loss: 27.7800\n",
      "Epoch 1292/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0952 - val_loss: 29.4951\n",
      "Epoch 1293/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4225 - val_loss: 28.8553\n",
      "Epoch 1294/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2170 - val_loss: 28.1365\n",
      "Epoch 1295/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2465 - val_loss: 28.2013\n",
      "Epoch 1296/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2983 - val_loss: 28.4985\n",
      "Epoch 1297/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4443 - val_loss: 28.5224\n",
      "Epoch 1298/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6223 - val_loss: 29.1190\n",
      "Epoch 1299/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2759 - val_loss: 28.8739\n",
      "Epoch 1300/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6171 - val_loss: 28.8953\n",
      "Epoch 1301/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3812 - val_loss: 28.2556\n",
      "Epoch 1302/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2322 - val_loss: 28.6374\n",
      "Epoch 1303/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2126 - val_loss: 28.5179\n",
      "Epoch 1304/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3605 - val_loss: 28.6298\n",
      "Epoch 1305/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2609 - val_loss: 28.6099\n",
      "Epoch 1306/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0713 - val_loss: 27.8884\n",
      "Epoch 1307/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2289 - val_loss: 29.0390\n",
      "Epoch 1308/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4435 - val_loss: 28.8607\n",
      "Epoch 1309/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3371 - val_loss: 28.4629\n",
      "Epoch 1310/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2563 - val_loss: 29.2572\n",
      "Epoch 1311/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2055 - val_loss: 28.6048\n",
      "Epoch 1312/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1342 - val_loss: 29.7379\n",
      "Epoch 1313/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6009 - val_loss: 28.7114\n",
      "Epoch 1314/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3341 - val_loss: 28.2192\n",
      "Epoch 1315/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.9886 - val_loss: 28.4967\n",
      "Epoch 1316/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2557 - val_loss: 29.2498\n",
      "Epoch 1317/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6153 - val_loss: 29.2776\n",
      "Epoch 1318/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7287 - val_loss: 28.4609\n",
      "Epoch 1319/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2529 - val_loss: 28.5979\n",
      "Epoch 1320/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1744 - val_loss: 28.4703\n",
      "Epoch 1321/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1855 - val_loss: 28.5407\n",
      "Epoch 1322/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3559 - val_loss: 28.9407\n",
      "Epoch 1323/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2596 - val_loss: 28.7253\n",
      "Epoch 1324/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3916 - val_loss: 28.6271\n",
      "Epoch 1325/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2718 - val_loss: 28.8901\n",
      "Epoch 1326/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3911 - val_loss: 28.4608\n",
      "Epoch 1327/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3432 - val_loss: 28.1537\n",
      "Epoch 1328/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0814 - val_loss: 29.1109\n",
      "Epoch 1329/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2498 - val_loss: 28.7329\n",
      "Epoch 1330/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3432 - val_loss: 28.5745\n",
      "Epoch 1331/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0960 - val_loss: 28.3894\n",
      "Epoch 1332/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1372 - val_loss: 29.2344\n",
      "Epoch 1333/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2499 - val_loss: 28.3774\n",
      "Epoch 1334/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2741 - val_loss: 28.9368\n",
      "Epoch 1335/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5125 - val_loss: 28.5995\n",
      "Epoch 1336/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2539 - val_loss: 28.4814\n",
      "Epoch 1337/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5761 - val_loss: 29.6266\n",
      "Epoch 1338/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7549 - val_loss: 29.0682\n",
      "Epoch 1339/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1833 - val_loss: 28.9785\n",
      "Epoch 1340/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2098 - val_loss: 29.1940\n",
      "Epoch 1341/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1612 - val_loss: 27.9100\n",
      "Epoch 1342/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0342 - val_loss: 28.5204\n",
      "Epoch 1343/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2601 - val_loss: 27.5256\n",
      "Epoch 1344/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0552 - val_loss: 28.7573\n",
      "Epoch 1345/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3080 - val_loss: 28.9611\n",
      "Epoch 1346/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2707 - val_loss: 27.8993\n",
      "Epoch 1347/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0616 - val_loss: 28.4995\n",
      "Epoch 1348/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2344 - val_loss: 28.9294\n",
      "Epoch 1349/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1747 - val_loss: 28.7554\n",
      "Epoch 1350/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2294 - val_loss: 28.8514\n",
      "Epoch 1351/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1646 - val_loss: 28.5833\n",
      "Epoch 1352/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2470 - val_loss: 28.4322\n",
      "Epoch 1353/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0279 - val_loss: 29.0143\n",
      "Epoch 1354/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0429 - val_loss: 28.9063\n",
      "Epoch 1355/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3111 - val_loss: 29.0079\n",
      "Epoch 1356/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2153 - val_loss: 28.9937\n",
      "Epoch 1357/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2409 - val_loss: 28.7256\n",
      "Epoch 1358/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2766 - val_loss: 29.3584\n",
      "Epoch 1359/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5815 - val_loss: 28.5929\n",
      "Epoch 1360/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3379 - val_loss: 28.6539\n",
      "Epoch 1361/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1164 - val_loss: 28.3493\n",
      "Epoch 1362/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1934 - val_loss: 29.7148\n",
      "Epoch 1363/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7699 - val_loss: 29.3267\n",
      "Epoch 1364/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5905 - val_loss: 29.4736\n",
      "Epoch 1365/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2079 - val_loss: 28.5812\n",
      "Epoch 1366/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3575 - val_loss: 28.7272\n",
      "Epoch 1367/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0840 - val_loss: 28.7327\n",
      "Epoch 1368/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1223 - val_loss: 29.1964\n",
      "Epoch 1369/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1782 - val_loss: 28.7398\n",
      "Epoch 1370/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1382 - val_loss: 28.9926\n",
      "Epoch 1371/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2104 - val_loss: 28.1361\n",
      "Epoch 1372/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0840 - val_loss: 28.3068\n",
      "Epoch 1373/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9741 - val_loss: 28.3527\n",
      "Epoch 1374/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9736 - val_loss: 28.0740\n",
      "Epoch 1375/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2631 - val_loss: 29.3656\n",
      "Epoch 1376/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5430 - val_loss: 29.4127\n",
      "Epoch 1377/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2662 - val_loss: 28.2716\n",
      "Epoch 1378/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6253 - val_loss: 27.8154\n",
      "Epoch 1379/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5663 - val_loss: 28.7150\n",
      "Epoch 1380/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3372 - val_loss: 29.0287\n",
      "Epoch 1381/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4339 - val_loss: 28.9269\n",
      "Epoch 1382/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1196 - val_loss: 28.9023\n",
      "Epoch 1383/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2206 - val_loss: 28.4200\n",
      "Epoch 1384/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8603 - val_loss: 28.7706\n",
      "Epoch 1385/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1982 - val_loss: 28.9077\n",
      "Epoch 1386/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1622 - val_loss: 28.2815\n",
      "Epoch 1387/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1690 - val_loss: 28.2254\n",
      "Epoch 1388/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1195 - val_loss: 28.2300\n",
      "Epoch 1389/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0075 - val_loss: 28.9616\n",
      "Epoch 1390/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9574 - val_loss: 28.5336\n",
      "Epoch 1391/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9118 - val_loss: 28.5537\n",
      "Epoch 1392/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0411 - val_loss: 28.2636\n",
      "Epoch 1393/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9584 - val_loss: 28.2402\n",
      "Epoch 1394/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3097 - val_loss: 28.9345\n",
      "Epoch 1395/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8227 - val_loss: 29.3367\n",
      "Epoch 1396/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2567 - val_loss: 29.2719\n",
      "Epoch 1397/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3257 - val_loss: 28.5497\n",
      "Epoch 1398/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3774 - val_loss: 28.3700\n",
      "Epoch 1399/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0925 - val_loss: 29.1377\n",
      "Epoch 1400/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 25.6374\n",
      "Epoch 01400: saving model to saved_models/latent32/cp-1400.h5\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 25.6374 - val_loss: 28.7546\n",
      "Epoch 1401/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2036 - val_loss: 28.7413\n",
      "Epoch 1402/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0813 - val_loss: 28.4569\n",
      "Epoch 1403/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2623 - val_loss: 28.7887\n",
      "Epoch 1404/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2538 - val_loss: 28.3975\n",
      "Epoch 1405/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0012 - val_loss: 29.2414\n",
      "Epoch 1406/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0922 - val_loss: 28.3985\n",
      "Epoch 1407/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9857 - val_loss: 28.2337\n",
      "Epoch 1408/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1967 - val_loss: 29.0084\n",
      "Epoch 1409/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2906 - val_loss: 28.8465\n",
      "Epoch 1410/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1488 - val_loss: 28.3546\n",
      "Epoch 1411/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0792 - val_loss: 28.3810\n",
      "Epoch 1412/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0508 - val_loss: 28.2839\n",
      "Epoch 1413/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9862 - val_loss: 28.4074\n",
      "Epoch 1414/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0346 - val_loss: 28.5258\n",
      "Epoch 1415/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8597 - val_loss: 28.7896\n",
      "Epoch 1416/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0055 - val_loss: 28.8107\n",
      "Epoch 1417/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0122 - val_loss: 27.8138\n",
      "Epoch 1418/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0040 - val_loss: 28.3457\n",
      "Epoch 1419/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9899 - val_loss: 27.9530\n",
      "Epoch 1420/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8156 - val_loss: 28.6174\n",
      "Epoch 1421/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0672 - val_loss: 28.3273\n",
      "Epoch 1422/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0111 - val_loss: 28.7598\n",
      "Epoch 1423/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8964 - val_loss: 28.7749\n",
      "Epoch 1424/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1194 - val_loss: 28.4045\n",
      "Epoch 1425/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9721 - val_loss: 28.3174\n",
      "Epoch 1426/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0113 - val_loss: 29.0096\n",
      "Epoch 1427/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0058 - val_loss: 28.2427\n",
      "Epoch 1428/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0399 - val_loss: 28.6414\n",
      "Epoch 1429/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0861 - val_loss: 29.0211\n",
      "Epoch 1430/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1669 - val_loss: 28.6446\n",
      "Epoch 1431/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9572 - val_loss: 29.7509\n",
      "Epoch 1432/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4223 - val_loss: 29.1168\n",
      "Epoch 1433/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3835 - val_loss: 28.9749\n",
      "Epoch 1434/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2439 - val_loss: 28.5149\n",
      "Epoch 1435/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1227 - val_loss: 28.9176\n",
      "Epoch 1436/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1202 - val_loss: 27.9410\n",
      "Epoch 1437/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0818 - val_loss: 28.8206\n",
      "Epoch 1438/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1191 - val_loss: 28.2601\n",
      "Epoch 1439/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2565 - val_loss: 28.8698\n",
      "Epoch 1440/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4048 - val_loss: 28.3986\n",
      "Epoch 1441/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4520 - val_loss: 28.7175\n",
      "Epoch 1442/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1543 - val_loss: 28.9414\n",
      "Epoch 1443/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4990 - val_loss: 29.6177\n",
      "Epoch 1444/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0928 - val_loss: 28.5213\n",
      "Epoch 1445/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6886 - val_loss: 29.2756\n",
      "Epoch 1446/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3881 - val_loss: 28.4690\n",
      "Epoch 1447/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1985 - val_loss: 28.5422\n",
      "Epoch 1448/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8674 - val_loss: 28.7396\n",
      "Epoch 1449/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9814 - val_loss: 27.8986\n",
      "Epoch 1450/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8977 - val_loss: 28.3671\n",
      "Epoch 1451/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0289 - val_loss: 28.3271\n",
      "Epoch 1452/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9526 - val_loss: 28.6811\n",
      "Epoch 1453/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9985 - val_loss: 29.6142\n",
      "Epoch 1454/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4015 - val_loss: 28.7579\n",
      "Epoch 1455/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3115 - val_loss: 29.3097\n",
      "Epoch 1456/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1230 - val_loss: 29.0416\n",
      "Epoch 1457/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1121 - val_loss: 28.9883\n",
      "Epoch 1458/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0410 - val_loss: 28.7011\n",
      "Epoch 1459/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0181 - val_loss: 29.4928\n",
      "Epoch 1460/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1847 - val_loss: 28.9342\n",
      "Epoch 1461/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1375 - val_loss: 28.9285\n",
      "Epoch 1462/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1712 - val_loss: 28.7236\n",
      "Epoch 1463/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1478 - val_loss: 27.9434\n",
      "Epoch 1464/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0037 - val_loss: 28.4185\n",
      "Epoch 1465/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9636 - val_loss: 28.3468\n",
      "Epoch 1466/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9895 - val_loss: 28.4093\n",
      "Epoch 1467/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9967 - val_loss: 29.3524\n",
      "Epoch 1468/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9932 - val_loss: 28.5997\n",
      "Epoch 1469/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9641 - val_loss: 28.9569\n",
      "Epoch 1470/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1500 - val_loss: 28.3959\n",
      "Epoch 1471/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0747 - val_loss: 29.1340\n",
      "Epoch 1472/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9792 - val_loss: 28.6408\n",
      "Epoch 1473/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2828 - val_loss: 28.8566\n",
      "Epoch 1474/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3078 - val_loss: 29.7399\n",
      "Epoch 1475/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1883 - val_loss: 28.6471\n",
      "Epoch 1476/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9647 - val_loss: 29.4151\n",
      "Epoch 1477/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0186 - val_loss: 28.6115\n",
      "Epoch 1478/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0592 - val_loss: 28.3634\n",
      "Epoch 1479/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1130 - val_loss: 28.7944\n",
      "Epoch 1480/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9557 - val_loss: 28.6173\n",
      "Epoch 1481/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0984 - val_loss: 28.9073\n",
      "Epoch 1482/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3070 - val_loss: 29.3097\n",
      "Epoch 1483/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1376 - val_loss: 28.6230\n",
      "Epoch 1484/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0017 - val_loss: 28.2266\n",
      "Epoch 1485/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2684 - val_loss: 28.9698\n",
      "Epoch 1486/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8361 - val_loss: 28.2893\n",
      "Epoch 1487/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8524 - val_loss: 28.0883\n",
      "Epoch 1488/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8310 - val_loss: 29.0013\n",
      "Epoch 1489/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9496 - val_loss: 29.3847\n",
      "Epoch 1490/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2917 - val_loss: 29.3911\n",
      "Epoch 1491/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3259 - val_loss: 29.2471\n",
      "Epoch 1492/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2073 - val_loss: 27.8833\n",
      "Epoch 1493/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1793 - val_loss: 29.0397\n",
      "Epoch 1494/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5574 - val_loss: 29.3830\n",
      "Epoch 1495/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2887 - val_loss: 28.8633\n",
      "Epoch 1496/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4214 - val_loss: 29.0766\n",
      "Epoch 1497/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0562 - val_loss: 28.5630\n",
      "Epoch 1498/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9657 - val_loss: 29.0773\n",
      "Epoch 1499/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9194 - val_loss: 28.7071\n",
      "Epoch 1500/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2160 - val_loss: 28.3715\n",
      "Epoch 1501/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9218 - val_loss: 28.5798\n",
      "Epoch 1502/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1989 - val_loss: 28.8113\n",
      "Epoch 1503/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0144 - val_loss: 28.6695\n",
      "Epoch 1504/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0453 - val_loss: 28.4598\n",
      "Epoch 1505/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0063 - val_loss: 28.6913\n",
      "Epoch 1506/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9023 - val_loss: 28.7021\n",
      "Epoch 1507/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2023 - val_loss: 29.1290\n",
      "Epoch 1508/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9938 - val_loss: 29.4051\n",
      "Epoch 1509/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0333 - val_loss: 28.7564\n",
      "Epoch 1510/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0588 - val_loss: 29.3130\n",
      "Epoch 1511/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9931 - val_loss: 29.3210\n",
      "Epoch 1512/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9135 - val_loss: 28.9428\n",
      "Epoch 1513/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8903 - val_loss: 27.9890\n",
      "Epoch 1514/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8201 - val_loss: 29.3567\n",
      "Epoch 1515/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0608 - val_loss: 28.4376\n",
      "Epoch 1516/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0131 - val_loss: 28.7028\n",
      "Epoch 1517/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9591 - val_loss: 28.8568\n",
      "Epoch 1518/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0077 - val_loss: 29.0733\n",
      "Epoch 1519/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9129 - val_loss: 29.3883\n",
      "Epoch 1520/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 24.9415Restoring model weights from the end of the best epoch.\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9415 - val_loss: 28.4973\n",
      "Epoch 01520: early stopping\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train VAE model with callbacks \"\"\"\n",
    "history = vae.fit(trainX, trainX, \n",
    "                  batch_size = batch_size, \n",
    "                  epochs = epochs, \n",
    "                  callbacks=[initial_checkpoint, checkpoint, EarlyStoppingAtMinLoss()],\n",
    "                  #callbacks=[initial_checkpoint, checkpoint],\n",
    "                  shuffle=True,\n",
    "                  validation_data=(valX, valX)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 7.0\n"
     ]
    }
   ],
   "source": [
    "# In this GPU implementation, it shows the number of batches/iterations for one epoch (and not the training samples), which is:\n",
    "print (\"Step size:\", np.ceil(trainX.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save the entire model \"\"\"\n",
    "# Save notes about hyperparameters used:\n",
    "epochs = len(history.history['loss'])\n",
    "with open(f'{SAVE_FOLDER}/notes.txt', 'w') as f:\n",
    "    f.write(f'Epochs: {epochs} \\n')\n",
    "    f.write(f'Batch size: {batch_size} \\n')\n",
    "    f.write(f'Latent dimension: {latent_dim} \\n')\n",
    "    f.write(f'Filter sizes: {filters} \\n')\n",
    "    f.write(f'img_width: {img_width} \\n')\n",
    "    f.write(f'img_height: {img_height} \\n')\n",
    "    f.write(f'num_channels: {num_channels}')\n",
    "    f.close()\n",
    "\n",
    "# Save weights for encoder model:\n",
    "encoder.save_weights(f'{SAVE_FOLDER}/ENCODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for decoder model:\n",
    "decoder.save_weights(f'{SAVE_FOLDER}/DECODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for total VAE:\n",
    "vae.save_weights(f'{SAVE_FOLDER}/VAE_WEIGHTS.h5')\n",
    "\n",
    "# Save the history at each epochs (cost of train and validation sets):\n",
    "np.save(f'{SAVE_FOLDER}/HISTORY.npy', history.history)\n",
    "\n",
    "# Save exact training, validation and testing data set (as numpy arrays):\n",
    "np.save(f'{SAVE_FOLDER}/trainX.npy', trainX)\n",
    "np.save(f'{SAVE_FOLDER}/valX.npy', valX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllX.npy', testAllX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllY.npy', testAllY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate training process\n",
    "\n",
    "Now that the training process is complete, let’s evaluate the average loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(13, 6))\n",
    "\n",
    "# summarize history for loss:\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='blue', alpha=0.4, label='validation')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Cost', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.title('Average Loss During Training', fontsize=18)\n",
    "#plt.xticks(range(0,epochs))\n",
    "plt.legend(loc='upper right', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
