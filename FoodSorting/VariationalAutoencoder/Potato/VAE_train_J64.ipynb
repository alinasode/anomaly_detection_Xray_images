{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import axis, colorbar, imshow, show, figure, subplot\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=18)\n",
    "mpl.rc('xtick', labelsize=16)\n",
    "mpl.rc('ytick', labelsize=16)\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## ----- GPU ------------------------------------\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'    # use of GPU 0 (ERDA) (use before importing torch or tensorflow/keeas)\n",
    "## ----------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization, ReLU, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.activations import relu\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K                         #contains calls for tensor manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL_TRAIN_AUG:       /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/NormalTrainAugmentations\n",
      "NORMAL_VALIDATION_AUG:  /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/NormalValidationAugmentations\n",
      "NORMAL_TEST_AUG:        /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/NormalTestAugmentations\n",
      "ANOMALY1_AUG:           /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/Anomaly1Augmentations\n",
      "ANOMALY2_AUG:           /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/Anomaly2Augmentations\n"
     ]
    }
   ],
   "source": [
    "from utils_vae_potato_paths import *\n",
    "\n",
    "# Get work directions for augmentated data set:\n",
    "print (\"NORMAL_TRAIN_AUG:      \", NORMAL_TRAIN_AUG)\n",
    "print (\"NORMAL_VALIDATION_AUG: \", NORMAL_VAL_AUG)\n",
    "print (\"NORMAL_TEST_AUG:       \", NORMAL_TEST_AUG)\n",
    "print (\"ANOMALY1_AUG:          \", ANOMALY1_AUG)\n",
    "print (\"ANOMALY2_AUG:          \", ANOMALY2_AUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which Folder The Models Are Saved In: saved_models/latent64\n"
     ]
    }
   ],
   "source": [
    "# What latent dimension and filter size are we using?:\n",
    "latent_dim = 64\n",
    "filters    = 32\n",
    "\n",
    "# Get work direction for saving files:\n",
    "SAVE_FOLDER = f'saved_models/latent{latent_dim}'\n",
    "print (\"Which Folder The Models Are Saved In:\", SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] deleting existing files in folder...\n",
      "         done in 0.052 minutes\n"
     ]
    }
   ],
   "source": [
    "# Delete all existing files in folder where models are saved:\n",
    "print(\"[INFO] deleting existing files in folder...\")\n",
    "t0 = time()\n",
    "\n",
    "files = glob.glob(f'{SAVE_FOLDER}/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "print (\"         done in %0.3f minutes\" % ((time() - t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_VAE_AE import get_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Investigation of Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"normal\" training images is:     1639\n",
      "Number of \"normal\" validation images is:   80\n",
      "Number of \"normal\" test images is:         320\n",
      "Total number of \"anomaly\" images is:       680\n"
     ]
    }
   ],
   "source": [
    "# Glob the directories and get the lists of good and bad images\n",
    "good_train_fns = [f for f in os.listdir(NORMAL_TRAIN_AUG) if not f.startswith('.')]\n",
    "good_val_fns = [f for f in os.listdir(NORMAL_VAL_AUG) if not f.startswith('.')]\n",
    "good_test_fns = [f for f in os.listdir(NORMAL_TEST_AUG) if not f.startswith('.')]\n",
    "bad1_fns = [f for f in os.listdir(ANOMALY1_AUG) if not f.startswith('.')]\n",
    "bad2_fns = [f for f in os.listdir(ANOMALY2_AUG) if not f.startswith('.')]\n",
    "\n",
    "print('Number of \"normal\" training images is:     {}'.format(len(good_train_fns)))\n",
    "print('Number of \"normal\" validation images is:   {}'.format(len(good_val_fns)))\n",
    "print('Number of \"normal\" test images is:         {}'.format(len(good_test_fns)))\n",
    "print('Total number of \"anomaly\" images is:       {}'.format(len(bad1_fns) + len(bad2_fns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Procentage of normal samples (ground truth):   74.99 %\n",
      "> Procentage of anomaly samples (ground truth):  25.01 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of bad images vs good images:\n",
    "normal_fns = len(good_train_fns) + len(good_val_fns) + len(good_test_fns)\n",
    "anomaly_fns = len(bad1_fns + bad2_fns)\n",
    "print(f\"> Procentage of normal samples (ground truth):   {(normal_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")\n",
    "print(f\"> Procentage of anomaly samples (ground truth):  {(anomaly_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Normal Data\n",
    "\n",
    "Load normal samples in pre-splitted data sets: train, valdiation and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal training images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal training images...\")\n",
    "trainX = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TRAIN_AUG}/*.png\")]  # read as grayscale\n",
    "trainX = np.array(trainX)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "trainY = get_labels(trainX, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal validation images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal validation images...\")\n",
    "x_normal_val = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_VAL_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_val = np.array(x_normal_val)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_val = get_labels(x_normal_val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal testing images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal testing images...\")\n",
    "x_normal_test = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TEST_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_test = np.array(x_normal_test)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_test = get_labels(x_normal_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Class 1 Data (i.e. 'metal' sampels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly class 1 ('metal') images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly class 1 ('metal') images...\")\n",
    "x_metal = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY1_AUG}/*.png\")]  # read as grayscale\n",
    "x_metal = np.array(x_metal)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_metal = get_labels(x_metal, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Class 2 Data (i.e. 'hollow' sampels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly class 2 ('hollow') images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly class 2 ('hollow') images...\")\n",
    "x_hollow = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY2_AUG}/*.png\")]  # read as grayscale\n",
    "x_hollow = np.array(x_hollow)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_hollow = get_labels(x_hollow, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine normal samples and anomaly samples and split them into training, validation and test sets\n",
    "\n",
    "\n",
    "`label 0` == Normal Samples (=> Perfect' Samples)\n",
    "\n",
    "`label 1` == Anomaly 1 Samples (=> 'Metal' Samples)\n",
    "\n",
    "`label 2` == Anomaly 2 Samples (=> 'Hollow' Samples)\n",
    "\n",
    "Train and Validation sets only consists of `Normal Data`, aka. `label 0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testvalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testvalY = get_labels(testvalX, 0)\n",
    "\n",
    "# randomly split in order to make new validation set:\n",
    "NoUsageX, valX, NoUsageY, valY = train_test_split(testvalX, testvalY, test_size=0.25, random_state=4345672)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Testing set \"\"\"\n",
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testNormalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testNormalY = get_labels(testNormalX, 0)\n",
    "\n",
    "# Anomaly Class 1 (i.e. 'metal' samples):\n",
    "testAnomaly1X, testAnomaly1Y = x_metal, y_metal\n",
    "\n",
    "# Anomaly Class 2 (i.e. 'hollow' samples):\n",
    "testAnomaly2X, testAnomaly2Y = x_hollow, y_hollow\n",
    "\n",
    "# Combine all testing data in one array (the test set is NOT used during any part but inference):\n",
    "testAllX = np.vstack((testNormalX, testAnomaly1X, testAnomaly2X))\n",
    "testAllY = np.hstack((testNormalY, testAnomaly1Y, testAnomaly2Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Dimensions and Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      " > images: (1639, 128, 128)\n",
      " > labels: (1639,)\n",
      "\n",
      "Validation set:\n",
      " > images: (100, 128, 128)\n",
      " > labels: (100,)\n",
      "\n",
      "Test set:\n",
      " > images: (1080, 128, 128)\n",
      " > labels: (1080,)\n",
      "\t'Normal' test set:\n",
      "\t  > images: (400, 128, 128)\n",
      "\t  > labels: (400,)\n",
      "\t'Anomaly 1' test set:\n",
      "\t  > images: (392, 128, 128)\n",
      "\t  > labels: (392,)\n",
      "\t'Anomaly 2' test set:\n",
      "\t  > images: (288, 128, 128)\n",
      "\t  > labels: (288,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "print(\" > images:\", trainX.shape)\n",
    "print(\" > labels:\", trainY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Validation set:\")\n",
    "print(\" > images:\", valX.shape)\n",
    "print(\" > labels:\", valY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Test set:\")\n",
    "print(\" > images:\", testAllX.shape)\n",
    "print(\" > labels:\", testAllY.shape)\n",
    "print(\"\\t'Normal' test set:\")\n",
    "print(\"\\t  > images:\", testNormalX.shape)\n",
    "print(\"\\t  > labels:\", testNormalY.shape)\n",
    "print(\"\\t'Anomaly 1' test set:\")\n",
    "print(\"\\t  > images:\", testAnomaly1X.shape)\n",
    "print(\"\\t  > labels:\", testAnomaly1Y.shape)\n",
    "print(\"\\t'Anomaly 2' test set:\")\n",
    "print(\"\\t  > images:\", testAnomaly2X.shape)\n",
    "print(\"\\t  > labels:\", testAnomaly2Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 76.62 %\n",
      "Procentage of validation samples: 4.68 %\n",
      "Procentage of (only normal) testing samples: 18.70 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets (only normal data):\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (only normal) testing samples: {(len(testNormalX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 58.14 %\n",
      "Procentage of validation samples: 3.55 %\n",
      "Procentage of (total) testing samples: 38.31 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets:\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (total) testing samples: {(len(testAllX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for (un)balanced data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['0: normal', '1: metal', '2: hollow']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,4))\n",
    "ax[0].hist(trainY, align=\"left\"); ax[0].set_title('train', fontsize=18); ax[0].set_xticks([0,1,2]); ax[0].set_xticklabels(labels); ax[0].tick_params(axis='both', which='major', labelsize=16); ax[0].set_xlim(-0.5, 2.5);\n",
    "ax[1].hist(valY, align=\"left\"); ax[1].set_title('validation', fontsize=18); ax[1].set_xticks([0,1,2]); ax[1].set_xticklabels(labels); ax[1].tick_params(axis='both', which='major', labelsize=16); ax[1].set_xlim(-0.5, 2.5);\n",
    "ax[2].hist(testAllY, align=\"left\"); ax[2].set_title('test', fontsize=18); ax[2].set_xticks([0,1,2]); ax[2].set_xticklabels(labels); ax[2].tick_params(axis='both', which='major', labelsize=16); ax[2].set_xlim(-0.5, 2.5);\n",
    "\n",
    "ax[0].set_ylabel(\"Number of images\", fontsize=18)\n",
    "ax[1].set_xlabel(\"Class Labels\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of normal test samples: 37.04 %\n",
      "Procentage of total anomaly test samples: 62.96 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of normal test images vs total anomaly test images:\n",
    "print(f\"Procentage of normal test samples: {(len(testNormalX) / (len(testNormalX) + (len(testAnomaly1X) + len(testAnomaly2X)))) * 100:.2f} %\")\n",
    "print(f\"Procentage of total anomaly test samples: {((len(testAnomaly1X) + len(testAnomaly2X)) / (len(testNormalX) + (len(testAnomaly1X) + len(testAnomaly2X)))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Only normalization has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "img_width, img_height = trainX.shape[1], trainX.shape[2]      # input image dimensions\n",
    "num_channels = 1                                              # gray-channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Neural Networks learns faster with normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to be in  the [0, 1] range:\n",
    "trainX = trainX.astype('float32') / 255.\n",
    "valX = valX.astype('float32') / 255.\n",
    "testAllX = testAllX.astype('float32') / 255.\n",
    "\n",
    "# reshape data to keras inputs --> (n_samples, img_rows, img_cols, n_channels):\n",
    "trainX = trainX.reshape(trainX.shape[0], img_height, img_width, num_channels)\n",
    "valX = valX.reshape(valX.shape[0], img_height, img_width, num_channels)\n",
    "testAllX = testAllX.reshape(testAllX.shape[0], img_height, img_width, num_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import architecture of VAE model and its hyperparameters:\n",
    "from J64_VAE_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Encoder Part*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 32)   320         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 64, 64, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 32)   9248        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 32, 32, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 32)   9248        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 16, 16, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 8, 32)     9248        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 8, 8, 32)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 4, 4, 32)     9248        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 4, 4, 32)     0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "latent (Dense)                  (None, 200)          102600      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 200)          0           latent[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 64)           12864       leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_log_mean (Dense)              (None, 64)           12864       leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 64)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_mean[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 165,640\n",
      "Trainable params: 165,640\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Decoder Part*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               33280     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 128, 128, 1)       289       \n",
      "=================================================================\n",
      "Total params: 79,809\n",
      "Trainable params: 79,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Total VAE Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         [(None, 64), (None, 64),  165640    \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 128, 128, 1)       79809     \n",
      "=================================================================\n",
      "Total params: 245,449\n",
      "Trainable params: 245,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "batch_size  = 256\n",
    "epochs      = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at 1'st epoch (inital model)\n",
    "\n",
    "https://stackoverflow.com/questions/54323960/save-keras-model-at-specific-epochs\n",
    "\"\"\"\n",
    "\n",
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 1:\n",
    "            self.model.save_weights(f\"{SAVE_FOLDER}/cp-0001.h5\")\n",
    "            \n",
    "# create and use callback:\n",
    "initial_checkpoint = CustomSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at every k'te epochs\n",
    "\"\"\"\n",
    "# Include the epoch in the file name (uses `str.format`):\n",
    "checkpoint_path = \"saved_models/latent64/cp-{epoch:04d}.h5\"\n",
    "\n",
    "# Create a callback that saves the model's weights every k'te epochs:\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path,\n",
    "                             monitor='loss',           # name of the metrics to monitor\n",
    "                             verbose=1, \n",
    "                             #save_best_only=False,     # if False, the best model will not be overridden.\n",
    "                             save_weights_only=True,   # if True, only the weights of the models will be saved.\n",
    "                                                        # if False, the whole models will be saved.\n",
    "                             #mode='auto', \n",
    "                             period=200                  # save after every k'te epochs\n",
    "                            )\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "vae.save_weights(checkpoint_path.format(epoch=0))          # save for zero epoch (inital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: Early stopping\n",
    "https://www.tensorflow.org/guide/keras/custom_callback\n",
    "\"\"\"\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=100):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create and Compile Model\"\"\"\n",
    "# import architecture of VAE model and its hyperparameters. learning rate choosen from graph above.\n",
    "vae = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "7/7 [==============================] - 1s 194ms/step - loss: 2319.2600 - val_loss: 2242.1807\n",
      "Epoch 2/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 2045.6786 - val_loss: 1625.6859\n",
      "Epoch 3/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 1327.4830 - val_loss: 1096.9008\n",
      "Epoch 4/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 901.3701 - val_loss: 719.4565\n",
      "Epoch 5/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 620.9555 - val_loss: 527.1929\n",
      "Epoch 6/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 474.1148 - val_loss: 430.4565\n",
      "Epoch 7/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 406.6296 - val_loss: 388.7356\n",
      "Epoch 8/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 374.1154 - val_loss: 371.3413\n",
      "Epoch 9/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 353.6815 - val_loss: 353.4060\n",
      "Epoch 10/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 345.0841 - val_loss: 344.0002\n",
      "Epoch 11/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 332.2746 - val_loss: 328.2566\n",
      "Epoch 12/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 323.4782 - val_loss: 328.0899\n",
      "Epoch 13/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 319.7596 - val_loss: 319.1394\n",
      "Epoch 14/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 312.5005 - val_loss: 323.7284\n",
      "Epoch 15/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 310.1834 - val_loss: 309.1184\n",
      "Epoch 16/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 302.2149 - val_loss: 302.0171\n",
      "Epoch 17/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 298.5410 - val_loss: 298.7975\n",
      "Epoch 18/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 293.0463 - val_loss: 292.1805\n",
      "Epoch 19/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 287.3254 - val_loss: 291.7070\n",
      "Epoch 20/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 282.3341 - val_loss: 279.0962\n",
      "Epoch 21/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 276.8810 - val_loss: 278.8291\n",
      "Epoch 22/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 273.1987 - val_loss: 286.0007\n",
      "Epoch 23/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 268.3478 - val_loss: 272.8569\n",
      "Epoch 24/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 266.8483 - val_loss: 276.4077\n",
      "Epoch 25/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 264.7877 - val_loss: 267.1397\n",
      "Epoch 26/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 260.6246 - val_loss: 260.7562\n",
      "Epoch 27/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 256.9626 - val_loss: 257.4912\n",
      "Epoch 28/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 254.2621 - val_loss: 255.0878\n",
      "Epoch 29/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 252.2691 - val_loss: 251.7830\n",
      "Epoch 30/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 247.8504 - val_loss: 249.6581\n",
      "Epoch 31/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 245.3557 - val_loss: 248.9255\n",
      "Epoch 32/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 243.5677 - val_loss: 250.0069\n",
      "Epoch 33/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 240.1228 - val_loss: 239.8804\n",
      "Epoch 34/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 235.9160 - val_loss: 235.4857\n",
      "Epoch 35/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 231.0582 - val_loss: 231.7277\n",
      "Epoch 36/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 227.5965 - val_loss: 223.9187\n",
      "Epoch 37/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 224.1994 - val_loss: 224.8454\n",
      "Epoch 38/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 221.3220 - val_loss: 223.2505\n",
      "Epoch 39/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 219.6583 - val_loss: 218.2300\n",
      "Epoch 40/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 217.0625 - val_loss: 222.4755\n",
      "Epoch 41/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 216.5210 - val_loss: 219.5188\n",
      "Epoch 42/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 215.6454 - val_loss: 213.1038\n",
      "Epoch 43/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 211.7434 - val_loss: 211.3142\n",
      "Epoch 44/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 208.2980 - val_loss: 208.9775\n",
      "Epoch 45/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 204.5899 - val_loss: 205.3339\n",
      "Epoch 46/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 203.9984 - val_loss: 203.7671\n",
      "Epoch 47/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 202.7255 - val_loss: 200.0198\n",
      "Epoch 48/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 200.4850 - val_loss: 201.4390\n",
      "Epoch 49/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 200.0048 - val_loss: 199.4918\n",
      "Epoch 50/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 196.0972 - val_loss: 196.6810\n",
      "Epoch 51/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 194.0548 - val_loss: 192.9886\n",
      "Epoch 52/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 192.0341 - val_loss: 193.2416\n",
      "Epoch 53/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 190.0748 - val_loss: 187.1236\n",
      "Epoch 54/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 188.0924 - val_loss: 187.9843\n",
      "Epoch 55/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 186.2041 - val_loss: 187.6212\n",
      "Epoch 56/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 187.5259 - val_loss: 183.6882\n",
      "Epoch 57/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 184.2320 - val_loss: 185.2826\n",
      "Epoch 58/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 182.5020 - val_loss: 179.2727\n",
      "Epoch 59/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 180.5440 - val_loss: 179.4289\n",
      "Epoch 60/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 178.6000 - val_loss: 180.5841\n",
      "Epoch 61/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 177.2718 - val_loss: 178.0436\n",
      "Epoch 62/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 175.2916 - val_loss: 175.1519\n",
      "Epoch 63/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 174.3240 - val_loss: 173.6876\n",
      "Epoch 64/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 171.4466 - val_loss: 171.9895\n",
      "Epoch 65/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 169.9832 - val_loss: 170.8866\n",
      "Epoch 66/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 168.6971 - val_loss: 169.0613\n",
      "Epoch 67/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 166.2336 - val_loss: 168.2587\n",
      "Epoch 68/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 165.1668 - val_loss: 164.0787\n",
      "Epoch 69/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 163.3976 - val_loss: 163.1640\n",
      "Epoch 70/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 162.3932 - val_loss: 163.1411\n",
      "Epoch 71/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 163.2943 - val_loss: 164.7065\n",
      "Epoch 72/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 162.7421 - val_loss: 159.4899\n",
      "Epoch 73/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 159.2324 - val_loss: 159.0455\n",
      "Epoch 74/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 158.3189 - val_loss: 157.8677\n",
      "Epoch 75/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 155.5306 - val_loss: 156.5819\n",
      "Epoch 76/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 154.4892 - val_loss: 157.3807\n",
      "Epoch 77/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 153.0847 - val_loss: 152.9068\n",
      "Epoch 78/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 151.3792 - val_loss: 151.6401\n",
      "Epoch 79/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 150.1659 - val_loss: 149.5680\n",
      "Epoch 80/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 149.4707 - val_loss: 148.6762\n",
      "Epoch 81/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 146.7711 - val_loss: 147.2946\n",
      "Epoch 82/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 146.2496 - val_loss: 146.2426\n",
      "Epoch 83/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 143.9566 - val_loss: 149.0662\n",
      "Epoch 84/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 144.0110 - val_loss: 145.0028\n",
      "Epoch 85/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 142.2327 - val_loss: 143.0810\n",
      "Epoch 86/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 141.1318 - val_loss: 141.7236\n",
      "Epoch 87/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 140.1140 - val_loss: 139.9721\n",
      "Epoch 88/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 138.2314 - val_loss: 139.2561\n",
      "Epoch 89/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 137.4034 - val_loss: 138.0941\n",
      "Epoch 90/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 136.2795 - val_loss: 137.7729\n",
      "Epoch 91/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 135.0689 - val_loss: 135.2415\n",
      "Epoch 92/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 132.7092 - val_loss: 132.9122\n",
      "Epoch 93/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 131.3272 - val_loss: 132.0912\n",
      "Epoch 94/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 129.6820 - val_loss: 131.6056\n",
      "Epoch 95/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 129.6512 - val_loss: 129.1786\n",
      "Epoch 96/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 128.6410 - val_loss: 129.2171\n",
      "Epoch 97/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 127.4279 - val_loss: 128.0039\n",
      "Epoch 98/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 126.3575 - val_loss: 129.1410\n",
      "Epoch 99/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 124.8427 - val_loss: 125.4679\n",
      "Epoch 100/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 124.4068 - val_loss: 127.4941\n",
      "Epoch 101/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 123.6132 - val_loss: 126.7257\n",
      "Epoch 102/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 122.7941 - val_loss: 124.1802\n",
      "Epoch 103/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 121.4226 - val_loss: 128.5593\n",
      "Epoch 104/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 120.5322 - val_loss: 120.5946\n",
      "Epoch 105/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 118.2661 - val_loss: 119.1573\n",
      "Epoch 106/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 117.7400 - val_loss: 119.4513\n",
      "Epoch 107/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 117.0923 - val_loss: 117.2339\n",
      "Epoch 108/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 115.3910 - val_loss: 116.1567\n",
      "Epoch 109/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 115.1526 - val_loss: 115.7006\n",
      "Epoch 110/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 114.4117 - val_loss: 114.9816\n",
      "Epoch 111/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 113.3874 - val_loss: 117.9483\n",
      "Epoch 112/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 112.7373 - val_loss: 112.7056\n",
      "Epoch 113/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 113.5202 - val_loss: 114.5013\n",
      "Epoch 114/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 114.6682 - val_loss: 116.0304\n",
      "Epoch 115/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 114.2383 - val_loss: 112.3200\n",
      "Epoch 116/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 110.8191 - val_loss: 110.7376\n",
      "Epoch 117/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 109.0661 - val_loss: 110.1020\n",
      "Epoch 118/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 108.2041 - val_loss: 108.4300\n",
      "Epoch 119/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 106.9803 - val_loss: 107.9416\n",
      "Epoch 120/2000\n",
      "7/7 [==============================] - 1s 108ms/step - loss: 106.1319 - val_loss: 107.1797\n",
      "Epoch 121/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 106.0199 - val_loss: 106.1115\n",
      "Epoch 122/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 104.3782 - val_loss: 103.9368\n",
      "Epoch 123/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 104.1919 - val_loss: 104.9514\n",
      "Epoch 124/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 102.7689 - val_loss: 102.7282\n",
      "Epoch 125/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 102.7176 - val_loss: 102.9831\n",
      "Epoch 126/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 101.3304 - val_loss: 101.3206\n",
      "Epoch 127/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 100.3143 - val_loss: 101.2677\n",
      "Epoch 128/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 100.3817 - val_loss: 100.6559\n",
      "Epoch 129/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 99.8701 - val_loss: 103.5639\n",
      "Epoch 130/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 98.8621 - val_loss: 98.8682\n",
      "Epoch 131/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 97.4009 - val_loss: 97.3747\n",
      "Epoch 132/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 97.9897 - val_loss: 99.6854\n",
      "Epoch 133/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 97.5345 - val_loss: 97.0217\n",
      "Epoch 134/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 95.2058 - val_loss: 97.1212\n",
      "Epoch 135/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 95.1676 - val_loss: 98.1562\n",
      "Epoch 136/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 95.8142 - val_loss: 97.7778\n",
      "Epoch 137/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 94.2845 - val_loss: 95.2626\n",
      "Epoch 138/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 92.7852 - val_loss: 95.3609\n",
      "Epoch 139/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 92.8320 - val_loss: 94.1980\n",
      "Epoch 140/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 91.6660 - val_loss: 92.2356\n",
      "Epoch 141/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 90.6512 - val_loss: 92.0462\n",
      "Epoch 142/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 90.8493 - val_loss: 90.8808\n",
      "Epoch 143/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 90.1632 - val_loss: 90.1989\n",
      "Epoch 144/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 88.7001 - val_loss: 91.4519\n",
      "Epoch 145/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 89.2928 - val_loss: 90.2537\n",
      "Epoch 146/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 88.8297 - val_loss: 89.0293\n",
      "Epoch 147/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 87.7578 - val_loss: 88.2237\n",
      "Epoch 148/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 86.4914 - val_loss: 88.0151\n",
      "Epoch 149/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 86.3391 - val_loss: 86.0940\n",
      "Epoch 150/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 85.7129 - val_loss: 85.9093\n",
      "Epoch 151/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 84.4471 - val_loss: 85.3320\n",
      "Epoch 152/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 84.8162 - val_loss: 85.5691\n",
      "Epoch 153/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 83.9901 - val_loss: 85.5348\n",
      "Epoch 154/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 83.7946 - val_loss: 87.5738\n",
      "Epoch 155/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 82.7929 - val_loss: 84.2387\n",
      "Epoch 156/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 82.2197 - val_loss: 82.3401\n",
      "Epoch 157/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 82.0113 - val_loss: 82.4533\n",
      "Epoch 158/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 81.1601 - val_loss: 85.0363\n",
      "Epoch 159/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 80.7096 - val_loss: 81.2088\n",
      "Epoch 160/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 80.4947 - val_loss: 80.2536\n",
      "Epoch 161/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 80.0377 - val_loss: 80.5327\n",
      "Epoch 162/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 79.3639 - val_loss: 79.5626\n",
      "Epoch 163/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 78.8403 - val_loss: 79.5196\n",
      "Epoch 164/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 78.1656 - val_loss: 79.7696\n",
      "Epoch 165/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 77.9139 - val_loss: 79.1813\n",
      "Epoch 166/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 77.6716 - val_loss: 78.1580\n",
      "Epoch 167/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 76.6624 - val_loss: 77.7184\n",
      "Epoch 168/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 76.8791 - val_loss: 77.9219\n",
      "Epoch 169/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 76.2559 - val_loss: 77.2181\n",
      "Epoch 170/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 75.8559 - val_loss: 75.9809\n",
      "Epoch 171/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 74.8554 - val_loss: 76.6915\n",
      "Epoch 172/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 74.6238 - val_loss: 75.3802\n",
      "Epoch 173/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 74.2829 - val_loss: 75.6581\n",
      "Epoch 174/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 73.8750 - val_loss: 74.6252\n",
      "Epoch 175/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 73.5259 - val_loss: 73.7854\n",
      "Epoch 176/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 73.4039 - val_loss: 74.3336\n",
      "Epoch 177/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 73.5708 - val_loss: 74.0027\n",
      "Epoch 178/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 72.6322 - val_loss: 73.9900\n",
      "Epoch 179/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 73.0269 - val_loss: 72.8931\n",
      "Epoch 180/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 72.0902 - val_loss: 72.6458\n",
      "Epoch 181/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 71.8253 - val_loss: 72.6395\n",
      "Epoch 182/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 70.8769 - val_loss: 71.2850\n",
      "Epoch 183/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 70.2918 - val_loss: 71.0172\n",
      "Epoch 184/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 70.5079 - val_loss: 71.4927\n",
      "Epoch 185/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 69.3388 - val_loss: 71.5361\n",
      "Epoch 186/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 69.0858 - val_loss: 70.1606\n",
      "Epoch 187/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 69.0760 - val_loss: 70.0014\n",
      "Epoch 188/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 69.4622 - val_loss: 70.1827\n",
      "Epoch 189/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 68.7947 - val_loss: 68.5481\n",
      "Epoch 190/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 67.5352 - val_loss: 69.0192\n",
      "Epoch 191/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 67.6432 - val_loss: 67.8607\n",
      "Epoch 192/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 67.5522 - val_loss: 67.8372\n",
      "Epoch 193/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 66.5180 - val_loss: 66.8878\n",
      "Epoch 194/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 67.6971 - val_loss: 69.6645\n",
      "Epoch 195/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 66.8779 - val_loss: 66.5490\n",
      "Epoch 196/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 65.9963 - val_loss: 66.4250\n",
      "Epoch 197/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 65.3961 - val_loss: 65.8746\n",
      "Epoch 198/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 64.7607 - val_loss: 65.4316\n",
      "Epoch 199/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 64.8628 - val_loss: 65.4592\n",
      "Epoch 200/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 64.3527\n",
      "Epoch 00200: saving model to saved_models/latent64/cp-0200.h5\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 64.3527 - val_loss: 64.5862\n",
      "Epoch 201/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 63.9120 - val_loss: 65.1957\n",
      "Epoch 202/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 63.8204 - val_loss: 64.1008\n",
      "Epoch 203/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 64.3019 - val_loss: 65.2156\n",
      "Epoch 204/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 63.7191 - val_loss: 64.8687\n",
      "Epoch 205/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 63.1984 - val_loss: 64.1363\n",
      "Epoch 206/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 62.7081 - val_loss: 63.8967\n",
      "Epoch 207/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 62.3354 - val_loss: 62.6334\n",
      "Epoch 208/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.2223 - val_loss: 62.5658\n",
      "Epoch 209/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 61.8185 - val_loss: 62.8514\n",
      "Epoch 210/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 61.5398 - val_loss: 62.6533\n",
      "Epoch 211/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 61.3653 - val_loss: 62.5808\n",
      "Epoch 212/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 61.0355 - val_loss: 61.9785\n",
      "Epoch 213/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 61.2395 - val_loss: 62.6530\n",
      "Epoch 214/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.0172 - val_loss: 63.1944\n",
      "Epoch 215/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 61.2724 - val_loss: 62.0547\n",
      "Epoch 216/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 60.3417 - val_loss: 61.7446\n",
      "Epoch 217/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 59.9788 - val_loss: 60.7746\n",
      "Epoch 218/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 59.2684 - val_loss: 61.8174\n",
      "Epoch 219/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 59.5890 - val_loss: 60.6248\n",
      "Epoch 220/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 59.6965 - val_loss: 59.4601\n",
      "Epoch 221/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 59.0571 - val_loss: 60.6301\n",
      "Epoch 222/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 58.3142 - val_loss: 59.1684\n",
      "Epoch 223/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 57.7411 - val_loss: 58.9318\n",
      "Epoch 224/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 57.6239 - val_loss: 58.6408\n",
      "Epoch 225/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 57.3809 - val_loss: 58.7770\n",
      "Epoch 226/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 57.0530 - val_loss: 57.6051\n",
      "Epoch 227/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.1560 - val_loss: 58.0823\n",
      "Epoch 228/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 56.5587 - val_loss: 57.8767\n",
      "Epoch 229/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 56.4449 - val_loss: 56.8008\n",
      "Epoch 230/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 56.0394 - val_loss: 57.7689\n",
      "Epoch 231/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 56.5033 - val_loss: 57.7714\n",
      "Epoch 232/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 56.0293 - val_loss: 56.7812\n",
      "Epoch 233/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 55.6949 - val_loss: 57.1636\n",
      "Epoch 234/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 55.5818 - val_loss: 56.1357\n",
      "Epoch 235/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 55.2264 - val_loss: 56.0663\n",
      "Epoch 236/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.4664 - val_loss: 56.1389\n",
      "Epoch 237/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.6141 - val_loss: 56.2556\n",
      "Epoch 238/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.8216 - val_loss: 54.9697\n",
      "Epoch 239/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 54.2488 - val_loss: 55.4062\n",
      "Epoch 240/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.5210 - val_loss: 55.3226\n",
      "Epoch 241/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.9764 - val_loss: 54.5661\n",
      "Epoch 242/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.0327 - val_loss: 54.7716\n",
      "Epoch 243/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.5111 - val_loss: 54.3603\n",
      "Epoch 244/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.2501 - val_loss: 54.7936\n",
      "Epoch 245/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.1148 - val_loss: 53.8963\n",
      "Epoch 246/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.7457 - val_loss: 53.8262\n",
      "Epoch 247/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.6597 - val_loss: 53.0940\n",
      "Epoch 248/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 52.2256 - val_loss: 53.6650\n",
      "Epoch 249/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 52.1030 - val_loss: 53.1058\n",
      "Epoch 250/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 52.2740 - val_loss: 52.4559\n",
      "Epoch 251/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.1522 - val_loss: 54.1826\n",
      "Epoch 252/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 51.9544 - val_loss: 52.6340\n",
      "Epoch 253/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 51.2655 - val_loss: 51.9506\n",
      "Epoch 254/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 51.3488 - val_loss: 51.7979\n",
      "Epoch 255/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 51.0135 - val_loss: 52.2113\n",
      "Epoch 256/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 51.2192 - val_loss: 52.0458\n",
      "Epoch 257/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.7204 - val_loss: 52.4712\n",
      "Epoch 258/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 51.2305 - val_loss: 52.2275\n",
      "Epoch 259/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.6762 - val_loss: 51.1177\n",
      "Epoch 260/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 50.2285 - val_loss: 51.2239\n",
      "Epoch 261/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.3754 - val_loss: 51.0617\n",
      "Epoch 262/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 49.8185 - val_loss: 51.0006\n",
      "Epoch 263/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.5226 - val_loss: 51.9020\n",
      "Epoch 264/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.7663 - val_loss: 51.1335\n",
      "Epoch 265/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.3291 - val_loss: 49.5269\n",
      "Epoch 266/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.8740 - val_loss: 50.2469\n",
      "Epoch 267/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.1663 - val_loss: 49.9267\n",
      "Epoch 268/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.5135 - val_loss: 49.7126\n",
      "Epoch 269/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 48.4871 - val_loss: 49.5434\n",
      "Epoch 270/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.2657 - val_loss: 49.1520\n",
      "Epoch 271/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.3383 - val_loss: 49.6254\n",
      "Epoch 272/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.3402 - val_loss: 48.5503\n",
      "Epoch 273/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.3741 - val_loss: 48.5356\n",
      "Epoch 274/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.6329 - val_loss: 48.5567\n",
      "Epoch 275/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.5894 - val_loss: 48.3214\n",
      "Epoch 276/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.5406 - val_loss: 47.9447\n",
      "Epoch 277/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.4448 - val_loss: 47.7881\n",
      "Epoch 278/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.2216 - val_loss: 49.4566\n",
      "Epoch 279/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.6096 - val_loss: 48.1099\n",
      "Epoch 280/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.0461 - val_loss: 47.5632\n",
      "Epoch 281/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 46.8549 - val_loss: 47.7757\n",
      "Epoch 282/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.8167 - val_loss: 47.7539\n",
      "Epoch 283/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 46.6710 - val_loss: 48.0567\n",
      "Epoch 284/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.1757 - val_loss: 47.2310\n",
      "Epoch 285/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.5044 - val_loss: 47.8502\n",
      "Epoch 286/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 46.3149 - val_loss: 47.8810\n",
      "Epoch 287/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 46.1257 - val_loss: 46.8667\n",
      "Epoch 288/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.4701 - val_loss: 47.2358\n",
      "Epoch 289/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.8372 - val_loss: 46.3739\n",
      "Epoch 290/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.9128 - val_loss: 45.9658\n",
      "Epoch 291/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.5266 - val_loss: 46.8723\n",
      "Epoch 292/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.1980 - val_loss: 46.1394\n",
      "Epoch 293/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.1869 - val_loss: 45.8051\n",
      "Epoch 294/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.0323 - val_loss: 45.3221\n",
      "Epoch 295/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.6284 - val_loss: 45.1222\n",
      "Epoch 296/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.8654 - val_loss: 45.1641\n",
      "Epoch 297/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.3931 - val_loss: 45.3733\n",
      "Epoch 298/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7667 - val_loss: 45.7389\n",
      "Epoch 299/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.6428 - val_loss: 45.0817\n",
      "Epoch 300/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 44.2890 - val_loss: 45.1847\n",
      "Epoch 301/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.1990 - val_loss: 45.8592\n",
      "Epoch 302/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 44.0964 - val_loss: 45.5812\n",
      "Epoch 303/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.0736 - val_loss: 44.7046\n",
      "Epoch 304/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.7776 - val_loss: 45.6998\n",
      "Epoch 305/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.8820 - val_loss: 44.4902\n",
      "Epoch 306/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.5296 - val_loss: 44.6142\n",
      "Epoch 307/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.5938 - val_loss: 44.5160\n",
      "Epoch 308/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 43.2223 - val_loss: 44.3190\n",
      "Epoch 309/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.2514 - val_loss: 44.2268\n",
      "Epoch 310/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.8650 - val_loss: 43.7923\n",
      "Epoch 311/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.7249 - val_loss: 43.2288\n",
      "Epoch 312/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.9324 - val_loss: 44.0444\n",
      "Epoch 313/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.9085 - val_loss: 43.6202\n",
      "Epoch 314/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.6111 - val_loss: 43.2249\n",
      "Epoch 315/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.2906 - val_loss: 43.2711\n",
      "Epoch 316/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.3864 - val_loss: 43.3017\n",
      "Epoch 317/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.2423 - val_loss: 42.9940\n",
      "Epoch 318/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 42.1864 - val_loss: 42.7894\n",
      "Epoch 319/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.0143 - val_loss: 42.9096\n",
      "Epoch 320/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.0370 - val_loss: 44.2431\n",
      "Epoch 321/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.3771 - val_loss: 42.9384\n",
      "Epoch 322/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 41.9833 - val_loss: 42.5953\n",
      "Epoch 323/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 41.6021 - val_loss: 42.3361\n",
      "Epoch 324/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.6417 - val_loss: 42.2020\n",
      "Epoch 325/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.6075 - val_loss: 42.1966\n",
      "Epoch 326/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.6892 - val_loss: 43.4466\n",
      "Epoch 327/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.0018 - val_loss: 43.4900\n",
      "Epoch 328/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.1527 - val_loss: 42.1514\n",
      "Epoch 329/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 41.3464 - val_loss: 41.6461\n",
      "Epoch 330/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 41.3452 - val_loss: 42.7151\n",
      "Epoch 331/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.4252 - val_loss: 41.7978\n",
      "Epoch 332/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.5376 - val_loss: 42.0096\n",
      "Epoch 333/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.6426 - val_loss: 41.7015\n",
      "Epoch 334/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.7844 - val_loss: 42.8170\n",
      "Epoch 335/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.1088 - val_loss: 41.3072\n",
      "Epoch 336/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.8534 - val_loss: 41.5527\n",
      "Epoch 337/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.7030 - val_loss: 41.1277\n",
      "Epoch 338/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.6201 - val_loss: 40.4282\n",
      "Epoch 339/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.3657 - val_loss: 41.5245\n",
      "Epoch 340/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.2775 - val_loss: 41.2866\n",
      "Epoch 341/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.1560 - val_loss: 40.5405\n",
      "Epoch 342/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.0799 - val_loss: 40.9231\n",
      "Epoch 343/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.4878 - val_loss: 40.8420\n",
      "Epoch 344/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.9422 - val_loss: 41.0575\n",
      "Epoch 345/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.7587 - val_loss: 41.6981\n",
      "Epoch 346/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.4458 - val_loss: 40.9873\n",
      "Epoch 347/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.6347 - val_loss: 39.9805\n",
      "Epoch 348/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.5405 - val_loss: 41.7746\n",
      "Epoch 349/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.0399 - val_loss: 39.8960\n",
      "Epoch 350/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.0329 - val_loss: 42.2406\n",
      "Epoch 351/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.0239 - val_loss: 40.3768\n",
      "Epoch 352/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.3154 - val_loss: 41.0243\n",
      "Epoch 353/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.5398 - val_loss: 40.6333\n",
      "Epoch 354/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.6420 - val_loss: 40.3154\n",
      "Epoch 355/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 39.0545 - val_loss: 40.4078\n",
      "Epoch 356/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.0919 - val_loss: 39.6646\n",
      "Epoch 357/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 39.0282 - val_loss: 39.9760\n",
      "Epoch 358/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 38.7405 - val_loss: 39.5674\n",
      "Epoch 359/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.8284 - val_loss: 39.0576\n",
      "Epoch 360/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 38.6929 - val_loss: 38.9965\n",
      "Epoch 361/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 38.6377 - val_loss: 38.9682\n",
      "Epoch 362/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 38.5409 - val_loss: 39.3645\n",
      "Epoch 363/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.4462 - val_loss: 38.8897\n",
      "Epoch 364/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 38.2443 - val_loss: 39.8136\n",
      "Epoch 365/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.2090 - val_loss: 38.9151\n",
      "Epoch 366/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.2098 - val_loss: 39.5511\n",
      "Epoch 367/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.2753 - val_loss: 39.4624\n",
      "Epoch 368/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.8876 - val_loss: 38.7437\n",
      "Epoch 369/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.8835 - val_loss: 38.8977\n",
      "Epoch 370/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.9212 - val_loss: 38.5600\n",
      "Epoch 371/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.8953 - val_loss: 38.9429\n",
      "Epoch 372/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.8881 - val_loss: 39.3669\n",
      "Epoch 373/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.6105 - val_loss: 38.9647\n",
      "Epoch 374/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 37.4310 - val_loss: 38.0375\n",
      "Epoch 375/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.5381 - val_loss: 37.9373\n",
      "Epoch 376/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.3346 - val_loss: 39.0374\n",
      "Epoch 377/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.5112 - val_loss: 37.6735\n",
      "Epoch 378/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.4035 - val_loss: 37.9927\n",
      "Epoch 379/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.2101 - val_loss: 38.4301\n",
      "Epoch 380/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.1536 - val_loss: 38.2281\n",
      "Epoch 381/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.2409 - val_loss: 37.9778\n",
      "Epoch 382/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.5121 - val_loss: 37.7671\n",
      "Epoch 383/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.0376 - val_loss: 38.2167\n",
      "Epoch 384/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.7793 - val_loss: 38.4687\n",
      "Epoch 385/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.2139 - val_loss: 36.8851\n",
      "Epoch 386/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.0392 - val_loss: 38.2743\n",
      "Epoch 387/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.8196 - val_loss: 37.3184\n",
      "Epoch 388/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.6475 - val_loss: 37.2504\n",
      "Epoch 389/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.9812 - val_loss: 37.5676\n",
      "Epoch 390/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.6939 - val_loss: 37.9213\n",
      "Epoch 391/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.2909 - val_loss: 37.9263\n",
      "Epoch 392/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.4203 - val_loss: 37.2588\n",
      "Epoch 393/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 36.1686 - val_loss: 37.4852\n",
      "Epoch 394/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.5315 - val_loss: 37.2189\n",
      "Epoch 395/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.4055 - val_loss: 36.4073\n",
      "Epoch 396/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.5711 - val_loss: 36.8585\n",
      "Epoch 397/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.4105 - val_loss: 38.2584\n",
      "Epoch 398/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.3175 - val_loss: 37.3573\n",
      "Epoch 399/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.3507 - val_loss: 36.9843\n",
      "Epoch 400/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 36.2313\n",
      "Epoch 00400: saving model to saved_models/latent64/cp-0400.h5\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 36.2313 - val_loss: 37.3348\n",
      "Epoch 401/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.2704 - val_loss: 37.3986\n",
      "Epoch 402/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.8091 - val_loss: 36.5701\n",
      "Epoch 403/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.8810 - val_loss: 36.7541\n",
      "Epoch 404/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 35.7290 - val_loss: 36.3511\n",
      "Epoch 405/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.5789 - val_loss: 36.9663\n",
      "Epoch 406/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.8125 - val_loss: 37.0661\n",
      "Epoch 407/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.4755 - val_loss: 37.0946\n",
      "Epoch 408/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.5298 - val_loss: 36.8143\n",
      "Epoch 409/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.5837 - val_loss: 36.4150\n",
      "Epoch 410/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.7015 - val_loss: 36.6042\n",
      "Epoch 411/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 35.3786 - val_loss: 36.5594\n",
      "Epoch 412/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.5795 - val_loss: 36.5784\n",
      "Epoch 413/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 35.3624 - val_loss: 36.7472\n",
      "Epoch 414/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.4131 - val_loss: 36.6938\n",
      "Epoch 415/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.3654 - val_loss: 36.5519\n",
      "Epoch 416/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 35.3287 - val_loss: 36.1634\n",
      "Epoch 417/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.4529 - val_loss: 35.7397\n",
      "Epoch 418/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.2121 - val_loss: 36.1832\n",
      "Epoch 419/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.3142 - val_loss: 36.3277\n",
      "Epoch 420/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.2224 - val_loss: 37.1809\n",
      "Epoch 421/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.1856 - val_loss: 36.0098\n",
      "Epoch 422/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.9547 - val_loss: 35.6054\n",
      "Epoch 423/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.8956 - val_loss: 35.9166\n",
      "Epoch 424/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.9657 - val_loss: 36.1370\n",
      "Epoch 425/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.0481 - val_loss: 36.2055\n",
      "Epoch 426/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 34.5970 - val_loss: 35.4281\n",
      "Epoch 427/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.6706 - val_loss: 36.2240\n",
      "Epoch 428/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.6995 - val_loss: 35.7963\n",
      "Epoch 429/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 34.5321 - val_loss: 35.3473\n",
      "Epoch 430/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.3416 - val_loss: 35.2424\n",
      "Epoch 431/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.5618 - val_loss: 35.8868\n",
      "Epoch 432/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.6043 - val_loss: 34.9857\n",
      "Epoch 433/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.5479 - val_loss: 35.7589\n",
      "Epoch 434/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.5725 - val_loss: 35.5137\n",
      "Epoch 435/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 34.1916 - val_loss: 35.9776\n",
      "Epoch 436/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 33.9581 - val_loss: 35.4139\n",
      "Epoch 437/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.2416 - val_loss: 35.0349\n",
      "Epoch 438/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.1051 - val_loss: 36.1699\n",
      "Epoch 439/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.2112 - val_loss: 35.3020\n",
      "Epoch 440/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.1065 - val_loss: 35.1768\n",
      "Epoch 441/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1605 - val_loss: 34.6328\n",
      "Epoch 442/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1317 - val_loss: 34.7941\n",
      "Epoch 443/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.9552 - val_loss: 34.9246\n",
      "Epoch 444/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1055 - val_loss: 36.1241\n",
      "Epoch 445/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1243 - val_loss: 35.8714\n",
      "Epoch 446/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1134 - val_loss: 34.7208\n",
      "Epoch 447/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 33.6504 - val_loss: 35.5442\n",
      "Epoch 448/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.0189 - val_loss: 35.4486\n",
      "Epoch 449/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.9623 - val_loss: 34.9814\n",
      "Epoch 450/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.8030 - val_loss: 34.8789\n",
      "Epoch 451/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.6067 - val_loss: 34.9271\n",
      "Epoch 452/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 33.4836 - val_loss: 34.2951\n",
      "Epoch 453/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.7240 - val_loss: 35.7766\n",
      "Epoch 454/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.0387 - val_loss: 34.5437\n",
      "Epoch 455/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.6179 - val_loss: 34.0144\n",
      "Epoch 456/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.3124 - val_loss: 33.9910\n",
      "Epoch 457/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.3152 - val_loss: 34.5030\n",
      "Epoch 458/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.3516 - val_loss: 34.3801\n",
      "Epoch 459/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.4456 - val_loss: 34.1518\n",
      "Epoch 460/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.4967 - val_loss: 34.0848\n",
      "Epoch 461/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.0671 - val_loss: 34.4038\n",
      "Epoch 462/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.0571 - val_loss: 34.1714\n",
      "Epoch 463/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.2368 - val_loss: 34.3253\n",
      "Epoch 464/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.1852 - val_loss: 34.6947\n",
      "Epoch 465/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.1766 - val_loss: 34.2459\n",
      "Epoch 466/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.1270 - val_loss: 34.6066\n",
      "Epoch 467/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.9272 - val_loss: 33.9568\n",
      "Epoch 468/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.1341 - val_loss: 34.3471\n",
      "Epoch 469/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.1613 - val_loss: 33.5621\n",
      "Epoch 470/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.9847 - val_loss: 34.8859\n",
      "Epoch 471/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.6883 - val_loss: 34.4353\n",
      "Epoch 472/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.9157 - val_loss: 34.3669\n",
      "Epoch 473/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.3566 - val_loss: 34.2627\n",
      "Epoch 474/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.7376 - val_loss: 33.9800\n",
      "Epoch 475/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.9187 - val_loss: 34.0524\n",
      "Epoch 476/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.2636 - val_loss: 33.7746\n",
      "Epoch 477/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 32.5440 - val_loss: 34.0292\n",
      "Epoch 478/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.6200 - val_loss: 33.8579\n",
      "Epoch 479/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.3863 - val_loss: 34.0743\n",
      "Epoch 480/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.6192 - val_loss: 34.1194\n",
      "Epoch 481/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.3884 - val_loss: 33.4264\n",
      "Epoch 482/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.4407 - val_loss: 33.6335\n",
      "Epoch 483/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.2901 - val_loss: 33.8262\n",
      "Epoch 484/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.7188 - val_loss: 33.7362\n",
      "Epoch 485/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.7462 - val_loss: 33.5849\n",
      "Epoch 486/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.4517 - val_loss: 33.6724\n",
      "Epoch 487/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.2609 - val_loss: 33.8216\n",
      "Epoch 488/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 32.1827 - val_loss: 34.0971\n",
      "Epoch 489/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.2362 - val_loss: 33.1090\n",
      "Epoch 490/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.3119 - val_loss: 33.3294\n",
      "Epoch 491/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.3798 - val_loss: 33.1998\n",
      "Epoch 492/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.4139 - val_loss: 33.5476\n",
      "Epoch 493/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 32.1504 - val_loss: 33.6454\n",
      "Epoch 494/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.9701 - val_loss: 33.1780\n",
      "Epoch 495/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.1951 - val_loss: 33.3050\n",
      "Epoch 496/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.2345 - val_loss: 34.7241\n",
      "Epoch 497/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5057 - val_loss: 33.5042\n",
      "Epoch 498/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.3562 - val_loss: 32.6740\n",
      "Epoch 499/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.4779 - val_loss: 33.3178\n",
      "Epoch 500/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.1649 - val_loss: 33.9492\n",
      "Epoch 501/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.3053 - val_loss: 32.7335\n",
      "Epoch 502/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.3132 - val_loss: 33.1267\n",
      "Epoch 503/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.3990 - val_loss: 33.5939\n",
      "Epoch 504/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5334 - val_loss: 33.1104\n",
      "Epoch 505/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.1847 - val_loss: 33.0773\n",
      "Epoch 506/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.3134 - val_loss: 33.7387\n",
      "Epoch 507/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.7376 - val_loss: 32.6601\n",
      "Epoch 508/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.4832 - val_loss: 34.3357\n",
      "Epoch 509/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.8075 - val_loss: 32.6892\n",
      "Epoch 510/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.6892 - val_loss: 33.3541\n",
      "Epoch 511/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.5179 - val_loss: 32.2090\n",
      "Epoch 512/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.6513 - val_loss: 33.0023\n",
      "Epoch 513/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.4007 - val_loss: 32.6151\n",
      "Epoch 514/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4463 - val_loss: 32.2575\n",
      "Epoch 515/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4134 - val_loss: 32.3659\n",
      "Epoch 516/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.7827 - val_loss: 33.3484\n",
      "Epoch 517/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.4399 - val_loss: 32.6089\n",
      "Epoch 518/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.3392 - val_loss: 32.5771\n",
      "Epoch 519/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.2151 - val_loss: 32.4057\n",
      "Epoch 520/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3188 - val_loss: 33.6813\n",
      "Epoch 521/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.8306 - val_loss: 33.1973\n",
      "Epoch 522/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.1280 - val_loss: 34.1171\n",
      "Epoch 523/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.7036 - val_loss: 32.7102\n",
      "Epoch 524/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4060 - val_loss: 33.0084\n",
      "Epoch 525/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.5182 - val_loss: 32.9838\n",
      "Epoch 526/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4550 - val_loss: 32.1843\n",
      "Epoch 527/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.1684 - val_loss: 32.1134\n",
      "Epoch 528/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3491 - val_loss: 33.3410\n",
      "Epoch 529/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.3413 - val_loss: 32.5802\n",
      "Epoch 530/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3379 - val_loss: 31.6242\n",
      "Epoch 531/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.1016 - val_loss: 32.8016\n",
      "Epoch 532/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.9058 - val_loss: 32.2419\n",
      "Epoch 533/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.9496 - val_loss: 32.6274\n",
      "Epoch 534/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7589 - val_loss: 32.1346\n",
      "Epoch 535/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.9674 - val_loss: 31.9252\n",
      "Epoch 536/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7317 - val_loss: 32.2026\n",
      "Epoch 537/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.6706 - val_loss: 31.7587\n",
      "Epoch 538/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.9872 - val_loss: 32.1290\n",
      "Epoch 539/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.9326 - val_loss: 32.1942\n",
      "Epoch 540/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.6345 - val_loss: 33.0134\n",
      "Epoch 541/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.6184 - val_loss: 32.3970\n",
      "Epoch 542/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.6433 - val_loss: 31.9613\n",
      "Epoch 543/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.6057 - val_loss: 31.5645\n",
      "Epoch 544/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.6200 - val_loss: 32.3224\n",
      "Epoch 545/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.6964 - val_loss: 31.7797\n",
      "Epoch 546/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.4362 - val_loss: 32.2704\n",
      "Epoch 547/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4743 - val_loss: 31.8666\n",
      "Epoch 548/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.7989 - val_loss: 32.8631\n",
      "Epoch 549/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.6200 - val_loss: 32.0204\n",
      "Epoch 550/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.5239 - val_loss: 31.8519\n",
      "Epoch 551/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.9697 - val_loss: 32.5171\n",
      "Epoch 552/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.7509 - val_loss: 32.2731\n",
      "Epoch 553/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.6216 - val_loss: 32.1486\n",
      "Epoch 554/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.5337 - val_loss: 31.0639\n",
      "Epoch 555/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4572 - val_loss: 31.2747\n",
      "Epoch 556/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.0777 - val_loss: 31.4356\n",
      "Epoch 557/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.3517 - val_loss: 31.3941\n",
      "Epoch 558/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.2545 - val_loss: 31.1912\n",
      "Epoch 559/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.2443 - val_loss: 32.1988\n",
      "Epoch 560/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.3975 - val_loss: 32.0211\n",
      "Epoch 561/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.5086 - val_loss: 32.0093\n",
      "Epoch 562/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.5495 - val_loss: 32.4620\n",
      "Epoch 563/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.6021 - val_loss: 31.5191\n",
      "Epoch 564/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.2096 - val_loss: 31.4731\n",
      "Epoch 565/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.2322 - val_loss: 31.7973\n",
      "Epoch 566/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.2257 - val_loss: 32.5889\n",
      "Epoch 567/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.0354 - val_loss: 31.3043\n",
      "Epoch 568/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.9366 - val_loss: 31.0963\n",
      "Epoch 569/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.1727 - val_loss: 31.8288\n",
      "Epoch 570/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4094 - val_loss: 31.9918\n",
      "Epoch 571/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.1778 - val_loss: 31.5935\n",
      "Epoch 572/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.6568 - val_loss: 31.2134\n",
      "Epoch 573/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.0782 - val_loss: 32.1653\n",
      "Epoch 574/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.2670 - val_loss: 31.8134\n",
      "Epoch 575/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.0986 - val_loss: 31.9654\n",
      "Epoch 576/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.5323 - val_loss: 31.0376\n",
      "Epoch 577/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.2927 - val_loss: 31.6861\n",
      "Epoch 578/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.3015 - val_loss: 31.4279\n",
      "Epoch 579/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.1406 - val_loss: 31.2780\n",
      "Epoch 580/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.0186 - val_loss: 31.7613\n",
      "Epoch 581/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.7392 - val_loss: 30.8031\n",
      "Epoch 582/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.8372 - val_loss: 30.7310\n",
      "Epoch 583/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.8304 - val_loss: 31.6813\n",
      "Epoch 584/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.0032 - val_loss: 31.0897\n",
      "Epoch 585/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.7677 - val_loss: 31.4503\n",
      "Epoch 586/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.6335 - val_loss: 30.9136\n",
      "Epoch 587/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.6465 - val_loss: 31.5751\n",
      "Epoch 588/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.9189 - val_loss: 31.7330\n",
      "Epoch 589/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.5524 - val_loss: 31.2007\n",
      "Epoch 590/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.6583 - val_loss: 31.8622\n",
      "Epoch 591/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.5106 - val_loss: 31.2771\n",
      "Epoch 592/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.7154 - val_loss: 30.9354\n",
      "Epoch 593/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.4939 - val_loss: 31.3176\n",
      "Epoch 594/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.5651 - val_loss: 31.2813\n",
      "Epoch 595/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.6030 - val_loss: 31.4219\n",
      "Epoch 596/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.9089 - val_loss: 31.5921\n",
      "Epoch 597/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.9459 - val_loss: 30.6719\n",
      "Epoch 598/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.5381 - val_loss: 30.6939\n",
      "Epoch 599/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.8449 - val_loss: 31.6397\n",
      "Epoch 600/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 29.5123\n",
      "Epoch 00600: saving model to saved_models/latent64/cp-0600.h5\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 29.5123 - val_loss: 31.3973\n",
      "Epoch 601/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.4081 - val_loss: 31.2722\n",
      "Epoch 602/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.8578 - val_loss: 31.5295\n",
      "Epoch 603/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.5502 - val_loss: 31.2961\n",
      "Epoch 604/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.6182 - val_loss: 31.3497\n",
      "Epoch 605/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.3078 - val_loss: 31.4513\n",
      "Epoch 606/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.3291 - val_loss: 30.7275\n",
      "Epoch 607/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.6638 - val_loss: 30.8739\n",
      "Epoch 608/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.4397 - val_loss: 31.0704\n",
      "Epoch 609/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.6196 - val_loss: 31.3032\n",
      "Epoch 610/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.2835 - val_loss: 30.3045\n",
      "Epoch 611/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.3309 - val_loss: 30.4694\n",
      "Epoch 612/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.6865 - val_loss: 31.4380\n",
      "Epoch 613/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.7933 - val_loss: 32.1823\n",
      "Epoch 614/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.8293 - val_loss: 30.8679\n",
      "Epoch 615/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.4160 - val_loss: 30.7623\n",
      "Epoch 616/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.2705 - val_loss: 30.6002\n",
      "Epoch 617/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.9879 - val_loss: 30.6479\n",
      "Epoch 618/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.0752 - val_loss: 31.1514\n",
      "Epoch 619/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.2597 - val_loss: 30.5033\n",
      "Epoch 620/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.4771 - val_loss: 31.3788\n",
      "Epoch 621/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.3398 - val_loss: 31.1686\n",
      "Epoch 622/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.2917 - val_loss: 31.3133\n",
      "Epoch 623/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.1057 - val_loss: 31.0085\n",
      "Epoch 624/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.2002 - val_loss: 30.2101\n",
      "Epoch 625/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.1184 - val_loss: 30.3626\n",
      "Epoch 626/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.9678 - val_loss: 31.1170\n",
      "Epoch 627/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.4191 - val_loss: 32.7445\n",
      "Epoch 628/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.7164 - val_loss: 30.8003\n",
      "Epoch 629/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.0931 - val_loss: 30.8545\n",
      "Epoch 630/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.1739 - val_loss: 31.5595\n",
      "Epoch 631/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.1936 - val_loss: 30.7654\n",
      "Epoch 632/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.2075 - val_loss: 29.9516\n",
      "Epoch 633/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.8957 - val_loss: 30.9023\n",
      "Epoch 634/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.8707 - val_loss: 30.0701\n",
      "Epoch 635/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.6755 - val_loss: 30.9780\n",
      "Epoch 636/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.0668 - val_loss: 30.0843\n",
      "Epoch 637/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.8284 - val_loss: 30.8412\n",
      "Epoch 638/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.7608 - val_loss: 30.3662\n",
      "Epoch 639/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.8838 - val_loss: 30.0834\n",
      "Epoch 640/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.8871 - val_loss: 30.2559\n",
      "Epoch 641/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6866 - val_loss: 30.4301\n",
      "Epoch 642/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.8001 - val_loss: 30.2611\n",
      "Epoch 643/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.8577 - val_loss: 30.2917\n",
      "Epoch 644/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.7298 - val_loss: 30.4677\n",
      "Epoch 645/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.7838 - val_loss: 30.2527\n",
      "Epoch 646/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.8553 - val_loss: 29.6440\n",
      "Epoch 647/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.8367 - val_loss: 30.5649\n",
      "Epoch 648/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.9241 - val_loss: 31.5098\n",
      "Epoch 649/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.9176 - val_loss: 30.9006\n",
      "Epoch 650/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.8307 - val_loss: 30.3004\n",
      "Epoch 651/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.7183 - val_loss: 29.8589\n",
      "Epoch 652/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.8238 - val_loss: 30.3512\n",
      "Epoch 653/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.5829 - val_loss: 29.8899\n",
      "Epoch 654/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.5490 - val_loss: 29.5583\n",
      "Epoch 655/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.7426 - val_loss: 30.1255\n",
      "Epoch 656/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.7467 - val_loss: 31.1944\n",
      "Epoch 657/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.7232 - val_loss: 29.7999\n",
      "Epoch 658/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.6909 - val_loss: 30.1916\n",
      "Epoch 659/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.5274 - val_loss: 30.0576\n",
      "Epoch 660/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.3039 - val_loss: 29.8531\n",
      "Epoch 661/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.8142 - val_loss: 30.8521\n",
      "Epoch 662/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.7997 - val_loss: 30.2178\n",
      "Epoch 663/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.5158 - val_loss: 30.0558\n",
      "Epoch 664/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.1679 - val_loss: 29.6523\n",
      "Epoch 665/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.1975 - val_loss: 29.8706\n",
      "Epoch 666/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.2474 - val_loss: 30.6040\n",
      "Epoch 667/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.2837 - val_loss: 30.0817\n",
      "Epoch 668/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.1432 - val_loss: 29.9830\n",
      "Epoch 669/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.2695 - val_loss: 30.3921\n",
      "Epoch 670/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2527 - val_loss: 29.3996\n",
      "Epoch 671/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.2809 - val_loss: 29.8601\n",
      "Epoch 672/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.4489 - val_loss: 29.2169\n",
      "Epoch 673/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.1088 - val_loss: 29.7835\n",
      "Epoch 674/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2233 - val_loss: 30.1238\n",
      "Epoch 675/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2059 - val_loss: 29.7412\n",
      "Epoch 676/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.1857 - val_loss: 30.3348\n",
      "Epoch 677/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.5516 - val_loss: 30.1727\n",
      "Epoch 678/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.4380 - val_loss: 29.9443\n",
      "Epoch 679/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.1199 - val_loss: 30.0331\n",
      "Epoch 680/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.7307 - val_loss: 30.7064\n",
      "Epoch 681/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6776 - val_loss: 30.6931\n",
      "Epoch 682/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.4020 - val_loss: 30.6386\n",
      "Epoch 683/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.3130 - val_loss: 30.0155\n",
      "Epoch 684/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2006 - val_loss: 30.0450\n",
      "Epoch 685/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2475 - val_loss: 29.5762\n",
      "Epoch 686/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.1706 - val_loss: 29.6453\n",
      "Epoch 687/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.1486 - val_loss: 30.2091\n",
      "Epoch 688/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.0072 - val_loss: 29.9037\n",
      "Epoch 689/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.4584 - val_loss: 30.7246\n",
      "Epoch 690/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.9047 - val_loss: 30.5023\n",
      "Epoch 691/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.1822 - val_loss: 29.9742\n",
      "Epoch 692/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.1423 - val_loss: 30.0887\n",
      "Epoch 693/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2339 - val_loss: 29.5575\n",
      "Epoch 694/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.0705 - val_loss: 29.1194\n",
      "Epoch 695/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.9341 - val_loss: 29.3646\n",
      "Epoch 696/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.0436 - val_loss: 29.4518\n",
      "Epoch 697/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.8282 - val_loss: 29.7829\n",
      "Epoch 698/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.9356 - val_loss: 30.0121\n",
      "Epoch 699/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8564 - val_loss: 30.5349\n",
      "Epoch 700/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.3244 - val_loss: 30.0333\n",
      "Epoch 701/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.0592 - val_loss: 29.6644\n",
      "Epoch 702/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.0865 - val_loss: 29.6184\n",
      "Epoch 703/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.4729 - val_loss: 29.9706\n",
      "Epoch 704/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.0174 - val_loss: 29.3140\n",
      "Epoch 705/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.1677 - val_loss: 29.4537\n",
      "Epoch 706/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.9527 - val_loss: 29.5658\n",
      "Epoch 707/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.8772 - val_loss: 29.4416\n",
      "Epoch 708/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9287 - val_loss: 29.6985\n",
      "Epoch 709/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2333 - val_loss: 30.1687\n",
      "Epoch 710/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.7335 - val_loss: 29.6936\n",
      "Epoch 711/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9817 - val_loss: 29.0906\n",
      "Epoch 712/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 27.8793 - val_loss: 29.0035\n",
      "Epoch 713/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8125 - val_loss: 29.6802\n",
      "Epoch 714/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9230 - val_loss: 29.2556\n",
      "Epoch 715/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.7243 - val_loss: 29.8179\n",
      "Epoch 716/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.5944 - val_loss: 29.4343\n",
      "Epoch 717/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8905 - val_loss: 29.9225\n",
      "Epoch 718/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.0911 - val_loss: 29.8368\n",
      "Epoch 719/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.8494 - val_loss: 29.5371\n",
      "Epoch 720/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.3613 - val_loss: 29.8164\n",
      "Epoch 721/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.9333 - val_loss: 30.4925\n",
      "Epoch 722/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2657 - val_loss: 30.4339\n",
      "Epoch 723/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.1243 - val_loss: 29.7042\n",
      "Epoch 724/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6101 - val_loss: 29.2222\n",
      "Epoch 725/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.6392 - val_loss: 30.1823\n",
      "Epoch 726/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9426 - val_loss: 29.7273\n",
      "Epoch 727/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.9397 - val_loss: 29.7981\n",
      "Epoch 728/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9429 - val_loss: 29.5298\n",
      "Epoch 729/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.6941 - val_loss: 30.5164\n",
      "Epoch 730/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.1039 - val_loss: 29.9781\n",
      "Epoch 731/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8143 - val_loss: 29.4767\n",
      "Epoch 732/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.7306 - val_loss: 28.9982\n",
      "Epoch 733/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.5663 - val_loss: 29.6780\n",
      "Epoch 734/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.7386 - val_loss: 30.1387\n",
      "Epoch 735/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.7568 - val_loss: 28.7438\n",
      "Epoch 736/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.3520 - val_loss: 29.0339\n",
      "Epoch 737/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4853 - val_loss: 28.9549\n",
      "Epoch 738/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.5687 - val_loss: 28.9649\n",
      "Epoch 739/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5082 - val_loss: 28.9617\n",
      "Epoch 740/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.3515 - val_loss: 29.3824\n",
      "Epoch 741/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5518 - val_loss: 29.2294\n",
      "Epoch 742/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6161 - val_loss: 28.8184\n",
      "Epoch 743/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3642 - val_loss: 29.5173\n",
      "Epoch 744/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.7477 - val_loss: 29.1141\n",
      "Epoch 745/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5634 - val_loss: 28.9291\n",
      "Epoch 746/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5384 - val_loss: 29.6571\n",
      "Epoch 747/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.5679 - val_loss: 29.7149\n",
      "Epoch 748/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3180 - val_loss: 29.6384\n",
      "Epoch 749/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3999 - val_loss: 29.2962\n",
      "Epoch 750/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4707 - val_loss: 28.7738\n",
      "Epoch 751/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3106 - val_loss: 29.4882\n",
      "Epoch 752/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4755 - val_loss: 29.4667\n",
      "Epoch 753/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6783 - val_loss: 30.0800\n",
      "Epoch 754/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3710 - val_loss: 29.3872\n",
      "Epoch 755/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3640 - val_loss: 29.3814\n",
      "Epoch 756/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 27.2503 - val_loss: 29.2347\n",
      "Epoch 757/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3232 - val_loss: 29.1314\n",
      "Epoch 758/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4329 - val_loss: 29.5749\n",
      "Epoch 759/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4937 - val_loss: 29.4678\n",
      "Epoch 760/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3534 - val_loss: 29.0516\n",
      "Epoch 761/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4594 - val_loss: 29.1651\n",
      "Epoch 762/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3923 - val_loss: 28.6321\n",
      "Epoch 763/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4330 - val_loss: 29.3179\n",
      "Epoch 764/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1025 - val_loss: 29.3512\n",
      "Epoch 765/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4752 - val_loss: 29.7503\n",
      "Epoch 766/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3899 - val_loss: 29.2427\n",
      "Epoch 767/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3483 - val_loss: 29.2363\n",
      "Epoch 768/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1927 - val_loss: 29.4227\n",
      "Epoch 769/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4091 - val_loss: 29.4456\n",
      "Epoch 770/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6763 - val_loss: 30.0435\n",
      "Epoch 771/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6546 - val_loss: 29.6591\n",
      "Epoch 772/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3585 - val_loss: 29.5438\n",
      "Epoch 773/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3685 - val_loss: 28.8784\n",
      "Epoch 774/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1062 - val_loss: 28.9342\n",
      "Epoch 775/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0946 - val_loss: 28.5007\n",
      "Epoch 776/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.0190 - val_loss: 29.0442\n",
      "Epoch 777/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1229 - val_loss: 28.9612\n",
      "Epoch 778/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1294 - val_loss: 29.3353\n",
      "Epoch 779/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5685 - val_loss: 30.0877\n",
      "Epoch 780/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.5876 - val_loss: 29.1501\n",
      "Epoch 781/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4181 - val_loss: 29.0702\n",
      "Epoch 782/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3022 - val_loss: 29.4915\n",
      "Epoch 783/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1682 - val_loss: 28.9710\n",
      "Epoch 784/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3685 - val_loss: 28.8645\n",
      "Epoch 785/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2910 - val_loss: 28.6451\n",
      "Epoch 786/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0088 - val_loss: 29.1545\n",
      "Epoch 787/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2289 - val_loss: 28.7924\n",
      "Epoch 788/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0879 - val_loss: 28.8783\n",
      "Epoch 789/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1861 - val_loss: 29.3582\n",
      "Epoch 790/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0115 - val_loss: 28.6034\n",
      "Epoch 791/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1667 - val_loss: 28.3797\n",
      "Epoch 792/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9341 - val_loss: 29.1960\n",
      "Epoch 793/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0623 - val_loss: 28.9074\n",
      "Epoch 794/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0597 - val_loss: 29.7431\n",
      "Epoch 795/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1314 - val_loss: 29.2784\n",
      "Epoch 796/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9540 - val_loss: 28.7295\n",
      "Epoch 797/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0025 - val_loss: 28.9593\n",
      "Epoch 798/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3248 - val_loss: 29.3650\n",
      "Epoch 799/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0888 - val_loss: 29.3849\n",
      "Epoch 800/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 27.2110\n",
      "Epoch 00800: saving model to saved_models/latent64/cp-0800.h5\n",
      "7/7 [==============================] - 1s 201ms/step - loss: 27.2110 - val_loss: 28.8864\n",
      "Epoch 801/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8572 - val_loss: 28.8368\n",
      "Epoch 802/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9550 - val_loss: 29.1773\n",
      "Epoch 803/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8222 - val_loss: 28.4331\n",
      "Epoch 804/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9608 - val_loss: 29.3330\n",
      "Epoch 805/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1312 - val_loss: 29.1174\n",
      "Epoch 806/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1158 - val_loss: 28.8152\n",
      "Epoch 807/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9747 - val_loss: 28.7628\n",
      "Epoch 808/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8477 - val_loss: 28.8921\n",
      "Epoch 809/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9618 - val_loss: 28.7962\n",
      "Epoch 810/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8760 - val_loss: 29.0302\n",
      "Epoch 811/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8818 - val_loss: 28.9990\n",
      "Epoch 812/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0283 - val_loss: 28.6619\n",
      "Epoch 813/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9264 - val_loss: 28.8581\n",
      "Epoch 814/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2633 - val_loss: 29.4202\n",
      "Epoch 815/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7588 - val_loss: 28.7534\n",
      "Epoch 816/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7712 - val_loss: 28.4757\n",
      "Epoch 817/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9309 - val_loss: 29.7612\n",
      "Epoch 818/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0035 - val_loss: 29.0378\n",
      "Epoch 819/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9186 - val_loss: 29.8938\n",
      "Epoch 820/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1434 - val_loss: 29.7761\n",
      "Epoch 821/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1226 - val_loss: 29.1087\n",
      "Epoch 822/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7411 - val_loss: 29.1328\n",
      "Epoch 823/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9323 - val_loss: 29.2560\n",
      "Epoch 824/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8039 - val_loss: 28.8321\n",
      "Epoch 825/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7343 - val_loss: 28.8820\n",
      "Epoch 826/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8475 - val_loss: 29.0412\n",
      "Epoch 827/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8687 - val_loss: 28.7458\n",
      "Epoch 828/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8902 - val_loss: 28.9756\n",
      "Epoch 829/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9548 - val_loss: 30.0411\n",
      "Epoch 830/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2132 - val_loss: 29.5647\n",
      "Epoch 831/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9178 - val_loss: 29.1150\n",
      "Epoch 832/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8897 - val_loss: 28.8811\n",
      "Epoch 833/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8068 - val_loss: 29.0701\n",
      "Epoch 834/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7347 - val_loss: 28.7025\n",
      "Epoch 835/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9705 - val_loss: 28.9609\n",
      "Epoch 836/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6267 - val_loss: 28.5444\n",
      "Epoch 837/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6662 - val_loss: 28.7534\n",
      "Epoch 838/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8746 - val_loss: 29.4280\n",
      "Epoch 839/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8365 - val_loss: 29.4511\n",
      "Epoch 840/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9739 - val_loss: 28.9795\n",
      "Epoch 841/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5592 - val_loss: 28.3119\n",
      "Epoch 842/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7115 - val_loss: 28.5707\n",
      "Epoch 843/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8071 - val_loss: 30.5795\n",
      "Epoch 844/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2386 - val_loss: 29.8282\n",
      "Epoch 845/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8831 - val_loss: 29.2219\n",
      "Epoch 846/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7130 - val_loss: 29.2759\n",
      "Epoch 847/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6803 - val_loss: 28.8378\n",
      "Epoch 848/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0684 - val_loss: 29.8422\n",
      "Epoch 849/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0035 - val_loss: 29.3375\n",
      "Epoch 850/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1281 - val_loss: 28.6502\n",
      "Epoch 851/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1829 - val_loss: 30.0688\n",
      "Epoch 852/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4241 - val_loss: 29.2476\n",
      "Epoch 853/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1424 - val_loss: 29.3958\n",
      "Epoch 854/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1402 - val_loss: 28.7343\n",
      "Epoch 855/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8899 - val_loss: 28.8195\n",
      "Epoch 856/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6470 - val_loss: 28.3151\n",
      "Epoch 857/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5942 - val_loss: 28.5084\n",
      "Epoch 858/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6882 - val_loss: 28.9188\n",
      "Epoch 859/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6991 - val_loss: 28.8100\n",
      "Epoch 860/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9418 - val_loss: 29.1629\n",
      "Epoch 861/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8647 - val_loss: 28.4660\n",
      "Epoch 862/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9367 - val_loss: 29.0764\n",
      "Epoch 863/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7246 - val_loss: 29.0941\n",
      "Epoch 864/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8174 - val_loss: 28.6871\n",
      "Epoch 865/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5951 - val_loss: 28.6006\n",
      "Epoch 866/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5785 - val_loss: 28.8947\n",
      "Epoch 867/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.5095 - val_loss: 28.5404\n",
      "Epoch 868/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6439 - val_loss: 28.7349\n",
      "Epoch 869/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4809 - val_loss: 28.2946\n",
      "Epoch 870/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6368 - val_loss: 28.8566\n",
      "Epoch 871/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7420 - val_loss: 29.0560\n",
      "Epoch 872/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9411 - val_loss: 28.2856\n",
      "Epoch 873/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5872 - val_loss: 28.5167\n",
      "Epoch 874/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7034 - val_loss: 28.9553\n",
      "Epoch 875/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6607 - val_loss: 28.5231\n",
      "Epoch 876/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5147 - val_loss: 28.9267\n",
      "Epoch 877/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3074 - val_loss: 29.0731\n",
      "Epoch 878/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7359 - val_loss: 29.2799\n",
      "Epoch 879/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4228 - val_loss: 29.3722\n",
      "Epoch 880/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3751 - val_loss: 28.5216\n",
      "Epoch 881/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5223 - val_loss: 28.8202\n",
      "Epoch 882/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7484 - val_loss: 28.7873\n",
      "Epoch 883/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5044 - val_loss: 28.8080\n",
      "Epoch 884/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3539 - val_loss: 28.4109\n",
      "Epoch 885/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3642 - val_loss: 28.2935\n",
      "Epoch 886/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4478 - val_loss: 28.1856\n",
      "Epoch 887/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6412 - val_loss: 29.3758\n",
      "Epoch 888/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5113 - val_loss: 28.5808\n",
      "Epoch 889/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3944 - val_loss: 28.5186\n",
      "Epoch 890/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3794 - val_loss: 28.4777\n",
      "Epoch 891/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.1727 - val_loss: 28.6152\n",
      "Epoch 892/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4087 - val_loss: 28.1368\n",
      "Epoch 893/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3144 - val_loss: 28.0768\n",
      "Epoch 894/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3296 - val_loss: 29.1526\n",
      "Epoch 895/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9403 - val_loss: 29.3758\n",
      "Epoch 896/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5863 - val_loss: 29.5135\n",
      "Epoch 897/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6086 - val_loss: 28.5782\n",
      "Epoch 898/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7307 - val_loss: 29.1514\n",
      "Epoch 899/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5765 - val_loss: 28.9055\n",
      "Epoch 900/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6294 - val_loss: 29.2318\n",
      "Epoch 901/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9536 - val_loss: 29.5739\n",
      "Epoch 902/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5891 - val_loss: 28.5246\n",
      "Epoch 903/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4570 - val_loss: 29.5327\n",
      "Epoch 904/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7287 - val_loss: 28.3895\n",
      "Epoch 905/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4328 - val_loss: 28.5044\n",
      "Epoch 906/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4890 - val_loss: 29.1386\n",
      "Epoch 907/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6902 - val_loss: 29.4469\n",
      "Epoch 908/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0001 - val_loss: 28.8664\n",
      "Epoch 909/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8887 - val_loss: 28.9019\n",
      "Epoch 910/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8501 - val_loss: 28.6515\n",
      "Epoch 911/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4373 - val_loss: 28.7528\n",
      "Epoch 912/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4268 - val_loss: 28.6298\n",
      "Epoch 913/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4081 - val_loss: 28.2582\n",
      "Epoch 914/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4472 - val_loss: 28.8659\n",
      "Epoch 915/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3494 - val_loss: 28.3767\n",
      "Epoch 916/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3849 - val_loss: 29.4535\n",
      "Epoch 917/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7242 - val_loss: 28.8355\n",
      "Epoch 918/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6118 - val_loss: 29.0817\n",
      "Epoch 919/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9423 - val_loss: 28.5012\n",
      "Epoch 920/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5008 - val_loss: 29.2346\n",
      "Epoch 921/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4004 - val_loss: 28.9788\n",
      "Epoch 922/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4538 - val_loss: 28.8049\n",
      "Epoch 923/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3571 - val_loss: 28.5236\n",
      "Epoch 924/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5004 - val_loss: 28.9304\n",
      "Epoch 925/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4977 - val_loss: 29.3673\n",
      "Epoch 926/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2858 - val_loss: 27.9635\n",
      "Epoch 927/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.1362 - val_loss: 28.5195\n",
      "Epoch 928/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2718 - val_loss: 28.9100\n",
      "Epoch 929/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3661 - val_loss: 29.4954\n",
      "Epoch 930/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6676 - val_loss: 29.9868\n",
      "Epoch 931/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4784 - val_loss: 28.7100\n",
      "Epoch 932/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3952 - val_loss: 29.2401\n",
      "Epoch 933/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3535 - val_loss: 28.8788\n",
      "Epoch 934/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1771 - val_loss: 28.2185\n",
      "Epoch 935/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3007 - val_loss: 28.2035\n",
      "Epoch 936/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2763 - val_loss: 28.8131\n",
      "Epoch 937/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3482 - val_loss: 28.5650\n",
      "Epoch 938/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2064 - val_loss: 28.5792\n",
      "Epoch 939/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0577 - val_loss: 28.5662\n",
      "Epoch 940/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0783 - val_loss: 28.9395\n",
      "Epoch 941/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1815 - val_loss: 28.8796\n",
      "Epoch 942/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1318 - val_loss: 28.7673\n",
      "Epoch 943/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2458 - val_loss: 28.2852\n",
      "Epoch 944/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0225 - val_loss: 28.4969\n",
      "Epoch 945/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3570 - val_loss: 29.1682\n",
      "Epoch 946/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.9592 - val_loss: 28.7778\n",
      "Epoch 947/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2052 - val_loss: 28.4376\n",
      "Epoch 948/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2454 - val_loss: 28.5037\n",
      "Epoch 949/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2739 - val_loss: 28.1228\n",
      "Epoch 950/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0770 - val_loss: 29.0734\n",
      "Epoch 951/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0699 - val_loss: 28.5725\n",
      "Epoch 952/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3331 - val_loss: 28.2717\n",
      "Epoch 953/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2111 - val_loss: 28.8065\n",
      "Epoch 954/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2191 - val_loss: 28.4424\n",
      "Epoch 955/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1471 - val_loss: 28.0885\n",
      "Epoch 956/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0873 - val_loss: 27.9495\n",
      "Epoch 957/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9287 - val_loss: 28.4251\n",
      "Epoch 958/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3778 - val_loss: 28.2159\n",
      "Epoch 959/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4321 - val_loss: 28.3351\n",
      "Epoch 960/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5026 - val_loss: 29.3839\n",
      "Epoch 961/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6619 - val_loss: 29.4040\n",
      "Epoch 962/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7429 - val_loss: 28.5455\n",
      "Epoch 963/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4305 - val_loss: 28.3847\n",
      "Epoch 964/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0319 - val_loss: 29.1122\n",
      "Epoch 965/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0932 - val_loss: 30.3219\n",
      "Epoch 966/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6983 - val_loss: 29.0481\n",
      "Epoch 967/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7075 - val_loss: 29.4307\n",
      "Epoch 968/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3100 - val_loss: 28.1190\n",
      "Epoch 969/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1864 - val_loss: 28.2650\n",
      "Epoch 970/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2203 - val_loss: 28.7196\n",
      "Epoch 971/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0037 - val_loss: 28.5117\n",
      "Epoch 972/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0042 - val_loss: 28.2073\n",
      "Epoch 973/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2325 - val_loss: 28.5990\n",
      "Epoch 974/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2435 - val_loss: 29.1299\n",
      "Epoch 975/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5031 - val_loss: 28.6469\n",
      "Epoch 976/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1636 - val_loss: 28.4321\n",
      "Epoch 977/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1636 - val_loss: 28.9536\n",
      "Epoch 978/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5062 - val_loss: 28.7588\n",
      "Epoch 979/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3823 - val_loss: 28.4591\n",
      "Epoch 980/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1084 - val_loss: 28.3647\n",
      "Epoch 981/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2155 - val_loss: 28.5926\n",
      "Epoch 982/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0971 - val_loss: 28.5768\n",
      "Epoch 983/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4784 - val_loss: 28.7954\n",
      "Epoch 984/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1192 - val_loss: 28.3764\n",
      "Epoch 985/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1258 - val_loss: 28.1956\n",
      "Epoch 986/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1598 - val_loss: 28.7445\n",
      "Epoch 987/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0929 - val_loss: 27.8551\n",
      "Epoch 988/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1521 - val_loss: 28.5625\n",
      "Epoch 989/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2277 - val_loss: 29.1302\n",
      "Epoch 990/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2038 - val_loss: 29.0571\n",
      "Epoch 991/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4564 - val_loss: 29.0856\n",
      "Epoch 992/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1108 - val_loss: 28.2109\n",
      "Epoch 993/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0784 - val_loss: 28.0102\n",
      "Epoch 994/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0419 - val_loss: 28.3490\n",
      "Epoch 995/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9339 - val_loss: 28.3935\n",
      "Epoch 996/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8117 - val_loss: 28.9079\n",
      "Epoch 997/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0309 - val_loss: 28.6121\n",
      "Epoch 998/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2218 - val_loss: 28.3133\n",
      "Epoch 999/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8259 - val_loss: 28.5606\n",
      "Epoch 1000/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 26.0521\n",
      "Epoch 01000: saving model to saved_models/latent64/cp-1000.h5\n",
      "7/7 [==============================] - 1s 151ms/step - loss: 26.0521 - val_loss: 28.0221\n",
      "Epoch 1001/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0208 - val_loss: 28.4299\n",
      "Epoch 1002/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0959 - val_loss: 28.8856\n",
      "Epoch 1003/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1018 - val_loss: 28.6025\n",
      "Epoch 1004/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3163 - val_loss: 28.4403\n",
      "Epoch 1005/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1658 - val_loss: 28.6721\n",
      "Epoch 1006/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0098 - val_loss: 28.4230\n",
      "Epoch 1007/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0553 - val_loss: 27.9044\n",
      "Epoch 1008/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0544 - val_loss: 28.0363\n",
      "Epoch 1009/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2743 - val_loss: 28.7963\n",
      "Epoch 1010/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1432 - val_loss: 28.5515\n",
      "Epoch 1011/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1965 - val_loss: 28.8773\n",
      "Epoch 1012/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3815 - val_loss: 28.9223\n",
      "Epoch 1013/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1645 - val_loss: 27.7624\n",
      "Epoch 1014/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8216 - val_loss: 28.1235\n",
      "Epoch 1015/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0299 - val_loss: 28.6612\n",
      "Epoch 1016/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0369 - val_loss: 28.6662\n",
      "Epoch 1017/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0415 - val_loss: 28.5859\n",
      "Epoch 1018/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0462 - val_loss: 28.5070\n",
      "Epoch 1019/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7420 - val_loss: 28.2560\n",
      "Epoch 1020/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7976 - val_loss: 28.5324\n",
      "Epoch 1021/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8752 - val_loss: 28.1154\n",
      "Epoch 1022/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7998 - val_loss: 28.5253\n",
      "Epoch 1023/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0227 - val_loss: 28.2929\n",
      "Epoch 1024/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1198 - val_loss: 28.8229\n",
      "Epoch 1025/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3761 - val_loss: 28.5814\n",
      "Epoch 1026/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3658 - val_loss: 28.4283\n",
      "Epoch 1027/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0974 - val_loss: 28.3568\n",
      "Epoch 1028/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0495 - val_loss: 28.7547\n",
      "Epoch 1029/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1418 - val_loss: 28.3911\n",
      "Epoch 1030/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8590 - val_loss: 28.2331\n",
      "Epoch 1031/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0107 - val_loss: 28.6365\n",
      "Epoch 1032/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0193 - val_loss: 28.5815\n",
      "Epoch 1033/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1520 - val_loss: 28.8144\n",
      "Epoch 1034/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0682 - val_loss: 28.1512\n",
      "Epoch 1035/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0921 - val_loss: 28.6810\n",
      "Epoch 1036/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2380 - val_loss: 28.7010\n",
      "Epoch 1037/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4223 - val_loss: 30.1843\n",
      "Epoch 1038/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3601 - val_loss: 29.0799\n",
      "Epoch 1039/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0981 - val_loss: 28.6113\n",
      "Epoch 1040/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2855 - val_loss: 29.7442\n",
      "Epoch 1041/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9755 - val_loss: 28.6124\n",
      "Epoch 1042/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1713 - val_loss: 28.4412\n",
      "Epoch 1043/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2582 - val_loss: 28.5443\n",
      "Epoch 1044/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0764 - val_loss: 28.3410\n",
      "Epoch 1045/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3279 - val_loss: 28.9975\n",
      "Epoch 1046/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4242 - val_loss: 28.8344\n",
      "Epoch 1047/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9863 - val_loss: 28.6386\n",
      "Epoch 1048/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8920 - val_loss: 28.1701\n",
      "Epoch 1049/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7444 - val_loss: 28.4968\n",
      "Epoch 1050/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9566 - val_loss: 27.9517\n",
      "Epoch 1051/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2278 - val_loss: 28.9904\n",
      "Epoch 1052/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0951 - val_loss: 28.6446\n",
      "Epoch 1053/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8634 - val_loss: 28.7783\n",
      "Epoch 1054/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9881 - val_loss: 28.4796\n",
      "Epoch 1055/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7894 - val_loss: 27.9448\n",
      "Epoch 1056/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7384 - val_loss: 28.3728\n",
      "Epoch 1057/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8115 - val_loss: 28.2439\n",
      "Epoch 1058/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8091 - val_loss: 29.0969\n",
      "Epoch 1059/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0539 - val_loss: 28.1430\n",
      "Epoch 1060/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7285 - val_loss: 27.9473\n",
      "Epoch 1061/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7583 - val_loss: 28.3776\n",
      "Epoch 1062/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9479 - val_loss: 28.4715\n",
      "Epoch 1063/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0981 - val_loss: 29.0081\n",
      "Epoch 1064/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8331 - val_loss: 28.9726\n",
      "Epoch 1065/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9009 - val_loss: 28.3756\n",
      "Epoch 1066/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9590 - val_loss: 27.7919\n",
      "Epoch 1067/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7480 - val_loss: 28.1414\n",
      "Epoch 1068/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6036 - val_loss: 28.6338\n",
      "Epoch 1069/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6606 - val_loss: 28.5909\n",
      "Epoch 1070/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8035 - val_loss: 28.0965\n",
      "Epoch 1071/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7880 - val_loss: 28.1044\n",
      "Epoch 1072/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7017 - val_loss: 28.2507\n",
      "Epoch 1073/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6176 - val_loss: 27.5494\n",
      "Epoch 1074/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7722 - val_loss: 28.0060\n",
      "Epoch 1075/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8076 - val_loss: 28.2689\n",
      "Epoch 1076/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9224 - val_loss: 28.0620\n",
      "Epoch 1077/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8091 - val_loss: 29.2026\n",
      "Epoch 1078/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9733 - val_loss: 28.3657\n",
      "Epoch 1079/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7827 - val_loss: 28.5086\n",
      "Epoch 1080/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7992 - val_loss: 28.0782\n",
      "Epoch 1081/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5872 - val_loss: 28.4852\n",
      "Epoch 1082/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8534 - val_loss: 27.9892\n",
      "Epoch 1083/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7690 - val_loss: 27.8879\n",
      "Epoch 1084/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8438 - val_loss: 29.1028\n",
      "Epoch 1085/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0807 - val_loss: 28.6138\n",
      "Epoch 1086/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7023 - val_loss: 28.6697\n",
      "Epoch 1087/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8428 - val_loss: 28.5433\n",
      "Epoch 1088/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7479 - val_loss: 28.9601\n",
      "Epoch 1089/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8125 - val_loss: 28.3380\n",
      "Epoch 1090/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7074 - val_loss: 28.6487\n",
      "Epoch 1091/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7242 - val_loss: 28.4875\n",
      "Epoch 1092/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0410 - val_loss: 29.1693\n",
      "Epoch 1093/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0870 - val_loss: 29.1190\n",
      "Epoch 1094/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0848 - val_loss: 28.1694\n",
      "Epoch 1095/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3087 - val_loss: 28.7016\n",
      "Epoch 1096/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7862 - val_loss: 27.9833\n",
      "Epoch 1097/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6031 - val_loss: 28.0191\n",
      "Epoch 1098/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6497 - val_loss: 28.1607\n",
      "Epoch 1099/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5666 - val_loss: 28.2287\n",
      "Epoch 1100/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7512 - val_loss: 28.1567\n",
      "Epoch 1101/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9893 - val_loss: 28.5114\n",
      "Epoch 1102/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0890 - val_loss: 28.0238\n",
      "Epoch 1103/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1221 - val_loss: 28.7362\n",
      "Epoch 1104/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1254 - val_loss: 29.1782\n",
      "Epoch 1105/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0078 - val_loss: 28.0436\n",
      "Epoch 1106/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7917 - val_loss: 27.8681\n",
      "Epoch 1107/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8360 - val_loss: 28.6187\n",
      "Epoch 1108/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8121 - val_loss: 28.8391\n",
      "Epoch 1109/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8690 - val_loss: 28.7558\n",
      "Epoch 1110/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9860 - val_loss: 28.0950\n",
      "Epoch 1111/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8138 - val_loss: 27.7630\n",
      "Epoch 1112/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6971 - val_loss: 28.2704\n",
      "Epoch 1113/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6400 - val_loss: 28.0380\n",
      "Epoch 1114/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7177 - val_loss: 28.4775\n",
      "Epoch 1115/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7712 - val_loss: 28.2759\n",
      "Epoch 1116/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6438 - val_loss: 29.0907\n",
      "Epoch 1117/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8958 - val_loss: 28.3891\n",
      "Epoch 1118/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6792 - val_loss: 28.3578\n",
      "Epoch 1119/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6803 - val_loss: 28.2921\n",
      "Epoch 1120/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6339 - val_loss: 28.9299\n",
      "Epoch 1121/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8372 - val_loss: 28.8756\n",
      "Epoch 1122/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8806 - val_loss: 28.1506\n",
      "Epoch 1123/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7801 - val_loss: 28.2728\n",
      "Epoch 1124/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9567 - val_loss: 28.6357\n",
      "Epoch 1125/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8618 - val_loss: 27.9944\n",
      "Epoch 1126/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9546 - val_loss: 28.1454\n",
      "Epoch 1127/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7230 - val_loss: 28.5875\n",
      "Epoch 1128/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7982 - val_loss: 28.5969\n",
      "Epoch 1129/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6100 - val_loss: 27.8924\n",
      "Epoch 1130/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6056 - val_loss: 28.3216\n",
      "Epoch 1131/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5704 - val_loss: 28.3917\n",
      "Epoch 1132/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7728 - val_loss: 28.5419\n",
      "Epoch 1133/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8076 - val_loss: 28.2120\n",
      "Epoch 1134/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8602 - val_loss: 28.9111\n",
      "Epoch 1135/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6413 - val_loss: 28.2827\n",
      "Epoch 1136/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9729 - val_loss: 28.2660\n",
      "Epoch 1137/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0514 - val_loss: 28.6004\n",
      "Epoch 1138/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9350 - val_loss: 28.0485\n",
      "Epoch 1139/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9070 - val_loss: 28.4379\n",
      "Epoch 1140/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8260 - val_loss: 27.3866\n",
      "Epoch 1141/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.5497 - val_loss: 28.0549\n",
      "Epoch 1142/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6099 - val_loss: 27.7302\n",
      "Epoch 1143/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5354 - val_loss: 28.5709\n",
      "Epoch 1144/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.4558 - val_loss: 28.2977\n",
      "Epoch 1145/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8388 - val_loss: 28.5113\n",
      "Epoch 1146/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5511 - val_loss: 28.6217\n",
      "Epoch 1147/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6957 - val_loss: 28.2169\n",
      "Epoch 1148/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7510 - val_loss: 29.1340\n",
      "Epoch 1149/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7050 - val_loss: 28.4968\n",
      "Epoch 1150/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9064 - val_loss: 29.0473\n",
      "Epoch 1151/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9670 - val_loss: 29.1517\n",
      "Epoch 1152/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7747 - val_loss: 27.7861\n",
      "Epoch 1153/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7708 - val_loss: 27.7674\n",
      "Epoch 1154/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5354 - val_loss: 28.2086\n",
      "Epoch 1155/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9565 - val_loss: 29.1977\n",
      "Epoch 1156/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0449 - val_loss: 29.1855\n",
      "Epoch 1157/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1223 - val_loss: 28.8405\n",
      "Epoch 1158/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7595 - val_loss: 28.2037\n",
      "Epoch 1159/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.8353 - val_loss: 28.2379\n",
      "Epoch 1160/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7055 - val_loss: 28.1198\n",
      "Epoch 1161/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6725 - val_loss: 28.4925\n",
      "Epoch 1162/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9613 - val_loss: 27.9915\n",
      "Epoch 1163/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0133 - val_loss: 28.5040\n",
      "Epoch 1164/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8548 - val_loss: 28.3595\n",
      "Epoch 1165/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7579 - val_loss: 28.7681\n",
      "Epoch 1166/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7465 - val_loss: 28.2170\n",
      "Epoch 1167/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5681 - val_loss: 28.4657\n",
      "Epoch 1168/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6108 - val_loss: 28.1174\n",
      "Epoch 1169/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5806 - val_loss: 28.4790\n",
      "Epoch 1170/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4615 - val_loss: 28.0486\n",
      "Epoch 1171/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4813 - val_loss: 28.2015\n",
      "Epoch 1172/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5793 - val_loss: 28.1781\n",
      "Epoch 1173/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7363 - val_loss: 27.6862\n",
      "Epoch 1174/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7309 - val_loss: 28.2186\n",
      "Epoch 1175/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8135 - val_loss: 28.7534\n",
      "Epoch 1176/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4605 - val_loss: 28.4599\n",
      "Epoch 1177/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5684 - val_loss: 28.5229\n",
      "Epoch 1178/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8057 - val_loss: 27.8644\n",
      "Epoch 1179/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7588 - val_loss: 28.3864\n",
      "Epoch 1180/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5787 - val_loss: 28.5077\n",
      "Epoch 1181/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6152 - val_loss: 27.9889\n",
      "Epoch 1182/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.4267 - val_loss: 28.3542\n",
      "Epoch 1183/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6488 - val_loss: 28.4019\n",
      "Epoch 1184/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5161 - val_loss: 28.3826\n",
      "Epoch 1185/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5162 - val_loss: 28.6062\n",
      "Epoch 1186/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6139 - val_loss: 28.4692\n",
      "Epoch 1187/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7170 - val_loss: 28.7264\n",
      "Epoch 1188/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9268 - val_loss: 27.7794\n",
      "Epoch 1189/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5887 - val_loss: 28.1310\n",
      "Epoch 1190/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6717 - val_loss: 28.6065\n",
      "Epoch 1191/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6916 - val_loss: 28.4234\n",
      "Epoch 1192/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6424 - val_loss: 28.3542\n",
      "Epoch 1193/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5596 - val_loss: 28.6781\n",
      "Epoch 1194/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5465 - val_loss: 27.8452\n",
      "Epoch 1195/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3361 - val_loss: 28.8663\n",
      "Epoch 1196/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4535 - val_loss: 29.5781\n",
      "Epoch 1197/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7182 - val_loss: 28.5160\n",
      "Epoch 1198/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6836 - val_loss: 28.1002\n",
      "Epoch 1199/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5730 - val_loss: 28.2394\n",
      "Epoch 1200/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 25.2745\n",
      "Epoch 01200: saving model to saved_models/latent64/cp-1200.h5\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 25.2745 - val_loss: 28.4021\n",
      "Epoch 1201/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6476 - val_loss: 28.3273\n",
      "Epoch 1202/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3380 - val_loss: 28.2931\n",
      "Epoch 1203/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5030 - val_loss: 27.9422\n",
      "Epoch 1204/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5250 - val_loss: 27.8242\n",
      "Epoch 1205/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6038 - val_loss: 28.2116\n",
      "Epoch 1206/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5502 - val_loss: 28.7808\n",
      "Epoch 1207/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4634 - val_loss: 28.2433\n",
      "Epoch 1208/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4733 - val_loss: 28.2721\n",
      "Epoch 1209/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5268 - val_loss: 27.9103\n",
      "Epoch 1210/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3905 - val_loss: 29.2073\n",
      "Epoch 1211/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3798 - val_loss: 28.5707\n",
      "Epoch 1212/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5957 - val_loss: 28.4814\n",
      "Epoch 1213/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7250 - val_loss: 28.2650\n",
      "Epoch 1214/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3440 - val_loss: 28.7195\n",
      "Epoch 1215/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5411 - val_loss: 28.2640\n",
      "Epoch 1216/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6351 - val_loss: 27.4440\n",
      "Epoch 1217/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1152 - val_loss: 28.4367\n",
      "Epoch 1218/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7064 - val_loss: 29.1386\n",
      "Epoch 1219/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8286 - val_loss: 28.1493\n",
      "Epoch 1220/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3615 - val_loss: 28.2233\n",
      "Epoch 1221/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5296 - val_loss: 28.3900\n",
      "Epoch 1222/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4451 - val_loss: 28.6604\n",
      "Epoch 1223/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4862 - val_loss: 28.3382\n",
      "Epoch 1224/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4080 - val_loss: 27.4015\n",
      "Epoch 1225/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5655 - val_loss: 28.4607\n",
      "Epoch 1226/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4182 - val_loss: 27.7399\n",
      "Epoch 1227/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5637 - val_loss: 28.4568\n",
      "Epoch 1228/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3509 - val_loss: 27.9949\n",
      "Epoch 1229/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4720 - val_loss: 28.3124\n",
      "Epoch 1230/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4826 - val_loss: 28.3820\n",
      "Epoch 1231/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4016 - val_loss: 28.8176\n",
      "Epoch 1232/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4149 - val_loss: 28.1771\n",
      "Epoch 1233/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3193 - val_loss: 27.7962\n",
      "Epoch 1234/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2913 - val_loss: 28.6696\n",
      "Epoch 1235/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.2567 - val_loss: 28.3901\n",
      "Epoch 1236/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2742 - val_loss: 27.8876\n",
      "Epoch 1237/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.2363 - val_loss: 28.6762\n",
      "Epoch 1238/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2347 - val_loss: 28.5470\n",
      "Epoch 1239/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4680 - val_loss: 28.3472\n",
      "Epoch 1240/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5367 - val_loss: 28.5313\n",
      "Epoch 1241/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.1767 - val_loss: 27.8557\n",
      "Epoch 1242/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4197 - val_loss: 28.3008\n",
      "Epoch 1243/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3953 - val_loss: 27.9488\n",
      "Epoch 1244/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3290 - val_loss: 28.6657\n",
      "Epoch 1245/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5275 - val_loss: 28.0298\n",
      "Epoch 1246/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3599 - val_loss: 28.5682\n",
      "Epoch 1247/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4157 - val_loss: 28.4251\n",
      "Epoch 1248/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5118 - val_loss: 28.4480\n",
      "Epoch 1249/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5936 - val_loss: 28.4193\n",
      "Epoch 1250/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5646 - val_loss: 28.2134\n",
      "Epoch 1251/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7623 - val_loss: 28.1578\n",
      "Epoch 1252/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4566 - val_loss: 28.6646\n",
      "Epoch 1253/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4715 - val_loss: 28.1160\n",
      "Epoch 1254/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3240 - val_loss: 28.1302\n",
      "Epoch 1255/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3245 - val_loss: 28.7075\n",
      "Epoch 1256/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4390 - val_loss: 28.7459\n",
      "Epoch 1257/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4308 - val_loss: 27.7924\n",
      "Epoch 1258/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2900 - val_loss: 27.8829\n",
      "Epoch 1259/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2119 - val_loss: 28.2852\n",
      "Epoch 1260/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3685 - val_loss: 28.1283\n",
      "Epoch 1261/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3618 - val_loss: 27.8893\n",
      "Epoch 1262/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3784 - val_loss: 28.6435\n",
      "Epoch 1263/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5693 - val_loss: 28.1035\n",
      "Epoch 1264/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2043 - val_loss: 28.2497\n",
      "Epoch 1265/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2017 - val_loss: 28.3763\n",
      "Epoch 1266/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2437 - val_loss: 28.1821\n",
      "Epoch 1267/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2116 - val_loss: 28.7633\n",
      "Epoch 1268/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2608 - val_loss: 28.0435\n",
      "Epoch 1269/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5406 - val_loss: 27.5852\n",
      "Epoch 1270/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4370 - val_loss: 28.2503\n",
      "Epoch 1271/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3390 - val_loss: 28.2295\n",
      "Epoch 1272/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2214 - val_loss: 27.8550\n",
      "Epoch 1273/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3243 - val_loss: 28.6313\n",
      "Epoch 1274/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6471 - val_loss: 28.0390\n",
      "Epoch 1275/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3447 - val_loss: 28.3794\n",
      "Epoch 1276/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2905 - val_loss: 28.1111\n",
      "Epoch 1277/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2082 - val_loss: 27.9382\n",
      "Epoch 1278/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4608 - val_loss: 28.2329\n",
      "Epoch 1279/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3697 - val_loss: 28.1733\n",
      "Epoch 1280/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4293 - val_loss: 28.1138\n",
      "Epoch 1281/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3839 - val_loss: 27.8318\n",
      "Epoch 1282/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5766 - val_loss: 28.1991\n",
      "Epoch 1283/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3151 - val_loss: 27.5767\n",
      "Epoch 1284/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2559 - val_loss: 28.1575\n",
      "Epoch 1285/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4162 - val_loss: 27.9272\n",
      "Epoch 1286/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2321 - val_loss: 28.3124\n",
      "Epoch 1287/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3070 - val_loss: 28.3652\n",
      "Epoch 1288/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1830 - val_loss: 27.8385\n",
      "Epoch 1289/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4727 - val_loss: 27.4304\n",
      "Epoch 1290/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.2322 - val_loss: 28.0285\n",
      "Epoch 1291/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2337 - val_loss: 28.5840\n",
      "Epoch 1292/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5955 - val_loss: 27.8958\n",
      "Epoch 1293/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3984 - val_loss: 28.0048\n",
      "Epoch 1294/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1948 - val_loss: 28.0903\n",
      "Epoch 1295/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.1489 - val_loss: 28.4119\n",
      "Epoch 1296/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3406 - val_loss: 28.2092\n",
      "Epoch 1297/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4778 - val_loss: 28.4582\n",
      "Epoch 1298/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4414 - val_loss: 27.8405\n",
      "Epoch 1299/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5196 - val_loss: 28.5648\n",
      "Epoch 1300/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4732 - val_loss: 28.0127\n",
      "Epoch 1301/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3829 - val_loss: 28.2817\n",
      "Epoch 1302/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3534 - val_loss: 27.7865\n",
      "Epoch 1303/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3044 - val_loss: 27.7183\n",
      "Epoch 1304/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1825 - val_loss: 28.3282\n",
      "Epoch 1305/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3273 - val_loss: 28.3535\n",
      "Epoch 1306/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8271 - val_loss: 28.7721\n",
      "Epoch 1307/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9344 - val_loss: 29.5184\n",
      "Epoch 1308/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6639 - val_loss: 28.4753\n",
      "Epoch 1309/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3189 - val_loss: 28.1714\n",
      "Epoch 1310/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4245 - val_loss: 28.3179\n",
      "Epoch 1311/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2311 - val_loss: 27.9429\n",
      "Epoch 1312/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2143 - val_loss: 28.2574\n",
      "Epoch 1313/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.1308 - val_loss: 27.9411\n",
      "Epoch 1314/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0729 - val_loss: 27.7228\n",
      "Epoch 1315/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2404 - val_loss: 27.7558\n",
      "Epoch 1316/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0916 - val_loss: 27.7202\n",
      "Epoch 1317/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4918 - val_loss: 28.4171\n",
      "Epoch 1318/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1615 - val_loss: 28.4110\n",
      "Epoch 1319/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3319 - val_loss: 28.0635\n",
      "Epoch 1320/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3713 - val_loss: 28.6313\n",
      "Epoch 1321/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2775 - val_loss: 27.7019\n",
      "Epoch 1322/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1998 - val_loss: 27.9762\n",
      "Epoch 1323/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1221 - val_loss: 27.6365\n",
      "Epoch 1324/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1913 - val_loss: 27.8638\n",
      "Epoch 1325/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2009 - val_loss: 27.6780\n",
      "Epoch 1326/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2261 - val_loss: 28.0142\n",
      "Epoch 1327/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3261 - val_loss: 28.3022\n",
      "Epoch 1328/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4549 - val_loss: 28.0419\n",
      "Epoch 1329/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4037 - val_loss: 28.0546\n",
      "Epoch 1330/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0490 - val_loss: 28.0839\n",
      "Epoch 1331/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2809 - val_loss: 27.8031\n",
      "Epoch 1332/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5423 - val_loss: 28.3531\n",
      "Epoch 1333/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4281 - val_loss: 28.6509\n",
      "Epoch 1334/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4997 - val_loss: 28.0060\n",
      "Epoch 1335/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1591 - val_loss: 28.2248\n",
      "Epoch 1336/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2493 - val_loss: 28.2072\n",
      "Epoch 1337/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3174 - val_loss: 29.0760\n",
      "Epoch 1338/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8279 - val_loss: 29.4778\n",
      "Epoch 1339/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6137 - val_loss: 27.9031\n",
      "Epoch 1340/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2583 - val_loss: 28.2369\n",
      "Epoch 1341/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3414 - val_loss: 28.0893\n",
      "Epoch 1342/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1892 - val_loss: 28.0035\n",
      "Epoch 1343/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1827 - val_loss: 28.3799\n",
      "Epoch 1344/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.0282 - val_loss: 28.5015\n",
      "Epoch 1345/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0661 - val_loss: 28.5101\n",
      "Epoch 1346/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1712 - val_loss: 28.4829\n",
      "Epoch 1347/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1924 - val_loss: 28.6452\n",
      "Epoch 1348/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3183 - val_loss: 27.8918\n",
      "Epoch 1349/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3343 - val_loss: 27.7763\n",
      "Epoch 1350/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1765 - val_loss: 27.8684\n",
      "Epoch 1351/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1606 - val_loss: 28.1440\n",
      "Epoch 1352/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1077 - val_loss: 28.1494\n",
      "Epoch 1353/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1717 - val_loss: 28.6222\n",
      "Epoch 1354/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2342 - val_loss: 29.0738\n",
      "Epoch 1355/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2406 - val_loss: 27.8165\n",
      "Epoch 1356/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2174 - val_loss: 27.7370\n",
      "Epoch 1357/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2127 - val_loss: 28.4703\n",
      "Epoch 1358/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2103 - val_loss: 27.6367\n",
      "Epoch 1359/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2400 - val_loss: 27.9634\n",
      "Epoch 1360/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1728 - val_loss: 27.7773\n",
      "Epoch 1361/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1270 - val_loss: 28.2743\n",
      "Epoch 1362/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3104 - val_loss: 29.1521\n",
      "Epoch 1363/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4493 - val_loss: 28.5312\n",
      "Epoch 1364/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3978 - val_loss: 28.4510\n",
      "Epoch 1365/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0328 - val_loss: 27.9163\n",
      "Epoch 1366/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3677 - val_loss: 27.8265\n",
      "Epoch 1367/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2026 - val_loss: 27.5250\n",
      "Epoch 1368/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1868 - val_loss: 27.8401\n",
      "Epoch 1369/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0499 - val_loss: 27.6613\n",
      "Epoch 1370/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1644 - val_loss: 28.1611\n",
      "Epoch 1371/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2587 - val_loss: 28.3108\n",
      "Epoch 1372/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2245 - val_loss: 28.4119\n",
      "Epoch 1373/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3214 - val_loss: 28.0329\n",
      "Epoch 1374/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0423 - val_loss: 28.6725\n",
      "Epoch 1375/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3501 - val_loss: 28.6667\n",
      "Epoch 1376/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4832 - val_loss: 29.0244\n",
      "Epoch 1377/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2017 - val_loss: 27.9865\n",
      "Epoch 1378/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3234 - val_loss: 27.9583\n",
      "Epoch 1379/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2551 - val_loss: 28.1807\n",
      "Epoch 1380/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0940 - val_loss: 28.3360\n",
      "Epoch 1381/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3853 - val_loss: 28.4928\n",
      "Epoch 1382/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2521 - val_loss: 27.8482\n",
      "Epoch 1383/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3738 - val_loss: 27.4526\n",
      "Epoch 1384/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1247 - val_loss: 27.8232\n",
      "Epoch 1385/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0997 - val_loss: 27.9881\n",
      "Epoch 1386/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1133 - val_loss: 28.2634\n",
      "Epoch 1387/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1128 - val_loss: 28.2906\n",
      "Epoch 1388/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1358 - val_loss: 28.0217\n",
      "Epoch 1389/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1892 - val_loss: 28.1338\n",
      "Epoch 1390/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2220 - val_loss: 27.9370\n",
      "Epoch 1391/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9488 - val_loss: 27.7019\n",
      "Epoch 1392/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1094 - val_loss: 27.4850\n",
      "Epoch 1393/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0759 - val_loss: 27.7259\n",
      "Epoch 1394/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2812 - val_loss: 28.6708\n",
      "Epoch 1395/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4633 - val_loss: 28.6629\n",
      "Epoch 1396/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4256 - val_loss: 28.1804\n",
      "Epoch 1397/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2785 - val_loss: 28.0393\n",
      "Epoch 1398/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0859 - val_loss: 28.1992\n",
      "Epoch 1399/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0686 - val_loss: 27.9115\n",
      "Epoch 1400/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 25.2014\n",
      "Epoch 01400: saving model to saved_models/latent64/cp-1400.h5\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 25.2014 - val_loss: 28.4552\n",
      "Epoch 1401/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2761 - val_loss: 28.1368\n",
      "Epoch 1402/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4038 - val_loss: 28.7001\n",
      "Epoch 1403/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3587 - val_loss: 28.4809\n",
      "Epoch 1404/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4474 - val_loss: 28.4162\n",
      "Epoch 1405/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2137 - val_loss: 27.5833\n",
      "Epoch 1406/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0041 - val_loss: 27.3593\n",
      "Epoch 1407/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0069 - val_loss: 27.7072\n",
      "Epoch 1408/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1888 - val_loss: 28.7480\n",
      "Epoch 1409/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5489 - val_loss: 27.8412\n",
      "Epoch 1410/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1867 - val_loss: 28.3769\n",
      "Epoch 1411/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0930 - val_loss: 28.3870\n",
      "Epoch 1412/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9507 - val_loss: 27.8794\n",
      "Epoch 1413/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0475 - val_loss: 27.9702\n",
      "Epoch 1414/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3687 - val_loss: 27.8448\n",
      "Epoch 1415/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2862 - val_loss: 27.4761\n",
      "Epoch 1416/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2969 - val_loss: 28.5684\n",
      "Epoch 1417/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0283 - val_loss: 27.5748\n",
      "Epoch 1418/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0274 - val_loss: 28.2086\n",
      "Epoch 1419/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1337 - val_loss: 27.8760\n",
      "Epoch 1420/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0178 - val_loss: 27.6264\n",
      "Epoch 1421/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2418 - val_loss: 28.9385\n",
      "Epoch 1422/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3539 - val_loss: 27.9429\n",
      "Epoch 1423/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1134 - val_loss: 28.0894\n",
      "Epoch 1424/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1871 - val_loss: 28.8177\n",
      "Epoch 1425/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4288 - val_loss: 28.0792\n",
      "Epoch 1426/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2260 - val_loss: 28.4626\n",
      "Epoch 1427/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5856 - val_loss: 28.0605\n",
      "Epoch 1428/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4843 - val_loss: 27.8120\n",
      "Epoch 1429/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3052 - val_loss: 27.9824\n",
      "Epoch 1430/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0551 - val_loss: 27.7394\n",
      "Epoch 1431/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9616 - val_loss: 28.9761\n",
      "Epoch 1432/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3981 - val_loss: 28.1630\n",
      "Epoch 1433/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2464 - val_loss: 27.7630\n",
      "Epoch 1434/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1431 - val_loss: 28.3412\n",
      "Epoch 1435/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2582 - val_loss: 27.5391\n",
      "Epoch 1436/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2065 - val_loss: 27.9161\n",
      "Epoch 1437/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9784 - val_loss: 27.8992\n",
      "Epoch 1438/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9643 - val_loss: 27.9124\n",
      "Epoch 1439/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2051 - val_loss: 28.7507\n",
      "Epoch 1440/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1665 - val_loss: 28.4493\n",
      "Epoch 1441/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0830 - val_loss: 28.3914\n",
      "Epoch 1442/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9174 - val_loss: 27.9430\n",
      "Epoch 1443/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1935 - val_loss: 27.7569\n",
      "Epoch 1444/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0629 - val_loss: 28.1348\n",
      "Epoch 1445/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0347 - val_loss: 27.6396\n",
      "Epoch 1446/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0150 - val_loss: 28.4893\n",
      "Epoch 1447/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0797 - val_loss: 27.9840\n",
      "Epoch 1448/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0861 - val_loss: 27.9346\n",
      "Epoch 1449/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0751 - val_loss: 28.0857\n",
      "Epoch 1450/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9395 - val_loss: 28.9268\n",
      "Epoch 1451/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1525 - val_loss: 28.3224\n",
      "Epoch 1452/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0529 - val_loss: 29.0076\n",
      "Epoch 1453/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9825 - val_loss: 28.3247\n",
      "Epoch 1454/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1721 - val_loss: 28.2337\n",
      "Epoch 1455/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3277 - val_loss: 28.4840\n",
      "Epoch 1456/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1386 - val_loss: 27.9697\n",
      "Epoch 1457/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0967 - val_loss: 28.0212\n",
      "Epoch 1458/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1609 - val_loss: 28.4927\n",
      "Epoch 1459/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1852 - val_loss: 28.1663\n",
      "Epoch 1460/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1611 - val_loss: 27.6479\n",
      "Epoch 1461/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1219 - val_loss: 28.1602\n",
      "Epoch 1462/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1309 - val_loss: 28.0747\n",
      "Epoch 1463/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0429 - val_loss: 27.6941\n",
      "Epoch 1464/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1252 - val_loss: 28.4456\n",
      "Epoch 1465/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0129 - val_loss: 27.9427\n",
      "Epoch 1466/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0357 - val_loss: 27.9733\n",
      "Epoch 1467/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1221 - val_loss: 28.4896\n",
      "Epoch 1468/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1797 - val_loss: 27.7772\n",
      "Epoch 1469/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2077 - val_loss: 28.6171\n",
      "Epoch 1470/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1940 - val_loss: 28.6154\n",
      "Epoch 1471/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1031 - val_loss: 28.0855\n",
      "Epoch 1472/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1833 - val_loss: 27.8483\n",
      "Epoch 1473/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0612 - val_loss: 28.2473\n",
      "Epoch 1474/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.9010 - val_loss: 28.3155\n",
      "Epoch 1475/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0386 - val_loss: 28.1994\n",
      "Epoch 1476/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0575 - val_loss: 28.3585\n",
      "Epoch 1477/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1447 - val_loss: 27.5850\n",
      "Epoch 1478/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.7949 - val_loss: 28.3230\n",
      "Epoch 1479/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9448 - val_loss: 28.1043\n",
      "Epoch 1480/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9949 - val_loss: 27.8261\n",
      "Epoch 1481/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2264 - val_loss: 29.1141\n",
      "Epoch 1482/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4002 - val_loss: 28.3705\n",
      "Epoch 1483/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4703 - val_loss: 28.5222\n",
      "Epoch 1484/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3232 - val_loss: 28.5457\n",
      "Epoch 1485/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4482 - val_loss: 27.7774\n",
      "Epoch 1486/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0009 - val_loss: 29.0824\n",
      "Epoch 1487/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2137 - val_loss: 28.5341\n",
      "Epoch 1488/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9669 - val_loss: 27.7701\n",
      "Epoch 1489/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0581 - val_loss: 28.6137\n",
      "Epoch 1490/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2084 - val_loss: 28.1774\n",
      "Epoch 1491/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2902 - val_loss: 28.0130\n",
      "Epoch 1492/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0606 - val_loss: 27.7858\n",
      "Epoch 1493/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9350 - val_loss: 28.2460\n",
      "Epoch 1494/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1176 - val_loss: 27.6117\n",
      "Epoch 1495/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8127 - val_loss: 28.4137\n",
      "Epoch 1496/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8995 - val_loss: 28.7697\n",
      "Epoch 1497/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8886 - val_loss: 28.1354\n",
      "Epoch 1498/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.7874 - val_loss: 28.5766\n",
      "Epoch 1499/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2720 - val_loss: 28.7134\n",
      "Epoch 1500/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4146 - val_loss: 28.3228\n",
      "Epoch 1501/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9930 - val_loss: 27.9970\n",
      "Epoch 1502/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0145 - val_loss: 28.4317\n",
      "Epoch 1503/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8963 - val_loss: 27.8623\n",
      "Epoch 1504/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1143 - val_loss: 27.5914\n",
      "Epoch 1505/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8809 - val_loss: 28.8891\n",
      "Epoch 1506/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8029 - val_loss: 28.0522\n",
      "Epoch 1507/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9449 - val_loss: 28.4169\n",
      "Epoch 1508/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9032 - val_loss: 28.5164\n",
      "Epoch 1509/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9458 - val_loss: 27.7088\n",
      "Epoch 1510/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0099 - val_loss: 28.9252\n",
      "Epoch 1511/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9161 - val_loss: 28.0938\n",
      "Epoch 1512/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9279 - val_loss: 27.7504\n",
      "Epoch 1513/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9157 - val_loss: 27.8538\n",
      "Epoch 1514/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7818 - val_loss: 27.8930\n",
      "Epoch 1515/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9560 - val_loss: 27.9324\n",
      "Epoch 1516/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0994 - val_loss: 28.2040\n",
      "Epoch 1517/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9880 - val_loss: 27.7110\n",
      "Epoch 1518/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0982 - val_loss: 28.3644\n",
      "Epoch 1519/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8797 - val_loss: 28.2389\n",
      "Epoch 1520/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8597 - val_loss: 28.5073\n",
      "Epoch 1521/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9832 - val_loss: 27.8997\n",
      "Epoch 1522/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9239 - val_loss: 28.0517\n",
      "Epoch 1523/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2053 - val_loss: 28.6219\n",
      "Epoch 1524/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2197 - val_loss: 28.7050\n",
      "Epoch 1525/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1215 - val_loss: 27.9775\n",
      "Epoch 1526/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9956 - val_loss: 28.2931\n",
      "Epoch 1527/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8421 - val_loss: 28.0383\n",
      "Epoch 1528/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9014 - val_loss: 28.1826\n",
      "Epoch 1529/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8988 - val_loss: 29.1663\n",
      "Epoch 1530/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8015 - val_loss: 28.4820\n",
      "Epoch 1531/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1519 - val_loss: 28.4406\n",
      "Epoch 1532/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2772 - val_loss: 28.0889\n",
      "Epoch 1533/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0752 - val_loss: 28.4710\n",
      "Epoch 1534/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9116 - val_loss: 27.6813\n",
      "Epoch 1535/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1062 - val_loss: 28.2984\n",
      "Epoch 1536/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8674 - val_loss: 28.2775\n",
      "Epoch 1537/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3069 - val_loss: 28.7363\n",
      "Epoch 1538/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0790 - val_loss: 28.0526\n",
      "Epoch 1539/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1045 - val_loss: 27.6027\n",
      "Epoch 1540/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0722 - val_loss: 28.2722\n",
      "Epoch 1541/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9897 - val_loss: 28.0140\n",
      "Epoch 1542/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0185 - val_loss: 27.9338\n",
      "Epoch 1543/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1013 - val_loss: 28.2464\n",
      "Epoch 1544/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9162 - val_loss: 28.2749\n",
      "Epoch 1545/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8400 - val_loss: 27.6879\n",
      "Epoch 1546/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0071 - val_loss: 28.1074\n",
      "Epoch 1547/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0386 - val_loss: 27.9805\n",
      "Epoch 1548/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1831 - val_loss: 28.3213\n",
      "Epoch 1549/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5544 - val_loss: 30.0353\n",
      "Epoch 1550/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5756 - val_loss: 28.2863\n",
      "Epoch 1551/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9575 - val_loss: 28.4503\n",
      "Epoch 1552/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0002 - val_loss: 27.8142\n",
      "Epoch 1553/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8743 - val_loss: 28.1799\n",
      "Epoch 1554/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0060 - val_loss: 28.2877\n",
      "Epoch 1555/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8717 - val_loss: 27.9577\n",
      "Epoch 1556/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8954 - val_loss: 27.8627\n",
      "Epoch 1557/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0740 - val_loss: 28.6314\n",
      "Epoch 1558/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1737 - val_loss: 27.7941\n",
      "Epoch 1559/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9660 - val_loss: 28.1085\n",
      "Epoch 1560/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1394 - val_loss: 27.8912\n",
      "Epoch 1561/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8106 - val_loss: 28.3412\n",
      "Epoch 1562/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9462 - val_loss: 27.7057\n",
      "Epoch 1563/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9312 - val_loss: 28.0461\n",
      "Epoch 1564/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8058 - val_loss: 28.3161\n",
      "Epoch 1565/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0923 - val_loss: 28.2836\n",
      "Epoch 1566/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8059 - val_loss: 27.8914\n",
      "Epoch 1567/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0257 - val_loss: 28.2031\n",
      "Epoch 1568/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8813 - val_loss: 27.8590\n",
      "Epoch 1569/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9099 - val_loss: 27.5141\n",
      "Epoch 1570/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8802 - val_loss: 28.1803\n",
      "Epoch 1571/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7348 - val_loss: 28.3261\n",
      "Epoch 1572/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9572 - val_loss: 28.6510\n",
      "Epoch 1573/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8054 - val_loss: 27.6737\n",
      "Epoch 1574/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7863 - val_loss: 27.8841\n",
      "Epoch 1575/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8184 - val_loss: 28.1473\n",
      "Epoch 1576/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1242 - val_loss: 27.9656\n",
      "Epoch 1577/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8809 - val_loss: 28.1127\n",
      "Epoch 1578/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7416 - val_loss: 28.5418\n",
      "Epoch 1579/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8157 - val_loss: 27.4371\n",
      "Epoch 1580/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7993 - val_loss: 27.8950\n",
      "Epoch 1581/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8387 - val_loss: 28.2959\n",
      "Epoch 1582/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9674 - val_loss: 28.3953\n",
      "Epoch 1583/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8982 - val_loss: 28.0556\n",
      "Epoch 1584/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0407 - val_loss: 27.9268\n",
      "Epoch 1585/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8564 - val_loss: 28.0489\n",
      "Epoch 1586/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0004 - val_loss: 28.2651\n",
      "Epoch 1587/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2279 - val_loss: 28.3919\n",
      "Epoch 1588/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0760 - val_loss: 28.5978\n",
      "Epoch 1589/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1139 - val_loss: 28.2536\n",
      "Epoch 1590/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1025 - val_loss: 27.8894\n",
      "Epoch 1591/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0958 - val_loss: 28.4856\n",
      "Epoch 1592/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8561 - val_loss: 28.3974\n",
      "Epoch 1593/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9765 - val_loss: 28.3437\n",
      "Epoch 1594/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9113 - val_loss: 28.3767\n",
      "Epoch 1595/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7962 - val_loss: 28.2513\n",
      "Epoch 1596/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9674 - val_loss: 27.9685\n",
      "Epoch 1597/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9591 - val_loss: 28.5101\n",
      "Epoch 1598/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9661 - val_loss: 28.5346\n",
      "Epoch 1599/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9985 - val_loss: 28.0388\n",
      "Epoch 1600/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 24.9183\n",
      "Epoch 01600: saving model to saved_models/latent64/cp-1600.h5\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 24.9183 - val_loss: 28.3107\n",
      "Epoch 1601/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0628 - val_loss: 28.4879\n",
      "Epoch 1602/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9850 - val_loss: 28.5890\n",
      "Epoch 1603/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9869 - val_loss: 28.2734\n",
      "Epoch 1604/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8425 - val_loss: 27.9938\n",
      "Epoch 1605/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0301 - val_loss: 27.9541\n",
      "Epoch 1606/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8521 - val_loss: 27.9510\n",
      "Epoch 1607/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9389 - val_loss: 28.3977\n",
      "Epoch 1608/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9171 - val_loss: 27.7321\n",
      "Epoch 1609/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7956 - val_loss: 27.6818\n",
      "Epoch 1610/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.6750 - val_loss: 27.6410\n",
      "Epoch 1611/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7007 - val_loss: 27.9306\n",
      "Epoch 1612/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9326 - val_loss: 28.1613\n",
      "Epoch 1613/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7513 - val_loss: 27.8162\n",
      "Epoch 1614/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8912 - val_loss: 28.6587\n",
      "Epoch 1615/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8253 - val_loss: 28.0201\n",
      "Epoch 1616/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0456 - val_loss: 27.9550\n",
      "Epoch 1617/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1985 - val_loss: 28.1089\n",
      "Epoch 1618/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0924 - val_loss: 28.1945\n",
      "Epoch 1619/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9419 - val_loss: 27.9893\n",
      "Epoch 1620/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7710 - val_loss: 27.8385\n",
      "Epoch 1621/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8569 - val_loss: 27.6730\n",
      "Epoch 1622/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7578 - val_loss: 28.2664\n",
      "Epoch 1623/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9098 - val_loss: 28.1562\n",
      "Epoch 1624/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1836 - val_loss: 28.8211\n",
      "Epoch 1625/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1727 - val_loss: 28.0954\n",
      "Epoch 1626/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0550 - val_loss: 28.3974\n",
      "Epoch 1627/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0010 - val_loss: 27.5497\n",
      "Epoch 1628/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0008 - val_loss: 29.1156\n",
      "Epoch 1629/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6798 - val_loss: 29.0854\n",
      "Epoch 1630/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0099 - val_loss: 27.8064\n",
      "Epoch 1631/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9851 - val_loss: 28.6521\n",
      "Epoch 1632/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8899 - val_loss: 27.9943\n",
      "Epoch 1633/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8010 - val_loss: 28.7303\n",
      "Epoch 1634/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9023 - val_loss: 28.3390\n",
      "Epoch 1635/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7354 - val_loss: 28.0975\n",
      "Epoch 1636/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7434 - val_loss: 28.1448\n",
      "Epoch 1637/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6522 - val_loss: 27.6556\n",
      "Epoch 1638/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9008 - val_loss: 27.8078\n",
      "Epoch 1639/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8053 - val_loss: 28.6066\n",
      "Epoch 1640/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0293 - val_loss: 29.3392\n",
      "Epoch 1641/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1204 - val_loss: 28.2983\n",
      "Epoch 1642/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8608 - val_loss: 28.4886\n",
      "Epoch 1643/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9928 - val_loss: 28.4154\n",
      "Epoch 1644/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8526 - val_loss: 28.2497\n",
      "Epoch 1645/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7374 - val_loss: 28.3169\n",
      "Epoch 1646/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9497 - val_loss: 28.0644\n",
      "Epoch 1647/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9630 - val_loss: 28.5056\n",
      "Epoch 1648/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3015 - val_loss: 29.0328\n",
      "Epoch 1649/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2997 - val_loss: 29.5132\n",
      "Epoch 1650/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0794 - val_loss: 28.0495\n",
      "Epoch 1651/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9538 - val_loss: 28.4766\n",
      "Epoch 1652/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9524 - val_loss: 28.2223\n",
      "Epoch 1653/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9985 - val_loss: 28.2946\n",
      "Epoch 1654/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8658 - val_loss: 28.0779\n",
      "Epoch 1655/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0710 - val_loss: 28.8462\n",
      "Epoch 1656/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1191 - val_loss: 27.9712\n",
      "Epoch 1657/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8454 - val_loss: 28.4178\n",
      "Epoch 1658/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8703 - val_loss: 28.4456\n",
      "Epoch 1659/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8585 - val_loss: 28.2540\n",
      "Epoch 1660/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9230 - val_loss: 28.3038\n",
      "Epoch 1661/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7938 - val_loss: 28.3727\n",
      "Epoch 1662/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8213 - val_loss: 28.2867\n",
      "Epoch 1663/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9100 - val_loss: 28.3710\n",
      "Epoch 1664/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8168 - val_loss: 28.0616\n",
      "Epoch 1665/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9108 - val_loss: 28.1899\n",
      "Epoch 1666/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7030 - val_loss: 28.8625\n",
      "Epoch 1667/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8281 - val_loss: 29.0040\n",
      "Epoch 1668/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8521 - val_loss: 27.5082\n",
      "Epoch 1669/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8386 - val_loss: 28.2038\n",
      "Epoch 1670/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8010 - val_loss: 27.8156\n",
      "Epoch 1671/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7986 - val_loss: 27.9718\n",
      "Epoch 1672/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8693 - val_loss: 28.8738\n",
      "Epoch 1673/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1419 - val_loss: 29.3590\n",
      "Epoch 1674/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1620 - val_loss: 28.0022\n",
      "Epoch 1675/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9345 - val_loss: 28.2051\n",
      "Epoch 1676/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9528 - val_loss: 28.3406\n",
      "Epoch 1677/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0458 - val_loss: 28.1083\n",
      "Epoch 1678/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8974 - val_loss: 27.9853\n",
      "Epoch 1679/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8067 - val_loss: 27.5419\n",
      "Epoch 1680/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1241 - val_loss: 27.9976\n",
      "Epoch 1681/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3506 - val_loss: 28.9023\n",
      "Epoch 1682/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3699 - val_loss: 27.6958\n",
      "Epoch 1683/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2793 - val_loss: 28.3290\n",
      "Epoch 1684/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0697 - val_loss: 27.9057\n",
      "Epoch 1685/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7688 - val_loss: 28.1269\n",
      "Epoch 1686/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7663 - val_loss: 28.2227\n",
      "Epoch 1687/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7824 - val_loss: 27.7508\n",
      "Epoch 1688/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9088 - val_loss: 28.6025\n",
      "Epoch 1689/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5931 - val_loss: 28.0247\n",
      "Epoch 1690/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8936 - val_loss: 28.2966\n",
      "Epoch 1691/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7131 - val_loss: 28.0795\n",
      "Epoch 1692/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7892 - val_loss: 27.8616\n",
      "Epoch 1693/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6536 - val_loss: 28.3846\n",
      "Epoch 1694/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6961 - val_loss: 28.2942\n",
      "Epoch 1695/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7500 - val_loss: 28.5441\n",
      "Epoch 1696/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1751 - val_loss: 28.8724\n",
      "Epoch 1697/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2500 - val_loss: 28.4464\n",
      "Epoch 1698/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8821 - val_loss: 28.2046\n",
      "Epoch 1699/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8959 - val_loss: 28.2745\n",
      "Epoch 1700/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5975 - val_loss: 28.8869\n",
      "Epoch 1701/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6065 - val_loss: 28.5679\n",
      "Epoch 1702/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7368 - val_loss: 28.4840\n",
      "Epoch 1703/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7304 - val_loss: 28.5164\n",
      "Epoch 1704/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8035 - val_loss: 27.7348\n",
      "Epoch 1705/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7543 - val_loss: 27.9951\n",
      "Epoch 1706/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9595 - val_loss: 29.3114\n",
      "Epoch 1707/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1198 - val_loss: 27.6352\n",
      "Epoch 1708/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0726 - val_loss: 28.1408\n",
      "Epoch 1709/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7014 - val_loss: 27.7509\n",
      "Epoch 1710/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9928 - val_loss: 28.9729\n",
      "Epoch 1711/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9993 - val_loss: 28.6091\n",
      "Epoch 1712/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6809 - val_loss: 27.9489\n",
      "Epoch 1713/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7441 - val_loss: 28.4748\n",
      "Epoch 1714/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9509 - val_loss: 27.7260\n",
      "Epoch 1715/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8452 - val_loss: 27.9497\n",
      "Epoch 1716/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7188 - val_loss: 27.5487\n",
      "Epoch 1717/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8569 - val_loss: 28.3825\n",
      "Epoch 1718/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0107 - val_loss: 28.2917\n",
      "Epoch 1719/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1064 - val_loss: 28.3478\n",
      "Epoch 1720/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9799 - val_loss: 28.1651\n",
      "Epoch 1721/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9790 - val_loss: 27.9221\n",
      "Epoch 1722/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.5270 - val_loss: 28.2799\n",
      "Epoch 1723/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7939 - val_loss: 28.3696\n",
      "Epoch 1724/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6047 - val_loss: 28.4813\n",
      "Epoch 1725/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7317 - val_loss: 27.7470\n",
      "Epoch 1726/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8302 - val_loss: 28.1637\n",
      "Epoch 1727/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6420 - val_loss: 27.6399\n",
      "Epoch 1728/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7498 - val_loss: 28.2106\n",
      "Epoch 1729/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0950 - val_loss: 28.6696\n",
      "Epoch 1730/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7687 - val_loss: 29.0098\n",
      "Epoch 1731/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7474 - val_loss: 28.0825\n",
      "Epoch 1732/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5656 - val_loss: 27.9986\n",
      "Epoch 1733/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7475 - val_loss: 28.2712\n",
      "Epoch 1734/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.4760 - val_loss: 27.9074\n",
      "Epoch 1735/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6467 - val_loss: 28.6067\n",
      "Epoch 1736/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8669 - val_loss: 27.8588\n",
      "Epoch 1737/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8431 - val_loss: 28.0130\n",
      "Epoch 1738/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5914 - val_loss: 27.6369\n",
      "Epoch 1739/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8117 - val_loss: 28.5008\n",
      "Epoch 1740/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9729 - val_loss: 28.7052\n",
      "Epoch 1741/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8506 - val_loss: 28.6826\n",
      "Epoch 1742/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7491 - val_loss: 28.1134\n",
      "Epoch 1743/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7020 - val_loss: 28.4178\n",
      "Epoch 1744/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7650 - val_loss: 28.6443\n",
      "Epoch 1745/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6389 - val_loss: 28.5048\n",
      "Epoch 1746/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6460 - val_loss: 28.2459\n",
      "Epoch 1747/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7200 - val_loss: 28.4854\n",
      "Epoch 1748/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5415 - val_loss: 27.8381\n",
      "Epoch 1749/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6442 - val_loss: 27.9488\n",
      "Epoch 1750/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8088 - val_loss: 28.1600\n",
      "Epoch 1751/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6245 - val_loss: 28.1615\n",
      "Epoch 1752/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9182 - val_loss: 29.3775\n",
      "Epoch 1753/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0572 - val_loss: 28.9541\n",
      "Epoch 1754/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0407 - val_loss: 28.2603\n",
      "Epoch 1755/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9667 - val_loss: 28.1896\n",
      "Epoch 1756/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7324 - val_loss: 29.0392\n",
      "Epoch 1757/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8978 - val_loss: 28.2094\n",
      "Epoch 1758/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7293 - val_loss: 28.2396\n",
      "Epoch 1759/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9904 - val_loss: 28.0658\n",
      "Epoch 1760/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8579 - val_loss: 28.9081\n",
      "Epoch 1761/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8497 - val_loss: 28.0719\n",
      "Epoch 1762/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7734 - val_loss: 28.0314\n",
      "Epoch 1763/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1397 - val_loss: 28.6974\n",
      "Epoch 1764/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8816 - val_loss: 29.9068\n",
      "Epoch 1765/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1084 - val_loss: 28.2764\n",
      "Epoch 1766/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8544 - val_loss: 28.1619\n",
      "Epoch 1767/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6477 - val_loss: 27.9850\n",
      "Epoch 1768/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6749 - val_loss: 27.8069\n",
      "Epoch 1769/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5107 - val_loss: 28.7720\n",
      "Epoch 1770/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8508 - val_loss: 28.1959\n",
      "Epoch 1771/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7078 - val_loss: 28.2989\n",
      "Epoch 1772/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8823 - val_loss: 28.5456\n",
      "Epoch 1773/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7657 - val_loss: 28.0089\n",
      "Epoch 1774/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8752 - val_loss: 28.7406\n",
      "Epoch 1775/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8327 - val_loss: 28.4646\n",
      "Epoch 1776/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1069 - val_loss: 28.3852\n",
      "Epoch 1777/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8362 - val_loss: 28.5311\n",
      "Epoch 1778/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8314 - val_loss: 28.4239\n",
      "Epoch 1779/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6206 - val_loss: 28.1970\n",
      "Epoch 1780/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8056 - val_loss: 28.0404\n",
      "Epoch 1781/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7178 - val_loss: 28.4091\n",
      "Epoch 1782/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7638 - val_loss: 28.6459\n",
      "Epoch 1783/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8585 - val_loss: 28.7656\n",
      "Epoch 1784/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7219 - val_loss: 28.3996\n",
      "Epoch 1785/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7298 - val_loss: 29.3970\n",
      "Epoch 1786/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6296 - val_loss: 28.0587\n",
      "Epoch 1787/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7638 - val_loss: 27.2594\n",
      "Epoch 1788/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7214 - val_loss: 28.7347\n",
      "Epoch 1789/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6211 - val_loss: 27.8437\n",
      "Epoch 1790/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6578 - val_loss: 28.3719\n",
      "Epoch 1791/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5671 - val_loss: 28.6938\n",
      "Epoch 1792/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7863 - val_loss: 28.7049\n",
      "Epoch 1793/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6471 - val_loss: 28.2041\n",
      "Epoch 1794/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7434 - val_loss: 27.9960\n",
      "Epoch 1795/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7882 - val_loss: 28.7185\n",
      "Epoch 1796/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6959 - val_loss: 28.0550\n",
      "Epoch 1797/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9721 - val_loss: 28.0106\n",
      "Epoch 1798/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7530 - val_loss: 28.2435\n",
      "Epoch 1799/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6605 - val_loss: 27.8760\n",
      "Epoch 1800/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 24.6141\n",
      "Epoch 01800: saving model to saved_models/latent64/cp-1800.h5\n",
      "7/7 [==============================] - 1s 156ms/step - loss: 24.6141 - val_loss: 28.9447\n",
      "Epoch 1801/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7496 - val_loss: 28.9794\n",
      "Epoch 1802/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6176 - val_loss: 28.0911\n",
      "Epoch 1803/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7941 - val_loss: 27.8712\n",
      "Epoch 1804/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8021 - val_loss: 28.1552\n",
      "Epoch 1805/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8864 - val_loss: 28.4647\n",
      "Epoch 1806/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9854 - val_loss: 28.3470\n",
      "Epoch 1807/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9141 - val_loss: 28.0492\n",
      "Epoch 1808/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9440 - val_loss: 29.5363\n",
      "Epoch 1809/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2946 - val_loss: 29.1590\n",
      "Epoch 1810/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0064 - val_loss: 28.3756\n",
      "Epoch 1811/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7888 - val_loss: 28.2670\n",
      "Epoch 1812/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7940 - val_loss: 28.6844\n",
      "Epoch 1813/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8292 - val_loss: 29.0050\n",
      "Epoch 1814/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2923 - val_loss: 28.5637\n",
      "Epoch 1815/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1387 - val_loss: 28.8639\n",
      "Epoch 1816/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8333 - val_loss: 28.0592\n",
      "Epoch 1817/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6423 - val_loss: 28.5081\n",
      "Epoch 1818/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6979 - val_loss: 28.2964\n",
      "Epoch 1819/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6593 - val_loss: 28.5837\n",
      "Epoch 1820/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6774 - val_loss: 28.1652\n",
      "Epoch 1821/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8196 - val_loss: 28.3953\n",
      "Epoch 1822/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2267 - val_loss: 28.5187\n",
      "Epoch 1823/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7501 - val_loss: 28.0070\n",
      "Epoch 1824/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6118 - val_loss: 28.3606\n",
      "Epoch 1825/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5665 - val_loss: 27.7239\n",
      "Epoch 1826/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6938 - val_loss: 27.9617\n",
      "Epoch 1827/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5527 - val_loss: 28.8150\n",
      "Epoch 1828/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6952 - val_loss: 27.9125\n",
      "Epoch 1829/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8647 - val_loss: 28.2277\n",
      "Epoch 1830/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7056 - val_loss: 28.3476\n",
      "Epoch 1831/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.6537 - val_loss: 28.0866\n",
      "Epoch 1832/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6209 - val_loss: 27.9546\n",
      "Epoch 1833/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5933 - val_loss: 28.0192\n",
      "Epoch 1834/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 24.6915Restoring model weights from the end of the best epoch.\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6915 - val_loss: 28.3651\n",
      "Epoch 01834: early stopping\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train VAE model with callbacks \"\"\"\n",
    "history = vae.fit(trainX, trainX, \n",
    "                  batch_size = batch_size, \n",
    "                  epochs = epochs, \n",
    "                  callbacks=[initial_checkpoint, checkpoint, EarlyStoppingAtMinLoss()],\n",
    "                  #callbacks=[initial_checkpoint, checkpoint],\n",
    "                  shuffle=True,\n",
    "                  validation_data=(valX, valX)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 7.0\n"
     ]
    }
   ],
   "source": [
    "# In this GPU implementation, it shows the number of batches/iterations for one epoch (and not the training samples), which is:\n",
    "print (\"Step size:\", np.ceil(trainX.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save the entire model \"\"\"\n",
    "# Save notes about hyperparameters used:\n",
    "epochs = len(history.history['loss'])\n",
    "with open(f'{SAVE_FOLDER}/notes.txt', 'w') as f:\n",
    "    f.write(f'Epochs: {epochs} \\n')\n",
    "    f.write(f'Batch size: {batch_size} \\n')\n",
    "    f.write(f'Latent dimension: {latent_dim} \\n')\n",
    "    f.write(f'Filter sizes: {filters} \\n')\n",
    "    f.write(f'img_width: {img_width} \\n')\n",
    "    f.write(f'img_height: {img_height} \\n')\n",
    "    f.write(f'num_channels: {num_channels}')\n",
    "    f.close()\n",
    "\n",
    "# Save weights for encoder model:\n",
    "encoder.save_weights(f'{SAVE_FOLDER}/ENCODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for decoder model:\n",
    "decoder.save_weights(f'{SAVE_FOLDER}/DECODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for total VAE:\n",
    "vae.save_weights(f'{SAVE_FOLDER}/VAE_WEIGHTS.h5')\n",
    "\n",
    "# Save the history at each epochs (cost of train and validation sets):\n",
    "np.save(f'{SAVE_FOLDER}/HISTORY.npy', history.history)\n",
    "\n",
    "# Save exact training, validation and testing data set (as numpy arrays):\n",
    "np.save(f'{SAVE_FOLDER}/trainX.npy', trainX)\n",
    "np.save(f'{SAVE_FOLDER}/valX.npy', valX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllX.npy', testAllX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllY.npy', testAllY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate training process\n",
    "\n",
    "Now that the training process is complete, let’s evaluate the average loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(13, 6))\n",
    "\n",
    "# summarize history for loss:\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='blue', alpha=0.4, label='validation')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Cost', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.title('Average Loss During Training', fontsize=18)\n",
    "#plt.xticks(range(0,epochs))\n",
    "plt.legend(loc='upper right', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
