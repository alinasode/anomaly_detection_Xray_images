{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import axis, colorbar, imshow, show, figure, subplot\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=18)\n",
    "mpl.rc('xtick', labelsize=16)\n",
    "mpl.rc('ytick', labelsize=16)\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## ----- GPU ------------------------------------\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'    # use of GPU 0 (ERDA) (use before importing torch or tensorflow/keeas)\n",
    "## ----------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization, ReLU, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.activations import relu\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K                         #contains calls for tensor manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL_TRAIN_AUG:       /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/NormalTrainAugmentations\n",
      "NORMAL_VALIDATION_AUG:  /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/NormalValidationAugmentations\n",
      "NORMAL_TEST_AUG:        /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/NormalTestAugmentations\n",
      "ANOMALY1_AUG:           /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/Anomaly1Augmentations\n",
      "ANOMALY2_AUG:           /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/Anomaly2Augmentations\n"
     ]
    }
   ],
   "source": [
    "from utils_vae_potato_paths import *\n",
    "\n",
    "# Get work directions for augmentated data set:\n",
    "print (\"NORMAL_TRAIN_AUG:      \", NORMAL_TRAIN_AUG)\n",
    "print (\"NORMAL_VALIDATION_AUG: \", NORMAL_VAL_AUG)\n",
    "print (\"NORMAL_TEST_AUG:       \", NORMAL_TEST_AUG)\n",
    "print (\"ANOMALY1_AUG:          \", ANOMALY1_AUG)\n",
    "print (\"ANOMALY2_AUG:          \", ANOMALY2_AUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which Folder The Models Are Saved In: saved_models/latent8\n"
     ]
    }
   ],
   "source": [
    "# What latent dimension and filter size are we using?:\n",
    "latent_dim = 8\n",
    "filters    = 32\n",
    "\n",
    "# Get work direction for saving files:\n",
    "SAVE_FOLDER = f'saved_models/latent{latent_dim}'\n",
    "print (\"Which Folder The Models Are Saved In:\", SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] deleting existing files in folder...\n",
      "         done in 0.005 minutes\n"
     ]
    }
   ],
   "source": [
    "# Delete all existing files in folder where models are saved:\n",
    "print(\"[INFO] deleting existing files in folder...\")\n",
    "t0 = time()\n",
    "\n",
    "files = glob.glob(f'{SAVE_FOLDER}/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "print (\"         done in %0.3f minutes\" % ((time() - t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_VAE_AE import get_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Investigation of Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"normal\" training images is:     1639\n",
      "Number of \"normal\" validation images is:   80\n",
      "Number of \"normal\" test images is:         320\n",
      "Total number of \"anomaly\" images is:       680\n"
     ]
    }
   ],
   "source": [
    "# Glob the directories and get the lists of good and bad images\n",
    "good_train_fns = [f for f in os.listdir(NORMAL_TRAIN_AUG) if not f.startswith('.')]\n",
    "good_val_fns = [f for f in os.listdir(NORMAL_VAL_AUG) if not f.startswith('.')]\n",
    "good_test_fns = [f for f in os.listdir(NORMAL_TEST_AUG) if not f.startswith('.')]\n",
    "bad1_fns = [f for f in os.listdir(ANOMALY1_AUG) if not f.startswith('.')]\n",
    "bad2_fns = [f for f in os.listdir(ANOMALY2_AUG) if not f.startswith('.')]\n",
    "\n",
    "print('Number of \"normal\" training images is:     {}'.format(len(good_train_fns)))\n",
    "print('Number of \"normal\" validation images is:   {}'.format(len(good_val_fns)))\n",
    "print('Number of \"normal\" test images is:         {}'.format(len(good_test_fns)))\n",
    "print('Total number of \"anomaly\" images is:       {}'.format(len(bad1_fns) + len(bad2_fns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Procentage of normal samples (ground truth):   74.99 %\n",
      "> Procentage of anomaly samples (ground truth):  25.01 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of bad images vs good images:\n",
    "normal_fns = len(good_train_fns) + len(good_val_fns) + len(good_test_fns)\n",
    "anomaly_fns = len(bad1_fns + bad2_fns)\n",
    "print(f\"> Procentage of normal samples (ground truth):   {(normal_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")\n",
    "print(f\"> Procentage of anomaly samples (ground truth):  {(anomaly_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Normal Data\n",
    "\n",
    "Load normal samples in pre-splitted data sets: train, valdiation and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal training images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal training images...\")\n",
    "trainX = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TRAIN_AUG}/*.png\")]  # read as grayscale\n",
    "trainX = np.array(trainX)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "trainY = get_labels(trainX, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal validation images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal validation images...\")\n",
    "x_normal_val = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_VAL_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_val = np.array(x_normal_val)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_val = get_labels(x_normal_val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal testing images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal testing images...\")\n",
    "x_normal_test = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TEST_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_test = np.array(x_normal_test)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_test = get_labels(x_normal_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Class 1 Data (i.e. 'metal' sampels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly class 1 ('metal') images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly class 1 ('metal') images...\")\n",
    "x_metal = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY1_AUG}/*.png\")]  # read as grayscale\n",
    "x_metal = np.array(x_metal)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_metal = get_labels(x_metal, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Class 2 Data (i.e. 'hollow' sampels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly class 2 ('hollow') images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly class 2 ('hollow') images...\")\n",
    "x_hollow = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY2_AUG}/*.png\")]  # read as grayscale\n",
    "x_hollow = np.array(x_hollow)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_hollow = get_labels(x_hollow, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine normal samples and anomaly samples and split them into training, validation and test sets\n",
    "\n",
    "\n",
    "`label 0` == Normal Samples (=> Perfect' Samples)\n",
    "\n",
    "`label 1` == Anomaly 1 Samples (=> 'Metal' Samples)\n",
    "\n",
    "`label 2` == Anomaly 2 Samples (=> 'Hollow' Samples)\n",
    "\n",
    "Train and Validation sets only consists of `Normal Data`, aka. `label 0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testvalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testvalY = get_labels(testvalX, 0)\n",
    "\n",
    "# randomly split in order to make new validation set:\n",
    "NoUsageX, valX, NoUsageY, valY = train_test_split(testvalX, testvalY, test_size=0.25, random_state=4345672)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Testing set \"\"\"\n",
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testNormalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testNormalY = get_labels(testNormalX, 0)\n",
    "\n",
    "# Anomaly Class 1 (i.e. 'metal' samples):\n",
    "testAnomaly1X, testAnomaly1Y = x_metal, y_metal\n",
    "\n",
    "# Anomaly Class 2 (i.e. 'hollow' samples):\n",
    "testAnomaly2X, testAnomaly2Y = x_hollow, y_hollow\n",
    "\n",
    "# Combine all testing data in one array (the test set is NOT used during any part but inference):\n",
    "testAllX = np.vstack((testNormalX, testAnomaly1X, testAnomaly2X))\n",
    "testAllY = np.hstack((testNormalY, testAnomaly1Y, testAnomaly2Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Dimensions and Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      " > images: (1639, 128, 128)\n",
      " > labels: (1639,)\n",
      "\n",
      "Validation set:\n",
      " > images: (100, 128, 128)\n",
      " > labels: (100,)\n",
      "\n",
      "Test set:\n",
      " > images: (1080, 128, 128)\n",
      " > labels: (1080,)\n",
      "\t'Normal' test set:\n",
      "\t  > images: (400, 128, 128)\n",
      "\t  > labels: (400,)\n",
      "\t'Anomaly 1' test set:\n",
      "\t  > images: (392, 128, 128)\n",
      "\t  > labels: (392,)\n",
      "\t'Anomaly 2' test set:\n",
      "\t  > images: (288, 128, 128)\n",
      "\t  > labels: (288,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "print(\" > images:\", trainX.shape)\n",
    "print(\" > labels:\", trainY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Validation set:\")\n",
    "print(\" > images:\", valX.shape)\n",
    "print(\" > labels:\", valY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Test set:\")\n",
    "print(\" > images:\", testAllX.shape)\n",
    "print(\" > labels:\", testAllY.shape)\n",
    "print(\"\\t'Normal' test set:\")\n",
    "print(\"\\t  > images:\", testNormalX.shape)\n",
    "print(\"\\t  > labels:\", testNormalY.shape)\n",
    "print(\"\\t'Anomaly 1' test set:\")\n",
    "print(\"\\t  > images:\", testAnomaly1X.shape)\n",
    "print(\"\\t  > labels:\", testAnomaly1Y.shape)\n",
    "print(\"\\t'Anomaly 2' test set:\")\n",
    "print(\"\\t  > images:\", testAnomaly2X.shape)\n",
    "print(\"\\t  > labels:\", testAnomaly2Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 76.62 %\n",
      "Procentage of validation samples: 4.68 %\n",
      "Procentage of (only normal) testing samples: 18.70 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets (only normal data):\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (only normal) testing samples: {(len(testNormalX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 58.14 %\n",
      "Procentage of validation samples: 3.55 %\n",
      "Procentage of (total) testing samples: 38.31 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets:\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (total) testing samples: {(len(testAllX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for (un)balanced data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['0: normal', '1: metal', '2: hollow']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,4))\n",
    "ax[0].hist(trainY, align=\"left\"); ax[0].set_title('train', fontsize=18); ax[0].set_xticks([0,1,2]); ax[0].set_xticklabels(labels); ax[0].tick_params(axis='both', which='major', labelsize=16); ax[0].set_xlim(-0.5, 2.5);\n",
    "ax[1].hist(valY, align=\"left\"); ax[1].set_title('validation', fontsize=18); ax[1].set_xticks([0,1,2]); ax[1].set_xticklabels(labels); ax[1].tick_params(axis='both', which='major', labelsize=16); ax[1].set_xlim(-0.5, 2.5);\n",
    "ax[2].hist(testAllY, align=\"left\"); ax[2].set_title('test', fontsize=18); ax[2].set_xticks([0,1,2]); ax[2].set_xticklabels(labels); ax[2].tick_params(axis='both', which='major', labelsize=16); ax[2].set_xlim(-0.5, 2.5);\n",
    "\n",
    "ax[0].set_ylabel(\"Number of images\", fontsize=18)\n",
    "ax[1].set_xlabel(\"Class Labels\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of normal test samples: 37.04 %\n",
      "Procentage of total anomaly test samples: 62.96 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of normal test images vs total anomaly test images:\n",
    "print(f\"Procentage of normal test samples: {(len(testNormalX) / (len(testNormalX) + (len(testAnomaly1X) + len(testAnomaly2X)))) * 100:.2f} %\")\n",
    "print(f\"Procentage of total anomaly test samples: {((len(testAnomaly1X) + len(testAnomaly2X)) / (len(testNormalX) + (len(testAnomaly1X) + len(testAnomaly2X)))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Only normalization has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "img_width, img_height = trainX.shape[1], trainX.shape[2]      # input image dimensions\n",
    "num_channels = 1                                              # gray-channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Neural Networks learns faster with normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to be in  the [0, 1] range:\n",
    "trainX = trainX.astype('float32') / 255.\n",
    "valX = valX.astype('float32') / 255.\n",
    "testAllX = testAllX.astype('float32') / 255.\n",
    "\n",
    "# reshape data to keras inputs --> (n_samples, img_rows, img_cols, n_channels):\n",
    "trainX = trainX.reshape(trainX.shape[0], img_height, img_width, num_channels)\n",
    "valX = valX.reshape(valX.shape[0], img_height, img_width, num_channels)\n",
    "testAllX = testAllX.reshape(testAllX.shape[0], img_height, img_width, num_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import architecture of VAE model and its hyperparameters:\n",
    "from J8_VAE_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Encoder Part*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 32)   320         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 64, 64, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 32)   9248        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 32, 32, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 32)   9248        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 16, 16, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 8, 32)     9248        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 8, 8, 32)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 4, 4, 32)     9248        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 4, 4, 32)     0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "latent (Dense)                  (None, 200)          102600      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 200)          0           latent[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 8)            1608        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_log_mean (Dense)              (None, 8)            1608        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 8)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_mean[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 143,128\n",
      "Trainable params: 143,128\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Decoder Part*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 8)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               4608      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 128, 128, 1)       289       \n",
      "=================================================================\n",
      "Total params: 51,137\n",
      "Trainable params: 51,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Total VAE Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         [(None, 8), (None, 8), (N 143128    \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 128, 128, 1)       51137     \n",
      "=================================================================\n",
      "Total params: 194,265\n",
      "Trainable params: 194,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "batch_size  = 256\n",
    "epochs      = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at 1'st epoch (inital model)\n",
    "\n",
    "https://stackoverflow.com/questions/54323960/save-keras-model-at-specific-epochs\n",
    "\"\"\"\n",
    "\n",
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 1:\n",
    "            self.model.save_weights(f\"{SAVE_FOLDER}/cp-0001.h5\")\n",
    "            \n",
    "# create and use callback:\n",
    "initial_checkpoint = CustomSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at every k'te epochs\n",
    "\"\"\"\n",
    "# Include the epoch in the file name (uses `str.format`):\n",
    "checkpoint_path = \"saved_models/latent8/cp-{epoch:04d}.h5\"\n",
    "\n",
    "# Create a callback that saves the model's weights every k'te epochs:\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path,\n",
    "                             monitor='loss',           # name of the metrics to monitor\n",
    "                             verbose=1, \n",
    "                             #save_best_only=False,     # if False, the best model will not be overridden.\n",
    "                             save_weights_only=True,   # if True, only the weights of the models will be saved.\n",
    "                                                        # if False, the whole models will be saved.\n",
    "                             #mode='auto', \n",
    "                             period=200                  # save after every k'te epochs\n",
    "                            )\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "vae.save_weights(checkpoint_path.format(epoch=0))          # save for zero epoch (inital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: Early stopping\n",
    "https://www.tensorflow.org/guide/keras/custom_callback\n",
    "\"\"\"\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=100):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create and Compile Model\"\"\"\n",
    "# import architecture of VAE model and its hyperparameters. learning rate choosen from graph above.\n",
    "vae = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "7/7 [==============================] - 1s 191ms/step - loss: 2324.8230 - val_loss: 2263.1094\n",
      "Epoch 2/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 2110.2874 - val_loss: 1804.9307\n",
      "Epoch 3/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 1521.1267 - val_loss: 1290.2426\n",
      "Epoch 4/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 1114.4152 - val_loss: 895.3164\n",
      "Epoch 5/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 767.3215 - val_loss: 634.8087\n",
      "Epoch 6/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 547.7434 - val_loss: 465.0831\n",
      "Epoch 7/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 428.9102 - val_loss: 399.6786\n",
      "Epoch 8/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 378.8867 - val_loss: 366.4644\n",
      "Epoch 9/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 352.2352 - val_loss: 345.9949\n",
      "Epoch 10/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 335.7679 - val_loss: 324.8402\n",
      "Epoch 11/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 313.8063 - val_loss: 320.9322\n",
      "Epoch 12/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 302.9558 - val_loss: 306.8869\n",
      "Epoch 13/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 303.8377 - val_loss: 302.6179\n",
      "Epoch 14/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 294.8625 - val_loss: 295.7178\n",
      "Epoch 15/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 287.0799 - val_loss: 290.0166\n",
      "Epoch 16/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 282.7829 - val_loss: 283.4182\n",
      "Epoch 17/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 277.8595 - val_loss: 281.0800\n",
      "Epoch 18/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 272.8926 - val_loss: 274.0068\n",
      "Epoch 19/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 269.7873 - val_loss: 270.4748\n",
      "Epoch 20/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 267.0945 - val_loss: 266.3923\n",
      "Epoch 21/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 260.4091 - val_loss: 262.6185\n",
      "Epoch 22/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 256.5703 - val_loss: 256.6399\n",
      "Epoch 23/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 253.5735 - val_loss: 255.5965\n",
      "Epoch 24/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 249.3480 - val_loss: 247.0310\n",
      "Epoch 25/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 244.3815 - val_loss: 241.3410\n",
      "Epoch 26/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 238.3371 - val_loss: 242.0714\n",
      "Epoch 27/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 236.8566 - val_loss: 238.1044\n",
      "Epoch 28/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 232.3482 - val_loss: 231.7868\n",
      "Epoch 29/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 228.3747 - val_loss: 228.2787\n",
      "Epoch 30/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 224.4981 - val_loss: 224.3975\n",
      "Epoch 31/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 221.4003 - val_loss: 221.9278\n",
      "Epoch 32/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 220.2913 - val_loss: 219.3940\n",
      "Epoch 33/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 217.6968 - val_loss: 216.5661\n",
      "Epoch 34/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 214.8248 - val_loss: 214.9845\n",
      "Epoch 35/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 212.4501 - val_loss: 212.4960\n",
      "Epoch 36/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 210.0659 - val_loss: 211.3407\n",
      "Epoch 37/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 208.2609 - val_loss: 210.9487\n",
      "Epoch 38/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 206.2873 - val_loss: 206.8441\n",
      "Epoch 39/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 204.1897 - val_loss: 204.3882\n",
      "Epoch 40/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 201.1706 - val_loss: 202.3467\n",
      "Epoch 41/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 199.5717 - val_loss: 199.9560\n",
      "Epoch 42/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 200.8172 - val_loss: 196.3260\n",
      "Epoch 43/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 199.1949 - val_loss: 200.7417\n",
      "Epoch 44/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 195.8538 - val_loss: 193.3278\n",
      "Epoch 45/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 191.6644 - val_loss: 192.5379\n",
      "Epoch 46/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 189.6486 - val_loss: 189.3911\n",
      "Epoch 47/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 188.5237 - val_loss: 188.4517\n",
      "Epoch 48/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 186.2745 - val_loss: 186.8176\n",
      "Epoch 49/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 185.1077 - val_loss: 185.5527\n",
      "Epoch 50/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 182.8858 - val_loss: 183.6122\n",
      "Epoch 51/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 181.0307 - val_loss: 182.3221\n",
      "Epoch 52/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 179.6453 - val_loss: 180.6307\n",
      "Epoch 53/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 178.2526 - val_loss: 177.7130\n",
      "Epoch 54/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 176.1633 - val_loss: 175.7547\n",
      "Epoch 55/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 174.4765 - val_loss: 175.3342\n",
      "Epoch 56/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 172.3330 - val_loss: 173.0019\n",
      "Epoch 57/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 171.5918 - val_loss: 171.0199\n",
      "Epoch 58/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 169.8862 - val_loss: 170.3757\n",
      "Epoch 59/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 167.5917 - val_loss: 168.5639\n",
      "Epoch 60/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 165.8495 - val_loss: 166.4182\n",
      "Epoch 61/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 165.2108 - val_loss: 166.2211\n",
      "Epoch 62/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 164.2084 - val_loss: 164.8128\n",
      "Epoch 63/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 162.4212 - val_loss: 163.2028\n",
      "Epoch 64/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 160.5011 - val_loss: 160.5767\n",
      "Epoch 65/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 158.4751 - val_loss: 160.2674\n",
      "Epoch 66/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 158.4215 - val_loss: 158.0986\n",
      "Epoch 67/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 156.4660 - val_loss: 156.7884\n",
      "Epoch 68/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 154.9971 - val_loss: 155.5401\n",
      "Epoch 69/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 152.9401 - val_loss: 153.9870\n",
      "Epoch 70/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 152.0139 - val_loss: 151.8327\n",
      "Epoch 71/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 150.7801 - val_loss: 150.8104\n",
      "Epoch 72/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 149.2729 - val_loss: 150.4500\n",
      "Epoch 73/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 147.6902 - val_loss: 149.9349\n",
      "Epoch 74/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 147.4434 - val_loss: 146.5648\n",
      "Epoch 75/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 145.1323 - val_loss: 145.8423\n",
      "Epoch 76/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 143.7437 - val_loss: 145.1486\n",
      "Epoch 77/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 142.2273 - val_loss: 143.7608\n",
      "Epoch 78/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 140.9113 - val_loss: 142.3250\n",
      "Epoch 79/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 139.7145 - val_loss: 141.1534\n",
      "Epoch 80/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 139.5325 - val_loss: 139.1556\n",
      "Epoch 81/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 137.3796 - val_loss: 137.5795\n",
      "Epoch 82/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 135.8199 - val_loss: 136.5066\n",
      "Epoch 83/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 134.3202 - val_loss: 136.4689\n",
      "Epoch 84/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 134.1711 - val_loss: 134.4566\n",
      "Epoch 85/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 132.3079 - val_loss: 133.9033\n",
      "Epoch 86/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 130.9305 - val_loss: 133.2860\n",
      "Epoch 87/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 130.4212 - val_loss: 132.0692\n",
      "Epoch 88/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 129.4925 - val_loss: 130.9762\n",
      "Epoch 89/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 128.3467 - val_loss: 128.6394\n",
      "Epoch 90/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 126.4359 - val_loss: 128.8184\n",
      "Epoch 91/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 125.8928 - val_loss: 126.8210\n",
      "Epoch 92/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 124.3615 - val_loss: 125.5875\n",
      "Epoch 93/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 123.3971 - val_loss: 124.7761\n",
      "Epoch 94/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 121.9745 - val_loss: 123.1969\n",
      "Epoch 95/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 121.1362 - val_loss: 122.9994\n",
      "Epoch 96/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 121.0467 - val_loss: 123.2349\n",
      "Epoch 97/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 121.0268 - val_loss: 123.7117\n",
      "Epoch 98/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 121.4475 - val_loss: 122.4822\n",
      "Epoch 99/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 118.6596 - val_loss: 120.2136\n",
      "Epoch 100/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 117.1264 - val_loss: 117.4215\n",
      "Epoch 101/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 115.3726 - val_loss: 116.7634\n",
      "Epoch 102/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 114.1870 - val_loss: 116.7522\n",
      "Epoch 103/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 114.0910 - val_loss: 115.6406\n",
      "Epoch 104/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 113.0833 - val_loss: 114.3097\n",
      "Epoch 105/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 111.7202 - val_loss: 114.2874\n",
      "Epoch 106/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 111.2087 - val_loss: 112.2420\n",
      "Epoch 107/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 110.7123 - val_loss: 111.6190\n",
      "Epoch 108/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 109.3851 - val_loss: 111.2881\n",
      "Epoch 109/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 108.9871 - val_loss: 110.0081\n",
      "Epoch 110/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 107.9344 - val_loss: 109.2296\n",
      "Epoch 111/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 106.9494 - val_loss: 108.5227\n",
      "Epoch 112/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 106.0638 - val_loss: 107.6596\n",
      "Epoch 113/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 105.6368 - val_loss: 106.5829\n",
      "Epoch 114/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 104.7024 - val_loss: 106.0101\n",
      "Epoch 115/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 104.2787 - val_loss: 106.0190\n",
      "Epoch 116/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 103.3876 - val_loss: 105.4705\n",
      "Epoch 117/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 102.3048 - val_loss: 104.0873\n",
      "Epoch 118/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 101.4124 - val_loss: 102.6741\n",
      "Epoch 119/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 100.7953 - val_loss: 104.2439\n",
      "Epoch 120/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 100.4957 - val_loss: 101.7371\n",
      "Epoch 121/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 99.9204 - val_loss: 102.3012\n",
      "Epoch 122/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 98.9579 - val_loss: 100.2016\n",
      "Epoch 123/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 97.9959 - val_loss: 100.9647\n",
      "Epoch 124/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 97.5230 - val_loss: 98.2431\n",
      "Epoch 125/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 96.4411 - val_loss: 98.5571\n",
      "Epoch 126/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 96.0068 - val_loss: 97.8771\n",
      "Epoch 127/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 94.9791 - val_loss: 96.7563\n",
      "Epoch 128/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 94.2403 - val_loss: 98.1462\n",
      "Epoch 129/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 94.0745 - val_loss: 95.5949\n",
      "Epoch 130/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 93.1473 - val_loss: 94.3444\n",
      "Epoch 131/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 92.4906 - val_loss: 94.0861\n",
      "Epoch 132/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 91.7310 - val_loss: 94.0174\n",
      "Epoch 133/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 91.2459 - val_loss: 93.0715\n",
      "Epoch 134/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 90.6524 - val_loss: 92.6180\n",
      "Epoch 135/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 90.4272 - val_loss: 91.8990\n",
      "Epoch 136/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 89.9825 - val_loss: 94.7412\n",
      "Epoch 137/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 88.8703 - val_loss: 90.9802\n",
      "Epoch 138/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 87.5103 - val_loss: 90.4863\n",
      "Epoch 139/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 87.7572 - val_loss: 89.4112\n",
      "Epoch 140/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 86.9976 - val_loss: 88.4561\n",
      "Epoch 141/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 86.4023 - val_loss: 88.5969\n",
      "Epoch 142/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 85.7203 - val_loss: 88.4436\n",
      "Epoch 143/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 85.6508 - val_loss: 87.2941\n",
      "Epoch 144/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 84.8290 - val_loss: 86.3522\n",
      "Epoch 145/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 84.4057 - val_loss: 86.7972\n",
      "Epoch 146/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 84.8104 - val_loss: 86.2107\n",
      "Epoch 147/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 83.3292 - val_loss: 84.8760\n",
      "Epoch 148/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 82.4455 - val_loss: 85.5054\n",
      "Epoch 149/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 82.8868 - val_loss: 84.0284\n",
      "Epoch 150/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 82.1276 - val_loss: 83.6527\n",
      "Epoch 151/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 80.9581 - val_loss: 83.2026\n",
      "Epoch 152/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 80.7665 - val_loss: 82.9713\n",
      "Epoch 153/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 80.2712 - val_loss: 82.5185\n",
      "Epoch 154/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 79.8979 - val_loss: 81.5845\n",
      "Epoch 155/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 78.8036 - val_loss: 83.5611\n",
      "Epoch 156/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 79.8528 - val_loss: 80.3921\n",
      "Epoch 157/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 78.3792 - val_loss: 79.6013\n",
      "Epoch 158/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 77.3777 - val_loss: 79.7052\n",
      "Epoch 159/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 77.4734 - val_loss: 79.3608\n",
      "Epoch 160/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 77.4330 - val_loss: 78.9014\n",
      "Epoch 161/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 77.3815 - val_loss: 78.3899\n",
      "Epoch 162/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 76.2254 - val_loss: 79.3465\n",
      "Epoch 163/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 75.3811 - val_loss: 77.0119\n",
      "Epoch 164/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 74.9556 - val_loss: 76.9280\n",
      "Epoch 165/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 74.5084 - val_loss: 77.7876\n",
      "Epoch 166/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 74.1551 - val_loss: 75.9482\n",
      "Epoch 167/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 73.7983 - val_loss: 76.4005\n",
      "Epoch 168/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 73.2946 - val_loss: 75.8573\n",
      "Epoch 169/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 72.7038 - val_loss: 75.0361\n",
      "Epoch 170/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 72.4616 - val_loss: 74.5837\n",
      "Epoch 171/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 71.7556 - val_loss: 74.5620\n",
      "Epoch 172/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 71.4518 - val_loss: 73.8006\n",
      "Epoch 173/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 70.8992 - val_loss: 73.6658\n",
      "Epoch 174/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 71.0715 - val_loss: 72.6765\n",
      "Epoch 175/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 70.3196 - val_loss: 72.6303\n",
      "Epoch 176/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 69.8882 - val_loss: 73.7124\n",
      "Epoch 177/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 69.6226 - val_loss: 72.0445\n",
      "Epoch 178/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 69.0469 - val_loss: 71.2611\n",
      "Epoch 179/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 68.6747 - val_loss: 70.2191\n",
      "Epoch 180/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 68.3968 - val_loss: 70.2660\n",
      "Epoch 181/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 68.2901 - val_loss: 70.0218\n",
      "Epoch 182/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 67.8224 - val_loss: 69.7969\n",
      "Epoch 183/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 68.2859 - val_loss: 72.0643\n",
      "Epoch 184/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 67.8424 - val_loss: 70.1697\n",
      "Epoch 185/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 66.9472 - val_loss: 69.7915\n",
      "Epoch 186/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 66.7422 - val_loss: 70.0002\n",
      "Epoch 187/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 66.3961 - val_loss: 69.4146\n",
      "Epoch 188/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 65.6979 - val_loss: 68.0063\n",
      "Epoch 189/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 65.4617 - val_loss: 67.6043\n",
      "Epoch 190/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 64.9505 - val_loss: 67.3230\n",
      "Epoch 191/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 64.9238 - val_loss: 66.5757\n",
      "Epoch 192/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 64.5133 - val_loss: 65.9752\n",
      "Epoch 193/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 63.9363 - val_loss: 67.0671\n",
      "Epoch 194/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 63.8158 - val_loss: 67.3240\n",
      "Epoch 195/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 63.5475 - val_loss: 66.3947\n",
      "Epoch 196/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 63.3781 - val_loss: 64.9895\n",
      "Epoch 197/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.7677 - val_loss: 69.0486\n",
      "Epoch 198/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 62.1732 - val_loss: 65.4919\n",
      "Epoch 199/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.4030 - val_loss: 68.7042\n",
      "Epoch 200/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 62.0089\n",
      "Epoch 00200: saving model to saved_models/latent8/cp-0200.h5\n",
      "7/7 [==============================] - 1s 153ms/step - loss: 62.0089 - val_loss: 64.3216\n",
      "Epoch 201/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 61.9632 - val_loss: 65.3832\n",
      "Epoch 202/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.3923 - val_loss: 65.3894\n",
      "Epoch 203/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.0447 - val_loss: 65.4020\n",
      "Epoch 204/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 61.9714 - val_loss: 62.4500\n",
      "Epoch 205/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 61.0043 - val_loss: 63.9702\n",
      "Epoch 206/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 60.3644 - val_loss: 62.1375\n",
      "Epoch 207/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 59.8678 - val_loss: 61.8733\n",
      "Epoch 208/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 59.5676 - val_loss: 61.6737\n",
      "Epoch 209/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 59.4518 - val_loss: 62.3511\n",
      "Epoch 210/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 59.3567 - val_loss: 61.9740\n",
      "Epoch 211/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 58.7166 - val_loss: 64.1977\n",
      "Epoch 212/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 58.6001 - val_loss: 60.6215\n",
      "Epoch 213/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 58.4387 - val_loss: 60.6244\n",
      "Epoch 214/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 58.2307 - val_loss: 60.5183\n",
      "Epoch 215/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 58.2288 - val_loss: 59.5634\n",
      "Epoch 216/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 57.5473 - val_loss: 59.7897\n",
      "Epoch 217/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 57.1994 - val_loss: 59.0751\n",
      "Epoch 218/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 56.4128 - val_loss: 59.3115\n",
      "Epoch 219/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 56.3777 - val_loss: 58.4517\n",
      "Epoch 220/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 56.2087 - val_loss: 62.7824\n",
      "Epoch 221/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 56.2512 - val_loss: 58.2858\n",
      "Epoch 222/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 56.2307 - val_loss: 59.8582\n",
      "Epoch 223/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 55.4914 - val_loss: 59.0012\n",
      "Epoch 224/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 55.7145 - val_loss: 59.2310\n",
      "Epoch 225/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 55.2070 - val_loss: 57.8394\n",
      "Epoch 226/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 55.1057 - val_loss: 57.9770\n",
      "Epoch 227/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.7640 - val_loss: 56.9261\n",
      "Epoch 228/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.5116 - val_loss: 58.7864\n",
      "Epoch 229/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.4063 - val_loss: 56.8996\n",
      "Epoch 230/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 54.0946 - val_loss: 57.2654\n",
      "Epoch 231/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.5664 - val_loss: 57.6758\n",
      "Epoch 232/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.6792 - val_loss: 56.0546\n",
      "Epoch 233/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.4236 - val_loss: 56.2871\n",
      "Epoch 234/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 53.2258 - val_loss: 56.9225\n",
      "Epoch 235/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.0923 - val_loss: 57.8840\n",
      "Epoch 236/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.2049 - val_loss: 55.8594\n",
      "Epoch 237/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.7731 - val_loss: 54.7705\n",
      "Epoch 238/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.4724 - val_loss: 55.0818\n",
      "Epoch 239/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 51.9929 - val_loss: 54.8856\n",
      "Epoch 240/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 51.8769 - val_loss: 54.4087\n",
      "Epoch 241/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 51.6390 - val_loss: 54.6071\n",
      "Epoch 242/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 51.9826 - val_loss: 54.4000\n",
      "Epoch 243/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 51.8797 - val_loss: 56.0196\n",
      "Epoch 244/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 51.2861 - val_loss: 54.5028\n",
      "Epoch 245/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 51.0665 - val_loss: 55.4588\n",
      "Epoch 246/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.8081 - val_loss: 54.7593\n",
      "Epoch 247/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.5604 - val_loss: 53.0364\n",
      "Epoch 248/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 50.4268 - val_loss: 54.1780\n",
      "Epoch 249/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 50.4091 - val_loss: 54.3174\n",
      "Epoch 250/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.1443 - val_loss: 53.8359\n",
      "Epoch 251/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 50.1064 - val_loss: 54.0811\n",
      "Epoch 252/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 49.5926 - val_loss: 52.6686\n",
      "Epoch 253/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 49.5050 - val_loss: 55.2541\n",
      "Epoch 254/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.6414 - val_loss: 51.4862\n",
      "Epoch 255/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 49.3190 - val_loss: 55.6843\n",
      "Epoch 256/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 48.9349 - val_loss: 53.2174\n",
      "Epoch 257/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.2969 - val_loss: 52.6928\n",
      "Epoch 258/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.8457 - val_loss: 53.6081\n",
      "Epoch 259/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.8227 - val_loss: 52.9424\n",
      "Epoch 260/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.3007 - val_loss: 52.5530\n",
      "Epoch 261/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 48.1953 - val_loss: 51.5371\n",
      "Epoch 262/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 47.9948 - val_loss: 50.8309\n",
      "Epoch 263/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.9428 - val_loss: 51.4221\n",
      "Epoch 264/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.0723 - val_loss: 52.0665\n",
      "Epoch 265/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.6384 - val_loss: 50.5025\n",
      "Epoch 266/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.1932 - val_loss: 50.7129\n",
      "Epoch 267/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 47.0290 - val_loss: 52.5027\n",
      "Epoch 268/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.1754 - val_loss: 50.3406\n",
      "Epoch 269/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.7821 - val_loss: 50.2787\n",
      "Epoch 270/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.0735 - val_loss: 49.8119\n",
      "Epoch 271/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.7200 - val_loss: 49.5261\n",
      "Epoch 272/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.4333 - val_loss: 49.4054\n",
      "Epoch 273/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.4627 - val_loss: 49.9859\n",
      "Epoch 274/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.5666 - val_loss: 49.6539\n",
      "Epoch 275/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.4586 - val_loss: 48.8523\n",
      "Epoch 276/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.0903 - val_loss: 48.6567\n",
      "Epoch 277/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.5686 - val_loss: 49.1806\n",
      "Epoch 278/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.5521 - val_loss: 48.5319\n",
      "Epoch 279/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.4455 - val_loss: 48.4707\n",
      "Epoch 280/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.1182 - val_loss: 48.7872\n",
      "Epoch 281/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.8977 - val_loss: 48.0486\n",
      "Epoch 282/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.9236 - val_loss: 48.8812\n",
      "Epoch 283/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.7198 - val_loss: 49.5148\n",
      "Epoch 284/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7567 - val_loss: 48.0804\n",
      "Epoch 285/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7273 - val_loss: 47.7472\n",
      "Epoch 286/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.6513 - val_loss: 49.7094\n",
      "Epoch 287/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.3464 - val_loss: 48.2392\n",
      "Epoch 288/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.3107 - val_loss: 46.8562\n",
      "Epoch 289/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.1668 - val_loss: 46.5187\n",
      "Epoch 290/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.0197 - val_loss: 48.2632\n",
      "Epoch 291/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.0852 - val_loss: 47.5446\n",
      "Epoch 292/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.9051 - val_loss: 47.8508\n",
      "Epoch 293/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.8564 - val_loss: 46.7691\n",
      "Epoch 294/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.4579 - val_loss: 47.3367\n",
      "Epoch 295/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.2971 - val_loss: 46.2662\n",
      "Epoch 296/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.2400 - val_loss: 46.7102\n",
      "Epoch 297/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.9991 - val_loss: 46.6604\n",
      "Epoch 298/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.1443 - val_loss: 45.2528\n",
      "Epoch 299/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.4450 - val_loss: 45.6335\n",
      "Epoch 300/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.7746 - val_loss: 47.4302\n",
      "Epoch 301/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.4323 - val_loss: 46.4771\n",
      "Epoch 302/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.5003 - val_loss: 46.3779\n",
      "Epoch 303/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.5432 - val_loss: 46.7510\n",
      "Epoch 304/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.1759 - val_loss: 45.2239\n",
      "Epoch 305/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.2946 - val_loss: 46.1183\n",
      "Epoch 306/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.3049 - val_loss: 46.4387\n",
      "Epoch 307/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 41.8142 - val_loss: 44.9030\n",
      "Epoch 308/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.9039 - val_loss: 46.5535\n",
      "Epoch 309/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.8481 - val_loss: 44.3769\n",
      "Epoch 310/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.1424 - val_loss: 44.2462\n",
      "Epoch 311/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.3893 - val_loss: 46.0623\n",
      "Epoch 312/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.4938 - val_loss: 45.1809\n",
      "Epoch 313/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.4035 - val_loss: 45.7132\n",
      "Epoch 314/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.9126 - val_loss: 44.7616\n",
      "Epoch 315/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.0057 - val_loss: 44.4943\n",
      "Epoch 316/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.5585 - val_loss: 44.8959\n",
      "Epoch 317/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.8773 - val_loss: 45.4276\n",
      "Epoch 318/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.8146 - val_loss: 45.4517\n",
      "Epoch 319/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.5246 - val_loss: 44.7985\n",
      "Epoch 320/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.6900 - val_loss: 43.5717\n",
      "Epoch 321/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.3783 - val_loss: 43.4541\n",
      "Epoch 322/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.9971 - val_loss: 44.7171\n",
      "Epoch 323/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.2087 - val_loss: 44.0502\n",
      "Epoch 324/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.5341 - val_loss: 45.0155\n",
      "Epoch 325/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.5625 - val_loss: 46.0539\n",
      "Epoch 326/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.9788 - val_loss: 44.2298\n",
      "Epoch 327/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.6593 - val_loss: 42.8856\n",
      "Epoch 328/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.9366 - val_loss: 42.5172\n",
      "Epoch 329/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.5730 - val_loss: 43.2659\n",
      "Epoch 330/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.6761 - val_loss: 41.7870\n",
      "Epoch 331/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.2710 - val_loss: 41.8482\n",
      "Epoch 332/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.9408 - val_loss: 43.0662\n",
      "Epoch 333/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.9575 - val_loss: 42.6858\n",
      "Epoch 334/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.2794 - val_loss: 43.2455\n",
      "Epoch 335/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.2442 - val_loss: 42.8234\n",
      "Epoch 336/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.6953 - val_loss: 44.5935\n",
      "Epoch 337/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.0002 - val_loss: 41.5524\n",
      "Epoch 338/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.6042 - val_loss: 43.1558\n",
      "Epoch 339/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.1657 - val_loss: 42.1913\n",
      "Epoch 340/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.8878 - val_loss: 41.8490\n",
      "Epoch 341/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.6618 - val_loss: 42.0377\n",
      "Epoch 342/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.5173 - val_loss: 41.4357\n",
      "Epoch 343/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.7350 - val_loss: 40.9415\n",
      "Epoch 344/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.6647 - val_loss: 41.1137\n",
      "Epoch 345/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.9252 - val_loss: 41.8153\n",
      "Epoch 346/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.7200 - val_loss: 40.5956\n",
      "Epoch 347/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.2684 - val_loss: 41.9483\n",
      "Epoch 348/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 38.1431 - val_loss: 41.7495\n",
      "Epoch 349/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.0581 - val_loss: 41.7352\n",
      "Epoch 350/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.8048 - val_loss: 41.2514\n",
      "Epoch 351/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.2927 - val_loss: 41.7099\n",
      "Epoch 352/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.7591 - val_loss: 42.7914\n",
      "Epoch 353/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.6568 - val_loss: 41.3346\n",
      "Epoch 354/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.5250 - val_loss: 41.2919\n",
      "Epoch 355/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.8688 - val_loss: 41.1457\n",
      "Epoch 356/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.5477 - val_loss: 39.7457\n",
      "Epoch 357/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.5381 - val_loss: 40.5410\n",
      "Epoch 358/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.0220 - val_loss: 40.4285\n",
      "Epoch 359/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.2614 - val_loss: 39.9479\n",
      "Epoch 360/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.4079 - val_loss: 40.9148\n",
      "Epoch 361/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.2506 - val_loss: 40.5414\n",
      "Epoch 362/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.3121 - val_loss: 41.0912\n",
      "Epoch 363/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.5347 - val_loss: 40.5793\n",
      "Epoch 364/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.5225 - val_loss: 39.6757\n",
      "Epoch 365/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.9738 - val_loss: 40.1030\n",
      "Epoch 366/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.7701 - val_loss: 39.7899\n",
      "Epoch 367/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.8948 - val_loss: 39.9154\n",
      "Epoch 368/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.5587 - val_loss: 39.6946\n",
      "Epoch 369/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.5575 - val_loss: 39.5922\n",
      "Epoch 370/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.5711 - val_loss: 39.5350\n",
      "Epoch 371/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.7806 - val_loss: 39.3185\n",
      "Epoch 372/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 36.2479 - val_loss: 41.1435\n",
      "Epoch 373/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.7064 - val_loss: 41.0524\n",
      "Epoch 374/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.0212 - val_loss: 40.0749\n",
      "Epoch 375/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.2377 - val_loss: 40.8663\n",
      "Epoch 376/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.3495 - val_loss: 38.7661\n",
      "Epoch 377/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.2552 - val_loss: 39.2336\n",
      "Epoch 378/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.7725 - val_loss: 39.5912\n",
      "Epoch 379/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.6974 - val_loss: 39.9228\n",
      "Epoch 380/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.4118 - val_loss: 38.6877\n",
      "Epoch 381/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.4765 - val_loss: 41.2658\n",
      "Epoch 382/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.1333 - val_loss: 38.5080\n",
      "Epoch 383/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.0322 - val_loss: 39.0603\n",
      "Epoch 384/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.0061 - val_loss: 39.1485\n",
      "Epoch 385/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.7574 - val_loss: 39.3940\n",
      "Epoch 386/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.9376 - val_loss: 39.2383\n",
      "Epoch 387/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.7064 - val_loss: 38.1539\n",
      "Epoch 388/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.6201 - val_loss: 38.8984\n",
      "Epoch 389/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.6624 - val_loss: 38.9075\n",
      "Epoch 390/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.2554 - val_loss: 38.4303\n",
      "Epoch 391/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.7172 - val_loss: 41.0519\n",
      "Epoch 392/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.8101 - val_loss: 38.1959\n",
      "Epoch 393/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.9835 - val_loss: 37.5011\n",
      "Epoch 394/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.8397 - val_loss: 37.7891\n",
      "Epoch 395/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.0177 - val_loss: 37.9700\n",
      "Epoch 396/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.1696 - val_loss: 37.7660\n",
      "Epoch 397/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.9721 - val_loss: 38.6026\n",
      "Epoch 398/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 34.6736 - val_loss: 39.0212\n",
      "Epoch 399/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.8914 - val_loss: 37.8253\n",
      "Epoch 400/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 34.7705\n",
      "Epoch 00400: saving model to saved_models/latent8/cp-0400.h5\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 34.7705 - val_loss: 37.9825\n",
      "Epoch 401/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.8659 - val_loss: 37.6104\n",
      "Epoch 402/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.7311 - val_loss: 37.6293\n",
      "Epoch 403/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.3449 - val_loss: 37.6065\n",
      "Epoch 404/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.9623 - val_loss: 39.3655\n",
      "Epoch 405/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 34.6712 - val_loss: 36.9738\n",
      "Epoch 406/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.7175 - val_loss: 37.0915\n",
      "Epoch 407/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.2599 - val_loss: 37.8851\n",
      "Epoch 408/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.6691 - val_loss: 37.3942\n",
      "Epoch 409/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.6733 - val_loss: 37.9050\n",
      "Epoch 410/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.5577 - val_loss: 39.3057\n",
      "Epoch 411/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.3787 - val_loss: 38.4655\n",
      "Epoch 412/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.5754 - val_loss: 38.1235\n",
      "Epoch 413/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.9764 - val_loss: 37.1811\n",
      "Epoch 414/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.2027 - val_loss: 37.1457\n",
      "Epoch 415/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.9787 - val_loss: 38.1889\n",
      "Epoch 416/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.9826 - val_loss: 37.6133\n",
      "Epoch 417/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.5988 - val_loss: 36.9644\n",
      "Epoch 418/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.5889 - val_loss: 37.2602\n",
      "Epoch 419/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.9441 - val_loss: 37.0117\n",
      "Epoch 420/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.7498 - val_loss: 37.1244\n",
      "Epoch 421/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.5182 - val_loss: 36.4835\n",
      "Epoch 422/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.5203 - val_loss: 37.6115\n",
      "Epoch 423/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.4031 - val_loss: 37.7497\n",
      "Epoch 424/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.6350 - val_loss: 35.9498\n",
      "Epoch 425/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 33.2817 - val_loss: 36.1928\n",
      "Epoch 426/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.4066 - val_loss: 36.5051\n",
      "Epoch 427/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.3506 - val_loss: 36.7428\n",
      "Epoch 428/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.4088 - val_loss: 37.0647\n",
      "Epoch 429/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.2464 - val_loss: 37.4165\n",
      "Epoch 430/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.1458 - val_loss: 36.8959\n",
      "Epoch 431/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.4435 - val_loss: 36.8682\n",
      "Epoch 432/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.3497 - val_loss: 36.7565\n",
      "Epoch 433/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.0954 - val_loss: 36.4318\n",
      "Epoch 434/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.1267 - val_loss: 36.5630\n",
      "Epoch 435/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.9142 - val_loss: 37.1146\n",
      "Epoch 436/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.0336 - val_loss: 37.7265\n",
      "Epoch 437/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.1261 - val_loss: 36.6167\n",
      "Epoch 438/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.2687 - val_loss: 35.6461\n",
      "Epoch 439/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.3245 - val_loss: 36.3735\n",
      "Epoch 440/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.0542 - val_loss: 37.1957\n",
      "Epoch 441/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.3755 - val_loss: 36.5039\n",
      "Epoch 442/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.1327 - val_loss: 36.2611\n",
      "Epoch 443/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.4716 - val_loss: 36.5198\n",
      "Epoch 444/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.8142 - val_loss: 36.9690\n",
      "Epoch 445/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.0624 - val_loss: 36.7224\n",
      "Epoch 446/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.9986 - val_loss: 35.3956\n",
      "Epoch 447/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.6108 - val_loss: 35.8087\n",
      "Epoch 448/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.6296 - val_loss: 36.0459\n",
      "Epoch 449/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.9071 - val_loss: 37.1295\n",
      "Epoch 450/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.2617 - val_loss: 36.8192\n",
      "Epoch 451/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.7686 - val_loss: 35.5518\n",
      "Epoch 452/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.6511 - val_loss: 36.0030\n",
      "Epoch 453/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.4181 - val_loss: 36.3850\n",
      "Epoch 454/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 32.3750 - val_loss: 35.8996\n",
      "Epoch 455/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.3627 - val_loss: 35.9681\n",
      "Epoch 456/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.1379 - val_loss: 35.3119\n",
      "Epoch 457/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.0154 - val_loss: 34.8882\n",
      "Epoch 458/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.9407 - val_loss: 37.1637\n",
      "Epoch 459/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.0210 - val_loss: 34.9838\n",
      "Epoch 460/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.9473 - val_loss: 35.1474\n",
      "Epoch 461/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.6915 - val_loss: 35.5362\n",
      "Epoch 462/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.8898 - val_loss: 35.1571\n",
      "Epoch 463/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.0987 - val_loss: 35.2674\n",
      "Epoch 464/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.9218 - val_loss: 35.2037\n",
      "Epoch 465/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.8133 - val_loss: 36.8150\n",
      "Epoch 466/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.9624 - val_loss: 35.9491\n",
      "Epoch 467/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.8638 - val_loss: 35.3621\n",
      "Epoch 468/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.8666 - val_loss: 36.0000\n",
      "Epoch 469/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.8437 - val_loss: 34.6581\n",
      "Epoch 470/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.0225 - val_loss: 35.3874\n",
      "Epoch 471/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.6349 - val_loss: 35.2628\n",
      "Epoch 472/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.6006 - val_loss: 35.4892\n",
      "Epoch 473/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.9726 - val_loss: 35.2266\n",
      "Epoch 474/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.4510 - val_loss: 35.4046\n",
      "Epoch 475/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.5133 - val_loss: 34.5486\n",
      "Epoch 476/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4706 - val_loss: 35.2242\n",
      "Epoch 477/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.7342 - val_loss: 33.9193\n",
      "Epoch 478/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4811 - val_loss: 35.3153\n",
      "Epoch 479/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.4019 - val_loss: 35.8355\n",
      "Epoch 480/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.2351 - val_loss: 34.9418\n",
      "Epoch 481/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.2590 - val_loss: 35.1004\n",
      "Epoch 482/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.0539 - val_loss: 35.4163\n",
      "Epoch 483/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.7692 - val_loss: 35.2751\n",
      "Epoch 484/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.9600 - val_loss: 35.0338\n",
      "Epoch 485/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4084 - val_loss: 35.4475\n",
      "Epoch 486/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.2839 - val_loss: 35.2546\n",
      "Epoch 487/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.2168 - val_loss: 36.1569\n",
      "Epoch 488/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.0454 - val_loss: 34.9333\n",
      "Epoch 489/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.2200 - val_loss: 34.5289\n",
      "Epoch 490/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3880 - val_loss: 35.6275\n",
      "Epoch 491/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1419 - val_loss: 34.6204\n",
      "Epoch 492/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3428 - val_loss: 34.5690\n",
      "Epoch 493/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.0324 - val_loss: 34.7934\n",
      "Epoch 494/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0545 - val_loss: 35.4533\n",
      "Epoch 495/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.0072 - val_loss: 34.8267\n",
      "Epoch 496/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.8873 - val_loss: 35.8555\n",
      "Epoch 497/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1988 - val_loss: 35.5237\n",
      "Epoch 498/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3968 - val_loss: 35.2593\n",
      "Epoch 499/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.6776 - val_loss: 34.2758\n",
      "Epoch 500/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7876 - val_loss: 34.5936\n",
      "Epoch 501/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0012 - val_loss: 34.6827\n",
      "Epoch 502/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 31.2058 - val_loss: 34.8335\n",
      "Epoch 503/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8528 - val_loss: 34.6881\n",
      "Epoch 504/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.0064 - val_loss: 34.9869\n",
      "Epoch 505/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.3661 - val_loss: 34.7288\n",
      "Epoch 506/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3359 - val_loss: 36.5762\n",
      "Epoch 507/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1619 - val_loss: 34.6310\n",
      "Epoch 508/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.6597 - val_loss: 35.0960\n",
      "Epoch 509/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.0299 - val_loss: 35.8968\n",
      "Epoch 510/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.2775 - val_loss: 35.1054\n",
      "Epoch 511/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0337 - val_loss: 34.4529\n",
      "Epoch 512/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.9286 - val_loss: 34.3044\n",
      "Epoch 513/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.7950 - val_loss: 34.8240\n",
      "Epoch 514/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7933 - val_loss: 34.5693\n",
      "Epoch 515/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.4790 - val_loss: 34.7610\n",
      "Epoch 516/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.3752 - val_loss: 33.3926\n",
      "Epoch 517/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4762 - val_loss: 34.0446\n",
      "Epoch 518/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.0331 - val_loss: 33.7748\n",
      "Epoch 519/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.0728 - val_loss: 34.9527\n",
      "Epoch 520/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.0221 - val_loss: 33.4307\n",
      "Epoch 521/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.0592 - val_loss: 33.2567\n",
      "Epoch 522/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.1218 - val_loss: 34.2423\n",
      "Epoch 523/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.0903 - val_loss: 33.5871\n",
      "Epoch 524/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.1136 - val_loss: 33.6726\n",
      "Epoch 525/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.5031 - val_loss: 33.3006\n",
      "Epoch 526/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.5226 - val_loss: 34.6251\n",
      "Epoch 527/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4660 - val_loss: 34.1473\n",
      "Epoch 528/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.3869 - val_loss: 34.4642\n",
      "Epoch 529/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.4812 - val_loss: 34.7282\n",
      "Epoch 530/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4141 - val_loss: 34.5521\n",
      "Epoch 531/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.2638 - val_loss: 34.3772\n",
      "Epoch 532/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.2163 - val_loss: 33.3427\n",
      "Epoch 533/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.0380 - val_loss: 34.6307\n",
      "Epoch 534/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.6980 - val_loss: 36.1890\n",
      "Epoch 535/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4345 - val_loss: 37.2559\n",
      "Epoch 536/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.3743 - val_loss: 33.7064\n",
      "Epoch 537/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.6668 - val_loss: 34.7415\n",
      "Epoch 538/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.1226 - val_loss: 34.2632\n",
      "Epoch 539/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.0360 - val_loss: 33.8157\n",
      "Epoch 540/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.7996 - val_loss: 33.8839\n",
      "Epoch 541/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.7912 - val_loss: 33.4557\n",
      "Epoch 542/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.6825 - val_loss: 33.6250\n",
      "Epoch 543/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.7384 - val_loss: 33.9768\n",
      "Epoch 544/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.6492 - val_loss: 33.7503\n",
      "Epoch 545/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.4235 - val_loss: 33.8700\n",
      "Epoch 546/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.4214 - val_loss: 33.1316\n",
      "Epoch 547/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.4799 - val_loss: 34.9823\n",
      "Epoch 548/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.8364 - val_loss: 34.0105\n",
      "Epoch 549/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.9384 - val_loss: 33.8386\n",
      "Epoch 550/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.1327 - val_loss: 33.8065\n",
      "Epoch 551/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.8333 - val_loss: 33.6835\n",
      "Epoch 552/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.7623 - val_loss: 34.3068\n",
      "Epoch 553/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.9645 - val_loss: 35.5344\n",
      "Epoch 554/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.1244 - val_loss: 33.4769\n",
      "Epoch 555/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.2137 - val_loss: 34.8604\n",
      "Epoch 556/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.0072 - val_loss: 36.1608\n",
      "Epoch 557/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.5958 - val_loss: 33.4019\n",
      "Epoch 558/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.6992 - val_loss: 34.4472\n",
      "Epoch 559/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.1558 - val_loss: 33.4909\n",
      "Epoch 560/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.8565 - val_loss: 34.7927\n",
      "Epoch 561/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.4055 - val_loss: 33.8998\n",
      "Epoch 562/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.6406 - val_loss: 34.3624\n",
      "Epoch 563/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.0046 - val_loss: 33.1035\n",
      "Epoch 564/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.4709 - val_loss: 33.4865\n",
      "Epoch 565/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.4307 - val_loss: 34.0889\n",
      "Epoch 566/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.1843 - val_loss: 34.0978\n",
      "Epoch 567/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.3969 - val_loss: 32.9579\n",
      "Epoch 568/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.1514 - val_loss: 33.2984\n",
      "Epoch 569/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.5137 - val_loss: 32.7840\n",
      "Epoch 570/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.8204 - val_loss: 34.2952\n",
      "Epoch 571/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.4864 - val_loss: 34.1351\n",
      "Epoch 572/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.4655 - val_loss: 33.4639\n",
      "Epoch 573/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.1118 - val_loss: 33.3072\n",
      "Epoch 574/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.5037 - val_loss: 34.2816\n",
      "Epoch 575/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.1436 - val_loss: 33.3099\n",
      "Epoch 576/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.7265 - val_loss: 34.0539\n",
      "Epoch 577/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.5107 - val_loss: 33.6624\n",
      "Epoch 578/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.1622 - val_loss: 32.9873\n",
      "Epoch 579/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.0407 - val_loss: 32.4542\n",
      "Epoch 580/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.9716 - val_loss: 33.5809\n",
      "Epoch 581/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.7186 - val_loss: 33.1217\n",
      "Epoch 582/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.7798 - val_loss: 32.7828\n",
      "Epoch 583/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.1484 - val_loss: 33.0178\n",
      "Epoch 584/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.0930 - val_loss: 32.9858\n",
      "Epoch 585/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.9219 - val_loss: 33.1956\n",
      "Epoch 586/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.8628 - val_loss: 32.3249\n",
      "Epoch 587/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.0605 - val_loss: 33.7495\n",
      "Epoch 588/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.9167 - val_loss: 33.0042\n",
      "Epoch 589/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.1975 - val_loss: 34.1169\n",
      "Epoch 590/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.2269 - val_loss: 32.7048\n",
      "Epoch 591/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.0635 - val_loss: 33.0778\n",
      "Epoch 592/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.9692 - val_loss: 33.0653\n",
      "Epoch 593/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.8862 - val_loss: 32.5896\n",
      "Epoch 594/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.6184 - val_loss: 34.0073\n",
      "Epoch 595/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.9282 - val_loss: 33.6749\n",
      "Epoch 596/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.0639 - val_loss: 32.6971\n",
      "Epoch 597/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.2365 - val_loss: 34.7449\n",
      "Epoch 598/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.2722 - val_loss: 33.1281\n",
      "Epoch 599/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.2139 - val_loss: 33.3598\n",
      "Epoch 600/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 28.7667\n",
      "Epoch 00600: saving model to saved_models/latent8/cp-0600.h5\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 28.7667 - val_loss: 33.5916\n",
      "Epoch 601/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.5446 - val_loss: 31.6667\n",
      "Epoch 602/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.8440 - val_loss: 32.9105\n",
      "Epoch 603/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.8741 - val_loss: 33.8716\n",
      "Epoch 604/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.7677 - val_loss: 33.4750\n",
      "Epoch 605/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.8831 - val_loss: 32.2472\n",
      "Epoch 606/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.7484 - val_loss: 33.2112\n",
      "Epoch 607/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.7855 - val_loss: 34.0079\n",
      "Epoch 608/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6819 - val_loss: 32.7612\n",
      "Epoch 609/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.7977 - val_loss: 33.4067\n",
      "Epoch 610/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.7694 - val_loss: 33.2191\n",
      "Epoch 611/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.4758 - val_loss: 33.4847\n",
      "Epoch 612/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.7259 - val_loss: 34.4281\n",
      "Epoch 613/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.6599 - val_loss: 32.9683\n",
      "Epoch 614/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.8702 - val_loss: 33.1911\n",
      "Epoch 615/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6337 - val_loss: 33.3574\n",
      "Epoch 616/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.5590 - val_loss: 32.7003\n",
      "Epoch 617/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.1610 - val_loss: 32.0029\n",
      "Epoch 618/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.3951 - val_loss: 32.6925\n",
      "Epoch 619/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.3817 - val_loss: 33.4337\n",
      "Epoch 620/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.9040 - val_loss: 34.0484\n",
      "Epoch 621/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6108 - val_loss: 32.4927\n",
      "Epoch 622/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.5171 - val_loss: 33.2796\n",
      "Epoch 623/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.4011 - val_loss: 32.8020\n",
      "Epoch 624/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.4221 - val_loss: 32.1804\n",
      "Epoch 625/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.5266 - val_loss: 33.1463\n",
      "Epoch 626/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.5483 - val_loss: 32.2008\n",
      "Epoch 627/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.4458 - val_loss: 32.9846\n",
      "Epoch 628/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2768 - val_loss: 32.1054\n",
      "Epoch 629/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.3704 - val_loss: 33.2208\n",
      "Epoch 630/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.1311 - val_loss: 32.0180\n",
      "Epoch 631/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.3354 - val_loss: 33.7624\n",
      "Epoch 632/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.4483 - val_loss: 33.0349\n",
      "Epoch 633/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.3478 - val_loss: 33.4723\n",
      "Epoch 634/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2292 - val_loss: 33.2379\n",
      "Epoch 635/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.3897 - val_loss: 34.0228\n",
      "Epoch 636/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.0602 - val_loss: 33.1967\n",
      "Epoch 637/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6519 - val_loss: 33.2457\n",
      "Epoch 638/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.4108 - val_loss: 32.3093\n",
      "Epoch 639/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.3156 - val_loss: 33.1308\n",
      "Epoch 640/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.1928 - val_loss: 33.2774\n",
      "Epoch 641/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2021 - val_loss: 32.0656\n",
      "Epoch 642/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.4399 - val_loss: 32.0704\n",
      "Epoch 643/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.4687 - val_loss: 32.8344\n",
      "Epoch 644/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.0218 - val_loss: 32.6794\n",
      "Epoch 645/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.1873 - val_loss: 32.2439\n",
      "Epoch 646/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.0161 - val_loss: 32.1017\n",
      "Epoch 647/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.1703 - val_loss: 32.5841\n",
      "Epoch 648/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.1837 - val_loss: 33.1004\n",
      "Epoch 649/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.0214 - val_loss: 33.0764\n",
      "Epoch 650/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.1252 - val_loss: 32.2202\n",
      "Epoch 651/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.3071 - val_loss: 32.6923\n",
      "Epoch 652/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.4954 - val_loss: 32.9541\n",
      "Epoch 653/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.0121 - val_loss: 31.9985\n",
      "Epoch 654/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.8482 - val_loss: 32.8015\n",
      "Epoch 655/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2232 - val_loss: 33.3376\n",
      "Epoch 656/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2973 - val_loss: 33.1319\n",
      "Epoch 657/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.0885 - val_loss: 32.6778\n",
      "Epoch 658/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.1378 - val_loss: 32.3063\n",
      "Epoch 659/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.4363 - val_loss: 32.9796\n",
      "Epoch 660/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.8997 - val_loss: 32.7950\n",
      "Epoch 661/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.0981 - val_loss: 34.6172\n",
      "Epoch 662/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.9523 - val_loss: 33.4302\n",
      "Epoch 663/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.7084 - val_loss: 34.3523\n",
      "Epoch 664/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2425 - val_loss: 32.6092\n",
      "Epoch 665/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.1679 - val_loss: 33.0926\n",
      "Epoch 666/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.5260 - val_loss: 31.9312\n",
      "Epoch 667/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.0433 - val_loss: 32.2337\n",
      "Epoch 668/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8592 - val_loss: 33.4880\n",
      "Epoch 669/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.5246 - val_loss: 32.5147\n",
      "Epoch 670/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.4130 - val_loss: 32.2865\n",
      "Epoch 671/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2859 - val_loss: 33.5751\n",
      "Epoch 672/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.3843 - val_loss: 31.9619\n",
      "Epoch 673/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.8231 - val_loss: 31.7698\n",
      "Epoch 674/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.6299 - val_loss: 32.0434\n",
      "Epoch 675/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6675 - val_loss: 32.0926\n",
      "Epoch 676/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.7582 - val_loss: 32.3966\n",
      "Epoch 677/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8138 - val_loss: 31.5898\n",
      "Epoch 678/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8428 - val_loss: 31.8762\n",
      "Epoch 679/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8595 - val_loss: 33.1636\n",
      "Epoch 680/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.8229 - val_loss: 32.2755\n",
      "Epoch 681/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.4491 - val_loss: 31.9780\n",
      "Epoch 682/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.0493 - val_loss: 32.4019\n",
      "Epoch 683/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.7534 - val_loss: 32.8304\n",
      "Epoch 684/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.1821 - val_loss: 33.6132\n",
      "Epoch 685/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9895 - val_loss: 31.4554\n",
      "Epoch 686/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.7686 - val_loss: 31.8699\n",
      "Epoch 687/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.7265 - val_loss: 32.1741\n",
      "Epoch 688/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.6042 - val_loss: 32.4846\n",
      "Epoch 689/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8565 - val_loss: 33.1899\n",
      "Epoch 690/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.7557 - val_loss: 32.0737\n",
      "Epoch 691/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6122 - val_loss: 31.7070\n",
      "Epoch 692/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4539 - val_loss: 31.3231\n",
      "Epoch 693/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8791 - val_loss: 32.6944\n",
      "Epoch 694/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.7327 - val_loss: 31.6595\n",
      "Epoch 695/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8106 - val_loss: 31.6843\n",
      "Epoch 696/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6196 - val_loss: 31.4042\n",
      "Epoch 697/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6056 - val_loss: 31.5729\n",
      "Epoch 698/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8813 - val_loss: 33.2998\n",
      "Epoch 699/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6938 - val_loss: 32.7191\n",
      "Epoch 700/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6367 - val_loss: 32.7978\n",
      "Epoch 701/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5228 - val_loss: 33.6431\n",
      "Epoch 702/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9939 - val_loss: 32.7228\n",
      "Epoch 703/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.3511 - val_loss: 34.6122\n",
      "Epoch 704/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2269 - val_loss: 32.7305\n",
      "Epoch 705/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9272 - val_loss: 33.4479\n",
      "Epoch 706/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.1435 - val_loss: 32.0897\n",
      "Epoch 707/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6124 - val_loss: 32.6895\n",
      "Epoch 708/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.7538 - val_loss: 33.3622\n",
      "Epoch 709/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9049 - val_loss: 33.4112\n",
      "Epoch 710/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4311 - val_loss: 32.8444\n",
      "Epoch 711/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5076 - val_loss: 31.7372\n",
      "Epoch 712/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4968 - val_loss: 32.1127\n",
      "Epoch 713/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3371 - val_loss: 32.2019\n",
      "Epoch 714/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.3143 - val_loss: 31.9940\n",
      "Epoch 715/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4415 - val_loss: 31.7346\n",
      "Epoch 716/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3337 - val_loss: 31.9369\n",
      "Epoch 717/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4380 - val_loss: 31.9373\n",
      "Epoch 718/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3981 - val_loss: 32.5444\n",
      "Epoch 719/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5139 - val_loss: 32.8155\n",
      "Epoch 720/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.7877 - val_loss: 34.7360\n",
      "Epoch 721/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2076 - val_loss: 32.5437\n",
      "Epoch 722/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5002 - val_loss: 31.8283\n",
      "Epoch 723/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3219 - val_loss: 33.0816\n",
      "Epoch 724/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.2811 - val_loss: 34.4928\n",
      "Epoch 725/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3829 - val_loss: 32.0153\n",
      "Epoch 726/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3445 - val_loss: 32.9721\n",
      "Epoch 727/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.8294 - val_loss: 33.0722\n",
      "Epoch 728/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.1106 - val_loss: 33.5986\n",
      "Epoch 729/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9029 - val_loss: 31.2055\n",
      "Epoch 730/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3451 - val_loss: 32.0206\n",
      "Epoch 731/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.2268 - val_loss: 32.8764\n",
      "Epoch 732/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1875 - val_loss: 33.4448\n",
      "Epoch 733/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3455 - val_loss: 32.4992\n",
      "Epoch 734/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3126 - val_loss: 31.9410\n",
      "Epoch 735/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1532 - val_loss: 31.9061\n",
      "Epoch 736/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0880 - val_loss: 31.7373\n",
      "Epoch 737/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4895 - val_loss: 32.5877\n",
      "Epoch 738/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2425 - val_loss: 32.4492\n",
      "Epoch 739/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1443 - val_loss: 31.7729\n",
      "Epoch 740/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3440 - val_loss: 32.4071\n",
      "Epoch 741/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0949 - val_loss: 32.0511\n",
      "Epoch 742/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5124 - val_loss: 32.0419\n",
      "Epoch 743/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4079 - val_loss: 32.1912\n",
      "Epoch 744/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4147 - val_loss: 31.7745\n",
      "Epoch 745/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3678 - val_loss: 32.1358\n",
      "Epoch 746/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3551 - val_loss: 32.3976\n",
      "Epoch 747/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.5944 - val_loss: 33.5044\n",
      "Epoch 748/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.6514 - val_loss: 32.9562\n",
      "Epoch 749/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3563 - val_loss: 32.8835\n",
      "Epoch 750/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1765 - val_loss: 32.4463\n",
      "Epoch 751/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9344 - val_loss: 32.1773\n",
      "Epoch 752/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0686 - val_loss: 31.6255\n",
      "Epoch 753/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2951 - val_loss: 32.3669\n",
      "Epoch 754/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1658 - val_loss: 32.3901\n",
      "Epoch 755/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2396 - val_loss: 32.3764\n",
      "Epoch 756/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4639 - val_loss: 32.1277\n",
      "Epoch 757/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2650 - val_loss: 31.8336\n",
      "Epoch 758/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1543 - val_loss: 33.0076\n",
      "Epoch 759/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1244 - val_loss: 31.0770\n",
      "Epoch 760/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9893 - val_loss: 32.2967\n",
      "Epoch 761/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5454 - val_loss: 32.9087\n",
      "Epoch 762/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6906 - val_loss: 32.3234\n",
      "Epoch 763/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4923 - val_loss: 32.0815\n",
      "Epoch 764/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4989 - val_loss: 32.8886\n",
      "Epoch 765/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1489 - val_loss: 31.4544\n",
      "Epoch 766/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1048 - val_loss: 32.1284\n",
      "Epoch 767/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1075 - val_loss: 31.7868\n",
      "Epoch 768/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1845 - val_loss: 33.4567\n",
      "Epoch 769/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4654 - val_loss: 32.1982\n",
      "Epoch 770/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1622 - val_loss: 32.0263\n",
      "Epoch 771/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3273 - val_loss: 32.8609\n",
      "Epoch 772/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1655 - val_loss: 32.7338\n",
      "Epoch 773/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0196 - val_loss: 32.4459\n",
      "Epoch 774/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.2202 - val_loss: 32.8774\n",
      "Epoch 775/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0341 - val_loss: 31.7434\n",
      "Epoch 776/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0725 - val_loss: 33.0749\n",
      "Epoch 777/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1726 - val_loss: 31.3358\n",
      "Epoch 778/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9525 - val_loss: 31.7772\n",
      "Epoch 779/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0716 - val_loss: 32.3109\n",
      "Epoch 780/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1513 - val_loss: 32.4103\n",
      "Epoch 781/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0513 - val_loss: 31.5240\n",
      "Epoch 782/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3603 - val_loss: 33.4693\n",
      "Epoch 783/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4034 - val_loss: 31.6695\n",
      "Epoch 784/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5049 - val_loss: 31.9306\n",
      "Epoch 785/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2425 - val_loss: 32.1705\n",
      "Epoch 786/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1742 - val_loss: 32.5546\n",
      "Epoch 787/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.7787 - val_loss: 31.7351\n",
      "Epoch 788/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0350 - val_loss: 32.9190\n",
      "Epoch 789/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.7311 - val_loss: 32.8311\n",
      "Epoch 790/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2733 - val_loss: 34.6320\n",
      "Epoch 791/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.7051 - val_loss: 32.3692\n",
      "Epoch 792/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.2849 - val_loss: 32.5731\n",
      "Epoch 793/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0117 - val_loss: 32.4156\n",
      "Epoch 794/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9005 - val_loss: 32.5071\n",
      "Epoch 795/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9627 - val_loss: 31.7850\n",
      "Epoch 796/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7979 - val_loss: 32.2073\n",
      "Epoch 797/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7995 - val_loss: 31.7590\n",
      "Epoch 798/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8012 - val_loss: 31.7849\n",
      "Epoch 799/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7504 - val_loss: 31.4946\n",
      "Epoch 800/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 26.8754\n",
      "Epoch 00800: saving model to saved_models/latent8/cp-0800.h5\n",
      "7/7 [==============================] - 1s 151ms/step - loss: 26.8754 - val_loss: 31.1732\n",
      "Epoch 801/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7700 - val_loss: 31.4334\n",
      "Epoch 802/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8533 - val_loss: 32.4339\n",
      "Epoch 803/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5125 - val_loss: 31.4261\n",
      "Epoch 804/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7585 - val_loss: 32.4400\n",
      "Epoch 805/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7630 - val_loss: 31.1575\n",
      "Epoch 806/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8322 - val_loss: 32.3078\n",
      "Epoch 807/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1186 - val_loss: 31.9413\n",
      "Epoch 808/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7821 - val_loss: 30.7195\n",
      "Epoch 809/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6936 - val_loss: 32.1829\n",
      "Epoch 810/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0512 - val_loss: 32.3654\n",
      "Epoch 811/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3963 - val_loss: 31.4943\n",
      "Epoch 812/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3097 - val_loss: 31.5807\n",
      "Epoch 813/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8809 - val_loss: 32.3129\n",
      "Epoch 814/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8905 - val_loss: 32.1594\n",
      "Epoch 815/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7327 - val_loss: 32.1429\n",
      "Epoch 816/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7213 - val_loss: 31.8204\n",
      "Epoch 817/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6258 - val_loss: 32.2636\n",
      "Epoch 818/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6552 - val_loss: 32.7425\n",
      "Epoch 819/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7116 - val_loss: 31.3139\n",
      "Epoch 820/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9404 - val_loss: 31.2770\n",
      "Epoch 821/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0359 - val_loss: 31.3398\n",
      "Epoch 822/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0939 - val_loss: 31.2459\n",
      "Epoch 823/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7600 - val_loss: 31.6119\n",
      "Epoch 824/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7041 - val_loss: 31.8445\n",
      "Epoch 825/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8637 - val_loss: 31.4557\n",
      "Epoch 826/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6728 - val_loss: 31.4585\n",
      "Epoch 827/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7835 - val_loss: 32.0488\n",
      "Epoch 828/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7000 - val_loss: 31.0145\n",
      "Epoch 829/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4259 - val_loss: 31.9095\n",
      "Epoch 830/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6443 - val_loss: 31.6443\n",
      "Epoch 831/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6695 - val_loss: 32.6001\n",
      "Epoch 832/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0897 - val_loss: 32.2489\n",
      "Epoch 833/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9640 - val_loss: 32.5109\n",
      "Epoch 834/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7964 - val_loss: 31.3435\n",
      "Epoch 835/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8494 - val_loss: 31.8724\n",
      "Epoch 836/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7895 - val_loss: 31.2108\n",
      "Epoch 837/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6611 - val_loss: 31.4175\n",
      "Epoch 838/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5878 - val_loss: 31.9433\n",
      "Epoch 839/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4788 - val_loss: 31.8775\n",
      "Epoch 840/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5416 - val_loss: 31.6428\n",
      "Epoch 841/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7587 - val_loss: 30.7938\n",
      "Epoch 842/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5489 - val_loss: 31.1015\n",
      "Epoch 843/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7209 - val_loss: 31.9905\n",
      "Epoch 844/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5435 - val_loss: 31.1420\n",
      "Epoch 845/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7856 - val_loss: 31.0216\n",
      "Epoch 846/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8551 - val_loss: 31.4104\n",
      "Epoch 847/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4360 - val_loss: 31.4565\n",
      "Epoch 848/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4632 - val_loss: 31.7036\n",
      "Epoch 849/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8081 - val_loss: 31.4385\n",
      "Epoch 850/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5969 - val_loss: 31.0457\n",
      "Epoch 851/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8647 - val_loss: 31.7514\n",
      "Epoch 852/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4585 - val_loss: 32.6615\n",
      "Epoch 853/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7555 - val_loss: 30.7534\n",
      "Epoch 854/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4926 - val_loss: 31.4085\n",
      "Epoch 855/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4635 - val_loss: 31.2502\n",
      "Epoch 856/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6725 - val_loss: 31.7811\n",
      "Epoch 857/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5233 - val_loss: 31.4292\n",
      "Epoch 858/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4569 - val_loss: 32.4552\n",
      "Epoch 859/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5790 - val_loss: 32.4800\n",
      "Epoch 860/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.3709 - val_loss: 31.4474\n",
      "Epoch 861/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5947 - val_loss: 31.8208\n",
      "Epoch 862/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6559 - val_loss: 31.1873\n",
      "Epoch 863/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5684 - val_loss: 32.3845\n",
      "Epoch 864/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9830 - val_loss: 30.9987\n",
      "Epoch 865/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5214 - val_loss: 30.9325\n",
      "Epoch 866/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9728 - val_loss: 32.0109\n",
      "Epoch 867/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5987 - val_loss: 31.2390\n",
      "Epoch 868/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5102 - val_loss: 31.5904\n",
      "Epoch 869/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3661 - val_loss: 32.4084\n",
      "Epoch 870/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4829 - val_loss: 31.8540\n",
      "Epoch 871/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4633 - val_loss: 31.6334\n",
      "Epoch 872/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4197 - val_loss: 31.6755\n",
      "Epoch 873/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3815 - val_loss: 31.0397\n",
      "Epoch 874/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4854 - val_loss: 31.9830\n",
      "Epoch 875/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5255 - val_loss: 31.2877\n",
      "Epoch 876/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5977 - val_loss: 33.0354\n",
      "Epoch 877/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5723 - val_loss: 30.9414\n",
      "Epoch 878/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6095 - val_loss: 32.6436\n",
      "Epoch 879/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3845 - val_loss: 32.5648\n",
      "Epoch 880/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4138 - val_loss: 31.5710\n",
      "Epoch 881/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2826 - val_loss: 31.2542\n",
      "Epoch 882/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4672 - val_loss: 31.7662\n",
      "Epoch 883/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3074 - val_loss: 31.9601\n",
      "Epoch 884/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3919 - val_loss: 31.4211\n",
      "Epoch 885/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1785 - val_loss: 31.4724\n",
      "Epoch 886/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3812 - val_loss: 31.0415\n",
      "Epoch 887/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3288 - val_loss: 31.5434\n",
      "Epoch 888/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5298 - val_loss: 31.8557\n",
      "Epoch 889/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.8524 - val_loss: 31.1851\n",
      "Epoch 890/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0436 - val_loss: 29.9429\n",
      "Epoch 891/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0014 - val_loss: 31.1017\n",
      "Epoch 892/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7445 - val_loss: 31.1959\n",
      "Epoch 893/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7198 - val_loss: 32.0047\n",
      "Epoch 894/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3218 - val_loss: 31.2669\n",
      "Epoch 895/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4022 - val_loss: 30.5346\n",
      "Epoch 896/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.1664 - val_loss: 31.8346\n",
      "Epoch 897/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1028 - val_loss: 30.8761\n",
      "Epoch 898/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4943 - val_loss: 32.9387\n",
      "Epoch 899/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4160 - val_loss: 31.5367\n",
      "Epoch 900/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4011 - val_loss: 31.5693\n",
      "Epoch 901/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3886 - val_loss: 30.8506\n",
      "Epoch 902/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4095 - val_loss: 31.1429\n",
      "Epoch 903/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2837 - val_loss: 31.5987\n",
      "Epoch 904/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4899 - val_loss: 32.2365\n",
      "Epoch 905/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4297 - val_loss: 31.9389\n",
      "Epoch 906/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2626 - val_loss: 31.8284\n",
      "Epoch 907/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3762 - val_loss: 31.8919\n",
      "Epoch 908/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4500 - val_loss: 31.4409\n",
      "Epoch 909/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7799 - val_loss: 32.2345\n",
      "Epoch 910/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9597 - val_loss: 30.6576\n",
      "Epoch 911/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2009 - val_loss: 31.2427\n",
      "Epoch 912/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2634 - val_loss: 30.9400\n",
      "Epoch 913/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3093 - val_loss: 31.3036\n",
      "Epoch 914/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3620 - val_loss: 30.8597\n",
      "Epoch 915/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5083 - val_loss: 31.3754\n",
      "Epoch 916/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1108 - val_loss: 31.7412\n",
      "Epoch 917/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1097 - val_loss: 30.8905\n",
      "Epoch 918/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1264 - val_loss: 31.1919\n",
      "Epoch 919/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3460 - val_loss: 31.5172\n",
      "Epoch 920/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3265 - val_loss: 31.6215\n",
      "Epoch 921/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2513 - val_loss: 31.4277\n",
      "Epoch 922/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4665 - val_loss: 31.3134\n",
      "Epoch 923/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3769 - val_loss: 31.0048\n",
      "Epoch 924/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7632 - val_loss: 31.0119\n",
      "Epoch 925/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3207 - val_loss: 31.5908\n",
      "Epoch 926/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1182 - val_loss: 30.7829\n",
      "Epoch 927/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0739 - val_loss: 31.7951\n",
      "Epoch 928/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1274 - val_loss: 30.7477\n",
      "Epoch 929/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0026 - val_loss: 30.4889\n",
      "Epoch 930/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1568 - val_loss: 31.1268\n",
      "Epoch 931/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0865 - val_loss: 31.3655\n",
      "Epoch 932/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1386 - val_loss: 31.8269\n",
      "Epoch 933/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.9268 - val_loss: 31.6689\n",
      "Epoch 934/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8469 - val_loss: 31.7818\n",
      "Epoch 935/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9296 - val_loss: 30.9041\n",
      "Epoch 936/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8404 - val_loss: 31.1104\n",
      "Epoch 937/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8851 - val_loss: 31.4768\n",
      "Epoch 938/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9733 - val_loss: 31.0164\n",
      "Epoch 939/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8526 - val_loss: 31.3026\n",
      "Epoch 940/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9355 - val_loss: 31.2890\n",
      "Epoch 941/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9773 - val_loss: 32.2573\n",
      "Epoch 942/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3319 - val_loss: 31.2294\n",
      "Epoch 943/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0774 - val_loss: 30.5582\n",
      "Epoch 944/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7562 - val_loss: 31.3212\n",
      "Epoch 945/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6464 - val_loss: 31.4508\n",
      "Epoch 946/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1401 - val_loss: 30.9270\n",
      "Epoch 947/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1993 - val_loss: 30.8495\n",
      "Epoch 948/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3715 - val_loss: 30.7878\n",
      "Epoch 949/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9990 - val_loss: 30.9585\n",
      "Epoch 950/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9470 - val_loss: 30.2346\n",
      "Epoch 951/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0208 - val_loss: 30.9794\n",
      "Epoch 952/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9734 - val_loss: 30.8079\n",
      "Epoch 953/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0433 - val_loss: 31.4766\n",
      "Epoch 954/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8392 - val_loss: 31.8972\n",
      "Epoch 955/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3692 - val_loss: 33.5212\n",
      "Epoch 956/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2431 - val_loss: 30.4609\n",
      "Epoch 957/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9011 - val_loss: 31.4394\n",
      "Epoch 958/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9233 - val_loss: 32.4017\n",
      "Epoch 959/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2926 - val_loss: 30.8700\n",
      "Epoch 960/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0276 - val_loss: 30.7693\n",
      "Epoch 961/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7665 - val_loss: 32.0541\n",
      "Epoch 962/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7938 - val_loss: 30.8245\n",
      "Epoch 963/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8003 - val_loss: 30.3857\n",
      "Epoch 964/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7925 - val_loss: 32.3549\n",
      "Epoch 965/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0112 - val_loss: 31.5832\n",
      "Epoch 966/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9940 - val_loss: 31.9807\n",
      "Epoch 967/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1418 - val_loss: 32.4631\n",
      "Epoch 968/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1804 - val_loss: 33.4733\n",
      "Epoch 969/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4462 - val_loss: 30.8062\n",
      "Epoch 970/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5863 - val_loss: 30.8066\n",
      "Epoch 971/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2849 - val_loss: 31.0059\n",
      "Epoch 972/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4169 - val_loss: 31.5272\n",
      "Epoch 973/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9812 - val_loss: 32.3721\n",
      "Epoch 974/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8480 - val_loss: 30.5753\n",
      "Epoch 975/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9317 - val_loss: 31.0476\n",
      "Epoch 976/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8275 - val_loss: 30.5024\n",
      "Epoch 977/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6410 - val_loss: 30.4509\n",
      "Epoch 978/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0124 - val_loss: 31.7917\n",
      "Epoch 979/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3460 - val_loss: 31.5627\n",
      "Epoch 980/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1581 - val_loss: 31.4159\n",
      "Epoch 981/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5860 - val_loss: 31.7180\n",
      "Epoch 982/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7878 - val_loss: 30.3538\n",
      "Epoch 983/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8293 - val_loss: 30.5450\n",
      "Epoch 984/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6790 - val_loss: 31.4511\n",
      "Epoch 985/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8351 - val_loss: 30.6939\n",
      "Epoch 986/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1055 - val_loss: 32.3825\n",
      "Epoch 987/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9630 - val_loss: 30.8088\n",
      "Epoch 988/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8666 - val_loss: 31.9163\n",
      "Epoch 989/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8007 - val_loss: 32.0228\n",
      "Epoch 990/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8319 - val_loss: 30.2130\n",
      "Epoch 991/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7784 - val_loss: 31.4881\n",
      "Epoch 992/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8077 - val_loss: 31.1817\n",
      "Epoch 993/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0721 - val_loss: 30.9995\n",
      "Epoch 994/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0570 - val_loss: 32.4330\n",
      "Epoch 995/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6953 - val_loss: 31.7830\n",
      "Epoch 996/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8951 - val_loss: 31.5324\n",
      "Epoch 997/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7799 - val_loss: 30.4355\n",
      "Epoch 998/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9120 - val_loss: 32.0486\n",
      "Epoch 999/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9637 - val_loss: 31.8086\n",
      "Epoch 1000/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 26.0300\n",
      "Epoch 01000: saving model to saved_models/latent8/cp-1000.h5\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 26.0300 - val_loss: 31.4035\n",
      "Epoch 1001/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7290 - val_loss: 31.4020\n",
      "Epoch 1002/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2123 - val_loss: 31.6990\n",
      "Epoch 1003/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0220 - val_loss: 30.8106\n",
      "Epoch 1004/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1500 - val_loss: 31.0056\n",
      "Epoch 1005/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9732 - val_loss: 32.5698\n",
      "Epoch 1006/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8634 - val_loss: 31.1963\n",
      "Epoch 1007/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2969 - val_loss: 31.3289\n",
      "Epoch 1008/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1845 - val_loss: 31.7100\n",
      "Epoch 1009/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2594 - val_loss: 32.1997\n",
      "Epoch 1010/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8855 - val_loss: 30.6450\n",
      "Epoch 1011/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7770 - val_loss: 30.7739\n",
      "Epoch 1012/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7837 - val_loss: 29.9559\n",
      "Epoch 1013/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6888 - val_loss: 31.1746\n",
      "Epoch 1014/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.4851 - val_loss: 32.5599\n",
      "Epoch 1015/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8281 - val_loss: 30.8893\n",
      "Epoch 1016/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8280 - val_loss: 31.0289\n",
      "Epoch 1017/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6973 - val_loss: 31.1388\n",
      "Epoch 1018/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6720 - val_loss: 31.5954\n",
      "Epoch 1019/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5513 - val_loss: 30.1106\n",
      "Epoch 1020/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5984 - val_loss: 30.9446\n",
      "Epoch 1021/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5666 - val_loss: 31.0058\n",
      "Epoch 1022/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6027 - val_loss: 30.5827\n",
      "Epoch 1023/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9015 - val_loss: 31.5637\n",
      "Epoch 1024/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7891 - val_loss: 30.7823\n",
      "Epoch 1025/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7616 - val_loss: 30.4829\n",
      "Epoch 1026/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8944 - val_loss: 30.6451\n",
      "Epoch 1027/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8430 - val_loss: 30.7417\n",
      "Epoch 1028/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6078 - val_loss: 32.1253\n",
      "Epoch 1029/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0129 - val_loss: 30.6798\n",
      "Epoch 1030/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6225 - val_loss: 31.3776\n",
      "Epoch 1031/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6264 - val_loss: 30.4052\n",
      "Epoch 1032/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7855 - val_loss: 30.9655\n",
      "Epoch 1033/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8552 - val_loss: 30.7852\n",
      "Epoch 1034/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6208 - val_loss: 31.2084\n",
      "Epoch 1035/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8367 - val_loss: 31.6382\n",
      "Epoch 1036/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7065 - val_loss: 30.6813\n",
      "Epoch 1037/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6986 - val_loss: 31.3730\n",
      "Epoch 1038/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6394 - val_loss: 31.3173\n",
      "Epoch 1039/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6926 - val_loss: 30.3075\n",
      "Epoch 1040/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6815 - val_loss: 31.5060\n",
      "Epoch 1041/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7350 - val_loss: 31.3443\n",
      "Epoch 1042/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6825 - val_loss: 31.1012\n",
      "Epoch 1043/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7520 - val_loss: 32.4330\n",
      "Epoch 1044/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0942 - val_loss: 31.8143\n",
      "Epoch 1045/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5045 - val_loss: 31.5433\n",
      "Epoch 1046/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4463 - val_loss: 31.2115\n",
      "Epoch 1047/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8166 - val_loss: 31.4610\n",
      "Epoch 1048/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0313 - val_loss: 31.2768\n",
      "Epoch 1049/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6300 - val_loss: 30.1873\n",
      "Epoch 1050/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8882 - val_loss: 30.4835\n",
      "Epoch 1051/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9806 - val_loss: 32.0193\n",
      "Epoch 1052/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1156 - val_loss: 30.9101\n",
      "Epoch 1053/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7589 - val_loss: 30.9989\n",
      "Epoch 1054/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5074 - val_loss: 31.1835\n",
      "Epoch 1055/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8723 - val_loss: 30.6004\n",
      "Epoch 1056/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0152 - val_loss: 32.3920\n",
      "Epoch 1057/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3337 - val_loss: 31.8391\n",
      "Epoch 1058/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9964 - val_loss: 31.3398\n",
      "Epoch 1059/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5821 - val_loss: 30.8686\n",
      "Epoch 1060/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5406 - val_loss: 30.8640\n",
      "Epoch 1061/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7275 - val_loss: 31.3920\n",
      "Epoch 1062/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4704 - val_loss: 30.5611\n",
      "Epoch 1063/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4462 - val_loss: 30.3839\n",
      "Epoch 1064/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4255 - val_loss: 30.5540\n",
      "Epoch 1065/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4799 - val_loss: 29.8669\n",
      "Epoch 1066/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4799 - val_loss: 30.7917\n",
      "Epoch 1067/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4972 - val_loss: 31.8757\n",
      "Epoch 1068/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7980 - val_loss: 30.8562\n",
      "Epoch 1069/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6663 - val_loss: 30.2439\n",
      "Epoch 1070/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8384 - val_loss: 32.2125\n",
      "Epoch 1071/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7296 - val_loss: 31.2352\n",
      "Epoch 1072/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6350 - val_loss: 31.1863\n",
      "Epoch 1073/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6055 - val_loss: 31.2840\n",
      "Epoch 1074/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8233 - val_loss: 32.5428\n",
      "Epoch 1075/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1488 - val_loss: 30.7222\n",
      "Epoch 1076/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7370 - val_loss: 31.1689\n",
      "Epoch 1077/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8786 - val_loss: 31.1737\n",
      "Epoch 1078/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7133 - val_loss: 31.0394\n",
      "Epoch 1079/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6514 - val_loss: 30.6771\n",
      "Epoch 1080/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4187 - val_loss: 30.8314\n",
      "Epoch 1081/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5253 - val_loss: 30.7395\n",
      "Epoch 1082/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6964 - val_loss: 32.2424\n",
      "Epoch 1083/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8730 - val_loss: 31.9820\n",
      "Epoch 1084/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6831 - val_loss: 31.8253\n",
      "Epoch 1085/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8826 - val_loss: 31.2541\n",
      "Epoch 1086/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6980 - val_loss: 30.9986\n",
      "Epoch 1087/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6125 - val_loss: 31.4248\n",
      "Epoch 1088/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8012 - val_loss: 32.0261\n",
      "Epoch 1089/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5042 - val_loss: 31.1976\n",
      "Epoch 1090/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3701 - val_loss: 30.4072\n",
      "Epoch 1091/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4958 - val_loss: 31.0582\n",
      "Epoch 1092/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4645 - val_loss: 31.5387\n",
      "Epoch 1093/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6403 - val_loss: 30.3281\n",
      "Epoch 1094/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6490 - val_loss: 30.8035\n",
      "Epoch 1095/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8881 - val_loss: 31.2598\n",
      "Epoch 1096/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6843 - val_loss: 30.8976\n",
      "Epoch 1097/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5299 - val_loss: 30.2567\n",
      "Epoch 1098/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5239 - val_loss: 32.2231\n",
      "Epoch 1099/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3928 - val_loss: 30.4029\n",
      "Epoch 1100/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5741 - val_loss: 31.3202\n",
      "Epoch 1101/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5647 - val_loss: 30.8048\n",
      "Epoch 1102/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3705 - val_loss: 32.0558\n",
      "Epoch 1103/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4477 - val_loss: 31.3160\n",
      "Epoch 1104/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6102 - val_loss: 31.9523\n",
      "Epoch 1105/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5372 - val_loss: 30.0569\n",
      "Epoch 1106/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4324 - val_loss: 31.2557\n",
      "Epoch 1107/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3915 - val_loss: 30.8209\n",
      "Epoch 1108/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5543 - val_loss: 30.7110\n",
      "Epoch 1109/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4776 - val_loss: 30.6430\n",
      "Epoch 1110/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4484 - val_loss: 31.5115\n",
      "Epoch 1111/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.5334 - val_loss: 31.4146\n",
      "Epoch 1112/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 25.3630 - val_loss: 31.3045\n",
      "Epoch 1113/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4443 - val_loss: 32.1377\n",
      "Epoch 1114/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4078 - val_loss: 30.0307\n",
      "Epoch 1115/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7669 - val_loss: 31.7217\n",
      "Epoch 1116/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8544 - val_loss: 30.6848\n",
      "Epoch 1117/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5614 - val_loss: 30.9023\n",
      "Epoch 1118/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4968 - val_loss: 31.6695\n",
      "Epoch 1119/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4083 - val_loss: 32.0943\n",
      "Epoch 1120/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5449 - val_loss: 31.7390\n",
      "Epoch 1121/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9738 - val_loss: 31.5411\n",
      "Epoch 1122/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8026 - val_loss: 30.9250\n",
      "Epoch 1123/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7393 - val_loss: 31.9886\n",
      "Epoch 1124/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4792 - val_loss: 31.0810\n",
      "Epoch 1125/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8710 - val_loss: 31.5404\n",
      "Epoch 1126/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9715 - val_loss: 31.0062\n",
      "Epoch 1127/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8066 - val_loss: 31.1644\n",
      "Epoch 1128/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9424 - val_loss: 29.9019\n",
      "Epoch 1129/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5048 - val_loss: 31.7068\n",
      "Epoch 1130/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7033 - val_loss: 31.8940\n",
      "Epoch 1131/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5413 - val_loss: 31.4928\n",
      "Epoch 1132/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5277 - val_loss: 31.7920\n",
      "Epoch 1133/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4192 - val_loss: 30.4827\n",
      "Epoch 1134/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4403 - val_loss: 31.5556\n",
      "Epoch 1135/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3562 - val_loss: 31.9911\n",
      "Epoch 1136/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5137 - val_loss: 30.8093\n",
      "Epoch 1137/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2300 - val_loss: 30.8693\n",
      "Epoch 1138/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3832 - val_loss: 31.0831\n",
      "Epoch 1139/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5390 - val_loss: 30.2405\n",
      "Epoch 1140/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5767 - val_loss: 31.3676\n",
      "Epoch 1141/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7589 - val_loss: 31.4493\n",
      "Epoch 1142/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4759 - val_loss: 30.9769\n",
      "Epoch 1143/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4587 - val_loss: 30.8670\n",
      "Epoch 1144/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6281 - val_loss: 31.3366\n",
      "Epoch 1145/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3689 - val_loss: 31.3118\n",
      "Epoch 1146/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3232 - val_loss: 30.7335\n",
      "Epoch 1147/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4751 - val_loss: 31.1536\n",
      "Epoch 1148/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5913 - val_loss: 32.7019\n",
      "Epoch 1149/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5051 - val_loss: 30.8968\n",
      "Epoch 1150/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4633 - val_loss: 31.2949\n",
      "Epoch 1151/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5043 - val_loss: 31.1622\n",
      "Epoch 1152/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5386 - val_loss: 30.6036\n",
      "Epoch 1153/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3946 - val_loss: 31.9686\n",
      "Epoch 1154/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4391 - val_loss: 30.7596\n",
      "Epoch 1155/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3502 - val_loss: 30.8949\n",
      "Epoch 1156/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3448 - val_loss: 30.8674\n",
      "Epoch 1157/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2961 - val_loss: 31.8839\n",
      "Epoch 1158/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5618 - val_loss: 31.9185\n",
      "Epoch 1159/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6513 - val_loss: 31.5466\n",
      "Epoch 1160/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2331 - val_loss: 31.1179\n",
      "Epoch 1161/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5422 - val_loss: 31.0156\n",
      "Epoch 1162/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3736 - val_loss: 31.4587\n",
      "Epoch 1163/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4405 - val_loss: 30.4644\n",
      "Epoch 1164/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6108 - val_loss: 31.0805\n",
      "Epoch 1165/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4344 - val_loss: 30.7372\n",
      "Epoch 1166/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3775 - val_loss: 31.7657\n",
      "Epoch 1167/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1815 - val_loss: 30.3694\n",
      "Epoch 1168/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5849 - val_loss: 30.9333\n",
      "Epoch 1169/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8854 - val_loss: 31.7318\n",
      "Epoch 1170/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7517 - val_loss: 33.4476\n",
      "Epoch 1171/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8457 - val_loss: 30.7213\n",
      "Epoch 1172/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3982 - val_loss: 31.8143\n",
      "Epoch 1173/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5194 - val_loss: 31.0262\n",
      "Epoch 1174/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5059 - val_loss: 31.1238\n",
      "Epoch 1175/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.7885 - val_loss: 31.0511\n",
      "Epoch 1176/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6310 - val_loss: 30.9013\n",
      "Epoch 1177/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1426 - val_loss: 30.8655\n",
      "Epoch 1178/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6458 - val_loss: 31.7142\n",
      "Epoch 1179/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5889 - val_loss: 31.0254\n",
      "Epoch 1180/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5239 - val_loss: 32.0588\n",
      "Epoch 1181/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6516 - val_loss: 31.7605\n",
      "Epoch 1182/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5756 - val_loss: 30.1957\n",
      "Epoch 1183/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5416 - val_loss: 31.5506\n",
      "Epoch 1184/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6402 - val_loss: 30.9974\n",
      "Epoch 1185/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9242 - val_loss: 31.3467\n",
      "Epoch 1186/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2781 - val_loss: 31.3626\n",
      "Epoch 1187/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0560 - val_loss: 33.0049\n",
      "Epoch 1188/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0745 - val_loss: 30.8697\n",
      "Epoch 1189/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5190 - val_loss: 29.8222\n",
      "Epoch 1190/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2424 - val_loss: 30.6470\n",
      "Epoch 1191/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4140 - val_loss: 31.0122\n",
      "Epoch 1192/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7624 - val_loss: 31.9799\n",
      "Epoch 1193/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6159 - val_loss: 30.9494\n",
      "Epoch 1194/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6977 - val_loss: 30.5087\n",
      "Epoch 1195/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5396 - val_loss: 31.3813\n",
      "Epoch 1196/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2517 - val_loss: 31.3789\n",
      "Epoch 1197/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3642 - val_loss: 30.7064\n",
      "Epoch 1198/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4013 - val_loss: 31.4215\n",
      "Epoch 1199/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5927 - val_loss: 32.7444\n",
      "Epoch 1200/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 25.4354\n",
      "Epoch 01200: saving model to saved_models/latent8/cp-1200.h5\n",
      "7/7 [==============================] - 1s 167ms/step - loss: 25.4354 - val_loss: 32.0873\n",
      "Epoch 1201/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4328 - val_loss: 31.2596\n",
      "Epoch 1202/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4367 - val_loss: 31.6073\n",
      "Epoch 1203/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8687 - val_loss: 31.4963\n",
      "Epoch 1204/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4228 - val_loss: 31.3222\n",
      "Epoch 1205/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4560 - val_loss: 31.3223\n",
      "Epoch 1206/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5549 - val_loss: 31.4191\n",
      "Epoch 1207/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3722 - val_loss: 30.6810\n",
      "Epoch 1208/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5567 - val_loss: 31.8965\n",
      "Epoch 1209/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3887 - val_loss: 31.0780\n",
      "Epoch 1210/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2169 - val_loss: 33.2564\n",
      "Epoch 1211/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4031 - val_loss: 30.6638\n",
      "Epoch 1212/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3911 - val_loss: 30.9625\n",
      "Epoch 1213/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2274 - val_loss: 31.1751\n",
      "Epoch 1214/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2352 - val_loss: 30.9913\n",
      "Epoch 1215/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4538 - val_loss: 30.9744\n",
      "Epoch 1216/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4013 - val_loss: 31.1168\n",
      "Epoch 1217/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3707 - val_loss: 31.5078\n",
      "Epoch 1218/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2770 - val_loss: 30.3605\n",
      "Epoch 1219/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1826 - val_loss: 31.4393\n",
      "Epoch 1220/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1506 - val_loss: 30.8115\n",
      "Epoch 1221/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3047 - val_loss: 30.6153\n",
      "Epoch 1222/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1840 - val_loss: 32.6473\n",
      "Epoch 1223/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3923 - val_loss: 31.6256\n",
      "Epoch 1224/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.9926 - val_loss: 30.8877\n",
      "Epoch 1225/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4287 - val_loss: 30.8546\n",
      "Epoch 1226/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5138 - val_loss: 32.5039\n",
      "Epoch 1227/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4569 - val_loss: 30.5544\n",
      "Epoch 1228/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2036 - val_loss: 31.1038\n",
      "Epoch 1229/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2558 - val_loss: 30.2990\n",
      "Epoch 1230/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2235 - val_loss: 31.3966\n",
      "Epoch 1231/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1367 - val_loss: 30.3731\n",
      "Epoch 1232/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2353 - val_loss: 30.1044\n",
      "Epoch 1233/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3051 - val_loss: 31.9739\n",
      "Epoch 1234/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7575 - val_loss: 31.8011\n",
      "Epoch 1235/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3901 - val_loss: 31.4315\n",
      "Epoch 1236/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5341 - val_loss: 32.4011\n",
      "Epoch 1237/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5148 - val_loss: 31.7785\n",
      "Epoch 1238/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4067 - val_loss: 31.5273\n",
      "Epoch 1239/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2329 - val_loss: 31.8804\n",
      "Epoch 1240/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5623 - val_loss: 31.3240\n",
      "Epoch 1241/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1605 - val_loss: 31.3858\n",
      "Epoch 1242/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2091 - val_loss: 31.3764\n",
      "Epoch 1243/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2088 - val_loss: 30.4771\n",
      "Epoch 1244/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2530 - val_loss: 32.8628\n",
      "Epoch 1245/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3503 - val_loss: 30.4931\n",
      "Epoch 1246/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3323 - val_loss: 32.1419\n",
      "Epoch 1247/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7796 - val_loss: 31.9054\n",
      "Epoch 1248/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9387 - val_loss: 30.1502\n",
      "Epoch 1249/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3067 - val_loss: 30.8165\n",
      "Epoch 1250/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3122 - val_loss: 31.5653\n",
      "Epoch 1251/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3693 - val_loss: 31.4507\n",
      "Epoch 1252/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1533 - val_loss: 30.7714\n",
      "Epoch 1253/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0900 - val_loss: 31.2363\n",
      "Epoch 1254/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2241 - val_loss: 30.9344\n",
      "Epoch 1255/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4787 - val_loss: 31.1532\n",
      "Epoch 1256/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1265 - val_loss: 30.7919\n",
      "Epoch 1257/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4693 - val_loss: 31.3983\n",
      "Epoch 1258/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2948 - val_loss: 30.7771\n",
      "Epoch 1259/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2557 - val_loss: 32.5347\n",
      "Epoch 1260/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2010 - val_loss: 31.4804\n",
      "Epoch 1261/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1812 - val_loss: 30.7649\n",
      "Epoch 1262/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3013 - val_loss: 30.7612\n",
      "Epoch 1263/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3819 - val_loss: 31.3647\n",
      "Epoch 1264/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1609 - val_loss: 31.8055\n",
      "Epoch 1265/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0167 - val_loss: 31.7615\n",
      "Epoch 1266/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1046 - val_loss: 31.1619\n",
      "Epoch 1267/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1225 - val_loss: 31.0446\n",
      "Epoch 1268/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2167 - val_loss: 31.6626\n",
      "Epoch 1269/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3182 - val_loss: 30.9910\n",
      "Epoch 1270/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0875 - val_loss: 30.6503\n",
      "Epoch 1271/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1837 - val_loss: 31.0226\n",
      "Epoch 1272/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1454 - val_loss: 31.5429\n",
      "Epoch 1273/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3854 - val_loss: 31.7069\n",
      "Epoch 1274/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2586 - val_loss: 31.7699\n",
      "Epoch 1275/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3726 - val_loss: 31.4043\n",
      "Epoch 1276/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3714 - val_loss: 31.1156\n",
      "Epoch 1277/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1954 - val_loss: 32.0433\n",
      "Epoch 1278/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6373 - val_loss: 31.6894\n",
      "Epoch 1279/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2830 - val_loss: 30.9726\n",
      "Epoch 1280/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2554 - val_loss: 30.5497\n",
      "Epoch 1281/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2560 - val_loss: 32.0558\n",
      "Epoch 1282/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3236 - val_loss: 31.7506\n",
      "Epoch 1283/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6768 - val_loss: 31.6464\n",
      "Epoch 1284/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5578 - val_loss: 30.8184\n",
      "Epoch 1285/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2789 - val_loss: 32.0317\n",
      "Epoch 1286/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2215 - val_loss: 31.0960\n",
      "Epoch 1287/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4115 - val_loss: 32.4598\n",
      "Epoch 1288/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9205 - val_loss: 32.8966\n",
      "Epoch 1289/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4237 - val_loss: 31.1048\n",
      "Epoch 1290/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4121 - val_loss: 31.5916\n",
      "Epoch 1291/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4412 - val_loss: 30.8821\n",
      "Epoch 1292/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3448 - val_loss: 31.2087\n",
      "Epoch 1293/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3048 - val_loss: 30.7781\n",
      "Epoch 1294/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2348 - val_loss: 32.2534\n",
      "Epoch 1295/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8435 - val_loss: 32.2084\n",
      "Epoch 1296/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1024 - val_loss: 31.7338\n",
      "Epoch 1297/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7034 - val_loss: 32.8311\n",
      "Epoch 1298/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3268 - val_loss: 31.1086\n",
      "Epoch 1299/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2649 - val_loss: 29.9192\n",
      "Epoch 1300/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3228 - val_loss: 31.6755\n",
      "Epoch 1301/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1542 - val_loss: 31.1648\n",
      "Epoch 1302/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2016 - val_loss: 31.1867\n",
      "Epoch 1303/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2341 - val_loss: 31.2423\n",
      "Epoch 1304/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0716 - val_loss: 32.2892\n",
      "Epoch 1305/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1669 - val_loss: 31.0368\n",
      "Epoch 1306/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0794 - val_loss: 32.1263\n",
      "Epoch 1307/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1757 - val_loss: 30.7777\n",
      "Epoch 1308/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0135 - val_loss: 30.9476\n",
      "Epoch 1309/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1564 - val_loss: 31.5494\n",
      "Epoch 1310/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2520 - val_loss: 31.1063\n",
      "Epoch 1311/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0394 - val_loss: 31.6466\n",
      "Epoch 1312/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1740 - val_loss: 31.9933\n",
      "Epoch 1313/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0335 - val_loss: 31.2883\n",
      "Epoch 1314/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1908 - val_loss: 30.8577\n",
      "Epoch 1315/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1142 - val_loss: 31.9923\n",
      "Epoch 1316/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0787 - val_loss: 31.0512\n",
      "Epoch 1317/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0795 - val_loss: 31.0613\n",
      "Epoch 1318/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9883 - val_loss: 30.3335\n",
      "Epoch 1319/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0331 - val_loss: 31.0999\n",
      "Epoch 1320/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1611 - val_loss: 30.9422\n",
      "Epoch 1321/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1847 - val_loss: 31.9969\n",
      "Epoch 1322/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2326 - val_loss: 31.6570\n",
      "Epoch 1323/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1314 - val_loss: 31.3992\n",
      "Epoch 1324/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1142 - val_loss: 31.2428\n",
      "Epoch 1325/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2651 - val_loss: 31.8202\n",
      "Epoch 1326/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3746 - val_loss: 30.8666\n",
      "Epoch 1327/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4597 - val_loss: 31.2739\n",
      "Epoch 1328/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0282 - val_loss: 31.6894\n",
      "Epoch 1329/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4552 - val_loss: 31.3827\n",
      "Epoch 1330/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3861 - val_loss: 31.6483\n",
      "Epoch 1331/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4773 - val_loss: 31.7433\n",
      "Epoch 1332/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4695 - val_loss: 31.1902\n",
      "Epoch 1333/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1373 - val_loss: 31.5251\n",
      "Epoch 1334/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1318 - val_loss: 31.9007\n",
      "Epoch 1335/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1304 - val_loss: 31.9294\n",
      "Epoch 1336/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5721 - val_loss: 32.3772\n",
      "Epoch 1337/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1033 - val_loss: 31.6823\n",
      "Epoch 1338/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0411 - val_loss: 30.7623\n",
      "Epoch 1339/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1345 - val_loss: 30.8404\n",
      "Epoch 1340/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9628 - val_loss: 31.9331\n",
      "Epoch 1341/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2834 - val_loss: 30.4788\n",
      "Epoch 1342/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2510 - val_loss: 32.2926\n",
      "Epoch 1343/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0628 - val_loss: 32.1898\n",
      "Epoch 1344/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9113 - val_loss: 31.7229\n",
      "Epoch 1345/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1226 - val_loss: 32.0644\n",
      "Epoch 1346/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3397 - val_loss: 32.7812\n",
      "Epoch 1347/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1419 - val_loss: 32.3214\n",
      "Epoch 1348/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1821 - val_loss: 31.9420\n",
      "Epoch 1349/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3983 - val_loss: 31.4790\n",
      "Epoch 1350/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2211 - val_loss: 32.3271\n",
      "Epoch 1351/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1639 - val_loss: 31.9939\n",
      "Epoch 1352/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1384 - val_loss: 30.1829\n",
      "Epoch 1353/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1549 - val_loss: 31.8907\n",
      "Epoch 1354/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0621 - val_loss: 32.0014\n",
      "Epoch 1355/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1733 - val_loss: 31.4139\n",
      "Epoch 1356/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1126 - val_loss: 32.5573\n",
      "Epoch 1357/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1026 - val_loss: 32.0287\n",
      "Epoch 1358/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1950 - val_loss: 32.0584\n",
      "Epoch 1359/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1892 - val_loss: 30.4210\n",
      "Epoch 1360/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1567 - val_loss: 30.6729\n",
      "Epoch 1361/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2006 - val_loss: 31.6917\n",
      "Epoch 1362/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0270 - val_loss: 31.6898\n",
      "Epoch 1363/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1173 - val_loss: 31.0183\n",
      "Epoch 1364/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0950 - val_loss: 30.5571\n",
      "Epoch 1365/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1007 - val_loss: 32.8508\n",
      "Epoch 1366/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1150 - val_loss: 30.4887\n",
      "Epoch 1367/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4150 - val_loss: 32.4481\n",
      "Epoch 1368/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4592 - val_loss: 30.9208\n",
      "Epoch 1369/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1684 - val_loss: 31.2790\n",
      "Epoch 1370/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2265 - val_loss: 31.6512\n",
      "Epoch 1371/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0704 - val_loss: 30.3385\n",
      "Epoch 1372/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2049 - val_loss: 31.9160\n",
      "Epoch 1373/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1979 - val_loss: 29.8434\n",
      "Epoch 1374/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9137 - val_loss: 31.7652\n",
      "Epoch 1375/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1251 - val_loss: 31.2202\n",
      "Epoch 1376/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2054 - val_loss: 31.2951\n",
      "Epoch 1377/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1193 - val_loss: 31.6797\n",
      "Epoch 1378/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2192 - val_loss: 30.8497\n",
      "Epoch 1379/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0149 - val_loss: 30.5269\n",
      "Epoch 1380/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9679 - val_loss: 32.2445\n",
      "Epoch 1381/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9388 - val_loss: 30.8993\n",
      "Epoch 1382/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0812 - val_loss: 31.3328\n",
      "Epoch 1383/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0222 - val_loss: 31.2724\n",
      "Epoch 1384/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0663 - val_loss: 32.6268\n",
      "Epoch 1385/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3218 - val_loss: 31.6992\n",
      "Epoch 1386/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2277 - val_loss: 31.2128\n",
      "Epoch 1387/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8915 - val_loss: 31.8289\n",
      "Epoch 1388/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0836 - val_loss: 31.2444\n",
      "Epoch 1389/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0010 - val_loss: 31.5630\n",
      "Epoch 1390/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1988 - val_loss: 30.4485\n",
      "Epoch 1391/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0301 - val_loss: 31.3933\n",
      "Epoch 1392/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2456 - val_loss: 31.8159\n",
      "Epoch 1393/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9424 - val_loss: 31.4449\n",
      "Epoch 1394/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0327 - val_loss: 30.8418\n",
      "Epoch 1395/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 25.2212Restoring model weights from the end of the best epoch.\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.2212 - val_loss: 31.7954\n",
      "Epoch 01395: early stopping\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train VAE model with callbacks \"\"\"\n",
    "history = vae.fit(trainX, trainX, \n",
    "                  batch_size = batch_size, \n",
    "                  epochs = epochs, \n",
    "                  callbacks=[initial_checkpoint, checkpoint, EarlyStoppingAtMinLoss()],\n",
    "                  #callbacks=[initial_checkpoint, checkpoint],\n",
    "                  shuffle=True,\n",
    "                  validation_data=(valX, valX)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 7.0\n"
     ]
    }
   ],
   "source": [
    "# In this GPU implementation, it shows the number of batches/iterations for one epoch (and not the training samples), which is:\n",
    "print (\"Step size:\", np.ceil(trainX.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save the entire model \"\"\"\n",
    "# Save notes about hyperparameters used:\n",
    "epochs = len(history.history['loss'])\n",
    "with open(f'{SAVE_FOLDER}/notes.txt', 'w') as f:\n",
    "    f.write(f'Epochs: {epochs} \\n')\n",
    "    f.write(f'Batch size: {batch_size} \\n')\n",
    "    f.write(f'Latent dimension: {latent_dim} \\n')\n",
    "    f.write(f'Filter sizes: {filters} \\n')\n",
    "    f.write(f'img_width: {img_width} \\n')\n",
    "    f.write(f'img_height: {img_height} \\n')\n",
    "    f.write(f'num_channels: {num_channels}')\n",
    "    f.close()\n",
    "\n",
    "# Save weights for encoder model:\n",
    "encoder.save_weights(f'{SAVE_FOLDER}/ENCODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for decoder model:\n",
    "decoder.save_weights(f'{SAVE_FOLDER}/DECODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for total VAE:\n",
    "vae.save_weights(f'{SAVE_FOLDER}/VAE_WEIGHTS.h5')\n",
    "\n",
    "# Save the history at each epochs (cost of train and validation sets):\n",
    "np.save(f'{SAVE_FOLDER}/HISTORY.npy', history.history)\n",
    "\n",
    "# Save exact training, validation and testing data set (as numpy arrays):\n",
    "np.save(f'{SAVE_FOLDER}/trainX.npy', trainX)\n",
    "np.save(f'{SAVE_FOLDER}/valX.npy', valX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllX.npy', testAllX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllY.npy', testAllY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate training process\n",
    "\n",
    "Now that the training process is complete, let’s evaluate the average loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(13, 6))\n",
    "\n",
    "# summarize history for loss:\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='blue', alpha=0.4, label='validation')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Cost', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.title('Average Loss During Training', fontsize=18)\n",
    "#plt.xticks(range(0,epochs))\n",
    "plt.legend(loc='upper right', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
